# Linear Regression

```{r, echo=FALSE}
rm(list = ls())
```

## Introduction

There is a broad range of statistical methods available for us to learn about data. Broadly speaking, these methods can be classified as **supervised** and **unsupervised**. Supervised methods involve relating a response variable with predictors, whereas unsupervised methods do not make a distinction between response variables and predictors and instead want to find structure or patterns in the data. 

Supervised methods generally have two primary uses:

1. **Association**: Quantify the relationship between variables. How does a change in the predictor variable change the value of the response variable?
2. **Prediction**: Predict a future value of a response variable, using information from predictor variables.

We always distinguish between a **response variable**, denoted by $y$, and a **predictor variable**, denoted by $x$. In most supervised methods, we say that the response variable can be approximated by some mathematical function, denoted by $f$, of the predictor variable, i.e.

$$
y \approx f(x).
$$

Oftentimes, we write this relationship as 

$$
y = f(x) + \epsilon,
$$

where $\epsilon$ denotes a **random error term**, with a mean of 0. The error term cannot be predicted based on the data we have. 

There are various methods to estimate $f$. Once we estimate $f$, we can use our method for association and / or prediction.

In this module, we will introduce one of the most traditional supervised methods: **linear regression**. It is used when there is a single response variable that is quantitative. The predictors could be quantitative or categorical. 

### Motivation

Why do we learn about linear regression?

- Linear regression is widely used in many fields, and, under certain conditions, does well in the two primary purposes of supervised methods: association and prediction. Other methods may be better at one of these purposes, but usually at the expense of the other purpose. The most important thing is to know what questions you have in order to select the right method that is best for your question.

- Linear regression is fairly easy to interpret and explain to others who may want to know how the method works. Other methods are generally more complicated and can feel like a black-box when explaining to others, leading to less confidence in the method. 

- A lot of the ideas used in other methods can be viewed as an extension or a variation of linear regression. Once you understand how linear regression works, it becomes easier to understand how other methods work. 

### Toy Example

The most common way of visualizing the relationship between one quantitative predictor variable and one quantitative response variable is with a scatter plot. In the simulated example below, we have data from 6000 UVa undergraduate students on the amount of time they spend studying in a week (in minutes), and how many courses they are taking in the semester (3 or 4 credit courses). Figure \@ref(fig:10-scatter) displays the scatter plot.

```{r, echo=FALSE}
gety <- function(x,intercept,slope,sigma)
{
  n<-length(x)
  y <- intercept + x*slope + rnorm(n,0,sigma)
  return(y)
}

##generate x values
courses<-c(rep(3,2000), rep(4,2000), rep(5,2000))

##generate parameters
b0<-60
b1<-120
sigma<-30

##simulate response
set.seed(6021)
study<-gety(courses, b0, b1, sigma)
```

```{r}
##create dataframe
df<-data.frame(study,courses)

##fit regression
result<-lm(study~courses, data=df)
```

```{r 10-scatter, fig.cap='Scatterplot of Study Time against Number of Courses Taken'}
##create scatterplot with regression line overlaid
plot(df$courses, df$study, xlab="# of Courses", ylab="Study Time (Mins)")
abline(result)
```

Figure \@ref(fig:10-scatter) could help us with the following questions:

- Are study time and the number of courses taken related to one another? 
- How strong is this relationship? 
- Could we use the data to make a prediction for the study time of a student who is not in this scatterplot? 

These questions can be answered using linear regression.

## Simple Linear Regression 

In this module, we will keep things simple by only considering a single quantitative predictor. Such a regression is called **simple linear regression (SLR)** to emphasize that only one predictor is being considered. We will briefly touch on multiple linear regression (MLR) when multiple predictors are involved later in this module, and you will learn more about linear regression next semester. 

### Model Setup

In SLR, the function $f$ that relates the predictor variable with the response variable is typically $\beta_0 + \beta_1 x$. Mathematically, we express this as 

$$
y \approx f(x) = \beta_0 + \beta_1 x,
$$

or in other words, that the response variable has an approximately linear relationship with the predictor variable. So the SLR model is written as

\begin{equation} 
y_i=\beta_0+\beta_{1}x_i + \epsilon_i,
(\#eq:10-SLRmod)
\end{equation} 

where

- $y_i$ denotes the value of the response variable for observation $i$,
- $x_i$ denotes the value of the predictor for observation $i$,
- $\epsilon_i$ denotes the value of the error for observation $i$,
- $\beta_0$ and $\beta_1$ are parameters in the SLR model, and we want to estimate them. These parameters are sometimes called **regression coefficients**.
- $\beta_1$ is also called the **slope**. 
- $\beta_0$ is also called the **intercept**. 

In linear regression, we make some assumptions about the error term $\epsilon$:

\begin{equation} 
\epsilon_1,\ldots,\epsilon_n \ i.i.d. \sim N(0,\sigma^2).
(\#eq:10-assumptions)
\end{equation}

What these assumptions mean is that for each value of the predictor variable $x$, the response variable:

1. follows a normal distribution,
2. with expected value equal to $\beta_0+\beta_{1} x$, i.e.

\begin{equation} 
E(Y|X=x) = \beta_0+\beta_{1} x 
(\#eq:10-SLR)
\end{equation} 

3. and variance equal to $\sigma^2$.

View the video below that explains how these are derived:

In other words, the conditional distribution of $Y|X=x$ is $N(\beta_0+\beta_{1} x, \sigma^2)$. Applying this to our study time example, it implies that:

  * for students who take 3 courses, their study time follows a $N(\beta_0 + 3\beta_1, \sigma^2)$ distribution,
  * for students who take 4 courses, their study time follows a $N(\beta_0 + 4\beta_1, \sigma^2)$ distribution,
  * for students who take 5 courses, their study time follows a $N(\beta_0 + 5\beta_1, \sigma^2)$ distribution.
  
So if we were to subset our dataframe into three subsets, one with students who take 3 courses, another subset for students who take 4 courses, and another subset for students who take 5 courses, and then create a density plot of study times for each subset, each density plot should follow a normal distribution, with different means, and the same spread.

Let us take a look at these density plots below in Figures \@ref(fig:10-conddist3), \@ref(fig:10-conddist4), and \@ref(fig:10-conddist5) below:

```{r, warning=FALSE, message=FALSE}
library(tidyverse)
```


```{r 10-conddist3, fig.cap='Distribution of Study Time for 3, 4, 5 Classes Taken'}
##subset dataframe
x.3<-df[which(df$courses==3),]
##density plot of study time for students taking 3 courses
ggplot(x.3,aes(x=study))+
  geom_density()+
  labs(x="Study Time (Mins)", title="Dist of Study Times with 3 Courses")
```

```{r 10-conddist4, fig.cap='Distribution of Study Time for 3, 4, 5 Classes Taken'}
##subset dataframe
x.4<-df[which(df$courses==4),]
##density plot of study time for students taking 4 courses
ggplot(x.4,aes(x=study))+
  geom_density()+
  labs(x="Study Time (Mins)", title="Dist of Study Times with 4 Courses")
```

```{r 10-conddist5, fig.cap='Distribution of Study Time for 3, 4, 5 Classes Taken'}
##subset dataframe
x.5<-df[which(df$courses==5),]
##density plot of study time for students taking 5 courses
ggplot(x.5,aes(x=study))+
  geom_density()+
  labs(x="Study Time (Mins)", title="Dist of Study Times with 5 Courses")
```

Notice all of these plots are normal, with different means (centers), and similar spreads.

The notation on the left hand side of \@ref(eq:10-SLR) denotes the **expected value** of the response variable, for a fixed value of the predictor variable. Therefore, the regression coefficients can be interpreted in the following manner:

- $\beta_1$ denotes the change in the response variable, on average, when the predictor increases by one unit.
- $\beta_0$ denotes the mean of the response variable when the predictor is 0.

### Assessing Assumptions

The assumptions for the error terms, $\epsilon$, expressed in equation \@ref(eq:10-assumptions), can be re-stated with words as the following 4 assumptions:

1. For each value of the predictor, the errors have mean 0. 
  - This implies that $f(x) = \beta_0 + \beta_1 x$ approximates the relationship between the variables well.
  - A scatter plot of the variables should show a linear relationship.
  - This is the most important assumption of the 4. If it is not met, predictions will be biased, in other words, predictions will systematically over- or under- predict the value of the response variable. 

The plots in Figure \@ref(fig:10-ass1) are based on simulated data. The scatter plot shown in Figure \@ref(fig:10-ass1)(a) is an example of when this assumption is met. As we move from left to right on the plot, the data points are generally evenly scattered on both sides of the regression line that is overlaid. 

The scatter plot shown in Figure \@ref(fig:10-ass1)(b) is an example of when this assumption is not met. As we move from left to right on the plot in Figure \@ref(fig:10-ass1)(b), the data points are generally not evenly scattered on both sides of the regression line that is overlaid. The shape of the plots look more like a cruve rather than a straight line.

```{r 10-ass1, fig.cap='Assumption 1 Assessment', echo=FALSE, message=FALSE}
knitr::include_graphics("images/10-ass1.jpg")
```
  
2. For each value of the predictor, the errors have variance denoted by $\sigma^2$.
  - This implies that in a scatter plot, the vertical variation of data points around the regression equation has the same magnitude everywhere.
  - If this assumption is not met, hypothesis tests and confidence intervals from the linear regression will be unreliable. 
  
The plots in Figure \@ref(fig:10-ass2) are based on simulated data. The scatter plot shown in Figure \@ref(fig:10-ass2)(a) is an example of when this assumption is met (this figure is actually the same as Figure \@ref(fig:10-ass1)(a), so the data that produced these plots satisfy both assumptions). As we move from left to right on the plot, the vertical variation of the data points about the regression line is approximately constant. 

The scatter plot shown in Figure \@ref(fig:10-ass2)(b) is an example of when this assumption is not met. As we move from left to right on the plot in Figure \@ref(fig:10-ass2)(b), the vertical variation of the data points about the regression line becomes larger as the value of the response variable gets larger, so the variance is not constant.

```{r 10-ass2, fig.cap='Assumption 2 Assessment', echo=FALSE, message=FALSE}
knitr::include_graphics("images/10-ass2.jpg")
```

3. The errors are independent. 
  - This implies that the observations are independent. This is usually a by-product of how the observations were sampled. So knowing the data collection method will help assess whether this assumption is met. 
  - If this assumption is not met, hypothesis tests and confidence intervals from the linear regression will be unreliable. 

4. The errors are normally distributed.
  - This is considered the least important of the 4 assumptions, especially with large sample sizes. This is due to the Central Limit Theorem, and that in linear regression, we write the conditional expectation (or mean) of the response variable to be equal to $f(x) = \beta_0 + \beta_1 x$, so hypothesis tests and confidence intervals from the linear regression are likely to be reliable. 

*Thought question*: Look at the scatter plot of the toy example in Figure \@ref(fig:10-scatter). Can you explain why this scatter plot shows the first two assumptions are met for linear regression?

## Estimating Regression Coefficients 

There are two methods in estimating the regression coefficients: the method of least squares and the method of maximum likelihood. For large sample sizes, these methods give similar results.

We will go over the method of least squares first, since this is the method that is usually used to explain to new learners and is conceptually easier to understand. 

### Method of Least Squares

From \@ref(eq:10-SLR) and \@ref(eq:10-SLRmod), we noted that we have to estimate the regression coefficients $\beta_0, \beta_1$. We are unable to obtain numerical values of these parameters as we do not have data from the entire population. So what we do is use the data from our sample to estimate these parameters. We estimate $\beta_0,\beta_1$ using $\hat{\beta}_0,\hat{\beta}_1$ based on a sample of observations $(x_i,y_i)$ of size $n$.

Following \@ref(eq:10-SLR) and \@ref(eq:10-SLRmod), the sample versions are

\begin{equation} 
\hat{y}=\hat{\beta}_0+\hat{\beta}_1 x
(\#eq:10-fitted)
\end{equation}

and 

\begin{equation} 
y=\hat{\beta}_0+\hat{\beta}_1 x + e
(\#eq:10-fitted-model)
\end{equation}

respectively. \@ref(eq:10-fitted) is called the **fitted line**. The line that was overlaid in the scatter plot for the toy example in Figure \@ref(fig:10-scatter) represents the fitted line. \@ref(eq:10-fitted-model) is called the **estimated SLR model**.

$\hat{\beta}_1,\hat{\beta}_0$ are the estimators for $\beta_1,\beta_0$ respectively. These estimators can be interpreted in the following manner:

- $\hat{\beta}_1$ denotes the change in the predicted $y$ when $x$ increases by 1 unit. Alternatively, it estimates the change in $y$, on average, when $x$ increases by 1 unit.
- $\hat{\beta}_0$ denotes the predicted $y$ when $x=0$. Alternatively, it estimates the average of $y$ when $x=0$.

From \@ref(eq:10-fitted-model), notice we use $e$ to denote the **residual**, or in other words, the "error" in the sample. 

From \@ref(eq:10-fitted) and \@ref(eq:10-fitted-model), we have the following quantities that we can compute:

- Fitted values:

\begin{equation}
\hat{y}_i = \hat{\beta}_0+\hat{\beta}_1 x_i.
(\#eq:10-fits)
\end{equation}

The **fitted values** are the predicted values of the response variable when the predictor is equal to some specific value. Visually, the fitted line represents each fitted value as we vary the value of the predictor. 

- Residuals:

\begin{equation} 
e_i = y_i-\hat{y}_i.
(\#eq:10-res)
\end{equation}

The **residuals** are the differences between the actual values of the response variable and their corresponding predicted values based on the fitted line. Visually, a residual is the vertical distance of a data point in the scatter plot from the fitted line, as shwon in Figure \@ref(fig:10-res) below:

```{r 10-res, fig.cap = "Example of Residuals. Picture from https://www.statology.org/residuals/", echo = FALSE}
knitr::include_graphics("images/10-residuals.png")
```

- Sum of Squared Errors:

\begin{equation} 
SS_{res} =  \sum\limits_{i=1}^n(y_i-\hat{y}_i)^2.
(\#eq:10-SSres)
\end{equation}

We compute the estimated coefficients $\hat{\beta}_1,\hat{\beta}_0$ using the **method of least squares**, i.e. choose the numerical values of $\hat{\beta}_1,\hat{\beta}_0$ that minimize $SS_{res}$ as given in \@ref(eq:10-SSres). We find the line which minimizes the sum of all the squared residuals on the scatter plot, hence the name method of least squares.

By minimizing $SS_{res}$ with respect to $\hat{\beta}_0$ and $\hat{\beta}_1$, the estimated coefficients in the simple linear regression equation are

\begin{equation} 
\hat{\beta}_1 = \frac{\sum\limits_{i=1}^n(x_i-\bar{x})(y_i-\bar{y})}{\sum\limits_{i=1}^n(x_i-\bar{x})^2}
(\#eq:10-b1)
\end{equation}

and

\begin{equation} 
\hat{\beta}_0 = \bar{y}- \hat{\beta}_1 \bar{x}
(\#eq:10-b0)
\end{equation}

$\hat{\beta}_1, \hat{\beta}_0$ are called **least squares estimators**, to emphasize that these values are found by minimizing $SS_{res}$.

The minimization of $SS_{res}$ with respect to $\hat{\beta}_0$ and $\hat{\beta}_1$ is done by taking the partial derivatives of \@ref(eq:SSres) with respect to $\hat{\beta}_1$ and $\hat{\beta}_0$, setting these two partial derivatives equal to 0, and solving these two equations for $\hat{\beta}_1$ and $\hat{\beta}_0$.

Let's take a look at the estimated coefficients for our study time example:

```{r}
##fit regression
result<-lm(study~courses, data=df) ##supply y, then x, and specify dataframe via data
##print out the estimated coefficients
result
```

From our sample of 6000 students, we have 

- $\hat{\beta}_1$ =  `r result$coefficients[2]`. The predicted study time increases by `r result$coefficients[2]` minutes for each additional course taken.
- $\hat{\beta}_0$ =  `r result$coefficients[1]`. The predicted study time is `r result$coefficients[1]` when no courses are taken. Notice this value does not make sense, as a student cannot be taking 0 courses. If you look at our data, the number of courses taken is 3, 4, or 5. So we should only use our regression when $3 \leq x \leq 5$. We cannot use it for values of $x$ outside the range of our data. Making predictions of the response variable for predictors outside the range of the data is called **extrapolation** and should not be done. 

*Thought question*: The response variable for the toy example is study time, in minutes. Suppose we convert these values to hours by dividing by 60. How will the numerical value of the estimated coefficients change? How will the interpretation of the estimated coefficients change?

Note: I often get a question about why we minimize $SS_{res}$ and not some other quantity such as minimizing the sum of the absolute value of the residuals $\sum_{i=1}^n |y_i - \hat{y}_i|$? We will state a famous theorem here without proving it. **Gauss Markov Theorem**: Under assumptions for a regression model (as stated in \@ref(eq:10-assumptions)), the least squares estimators $\hat{\beta}_1$ and $\hat{\beta}_0$ are unbiased and have minimum variance among all unbiased linear estimators. This means that any other way of deriving unbiased estimators will result in the estimators having larger variance than least squares estimators. 

### Method of Maximum Likelihood

We will give a brief overview of maximum likelihood estimation is carried out for simple linear regression. We had earlier written that the conditional distribution of $Y|X=x$ is $N(\beta_0+\beta_{1} x, \sigma^2)$. We know the PDF of any normal distribution takes the form in equation \@ref(eq:4-pdfNormal), which implies that we can write the PDF of this distribution as 

\begin{equation} 
f_{Y|X}(y|x) = \frac{1}{\sigma \sqrt{2 \pi}} \exp \left(-\frac{(y - (\beta_0+\beta_{1} x))^2}{2 \sigma^2} \right).
(\#eq:10-SLRpdf)
\end{equation}

We can then write the corresponding log-likelihood function, using equation \@ref(eq:7-loglike), as

\begin{equation}
\ell(\beta_0, \beta_1, \sigma^2 | \boldsymbol{y}, \boldsymbol{x}) = -\frac{n}{2} \log(2\pi) - n \log(\sigma) - \frac{1}{2 \sigma^2} \sum_{i=1}^n \left(y_i - (\beta_0 + \beta_1 x_i) \right)^2.
(\#eq:10-SLRloglike)
\end{equation}

We then take the partial derivatives of \@ref(eq:10-SLRloglike) with respect to $\hat{\beta}_1$ and $\hat{\beta}_0$, setting these two partial derivatives equal to 0, and solving these two equations for $\hat{\beta}_1$ and $\hat{\beta}_0$. We end up with the same solutions as the method of least squares. 

```{r}
##fit regression
result.mle<-glm(study~courses, data=df) ##we use glm() function for ML instead
##print out the estimated coefficients. Notice they are the same. 
result.mle
```

## Inference with Simple Linear Regression {#SLRinf}

The process of using data from a sample to draw a conclusion about the population is called (statistical) **inference**. Two methods associated with inference are confidence intervals and hypothesis testing. 

The most common inference deals with the slope, $\beta_1$. We are usually assessing whether the slope is 0 or not, as a slope of 0 implies that there is no linear relationship between the variables (if the slope is 0, the value of the response is not affected by the value of the predictor).

We will cover the confidence interval and hypothesis test for the slope. 

### Properties of Least Squares Estimators

Before proceeding, we will state a few properties associated with the least squares estimators $\hat{\beta}_1, \hat{\beta}_0$. The key here is to realize that the sampling distribution of the least squares estimators are known. Do not be too torn up about these formulas as a lot of these are computed using R. The proof of these are beyond the scope of this class, and there are countless books that provide these proofs if you are interested.

1. $\mbox{E}(\hat{\beta}_1) = \beta_1$, $\mbox{E}(\hat{\beta}_0) = \beta_0$. In other words, the least squares estimators are unbiased. 

2. The variance of $\hat{\beta}_1$ is

\begin{equation} 
\mbox{Var}(\hat{\beta}_1) = \frac{\sigma^{2}}{\sum_{i=1}^n{(x_{i}-\bar{x})^{2}}}
(\#eq:10-varb1)
\end{equation}

3. The variance of $\hat{\beta}_0$ is

\begin{equation} 
\mbox{Var}(\hat{\beta}_0) = \sigma^2 \left[\frac{1}{n} + \frac{\bar{x}^2}{\sum_{i=1}^n (x_i -\bar{x})^2}\right]
(\#eq:10-varb0)
\end{equation}

4. $\hat{\beta}_1$ and $\hat{\beta}_0$ both follow a normal distribution.

Note that in \@ref(eq:10-varb1) and \@ref(eq:10-varb0), we use $s^2 = \frac{SS_{res}}{n-2} = MS_{res}$ to estimate $\sigma^2$ since $\sigma^2$ is a unknown value.

What these imply is that if we standardize $\hat{\beta}_1$ and $\hat{\beta}_0$, these standardized quantities will follow a $t_{n-2}$ distribution, i.e.

\begin{equation} 
\frac{\hat{\beta}_1 - \beta_1}{se(\hat{\beta}_1)}\sim t_{n-2}
(\#eq:10-distb1)
\end{equation}

and

\begin{equation} 
\frac{\hat{\beta}_0 - \beta_0}{se(\hat{\beta}_0)}\sim t_{n-2},
(\#eq:10-distb0)
\end{equation}

where 

\begin{equation}
se(\hat{\beta}_1) = \sqrt{\frac{MS_{res}}{\sum_{i=1}^n{(x_{i}-\bar{x})^{2}}}} = \frac{s}{\sqrt{\sum_{i=1}^n{(x_{i}-\bar{x})^{2}}}}
(\#eq:10-seb1)
\end{equation}

and 

\begin{equation} 
se(\hat{\beta}_0) = \sqrt{MS_{res}\left[\frac{1}{n} + \frac{\bar{x}^2}{\sum_{i=1}^n (x_i -\bar{x})^2}\right]} = s \sqrt{\frac{1}{n} + \frac{\bar{x}^2}{\sum_{i=1}^n (x_i -\bar{x})^2}}
(\#eq:10-seb0)
\end{equation}

Note: The standardized quantities in equations \@ref(eq:10-distb1) and \@ref(eq:10-distb0) follow $t_{n-2}$ distribution. As mentioned in Section \@ref(df), we lose 1 degree of freedom for every equation that must be satisfied. In SLR, we have 2 equations, equations \@ref(eq:10-b1) and \@ref(eq:10-b0), that must be satisfied when calculating the least squares estimators, therefore we have $n-2$ degrees of freedom. 

### Confidence Interval

From equation \@ref(eq:10-distb1), we note that the standardized $\hat{\beta}_1$ follows a $t_{n-2}$ distribution, which is symmetric. This implies that the confidence interval for the slope takes on the form expressed in equation \@ref(eq:8-CIbasic2), which is $\text{point estimate } \pm \text{ margin of error}$. We also know that the margin of error is the critical value multiplied by the standard error of the estimator.

*Thought question*: Before scrolling any further, can you write out the formula for a confidence interval for the slope?

The $100(1-\alpha)\%$ CI for $\beta_1$ is

\begin{equation} 
\hat{\beta}_1 \pm t_{n-2}^*  se(\hat{\beta}_1) = \hat{\beta}_1 \pm t_{n-2}^* {\sqrt\frac{MS_{res}}{\sum_{i=1}^n(x_i - \bar{x})^{2}}}.
(\#eq:10-CIb1)
\end{equation}

The critical value $t_{n-2}^*$ is found in the usual manner; it corresponds to the $(1 - \alpha/2) \times 100$th percentile from a $t_{n-2}$ distribution. 

Going back to our study time example, the 95% CI for the slope is (`r confint(result,level = 0.95)[2,1]`, `r confint(result,level = 0.95)[2,2]`).

```{r}
##CI for coefficients
confint(result,level = 0.95)[2,]
```

An interpretation of this CI is that there is 95% probability that the random interval  (`r confint(result,level = 0.95)[2,1]`, `r confint(result,level = 0.95)[2,2]`) contains the true value of the slop.

The interval excludes 0, so there is a linear relationship between number of courses and study time.

### Hypothesis Testing

In the context of SLR, we usually want to test if the slope $\beta_1$ is 0 or not. If the slope is 0, there is no linear relationship between the variables. So the hypotheses are typically $H_0: \beta_1 = 0, H_a: \beta_1 \neq 0.$

In equation \@ref(eq:10-distb1), we noted that the standardized $\hat{\beta}_1$ follows a $t_{n-2}$ distribution, so the test statistic will be a t-statistic which takes on the general form in equation \@ref(eq:9-tstatGen).

*Thought question*: Before scrolling down any further, can you make an educated guess as to the formula of the t-statistic when testing the slope?

The test statistic is

\begin{equation} 
t = \frac{\hat{\beta}_1 - \text{ value in null}}{se(\hat{\beta}_1)}.
(\#eq:10-testslope)
\end{equation}

The p-value and critical value are found in the usual manner, based on a $t_{n-2}$ distribution, and are used in the usual manner. 

We go back to the study time example:

```{r}
summary(result)$coefficients ##print out est coeffs, SEs, t statistics, and p-vals
```
From R, the reported t-statistic and p-value are 255.74125 and approximately 0 respectively. So we reject the null hypothesis: our data support the claim that there is a linear relationship between study time and number of courses taken. 

Note: The reported values are based on a two-sided alternative hypothesis and a null hypothesis of $H_0: \beta_1 = 0$. If we have different hypotheses we will have to perform these calculations on our own. 

### Correlation

Another commonly used measure in linear regression is the correlation, which we explored earlier in Section \@ref(corr). Visually, variables with a higher correlation (in magnitude) means their observations will fall closer to the fitted line. A correlation of 1 or -1 will mean the observations fall perfectly on the fitted line.

### Coefficient of Determination

Another measure that is commonly used in linear regression is the **coefficient of determination**, usually denoted by $R^2$. The value represents the proportion of variation in the response variable that can be explained by our linear regression. In SLR, it is numerically equal to the squared of the sample correlation. 

A few notes about $R^2$:

- $0 \leq R^2 \leq 1$.
- Values closer to 1 indicate a better fit; values closer to 0 indicate a poorer fit.
- Often reported as a percentage. 

### Linear Regression in R

Next, we will perform a simple linear regression on a real dataset using R. We will work with the dataset `elmhurst` from the `openintro` package in R.

```{r, echo=FALSE}
rm(list = ls())
```

```{r, warning=FALSE, message=FALSE}
library(tidyverse)
library(openintro)
Data<-openintro::elmhurst
```

Type `?openintro::elmhurst` to read the documentation for datasets in R. Always seek to understand the background of your data! The key pieces of information are:

- A random sample of 50 students (all freshman from the 2011 class at Elmhurst College).
- Family income of the student (units are missing).
- Gift aid, in $1000s.

We want to explore how family income may be related to gift aid, in a simple linear regression framework.

#### Visualization {-}

We should always verify with scatter plot that the relationship is (approximately) linear before proceeding with correlation and simple linear regression! 

```{r 10-scatter-eg, fig.cap='Scatter Plot for Worked Example'}
##scatterplot of gift aid against family income
ggplot2::ggplot(Data, aes(x=family_income,y=gift_aid))+
  geom_point()+
  geom_smooth(method = "lm", se=FALSE)+
  labs(x="Family Income", y="Gift Aid", title="Scatterplot of Gift Aid against Family")
```

We note that the observations are fairly evenly scattered on both sides of the regression line, so a linear association exists. We see a negative linear association. As family income increases, the gift aid, on average, decreases. 

We also do not see any observation with weird values that may warrant further investigation.

#### Regression {-}

We use the `lm()` function to fit a regression model. We supply the response variable, then the predictor, with a ``~`` in between, then specify the dataframe via the `data` argument.

```{r}
##regress gift aid against family income
result<-lm(gift_aid~family_income, data=Data)
```

Use the `summary()` function to display relevant information from this regression:

```{r}
##look at information regarding regression
summary(result)
```

We see the following values:

- $\hat{\beta}_1 =$ `r summary(result)$coefficients[2,1]`. The estimated slope informs us the the predicted gift aid decreases by `r -summary(result)$coefficients[2,1]` thousands of dollars (or \$43.07) per unit increase in family income.
- $\hat{\beta}_0 =$ `r summary(result)$coefficients[1,1]`. For students with no family income, their predicted gift aid is \$24 319.33. Note: from the scatter plot, we have an observation with 0 family income. We must be careful in not extrapolating when making predictions with our regression. We should only make predictions for family incomes between the minimum and maximum values of family incomes in our data.
- $s$  = `r summary(result)$sigma`, is the estimate of the standard deviation of the error terms, $\sigma$. This is reported as residual standard error in R. Squaring this gives the estimated variance of the error terms.
- $R^2 =$ `r summary(result)$r.squared`. The coefficient of determination informs us that about 24.86% of the variation in gift aid can be explained by family income. This is reported as multiple R-squared in R.

#### Hypothesis Testing {-}

Under coefficients, we can see the results of the hypothesis tests for $\beta_1$ and $\beta_0$. Specifically, for $\beta_1$:

- $\hat{\beta}_1$ = `r summary(result)$coefficients[2,1]`
- $se(\hat{\beta}_1)$ = `r summary(result)$coefficients[2,2]`
- the test statistic is $t$ = `r summary(result)$coefficients[2,3]`
- the corresponding p-value is `r summary(result)$coefficients[2,4]`

You can work out the p-value using R (slight difference due to rounding):

```{r}
##pvalue
2*pt(-abs(-3.985), df = 50-2)
```

Or find the critical value using R:

```{r}
##critical value
qt(1-0.05/2, df = 50-2)
```

Either way, we end up rejecting the null hypothesis. The data support the claim that there is a linear association between gift aid and family income.

Remember the $t$ tests for the regression coefficients are based on $H_0: \beta_j = 0, H_a: \beta_j \neq 0$. The reported p-value is based on this set of null and alternative hypotheses. If your null and alternative hypotheses are different, you will need to compute your own test statistic and p-value.

#### Confidence Intervals {-}

To find the 95% confidence intervals for the coefficients, we use the `confint()` function:

```{r}
##to produce 95% CIs for all regression coefficients
confint(result,level = 0.95)
```

The 95% CI for $\beta_1$ is (`r confint(result,level = 0.95)[2,1]`, `r confint(result,level = 0.95)[2,2]`). We have 95% confidence that for each additional thousand dollars in family income, the predicted gift aid decreases between \$21.3378 and \$64.8056.

#### Extract Values from R Objects {-}

We can actually extract these values that are being reported from `summary(result)`. To see what can be extracted from an R object, use the `names()` function:

```{r}
##see what can be extracted from summary(result)
names(summary(result))
```

To extract the estimated coefficients:

```{r}
##extract coefficients
summary(result)$coefficients
```

Notice the information is presented in a table. To extract a specific value, we can specify the row and column indices:

```{r}
##extract slope
summary(result)$coefficients[2,1]
##extract intercept
summary(result)$coefficients[1,1]
```

On your own, extract the values of the residual standard error and $R^2$.

#### Prediction {-}

A use of regression models is prediction. Suppose I want to predict the gift aid of a student with family income of 50 thousand dollars (assuming the unit is in thousands of dollars). We use the `predict()` function:

```{r}
##create data point for prediction
newdata<-data.frame(family_income=50)
##predicted gift aid when x=50
predict(result,newdata)
```

This student's predicted gift aid is \$22 165.75. Alternatively, you could have calculated this by plugging $x=50$ into the estimated SLR equation:

```{r}
summary(result)$coefficients[1,1] + summary(result)$coefficients[2,1]*50
```

#### Correlation {-}

We use the `cor()` function to find the correlation between two quantitative variables:

```{r}
##correlation
cor(Data$family_income,Data$gift_aid)
```

The correlation is `r cor(Data$family_income,Data$gift_aid)`. We have a moderate, negative linear association between family income and gift aid. 

## Assessing Model Accuracy

In the introduction of this module, we wrote that supervised methods such as linear regression have two primary uses:

1. Association: Quantify the relationship between variables. How does a change in the predictor variable change the value of the response variable?
2. Prediction: Predict a future value of a response variable, using information from predictor variables.

In Section \@ref(SLRinf), we focused on the sampling distribution of the least squares estimators which was then used to construct confidence intervals and perform hypothesis tests. These tools are useful in trying to explain the relationship between the response variable and predictor, which is the first primary use of supervised methods. This has been the focus of a lot of traditional research in a wide variety of applications, for example:

- Medicine: Does increasing the dose of a medication reduce severity of symptoms? 
- Advertising: How does advertising in various media affect sales?
- Environmental science: How does increased greenhouse emissions affect air quality?
- Education: Does the use of AI tools aid in student learning outcomes?

In this section, we will focus on the second primary use of supervised methods: prediction. When we focus on prediction, we mainly care about how close are the predicted (or fitted) values with their true values. A lot of what was mentioned in Section \@ref(SLRinf) can actually be ignored (other than assumption 1 of the regression assumptions, since predictions will definitely suffer if this is not met) when our focus is on prediction. Prediction is in the domain of predictive analytics. While predictive analytics can be used in a wide variety of applications, the kind of questions they answer differ from the earlier questions, for example:

- Hospitality: Determining staffing needs for a casino. We want to be able to predict the right number of staff to hire: overstaffing costs money and understaffing results in unhappy customers. 
- Online ads: Based on a user's history on Amazon, Amazon would like to be able to recommend products that the user is highly likely to purchase.
- Personalized medicine: Based on a patient's history, what is the most effective treatment plan?
- Dynamic pricing: airlines use dynamic pricing that adjusts price to maximize profits, based on real-time market conditions. 
