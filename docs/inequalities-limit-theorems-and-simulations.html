<!DOCTYPE html>
<html lang="en">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<title>Chapter 6 Inequalities, Limit Theorems, and Simulations | Understanding Uncertainty Course Notes</title>
<meta name="author" content="Jeffrey Woo">
<meta name="description" content="This module is based on Introduction to Probability (Blitzstein, Hwang), Chapter 10. You can access the book for free at https://stat110.hsites.harvard.edu/ (and then click on Book). Please note...">
<meta name="generator" content="bookdown 0.43 with bs4_book()">
<meta property="og:title" content="Chapter 6 Inequalities, Limit Theorems, and Simulations | Understanding Uncertainty Course Notes">
<meta property="og:type" content="book">
<meta property="og:description" content="This module is based on Introduction to Probability (Blitzstein, Hwang), Chapter 10. You can access the book for free at https://stat110.hsites.harvard.edu/ (and then click on Book). Please note...">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="Chapter 6 Inequalities, Limit Theorems, and Simulations | Understanding Uncertainty Course Notes">
<meta name="twitter:description" content="This module is based on Introduction to Probability (Blitzstein, Hwang), Chapter 10. You can access the book for free at https://stat110.hsites.harvard.edu/ (and then click on Book). Please note...">
<!-- JS --><script src="https://cdnjs.cloudflare.com/ajax/libs/clipboard.js/2.0.6/clipboard.min.js" integrity="sha256-inc5kl9MA1hkeYUt+EC3BhlIgyp/2jDIyBLS6k3UxPI=" crossorigin="anonymous"></script><script src="https://cdnjs.cloudflare.com/ajax/libs/fuse.js/6.4.6/fuse.js" integrity="sha512-zv6Ywkjyktsohkbp9bb45V6tEMoWhzFzXis+LrMehmJZZSys19Yxf1dopHx7WzIKxr5tK2dVcYmaCk2uqdjF4A==" crossorigin="anonymous"></script><script src="https://kit.fontawesome.com/6ecbd6c532.js" crossorigin="anonymous"></script><script src="libs/jquery-3.6.0/jquery-3.6.0.min.js"></script><meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<link href="libs/bootstrap-4.6.0/bootstrap.min.css" rel="stylesheet">
<script src="libs/bootstrap-4.6.0/bootstrap.bundle.min.js"></script><script src="libs/bs3compat-0.9.0/transition.js"></script><script src="libs/bs3compat-0.9.0/tabs.js"></script><script src="libs/bs3compat-0.9.0/bs3compat.js"></script><link href="libs/bs4_book-1.0.0/bs4_book.css" rel="stylesheet">
<script src="libs/bs4_book-1.0.0/bs4_book.js"></script><script src="https://cdnjs.cloudflare.com/ajax/libs/autocomplete.js/0.38.0/autocomplete.jquery.min.js" integrity="sha512-GU9ayf+66Xx2TmpxqJpliWbT5PiGYxpaG8rfnBEk1LL8l1KGkRShhngwdXK1UgqhAzWpZHSiYPc09/NwDQIGyg==" crossorigin="anonymous"></script><script src="https://cdnjs.cloudflare.com/ajax/libs/mark.js/8.11.1/mark.min.js" integrity="sha512-5CYOlHXGh6QpOFA/TeTylKLWfB3ftPsde7AnmhuitiTX4K5SqCLBeKro6sPS8ilsz1Q4NRx3v8Ko2IBiszzdww==" crossorigin="anonymous"></script><!-- CSS --><style type="text/css">
    
    div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
  </style>
</head>
<body data-spy="scroll" data-target="#toc">

<div class="container-fluid">
<div class="row">
  <header class="col-sm-12 col-lg-3 sidebar sidebar-book"><a class="sr-only sr-only-focusable" href="#content">Skip to main content</a>

    <div class="d-flex align-items-start justify-content-between">
      <h1>
        <a href="index.html" title="">Understanding Uncertainty Course Notes</a>
      </h1>
      <button class="btn btn-outline-primary d-lg-none ml-2 mt-1" type="button" data-toggle="collapse" data-target="#main-nav" aria-expanded="true" aria-controls="main-nav"><i class="fas fa-bars"></i><span class="sr-only">Show table of contents</span></button>
    </div>

    <div id="main-nav" class="collapse-lg">
      <form role="search">
        <input id="search" class="form-control" type="search" placeholder="Search" aria-label="Search">
</form>

      <nav aria-label="Table of contents"><h2>Table of contents</h2>
        <ul class="book-toc list-unstyled">
<li><a class="" href="index.html">Preface</a></li>
<li><a class="" href="descriptive.html"><span class="header-section-number">1</span> Descriptive Statistics</a></li>
<li><a class="" href="probability.html"><span class="header-section-number">2</span> Probability</a></li>
<li><a class="" href="discrete-random-variables.html"><span class="header-section-number">3</span> Discrete Random Variables</a></li>
<li><a class="" href="continuous-random-variables.html"><span class="header-section-number">4</span> Continuous Random Variables</a></li>
<li><a class="" href="joint-distributions.html"><span class="header-section-number">5</span> Joint Distributions</a></li>
<li><a class="active" href="inequalities-limit-theorems-and-simulations.html"><span class="header-section-number">6</span> Inequalities, Limit Theorems, and Simulations</a></li>
<li><a class="" href="est.html"><span class="header-section-number">7</span> Estimation</a></li>
<li><a class="" href="confidence-intervals.html"><span class="header-section-number">8</span> Confidence Intervals</a></li>
<li><a class="" href="hypothesis-testing.html"><span class="header-section-number">9</span> Hypothesis Testing</a></li>
<li><a class="" href="linear-regression.html"><span class="header-section-number">10</span> Linear Regression</a></li>
</ul>

        <div class="book-extra">
          
        </div>
      </nav>
</div>
  </header><main class="col-sm-12 col-md-9 col-lg-7" id="content"><div id="inequalities-limit-theorems-and-simulations" class="section level1" number="6">
<h1>
<span class="header-section-number">6</span> Inequalities, Limit Theorems, and Simulations<a class="anchor" aria-label="anchor" href="#inequalities-limit-theorems-and-simulations"><i class="fas fa-link"></i></a>
</h1>
<p>This module is based on Introduction to Probability (Blitzstein, Hwang), Chapter 10. You can access the book for free at <a href="https://stat110.hsites.harvard.edu/" class="uri">https://stat110.hsites.harvard.edu/</a> (and then click on Book). Please note that I cover additional topics, and skip certain topics from the book. You may skip Example 10.1.3, 10.1.4, 10.1.7 to 10.1.9, Theorem 10.1.12, Example 10.2.5, 10.2.6, 10.3.7, and Section 10.4 from the book.</p>
<div id="introduction-2" class="section level2" number="6.1">
<h2>
<span class="header-section-number">6.1</span> Introduction<a class="anchor" aria-label="anchor" href="#introduction-2"><i class="fas fa-link"></i></a>
</h2>
<p>It can be difficult to calculate probabilities and expected values, for example, when the PDF of a distribution is unknown, or its integral is too difficult to work out. You may notice that we used simulations to approximate probabilities and expected values in some of the examples in previous modules. With improvement in computing capabilities, simulations can now be performed faster and is a tool that is used more and more. Other tools to calculate difficult probabilities and expected values include using inequalities to bound the probabilities (e.g. the probability cannot be greater or less than a certain value), or approximating using known theorems. We’ll look at these three tools in this module.</p>
<div id="module-roadmap-4" class="section level3" number="6.1.1">
<h3>
<span class="header-section-number">6.1.1</span> Module Roadmap<a class="anchor" aria-label="anchor" href="#module-roadmap-4"><i class="fas fa-link"></i></a>
</h3>
<ul>
<li>Section <a href="inequalities-limit-theorems-and-simulations.html#inequalities">6.2</a> covers 3 famous inequalities that are used in probability (Cauchy-Schwartz, Jensen, Chebyshev), as well as their implications.</li>
<li>Section <a href="inequalities-limit-theorems-and-simulations.html#limits">6.3</a> covers 2 of the most consequential theorems in probability, the Law of Large Numbers and the Central Limit Theorem. We will be invoking these 2 theorems in a number of the remaining modules .</li>
<li>Section <a href="inequalities-limit-theorems-and-simulations.html#MCsims">6.4</a> covers Monte Carlo simulations, which is a computational tool used to estimate probabilities, especially when they may be difficult to derive “by hand”. We will be using Monte Carlo simulations extensively for the rest of this class.</li>
</ul>
</div>
</div>
<div id="inequalities" class="section level2" number="6.2">
<h2>
<span class="header-section-number">6.2</span> Inequalities<a class="anchor" aria-label="anchor" href="#inequalities"><i class="fas fa-link"></i></a>
</h2>
<p>If a probability or expected value is difficult to calculate, it may be easier to find a bound via an inequality. This usually means that we can guarantee that a certain probability or expected value is within a certain range of values, which narrows down the possible values for the exact answer. For example, instead of being able to calculate the probability of a certain event, we may be able to show that its probability is no more than 0.1, so we know the event is unlikely to happen. We will cover a couple of the most well-known inequalities in probability.</p>
<div id="cauchy-schwartz-inequality" class="section level3" number="6.2.1">
<h3>
<span class="header-section-number">6.2.1</span> Cauchy-Schwartz Inequality<a class="anchor" aria-label="anchor" href="#cauchy-schwartz-inequality"><i class="fas fa-link"></i></a>
</h3>
<p>The <strong>Cauchy-Schwarz inequality</strong> is one of the most famous inequalities in mathematics and has many applications. In the context of probability, it is written as: For any random variables <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span> with finite variances</p>
<p><span class="math display" id="eq:6-CS">\[\begin{equation}
|E(XY)| \leq \sqrt{E(X^2)E(Y^2)}.
\tag{6.1}
\end{equation}\]</span></p>
<p>Next, we use the Cauchy-Schwartz inequality to prove a couple of properties that we have stated in earlier modules:</p>
<ol style="list-style-type: decimal">
<li>The Cauchy-Schwartz inequality can be used to show the correlation between any two random variables with finite variances must be between -1 and 1. A quick proof is as follows: we apply equation <a href="inequalities-limit-theorems-and-simulations.html#eq:6-CS">(6.1)</a> to the <strong>centered</strong> random variables <span class="math inline">\(X - \mu_X\)</span> and <span class="math inline">\(Y - \mu_Y\)</span>:</li>
</ol>
<p><span class="math display">\[
\begin{split}
|E[(X - \mu_X)(Y - \mu_Y)]| &amp; \leq \sqrt{E[(X - \mu_X)^2] E[(Y - \mu_Y)^2]} \\
\implies |Cov(X,Y)| &amp; \leq \sqrt{Var(X) Var(Y)} \\
\implies |Corr(X,Y)| &amp; \leq 1.
\end{split}
\]</span></p>
<p>View the video below for a more detailed explanation of this proof:</p>
<iframe width="560px" height="320px" allowfullscreen="true" allow="autoplay *" title="Module 06: Cauchy Schwartz and Correlation" src="https://virginiauniversity.instructuremedia.com/embed/bf9948b4-1c86-4529-9b49-53c808b4a0be" frameborder="0">
</iframe>
<ol start="2" style="list-style-type: decimal">
<li>The Cauchy-Schwarz inequality can also be used to show that the variance of any random variable has to be non negative. A quick proof is as follows: we apply equation <a href="inequalities-limit-theorems-and-simulations.html#eq:6-CS">(6.1)</a> to the random variable <span class="math inline">\(X\)</span> and to a constant 1:</li>
</ol>
<p><span class="math display">\[
\begin{split}
|E(X)| &amp; \leq \sqrt{E(X^2)E(1^2)}. \\
\implies |E(X)| &amp; \leq \sqrt{E(X^2)} \\
\implies E(X)^2 &amp; \leq E(X^2) \\
\implies 0 &amp; \leq E(X^2) - E(X)^2 = Var(X).
\end{split}
\]</span>
View the video below for a more detailed explanation of this proof:</p>
<iframe width="560px" height="320px" allowfullscreen="true" allow="autoplay *" title="Module 06: Cauchy Schwartz and Variance" src="https://virginiauniversity.instructuremedia.com/embed/856ad59e-6e7f-4039-97e5-d1ce7397752c" frameborder="0">
</iframe>
<p>Note: One other place that you may seen the Cauchy-Schwarz inequality is in the proof of the triangle inequality in geometry.</p>
</div>
<div id="jensens-inequality" class="section level3" number="6.2.2">
<h3>
<span class="header-section-number">6.2.2</span> Jensen’s Inequality<a class="anchor" aria-label="anchor" href="#jensens-inequality"><i class="fas fa-link"></i></a>
</h3>
<p>You may have noticed in previous modules, we have written about transforming a random variable. One way of transforming a random variable is through a scale change, in other words, the value of the random variable is multiplied by a constant. This can happen when we change the units of the variable. For example we want to convert a random variable based on weight from kilograms to pounds. If <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span> denote the weight in kilograms and pounds respectively, we can write <span class="math inline">\(Y = 2.2X\)</span>. If we know the expected value of <span class="math inline">\(X\)</span>, we can easily find the expected value for <span class="math inline">\(Y\)</span> by multiplying <span class="math inline">\(E(X)\)</span> by 2.2. This is fairly intuitive and is based on the linearity of expectations using equation <a href="discrete-random-variables.html#eq:3-linEX">(3.3)</a>.</p>
<p>Before stating Jensen’s inequality, we have to cover a couple of concepts: linear vs non linear transformations, and convex vs concave functions.</p>
<div id="linnonlin" class="section level4" number="6.2.2.1">
<h4>
<span class="header-section-number">6.2.2.1</span> Linear and Non Linear Transformations<a class="anchor" aria-label="anchor" href="#linnonlin"><i class="fas fa-link"></i></a>
</h4>
<p>A way to think about transformations is to write <span class="math inline">\(Y = g(X)\)</span>, where <span class="math inline">\(g\)</span> is a function that describes the transformation. In the kilograms to pounds example, <span class="math inline">\(g\)</span> is exactly 2.2, so <span class="math inline">\(Y = 2.2X\)</span>. This transformation is a <strong>linear transformation</strong> since the graph of <span class="math inline">\(Y = 2.2X\)</span> is a straight line. In this example, <span class="math inline">\(E(Y) = E(2.2X) = 2.2E(X)\)</span>.</p>
<p>What if we use a <strong>non linear transformation</strong>? A popular non linear transformation is a log transformation. This is used when a random variable is right skewed (which happens pretty often in real data, such as wages, since only a few people make really high wages and the vast majority of people have wages on the lower end). Expected values are often used in statistical models for predictions; however, we know that the mean may not be the best measure of centrality with skewed data. One way to transform right skewed data to become less skewed is to log transform the data. In this example, we have <span class="math inline">\(Y = \log(X)\)</span>, so <span class="math inline">\(g(x) = \log(x)\)</span>. If we know the expected value of the original variable, <span class="math inline">\(E(X)\)</span>, can we easily find the expected value of <span class="math inline">\(Y\)</span>? Can we write <span class="math inline">\(E(Y) = E(\log(X)) = \log E(X)\)</span>? This is actually incorrect. It turns out that such operations do not work for non linear transformations, i.e. if <span class="math inline">\(g\)</span> is non linear, <span class="math inline">\(E(g(X))\)</span> is not necessarily equal to <span class="math inline">\(g(E(X))\)</span>. A log transformation is not linear since the graph of <span class="math inline">\(Y = \log(X)\)</span> is not a straight line.</p>
<p>Let us use a toy example to show this. Suppose we roll a fair six-sided die, and let <span class="math inline">\(X\)</span> denote the number of dots the die shows. For this game, we get to win money based on the result of the roll, specifically twice the result. Let <span class="math inline">\(D\)</span> denote the winnings for this game, so <span class="math inline">\(D = 2X\)</span>. Since we know <span class="math inline">\(E(X) = 3.5\)</span>, this means that the expected winnings for this game is <span class="math inline">\(E(D) = E(2X) = 2E(X) = 7\)</span>, since we have linear transformation here. The code below verifies these:</p>
<div class="sourceCode" id="cb89"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">X</span><span class="op">&lt;-</span><span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span><span class="fl">1</span>,<span class="fl">2</span>,<span class="fl">3</span>,<span class="fl">4</span>,<span class="fl">5</span>,<span class="fl">6</span><span class="op">)</span> <span class="co">##support for X</span></span>
<span></span>
<span><span class="va">D</span><span class="op">&lt;-</span><span class="fl">2</span><span class="op">*</span> <span class="va">X</span> <span class="co">##winnings</span></span>
<span></span>
<span><span class="fu"><a href="https://rdrr.io/r/base/mean.html">mean</a></span><span class="op">(</span><span class="va">X</span><span class="op">)</span> <span class="co">##EX since die is fair</span></span></code></pre></div>
<pre><code>## [1] 3.5</code></pre>
<div class="sourceCode" id="cb91"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="fu"><a href="https://rdrr.io/r/base/mean.html">mean</a></span><span class="op">(</span><span class="va">D</span><span class="op">)</span> <span class="co">##Expected winnings. This is equal to 2 times mean(X)</span></span></code></pre></div>
<pre><code>## [1] 7</code></pre>
<p>Now suppose the winnings is now defined as the squared of the number of dots the die shows. Let <span class="math inline">\(T\)</span> denote the new winnings, so <span class="math inline">\(T = X^2\)</span>. Since this is a non linear transformation, <span class="math inline">\(E(T) = E(X^2)\)</span> may not equal to <span class="math inline">\(E(X)^2\)</span>:</p>
<div class="sourceCode" id="cb93"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">X</span><span class="op">&lt;-</span><span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span><span class="fl">1</span>,<span class="fl">2</span>,<span class="fl">3</span>,<span class="fl">4</span>,<span class="fl">5</span>,<span class="fl">6</span><span class="op">)</span> <span class="co">##support for X</span></span>
<span></span>
<span><span class="cn">T</span><span class="op">&lt;-</span><span class="va">X</span><span class="op">^</span><span class="fl">2</span> <span class="co">##winnings</span></span>
<span></span>
<span><span class="fu"><a href="https://rdrr.io/r/base/mean.html">mean</a></span><span class="op">(</span><span class="cn">T</span><span class="op">)</span> <span class="co">##Expected winnings. </span></span></code></pre></div>
<pre><code>## [1] 15.16667</code></pre>
<div class="sourceCode" id="cb95"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="fu"><a href="https://rdrr.io/r/base/mean.html">mean</a></span><span class="op">(</span><span class="va">X</span><span class="op">)</span><span class="op">^</span><span class="fl">2</span> <span class="co">##not equal</span></span></code></pre></div>
<pre><code>## [1] 12.25</code></pre>
<p>In this example, we see that <span class="math inline">\(E(T) &gt; E(X)^2\)</span>, in other words, <span class="math inline">\(E(g(X)) &gt; g(E(X))\)</span>, when <span class="math inline">\(g(x) = x^2\)</span>. Is <span class="math inline">\(E(g(X)) &gt; g(E(X))\)</span> always for any non linear function <span class="math inline">\(g\)</span>? It turns out that this is not always the case.</p>
<p>To summarize:</p>
<ul>
<li>If <span class="math inline">\(g\)</span> is linear, then <span class="math inline">\(E(g(X)) = g(E(X))\)</span>, and we can use linearity of expectations.</li>
<li>If <span class="math inline">\(g\)</span> is non linear, then <span class="math inline">\(E(g(X)) \neq g(E(X))\)</span>.</li>
</ul>
</div>
<div id="convex-and-concave-functions" class="section level4" number="6.2.2.2">
<h4>
<span class="header-section-number">6.2.2.2</span> Convex and Concave Functions<a class="anchor" aria-label="anchor" href="#convex-and-concave-functions"><i class="fas fa-link"></i></a>
</h4>
<p>In the example above, we have an instance where <span class="math inline">\(E(g(X)) \neq g(E(X))\)</span>. The direction of the inequality depends on whether the function <span class="math inline">\(g\)</span> is convex or concave. There are a couple of ways to decide if a function is convex or concave:</p>
<ul>
<li>Using derivatives:
<ul>
<li>A function <span class="math inline">\(g(x)\)</span> is <strong>convex</strong> if its second derivative is non negative, i.e. <span class="math inline">\(g^{\prime \prime}(x) \geq 0\)</span> over its domain. The domain is the set of all values of <span class="math inline">\(x\)</span> for which <span class="math inline">\(g(x)\)</span> is defined.</li>
<li>A function <span class="math inline">\(g(x)\)</span> is <strong>concave</strong> if its second derivative is non positive, i.e. <span class="math inline">\(g^{\prime \prime}(x) \leq 0\)</span> over its domain.</li>
</ul>
</li>
<li>Using visuals:
<ul>
<li>A function <span class="math inline">\(g(x)\)</span> is <strong>convex</strong> if every line segment joining two points on its graph is never below the graph.</li>
<li>A function <span class="math inline">\(g(x)\)</span> is <strong>concave</strong> if every line segment joining two points on its graph is never above the graph.</li>
</ul>
</li>
</ul>
<p>We now look at a couple of functions to see if they are convex or concave:</p>
<ul>
<li>
<span class="math inline">\(g(x) = \log(x)\)</span> is a concave function.
<ul>
<li>Its second derivative is <span class="math inline">\(g^{\prime \prime}(x) = -\frac{1}{x^2}\)</span>. Note the domain of <span class="math inline">\(\log(x)\)</span> is positive real numbers (it is undefined when <span class="math inline">\(x \leq 0\)</span>), so its second derivative is always negative.</li>
<li>We can also look at a graph of <span class="math inline">\(y = \log(x)\)</span>, and draw line segments that join two points on its graph. All of these lines are never above the graph. Figure <a href="inequalities-limit-theorems-and-simulations.html#fig:concave">6.1</a> below shows an example with one line segment, but we can see that any line segment that joins two points on the graph will never be above the graph.</li>
</ul>
</li>
</ul>
<div class="figure">
<span style="display:block;" id="fig:concave"></span>
<img src="images/06-concave.jpeg" alt="Example of Concave Function"><p class="caption">
Figure 6.1: Example of Concave Function
</p>
</div>
<ul>
<li>
<span class="math inline">\(g(x) = x^2\)</span> is a convex function.
<ul>
<li>Its second derivative is <span class="math inline">\(g^{\prime \prime}(x) = 2\)</span>, which is always positive.</li>
<li>We can also look at a graph of <span class="math inline">\(y = x^2\)</span>, and draw line segments that join two points on its graph. All of these lines are never below the graph. Figure <a href="inequalities-limit-theorems-and-simulations.html#fig:convex">6.2</a> below shows an example with one line segment, but we can see that any line segment that joins two points on the graph will never be below the graph.</li>
</ul>
</li>
</ul>
<div class="figure">
<span style="display:block;" id="fig:convex"></span>
<img src="images/06-convex.jpeg" alt="Example of Convex Function"><p class="caption">
Figure 6.2: Example of Convex Function
</p>
</div>
<p><em>Thought question</em>: Consider the function <span class="math inline">\(g(x) = \frac{1}{x}\)</span>, i.e. the inverse function. Can you explain why this function is convex when <span class="math inline">\(x&gt;0\)</span> and is concave when <span class="math inline">\(x&lt;0\)</span>?</p>
</div>
<div id="jensens-inequality-1" class="section level4" number="6.2.2.3">
<h4>
<span class="header-section-number">6.2.2.3</span> Jensen’s Inequality<a class="anchor" aria-label="anchor" href="#jensens-inequality-1"><i class="fas fa-link"></i></a>
</h4>
<p>We are now ready to state <strong>Jensen’s inequality</strong>. Let <span class="math inline">\(X\)</span> denote a random variable. If <span class="math inline">\(g\)</span> is convex, then <span class="math inline">\(E(g(X)) \geq g(E(X))\)</span>. If <span class="math inline">\(g\)</span> is concave, then <span class="math inline">\(E(g(X)) \leq g(E(X))\)</span>.</p>
<p>The equality holds only if <span class="math inline">\(g\)</span> is a linear function. It turns out linear functions are both convex and concave. The book goes through a simple proof of Jensen’s inequality and is worth reading. Next, we apply Jensen’s inequality to a few examples:</p>
<ol style="list-style-type: decimal">
<li><p>We apply Jensen’s inequality to the toy example in Section <a href="inequalities-limit-theorems-and-simulations.html#linnonlin">6.2.2.1</a>. As a reminder, suppose we roll a fair six-sided die, and let <span class="math inline">\(X\)</span> denote the number of dots the die shows. The winnings is defined as the squared of the number of dots the die shows. Let <span class="math inline">\(T\)</span> denote the new winnings, so <span class="math inline">\(T = X^2\)</span>, so <span class="math inline">\(g(x) = x^2\)</span> is the function representing this non linear transformation. We established that the quadratic function is convex, so Jensen’s inequality tells us that <span class="math inline">\(E(g(X)) \geq g(E(X))\)</span>, i.e. that <span class="math inline">\(E(T) &gt; E(X)^2\)</span> which we showed in the code.</p></li>
<li><p>As mentioned in Section <a href="inequalities-limit-theorems-and-simulations.html#linnonlin">6.2.2.1</a>, a log transformation is often applied to make data that are right skewed less skewed, so that popular methods such as linear regression, tree based methods, <span class="math inline">\(K\)</span> nearest neighbors can be used (these methods can be sensitive to outliers since they are based on conditional expectations or conditional means). What often happens is the log transformation is applied to the variable of interest, the model is fit, a prediction is made for the log transformed variable using conditional expectations, and the exponential is applied to this predicted value to convert it back to the original variable. Jensen’s inequality tells us that the exponential of the average log variable is greater than the average variable, and our model over estimates.</p></li>
<li><p>Jensen’s inequality can also be used to show that the sample standard deviation is a biased estimator of the population standard deviation, which appears counter intuitive, since the sample variance is an unbiased estimator of the population variance, i.e. <span class="math inline">\(E(s^2) = \sigma^2\)</span>, but <span class="math inline">\(E(s) \neq \sigma\)</span>. The quick proof is</p></li>
</ol>
<p><span class="math display">\[
E(s) = E(\sqrt{s^2}) \leq \sqrt{E(s^2)} = \sigma.
\]</span></p>
<p>So the sample standard deviation underestimates the population standard deviation. However, this bias tends to be small if the sample size is large. We will cover ideas relating to unbiased estimators in a future module in more detail.</p>
</div>
</div>
<div id="chebyshevs-inequality" class="section level3" number="6.2.3">
<h3>
<span class="header-section-number">6.2.3</span> Chebyshev’s Inequality<a class="anchor" aria-label="anchor" href="#chebyshevs-inequality"><i class="fas fa-link"></i></a>
</h3>
<p>A common inequality that is used for probability is <strong>Chebyshev’s inequality</strong>. It provides an upper bound on the probability that a random variable is at least a certain distance from its mean. Let <span class="math inline">\(X\)</span> be a random variable with mean <span class="math inline">\(\mu\)</span> and variance <span class="math inline">\(\sigma^2\)</span>. Then for any <span class="math inline">\(a&gt;0\)</span>,</p>
<p><span class="math display" id="eq:6-CI">\[\begin{equation}
P(|X-\mu| \geq a) \leq \frac{\sigma^2}{a^2}.
\tag{6.2}
\end{equation}\]</span></p>
<p>An alternative way of expressing Chebyshev’s inequality is to let <span class="math inline">\(a = c \sigma\)</span> in equation <a href="inequalities-limit-theorems-and-simulations.html#eq:6-CI">(6.2)</a>, so that it can be interpreted as providing an upper bound on the probability that a random variable is at least <span class="math inline">\(c\)</span> standard deviations from its mean:</p>
<p><span class="math display" id="eq:6-CIalt">\[\begin{equation}
P(|X-\mu| \geq c \sigma) \leq \frac{\sigma^2}{c^2 \sigma^2} = \frac{1}{c^2}.
\tag{6.3}
\end{equation}\]</span></p>
<p>Using equation <a href="inequalities-limit-theorems-and-simulations.html#eq:6-CIalt">(6.3)</a>, we can say the following about the upper bond on the probability that a random variable is at least 1, 2, and 3 standard deviations from its mean:</p>
<ul>
<li>When <span class="math inline">\(c=1\)</span>,</li>
</ul>
<p><span class="math display">\[
P(|X-\mu| \geq \sigma) \leq \frac{1}{1^2} = 1.
\]</span>
This informs us that the probability that a random variable is at least one standard deviation from its mean is no more than 1. The upper bound is not very informative in this setting since we know probabilities cannot be greater than 1.</p>
<ul>
<li>When <span class="math inline">\(c=2\)</span>,</li>
</ul>
<p><span class="math display">\[
P(|X-\mu| \geq 2\sigma) \leq \frac{1}{2^2} = 0.25.
\]</span></p>
<p>This informs us the probability that a random variable is at least two standard deviations from its mean is no more than 0.25. In other words, there cannot be more than a 25% chance that a random variable is at least 2 standard deviations from its mean, or there cannot be less than a 75% chance that a random variable is within 2 standard deviations from its mean, since <span class="math inline">\(P(|X-\mu| \leq 2\sigma)\)</span> is the complement of <span class="math inline">\(P(|X-\mu| \geq 2\sigma)\)</span>.</p>
<ul>
<li>When <span class="math inline">\(c=3\)</span>,</li>
</ul>
<p><span class="math display">\[
P(|X-\mu| \geq 3\sigma) \leq \frac{1}{3^2} = \frac{1}{9}.
\]</span></p>
<p>There cannot be more than a 11.11% chance that a random variable is at least 3 standard deviations from its mean, or there cannot be less than a 88.89% chance that a random variable is within 3 standard deviations from its mean.</p>
<p><em>Thought question</em>: Can you explain how these results are consistent with the 68-99-99.7% rule for normal distributions, as stated in Section <a href="continuous-random-variables.html#rulenorm">4.5.2.3</a>?</p>
<p>Notice that Chebyshev’s inequality can be applied to any distribution, and can be used to provide bounds on how data can be spread out. It is more flexible than the 68-99-99.7% rule for normal distributions as it can be applied to any distribution, but the bounds are not as exact as they are an inequality. There can be a trade-off in relaxing assumptions and accuracy of results.</p>
</div>
</div>
<div id="limits" class="section level2" number="6.3">
<h2>
<span class="header-section-number">6.3</span> Limit Theorems<a class="anchor" aria-label="anchor" href="#limits"><i class="fas fa-link"></i></a>
</h2>
<p>In the previous subsection, we used inequalities to provide bounds on probabilities and expectations that may be difficult to calculate. Another way of handing difficult calculations would be to use approximations for the distribution of the random variable, instead of the exact distribution of the random variable. Generally speaking, these approximations work better when we have more data (i.e. when the sample size is larger). These approximations are covered by two of the most important limit theorems: the Law of Large Numbers and the Central Limit Theorem. These theorems approximate the distribution of the sample mean of i.i.d. (independent and identically distributed) random variables as the sample size gets larger.</p>
<p>Note: The idea of i.i.d. random variables implies that each observed value of the random variable is independent of each other, and that each observed value come from the same random variable. For example, let <span class="math inline">\(X\)</span> denote the number of dots from a roll of a 6-sided fair die, and let <span class="math inline">\(X_1, X_2\)</span> denote the value of the first and second roll respectively. <span class="math inline">\(X_1\)</span> and <span class="math inline">\(X_2\)</span> are i.i.d. since the outcomes from the first and second roll do not influence each other, so they are independent. <span class="math inline">\(X_1\)</span> and <span class="math inline">\(X_2\)</span> are identically distributed as they both follow the same distribution, <span class="math inline">\(Mult_6(1, (1/6, 1/6, 1/6, 1/6, 1/6, 1/6))\)</span>.</p>
<p>For the rest of this section, Section <a href="inequalities-limit-theorems-and-simulations.html#limits">6.3</a>, assume we have i.i.d. <span class="math inline">\(X_1, \cdots, X_n\)</span> with finite mean <span class="math inline">\(\mu\)</span> and finite variance <span class="math inline">\(\sigma^2\)</span>. For all positive integers <span class="math inline">\(n\)</span> (i.e. for any possible sample size), define the sample mean as <span class="math inline">\(\bar{X}_n = \frac{X_1 + \cdots + X_n}{n}\)</span>. We can easily derive the expected value and variance of the sample mean using properties of expectations and variances. Its expected value is</p>
<p><span class="math display" id="eq:6-Emean">\[\begin{equation}
\begin{split}
E(\bar{X}_n) &amp;= E(\frac{X_1 + \cdots + X_n}{n}) \\
             &amp;= \frac{1}{n}E(X_1 + \cdots + X_n) \\
             &amp;= \frac{1}{n} (E(X_1) + \cdots + E(X_n)) \\
             &amp;= \frac{1}{n} (\mu + \cdots + \mu) \\
             &amp;= \mu.
\end{split}
\tag{6.4}
\end{equation}\]</span></p>
<p>Its variance is</p>
<p><span class="math display" id="eq:6-Varmean">\[\begin{equation}
\begin{split}
Var(\bar{X}_n) &amp;= Var(\frac{X_1 + \cdots + X_n}{n}) \\
             &amp;= \frac{1}{n^2}Var(X_1 + \cdots + X_n) \\
             &amp;= \frac{1}{n^2} (Var(X_1) + \cdots + Var(X_n)) \\
             &amp;= \frac{1}{n^2} (\sigma^2 + \cdots + \sigma^2) \\
             &amp;= \frac{\sigma^2}{n}.
\end{split}
\tag{6.5}
\end{equation}\]</span></p>
<p>View the video below for a more detailed explanation of these results:</p>
<iframe width="560px" height="320px" allowfullscreen="true" allow="autoplay *" title="Module 06: Properties of Sample Mean" src="https://virginiauniversity.instructuremedia.com/embed/86d05cc6-56b2-4186-96ce-c1a641acb2b0" frameborder="0">
</iframe>
<p>Equation <a href="inequalities-limit-theorems-and-simulations.html#eq:6-Emean">(6.4)</a> informs us that the long-run average of sample means is equal to the population mean. We can imagine this if we had taken different random samples of size <span class="math inline">\(n\)</span> from a population, and for each random sample we find the sample mean, and then average all these sample means. This average equals to the population mean <span class="math inline">\(\mu\)</span>. The code below provides a demonstration of these steps:</p>
<ul>
<li>We simulate a random sample of <span class="math inline">\(X_1, \cdots, X_{500}\)</span> i.i.d. from standard normal.</li>
<li>Compute the sample mean and store it.</li>
<li>Repeat the previous steps for a total of 10 thousand reps.</li>
<li>Find the average of the 10 thousand sample means.</li>
</ul>
<div class="sourceCode" id="cb97"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">reps</span><span class="op">&lt;-</span> <span class="fl">10000</span> <span class="co">##take 10000 random samples. This value should be large</span></span>
<span><span class="va">n</span><span class="op">&lt;-</span><span class="fl">500</span> <span class="co">##sample size for each random sample</span></span>
<span><span class="va">xbar</span><span class="op">&lt;-</span><span class="fu"><a href="https://rdrr.io/r/base/array.html">array</a></span><span class="op">(</span><span class="fl">0</span>,<span class="va">reps</span><span class="op">)</span> <span class="co">##store the sample mean for each random sample</span></span>
<span></span>
<span><span class="fu"><a href="https://rdrr.io/r/base/Random.html">set.seed</a></span><span class="op">(</span><span class="fl">90</span><span class="op">)</span></span>
<span></span>
<span><span class="kw">for</span> <span class="op">(</span><span class="va">i</span> <span class="kw">in</span> <span class="fl">1</span><span class="op">:</span><span class="va">reps</span><span class="op">)</span></span>
<span>  </span>
<span><span class="op">{</span></span>
<span>  </span>
<span>  <span class="va">xbar</span><span class="op">[</span><span class="va">i</span><span class="op">]</span><span class="op">&lt;-</span><span class="fu"><a href="https://rdrr.io/r/base/mean.html">mean</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/stats/Normal.html">rnorm</a></span><span class="op">(</span><span class="va">n</span><span class="op">)</span><span class="op">)</span> <span class="co">##find and store sample mean for each random sample</span></span>
<span>  </span>
<span><span class="op">}</span></span>
<span></span>
<span><span class="fu"><a href="https://rdrr.io/r/base/mean.html">mean</a></span><span class="op">(</span><span class="va">xbar</span><span class="op">)</span> <span class="co">##average the sample means. This should be close to 0. </span></span></code></pre></div>
<pre><code>## [1] -0.0001034368</code></pre>
<p>Equation <a href="inequalities-limit-theorems-and-simulations.html#eq:6-Varmean">(6.5)</a> informs us how to calculate the variance of the sample means. We can imagine this if we had taken different random samples of size <span class="math inline">\(n\)</span> from a population, and for each random sample we find the sample mean, and then find the variance of all these sample means. It is the variance of the original random variable divided by <span class="math inline">\(n\)</span>. This means as the sample size gets larger, the variance of the sample means get smaller, in other words, the sample means tend to get closer to the population mean. We re run the code from above and also find the variance of the sample means.</p>
<div class="sourceCode" id="cb99"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="fu"><a href="https://rdrr.io/r/stats/cor.html">var</a></span><span class="op">(</span><span class="va">xbar</span><span class="op">)</span> <span class="co">##variance of sample means. This should be close to 1/500, since n=500. </span></span></code></pre></div>
<pre><code>## [1] 0.001979948</code></pre>
<div id="law-of-large-numbers" class="section level3" number="6.3.1">
<h3>
<span class="header-section-number">6.3.1</span> Law of Large Numbers<a class="anchor" aria-label="anchor" href="#law-of-large-numbers"><i class="fas fa-link"></i></a>
</h3>
<p>The <strong>Law of Large Numbers (LLN)</strong> states that as <span class="math inline">\(n\)</span> gets larger and approaches infinity, the sample mean <span class="math inline">\(\bar{X}_n\)</span> converges to the true mean <span class="math inline">\(\mu\)</span>. This implies that the sample mean tends to get closer to the population mean with larger sample sizes. The key word here is tends to, it is not a guarantee that the sample mean always gets closer to the population mean whenever <span class="math inline">\(n\)</span> gets larger, but it generally does. This explains why we tend to trust results from larger sample sizes.</p>
<p>Another implication of the LLN is that we can use simulations to verify theoretical results, since these results usually require us to simulate data based on a large number of independent replications.</p>
<p>The LLN is a by product of equations <a href="inequalities-limit-theorems-and-simulations.html#eq:6-Emean">(6.4)</a> and <a href="inequalities-limit-theorems-and-simulations.html#eq:6-Varmean">(6.5)</a>. Equation <a href="inequalities-limit-theorems-and-simulations.html#eq:6-Varmean">(6.5)</a> informs us that as <span class="math inline">\(n\)</span> gets larger, the variance of the sample mean gets smaller. Equation <a href="inequalities-limit-theorems-and-simulations.html#eq:6-Emean">(6.4)</a> informs us that the sample mean is unbiased, i.e. its long run average is equal to the true mean. Collectively, these inform us that as <span class="math inline">\(n\)</span> gets larger, the sample mean is more likely to be closer to the true mean.</p>
<p>We use an example to illustrate the LLN, which comes from flipping a fair coin. Let <span class="math inline">\(X\)</span> denote whether the coin lands heads or tails, and let <span class="math inline">\(X=1\)</span> for heads and <span class="math inline">\(X=0\)</span> for tails. We can say that <span class="math inline">\(X \sim Bern(0.5)\)</span> since the coin is fair. Imagine flipping the coin <span class="math inline">\(n\)</span> times, and record the outcome after each flip, so <span class="math inline">\(X_1, \cdots, X_n\)</span> denote the outcome of each flip. We know that <span class="math inline">\(E(X) = 0.5\)</span> since <span class="math inline">\(X \sim Bern(0.5)\)</span>. The LLN informs us that <span class="math inline">\(\bar{X}_1, \cdots, \bar{X}_n\)</span> should usually get closer to 0.5 as <span class="math inline">\(n\)</span> increases. In other words, the value of the sample proportion after each flip should get usually closer to 0.5 with more flips. The code below simulates this example for <span class="math inline">\(n=500\)</span>, and Figure <a href="inequalities-limit-theorems-and-simulations.html#fig:4-LLN2">6.3</a> shows how the sample proportions get closer to 0.5, in general, as <span class="math inline">\(n\)</span> increases.</p>
<div class="sourceCode" id="cb101"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">n</span><span class="op">&lt;-</span><span class="fl">500</span> <span class="co">##make this big, but not too big otherwise picture is difficult to see</span></span>
<span></span>
<span><span class="fu"><a href="https://rdrr.io/r/base/Random.html">set.seed</a></span><span class="op">(</span><span class="fl">23</span><span class="op">)</span></span>
<span></span>
<span><span class="va">X</span><span class="op">&lt;-</span><span class="fu"><a href="https://rdrr.io/r/stats/Binomial.html">rbinom</a></span><span class="op">(</span><span class="va">n</span>,<span class="fl">1</span>,<span class="fl">0.5</span><span class="op">)</span> <span class="co">##simulate 500 flips of fair coin</span></span>
<span></span>
<span><span class="va">totals</span><span class="op">&lt;-</span><span class="fu"><a href="https://rdrr.io/r/base/cumsum.html">cumsum</a></span><span class="op">(</span><span class="va">X</span><span class="op">)</span> <span class="co">##count total number of heads after each flip</span></span>
<span><span class="va">index</span><span class="op">&lt;-</span><span class="fl">1</span><span class="op">:</span><span class="va">n</span></span>
<span><span class="va">props</span><span class="op">&lt;-</span><span class="va">totals</span><span class="op">/</span><span class="va">index</span> <span class="co">##find proportion of heads after each flip</span></span>
<span></span>
<span><span class="co">##create visual. LLN says that as n gets larger, the value of the sample proportion tends to get closer to 0.5</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/graphics/plot.default.html">plot</a></span><span class="op">(</span><span class="va">props</span>, type<span class="op">=</span><span class="st">"l"</span>, main<span class="op">=</span><span class="st">"Prop vs Sample Size"</span>, ylab<span class="op">=</span><span class="st">"Proportion"</span>, xlab<span class="op">=</span><span class="st">"n"</span><span class="op">)</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/graphics/abline.html">abline</a></span><span class="op">(</span>h<span class="op">=</span><span class="fl">0.5</span>, col<span class="op">=</span><span class="st">"blue"</span><span class="op">)</span> <span class="co">##overlay 0.5 for easy comparison</span></span></code></pre></div>
<div class="figure">
<span style="display:block;" id="fig:4-LLN2"></span>
<img src="bookdown-demo_files/figure-html/4-LLN2-1.png" alt="LLN Example 2" width="672"><p class="caption">
Figure 6.3: LLN Example 2
</p>
</div>
<p>View the video below for a more detailed explanation of the code:</p>
<iframe width="560px" height="320px" allowfullscreen="true" allow="autoplay *" title="Module 06: Law of Large Numbers" src="https://virginiauniversity.instructuremedia.com/embed/d5516684-77b9-474c-a862-8f2a7377d0b8" frameborder="0">
</iframe>
<p>Note: <code><a href="https://rdrr.io/r/base/Random.html">set.seed()</a></code> was used so you can reproduce these results exactly. However, the observation that the sample mean tends to get closer to the true mean as <span class="math inline">\(n\)</span> increases will happen regardless of what <code><a href="https://rdrr.io/r/base/Random.html">set.seed()</a></code> was used, or even if <code><a href="https://rdrr.io/r/base/Random.html">set.seed()</a></code> was not used.</p>
<p>Note: The LLN actually comes in two versions, the Weak Law of Large Numbers (WLLN), and the Strong Law of Large Numbers (SLLN). The book goes into some detail about their definitions and differences. What I have written gives an intuitive explanation of what the LLN implies.</p>
<div id="misconceptions-with-lln" class="section level4" number="6.3.1.1">
<h4>
<span class="header-section-number">6.3.1.1</span> Misconceptions with LLN<a class="anchor" aria-label="anchor" href="#misconceptions-with-lln"><i class="fas fa-link"></i></a>
</h4>
<p>One key idea with the LLN is that the sample mean <strong>tends to get closer to the true mean as <span class="math inline">\(n\)</span> gets larger</strong>. The key words here are “tends” and “as <span class="math inline">\(n\)</span> gets larger”.</p>
<p>A misunderstanding of the LLN is the <strong>gambler’s fallacy</strong>, which erroneously believes that the sample mean must “self correct” and get closer to the population mean with small increments of <span class="math inline">\(n\)</span>.</p>
<p>Using the example from flipping a fair coin. The gambler’s fallacy erroneously thinks that:</p>
<ul>
<li>The proportion of heads should be close to 0.5, even with small <span class="math inline">\(n\)</span>.</li>
<li>The results of subsequent flips should self correct, i.e. the proportion of heads get closer to 0.5 with the next flip. For example, if the first 5 flips are heads, the next flip is “due” to be tails since the proportion should get closer to 0.5 with the next flip.</li>
</ul>
<p>The convergence to 0.5 comes from flipping the coin many times.</p>
</div>
</div>
<div id="CLT" class="section level3" number="6.3.2">
<h3>
<span class="header-section-number">6.3.2</span> Central Limit Theorem<a class="anchor" aria-label="anchor" href="#CLT"><i class="fas fa-link"></i></a>
</h3>
<p>The LLN informs us that the sample mean converges to the true mean. Statistical theory informs us about the expected value and variance of the sample mean in equations <a href="inequalities-limit-theorems-and-simulations.html#eq:6-Emean">(6.4)</a> and <a href="inequalities-limit-theorems-and-simulations.html#eq:6-Varmean">(6.5)</a>. But these do not inform us about the distribution of <span class="math inline">\(\bar{X}_n\)</span>. This is where the <strong>Central Limit Theorem (CLT)</strong> comes in.</p>
<p>The CLT states that as the sample size gets larger and tends to infinity, the distribution of <span class="math inline">\(\bar{X}_n\)</span> after standardization approaches a standard normal distribution, i.e.</p>
<p><span class="math display" id="eq:6-CLT">\[\begin{equation}
\sqrt{n} \left(\frac{\bar{X}_n \ - \mu}{\sigma} \right) \to N(0,1).
\tag{6.6}
\end{equation}\]</span></p>
<p>The CLT is called an <strong>asymptotic</strong> result, as it informs us about the <strong>limiting distribution</strong> of <span class="math inline">\(\bar{X}_n\)</span> as <span class="math inline">\(n\)</span> gets larger and tends to infinity. The CLT implies an approximation when <span class="math inline">\(n\)</span> is large enough. For large <span class="math inline">\(n\)</span>, the distribution of <span class="math inline">\(\bar{X}_n\)</span> is approximately <span class="math inline">\(N(\mu, \frac{\sigma^2}{n})\)</span>.</p>
<p>The implication of the CLT is that even if our data do not follow a normal distribution, the average value of the data can be approximated by a normal distribution if our sample size is large enough. As mentioned in Section <a href="continuous-random-variables.html#normdist">4.5.2</a>, a lot of questions in research deal with averages.</p>
<p>We consider this hypothetical situation. Suppose the waiting time for customers calling customer service during lunch time is known have a mean of 600 seconds with standard deviation 30 seconds. The company decides to cut costs and reduces staffing at the call center, and claims that wait times are not affected negatively. The customers are convinced otherwise. A researchers obtains the wait times from 500 customers who call in during lunch time after staffing is reduced. The sample mean of the wait times for these customers is 700 seconds. Can this data be used to counter the company’s claim that wait times have been affected?</p>
<p>One possible calculation will be to assume the company is correct, that wait times have not changed, on average. If so, the sample means will be approximately normal, with mean 600 and variance <span class="math inline">\(\frac{30^2}{\sqrt{500}}\)</span>, i.e. <span class="math inline">\(\bar{X}_{500} \sim N(600, \frac{30^2}{\sqrt{500}})\)</span>. We then calculate <span class="math inline">\(P(\bar{X}_{500} \geq 700)\)</span>, the probability that we have sample mean that is equal to or greater than 700 seconds. Using R, this probability is about 0.0065, which is very small, indicating that our data is inconsistent with the company’s claim.</p>
<div class="sourceCode" id="cb102"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="fl">1</span><span class="op">-</span><span class="fu"><a href="https://rdrr.io/r/stats/Normal.html">pnorm</a></span><span class="op">(</span><span class="fl">700</span>, <span class="fl">600</span>, <span class="fl">30</span><span class="op">^</span><span class="fl">2</span><span class="op">/</span><span class="fu"><a href="https://rdrr.io/r/base/MathFun.html">sqrt</a></span><span class="op">(</span><span class="fl">500</span><span class="op">)</span><span class="op">)</span></span></code></pre></div>
<pre><code>## [1] 0.006486311</code></pre>
<p>The CLT is traditionally associated with the distribution of the sample mean <span class="math inline">\(\bar{X}_n\)</span>. It can be applied to the sum as well, due to properties of expectations and variances. Let <span class="math inline">\(T_n = X_1 + \cdots + X_n = n \bar{X}_n\)</span> denote the sum of <span class="math inline">\(n\)</span> i.i.d. random variables. The CLT says that, for large <span class="math inline">\(n\)</span>, the distribution of <span class="math inline">\(T_n\)</span> is approximately <span class="math inline">\(N(n\mu, n\sigma^2)\)</span>.</p>
<div id="considerCLT" class="section level4" number="6.3.2.1">
<h4>
<span class="header-section-number">6.3.2.1</span> Considerations with CLT<a class="anchor" aria-label="anchor" href="#considerCLT"><i class="fas fa-link"></i></a>
</h4>
<p>One question that is raised when the CLT is used is how large does the sample size <span class="math inline">\(n\)</span> have to be for the approximation to be accurate? While suggestions are plentiful (usually along the lines to sample size being at least 25 or 30), there is no fixed answer to this question. It depends on the distribution of <span class="math inline">\(X\)</span>. In general, the more skewed <span class="math inline">\(X\)</span> is, <span class="math inline">\(n\)</span> needs to be larger for the approximation to work. On the other hand, if <span class="math inline">\(X\)</span> is already normal, then the distribution of <span class="math inline">\(\bar{X}_n\)</span> is exactly <span class="math inline">\(N(\mu, \frac{\sigma^2}{n})\)</span> for any sample size <span class="math inline">\(n\)</span>. We look at a couple of examples based on different distributions.</p>
<ol style="list-style-type: decimal">
<li>
<span class="math inline">\(X\)</span> is standard normal. Our code will do the following:</li>
</ol>
<ul>
<li>We simulate <span class="math inline">\(n\)</span> draws from <span class="math inline">\(X\)</span> for <span class="math inline">\(n = 1\)</span>.</li>
<li>To obtain the distribution of <span class="math inline">\(\bar{X}_n\)</span> for each value of <span class="math inline">\(n\)</span>, we repeat the previous step for a total of 10 thousand reps, then produce a histogram of the 10 thousand values of <span class="math inline">\(\bar{X}_n\)</span>.</li>
<li>Repeat the previous two steps, with different values of <span class="math inline">\(n\)</span>. We will use <span class="math inline">\(n= 5, 30, 100\)</span> as well</li>
<li>We expect the histograms for <span class="math inline">\(\bar{X}_n\)</span> to all look normal for all values of <span class="math inline">\(n\)</span> we used.</li>
</ul>
<div class="figure">
<span style="display:block;" id="fig:6-CLTnorm"></span>
<img src="bookdown-demo_files/figure-html/6-CLTnorm-1.png" alt="Distribution of Sample Means when X is N(0,1), n varied" width="672"><p class="caption">
Figure 6.4: Distribution of Sample Means when X is N(0,1), n varied
</p>
</div>
<p>Figure <a href="inequalities-limit-theorems-and-simulations.html#fig:6-CLTnorm">6.4</a> displays the histograms from this simulation, and matches what we expect from the CLT. Since <span class="math inline">\(X\)</span> is normal, <span class="math inline">\(\bar{X}_n\)</span> follows a normal distribution for any value of <span class="math inline">\(n\)</span>.</p>
<ol start="2" style="list-style-type: decimal">
<li>
<span class="math inline">\(X\)</span> is Poisson with parameter 1. This is a skewed distribution. We use code that mimics the previous example, with the only difference being that our data are simulated from <span class="math inline">\(Pois(1)\)</span> instead of standard normal. When <span class="math inline">\(n\)</span> is small, we expect the distribution of <span class="math inline">\(\bar{X}_n\)</span> to not look normal. As <span class="math inline">\(n\)</span> gets larger, we expect the distribution of <span class="math inline">\(\bar{X}_n\)</span> will look more normal.</li>
</ol>
<div class="figure">
<span style="display:block;" id="fig:6-CLTpois"></span>
<img src="bookdown-demo_files/figure-html/6-CLTpois-1.png" alt="Distribution of Sample Means when X is Pois(1), n varied" width="672"><p class="caption">
Figure 6.5: Distribution of Sample Means when X is Pois(1), n varied
</p>
</div>
<p>Figure <a href="inequalities-limit-theorems-and-simulations.html#fig:6-CLTpois">6.5</a> displays the histograms from this simulation, and matches what we expect. When <span class="math inline">\(n\)</span> is 1 or 5, the histograms are clearly not normal, so the CLT approximation will not work well. When <span class="math inline">\(n=30\)</span> the histogram looks approximately normal, and when <span class="math inline">\(n=100\)</span>, the histogram looks even closer to a normal distribution.</p>
</div>
</div>
</div>
<div id="MCsims" class="section level2" number="6.4">
<h2>
<span class="header-section-number">6.4</span> Monte Carlo Simulations<a class="anchor" aria-label="anchor" href="#MCsims"><i class="fas fa-link"></i></a>
</h2>
<p>You may have noticed that we have used simulations in the earlier sections of this module (and previous modules) to help explain certain concepts. These simulations are called <strong>Monte Carlo</strong> methods, or Monte Carlo simulations. The idea behind Monte Carlo methods is to used repeated random sampling (and by repeated, we mean repeated a large number of times) to estimate features of data, usually probabilities and expected values. Monte Carlo methods are used for the following purposes:</p>
<ol style="list-style-type: decimal">
<li><p>When the probability or expectation is too complicated to work out by hand. Recall that finding probabilities and expectations by hand involve summations or integrals, and it becomes obvious that working with summations and especially integrals can get onerous.</p></li>
<li><p>To verify theoretical results involving probability or expectations. While a lot of theory is proved using mathematics, most academic papers include Monte Carlo simulations to verify the theoretical results. We have done these to verify the LLN and CLT in the previous subsection (under some circumstances).</p></li>
<li><p>To help confirm that you understand the meaning of theoretical results. The only way your code matches the theory is if you understand the theory.</p></li>
</ol>
<div id="monte-carlo-methods-for-expected-values" class="section level3" number="6.4.1">
<h3>
<span class="header-section-number">6.4.1</span> Monte Carlo Methods for Expected Values<a class="anchor" aria-label="anchor" href="#monte-carlo-methods-for-expected-values"><i class="fas fa-link"></i></a>
</h3>
<p>Suppose we want to find some expectation for a continuous random variable <span class="math inline">\(X\)</span>, <span class="math inline">\(E(g(X))\)</span>, where <span class="math inline">\(g\)</span> is some function. LOTUS says that we need to use equation <a href="continuous-random-variables.html#eq:4-lotus">(4.4)</a>, i.e. <span class="math inline">\(E(g(X)) = \int_{-\infty}^{\infty} g(x) f_X(x).\)</span> Monte Carlo methods avoid doing this integration by simulating <span class="math inline">\(X_1, \cdots, X_M\)</span> from <span class="math inline">\(X\)</span> and estimate <span class="math inline">\(E(g(X))\)</span> with the sample mean of <span class="math inline">\(g(X)\)</span>, <span class="math inline">\(\frac{1}{M} \sum_{i=1}^M g(X_i)\)</span>. The LLN tells us that as <span class="math inline">\(M\)</span> gets larger, this sample mean converges to <span class="math inline">\(E(g(X))\)</span>.</p>
<p>Monte Carlo methods replace the integral (or summation) with simulating a random variable repeatedly many times. We use a simple example to illustrate this idea.</p>
<p>Let <span class="math inline">\(X\)</span> be a standard normal distribution. Suppose we want to find the value of <span class="math inline">\(E(X^2)\)</span>. If we try to find this using LOTUS, we will need to find <span class="math inline">\(\int_{-\infty}^{\infty} x^2 \frac{1}{\sqrt{2 \pi}} e^{-x^2/2} dx\)</span>. Instead of working out this integral by hand, we carry out a Monte Carlo simulation by doing these steps:</p>
<ul>
<li>Simulate <span class="math inline">\(M\)</span> random values from a standard normal, where <span class="math inline">\(M\)</span> is large.</li>
<li>Calculate <span class="math inline">\(X_1^2, \cdots, X_M^2\)</span>.</li>
<li>Find the sample average of <span class="math inline">\(X_1^2, \cdots, X_M^2\)</span>.</li>
</ul>
<div class="sourceCode" id="cb104"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="fu"><a href="https://rdrr.io/r/base/Random.html">set.seed</a></span><span class="op">(</span><span class="fl">5</span><span class="op">)</span></span>
<span></span>
<span><span class="va">reps</span><span class="op">&lt;-</span><span class="fl">10000</span> <span class="co">## this is M</span></span>
<span></span>
<span><span class="va">Xs</span><span class="op">&lt;-</span><span class="fu"><a href="https://rdrr.io/r/stats/Normal.html">rnorm</a></span><span class="op">(</span><span class="va">reps</span><span class="op">)</span> <span class="co">##generate M values of X</span></span>
<span></span>
<span><span class="va">squared.values</span><span class="op">&lt;-</span><span class="va">Xs</span><span class="op">^</span><span class="fl">2</span> <span class="co">##square each X</span></span>
<span></span>
<span><span class="fu"><a href="https://rdrr.io/r/base/mean.html">mean</a></span><span class="op">(</span><span class="va">squared.values</span><span class="op">)</span> <span class="co">##sample average of squared values. </span></span></code></pre></div>
<pre><code>## [1] 1.024546</code></pre>
<div class="sourceCode" id="cb106"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="co">##when reps is large, this sample mean should be close to the true E(X^2), which is 1</span></span></code></pre></div>
<p>Since <span class="math inline">\(X\)</span> is standard normal, we know <span class="math inline">\(E(X) = 0\)</span> and <span class="math inline">\(Var(X) = E(X^2) - E(X)^2 = E(X^2) = 1\)</span>. We see in our simulation that our estimated value for <span class="math inline">\(E(X^2)\)</span> is pretty close to its theoretical value.</p>
</div>
<div id="monte-carlo-methods-for-probabilities" class="section level3" number="6.4.2">
<h3>
<span class="header-section-number">6.4.2</span> Monte Carlo Methods for Probabilities<a class="anchor" aria-label="anchor" href="#monte-carlo-methods-for-probabilities"><i class="fas fa-link"></i></a>
</h3>
<p>Suppose we want to find the probability that a random variable satisfies some event <span class="math inline">\(E\)</span>, <span class="math inline">\(P(E)\)</span>. We could perform a summation or integral to find this probability, or estimate the probability using Monte Carlo methods. What we will do is simulate <span class="math inline">\(X_1, \cdots, X_M\)</span> from <span class="math inline">\(X\)</span>, where <span class="math inline">\(M\)</span> is large, in other words, simulate a large number of replicates of <span class="math inline">\(X\)</span>. We then count how many of the <span class="math inline">\(X_i\)</span>s correspond to event <span class="math inline">\(E\)</span> happening, and then divide this number by <span class="math inline">\(M\)</span>, the number of replicates. We use an example to illustrate this idea.</p>
<p>Let <span class="math inline">\(X\)</span> be a standard normal distribution. Suppose we want to find the probability <span class="math inline">\(P(X^2 &gt; 1)\)</span>. We carry out a Monte Carlo simulation by doing these steps:</p>
<ul>
<li>Simulate <span class="math inline">\(M\)</span> random values from a standard normal, where <span class="math inline">\(M\)</span> is large.</li>
<li>Calculate <span class="math inline">\(X_1^2, \cdots, X_M^2\)</span>.</li>
<li>Count the number of times <span class="math inline">\(X_i^2\)</span> is greater than 1.</li>
<li>Divide this number by <span class="math inline">\(M\)</span> to estimate the probability, since probability can be interpreted as a long-run proportion.</li>
</ul>
<div class="sourceCode" id="cb107"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="fu"><a href="https://rdrr.io/r/base/Random.html">set.seed</a></span><span class="op">(</span><span class="fl">5</span><span class="op">)</span></span>
<span></span>
<span><span class="va">reps</span><span class="op">&lt;-</span><span class="fl">10000</span> <span class="co">## this is M</span></span>
<span></span>
<span><span class="va">Xs</span><span class="op">&lt;-</span><span class="fu"><a href="https://rdrr.io/r/stats/Normal.html">rnorm</a></span><span class="op">(</span><span class="va">reps</span><span class="op">)</span> <span class="co">##generate M values of X</span></span>
<span></span>
<span><span class="va">squared.values</span><span class="op">&lt;-</span><span class="va">Xs</span><span class="op">^</span><span class="fl">2</span> <span class="co">##square each X</span></span>
<span></span>
<span><span class="fu"><a href="https://rdrr.io/r/base/sum.html">sum</a></span><span class="op">(</span><span class="va">squared.values</span><span class="op">&gt;</span><span class="fl">1</span><span class="op">)</span><span class="op">/</span><span class="va">reps</span> <span class="co">##count the number of times X^2 is greater than 1, and divide by M</span></span></code></pre></div>
<pre><code>## [1] 0.3178</code></pre>
<div class="sourceCode" id="cb109"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="co">##when reps is large, this proportion should be close to</span></span>
<span><span class="fl">1</span><span class="op">-</span><span class="fu"><a href="https://rdrr.io/r/stats/Chisquare.html">pchisq</a></span><span class="op">(</span><span class="fl">1</span>, df<span class="op">=</span><span class="fl">1</span><span class="op">)</span></span></code></pre></div>
<pre><code>## [1] 0.3173105</code></pre>
<div class="sourceCode" id="cb111"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="co">##it turns out that squaring a standard normal gives a chi-squared distribution with 1 df.</span></span></code></pre></div>
<p>We see the estimated probability <span class="math inline">\(P(X^2 &gt; 1)\)</span> is close to its theoretical probability.</p>
</div>
<div id="monte-carlo-methods-for-other-purposes" class="section level3" number="6.4.3">
<h3>
<span class="header-section-number">6.4.3</span> Monte Carlo Methods for Other Purposes<a class="anchor" aria-label="anchor" href="#monte-carlo-methods-for-other-purposes"><i class="fas fa-link"></i></a>
</h3>
<p>Monte Carlo methods are not exclusively used estimating expected values and probabilities. They are versatile and can be used for a number of purposes, as long we need repeated random sampling.</p>
<p>A fun example that is pretty famous uses Monte Carlo simulations to estimate the value of <span class="math inline">\(\pi\)</span>. We can consider the following hypothetical dart throwing experiment to do so, based on the figure below:</p>
<div class="figure">
<span style="display:block;" id="fig:circle"></span>
<img src="images/06-circle.jpg" alt="Board for Dart Throwing Experiment"><p class="caption">
Figure 6.6: Board for Dart Throwing Experiment
</p>
</div>
<p>The experiment works in this way:</p>
<ul>
<li>The dart will always land in the square, and has an equal probability of landing on any spot on the square.
<ul>
<li>We can let <span class="math inline">\(X \sim U(-1,1)\)</span> to represent the position of the dart on the x-axis of the circle in Figure <a href="inequalities-limit-theorems-and-simulations.html#fig:circle">6.6</a>.</li>
<li>We can let <span class="math inline">\(Y \sim U(-1,1)\)</span> to represent the position of the dart on the y-axis of the circle in Figure <a href="inequalities-limit-theorems-and-simulations.html#fig:circle">6.6</a>.</li>
</ul>
</li>
<li>We will throw a large number of darts. For each dart, we will see if it lands in the circle or not.
<ul>
<li>To assess if a dart lies in the circle, we assess whether <span class="math inline">\(x_i^2 + y_i^2 \leq 1\)</span> for dart <span class="math inline">\(i\)</span>. If this condition is met, we know dart <span class="math inline">\(i\)</span> lies in the circle, if not, it lies outside the circle.</li>
</ul>
</li>
<li>It stands to reason that <span class="math inline">\(\frac{\text{Area of circle}}{\text{Are of square}} = \frac{\pi}{4} \approx \frac{\text{Number of darts landing in circle}}{\text{Number of darts thrown}}\)</span>.</li>
<li>Therefore, after throwing a large number of darts, <span class="math inline">\(\pi \approx 4 \times \frac{\text{Number of darts landing in circle}}{\text{Number of darts thrown}}.\)</span>
</li>
</ul>
<p>The code below carries out this experiment with 10 thousand reps (or 10 thousand dart throws):</p>
<div class="sourceCode" id="cb112"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">reps</span><span class="op">&lt;-</span><span class="fl">10000</span> <span class="co">##number of dart throws</span></span>
<span></span>
<span><span class="va">count</span><span class="op">&lt;-</span><span class="fl">0</span> <span class="co">##counter that keeps track of number of throws inside circle</span></span>
<span></span>
<span><span class="fu"><a href="https://rdrr.io/r/base/Random.html">set.seed</a></span><span class="op">(</span><span class="fl">222</span><span class="op">)</span></span>
<span></span>
<span><span class="kw">for</span> <span class="op">(</span><span class="va">i</span> <span class="kw">in</span> <span class="fl">1</span><span class="op">:</span><span class="va">reps</span><span class="op">)</span> <span class="op">{</span></span>
<span></span>
<span><span class="va">x</span><span class="op">&lt;-</span><span class="fu"><a href="https://rdrr.io/r/stats/Uniform.html">runif</a></span><span class="op">(</span><span class="fl">1</span>,min<span class="op">=</span><span class="op">-</span><span class="fl">1</span>, max<span class="op">=</span><span class="fl">1</span><span class="op">)</span> <span class="co">##simulate landing spot on x axis</span></span>
<span><span class="va">y</span><span class="op">&lt;-</span><span class="fu"><a href="https://rdrr.io/r/stats/Uniform.html">runif</a></span><span class="op">(</span><span class="fl">1</span>,min<span class="op">=</span><span class="op">-</span><span class="fl">1</span>, max<span class="op">=</span><span class="fl">1</span><span class="op">)</span> <span class="co">##simulate landing spot on y axis</span></span>
<span></span>
<span>  <span class="kw">if</span> <span class="op">(</span><span class="va">x</span><span class="op">^</span><span class="fl">2</span> <span class="op">+</span> <span class="va">y</span><span class="op">^</span><span class="fl">2</span> <span class="op">&lt;=</span> <span class="fl">1</span><span class="op">)</span><span class="op">{</span></span>
<span>    <span class="va">count</span> <span class="op">&lt;-</span> <span class="va">count</span><span class="op">+</span><span class="fl">1</span> <span class="co">##counter adds 1 if dart lands in circle</span></span>
<span>  <span class="op">}</span></span>
<span></span>
<span><span class="op">}</span></span>
<span></span>
<span><span class="co">##estimate pi. should be close to real value of pi. Gets closer if we throw more darts</span></span>
<span><span class="va">count</span><span class="op">/</span><span class="va">reps</span> <span class="op">*</span> <span class="fl">4</span> </span></code></pre></div>
<pre><code>## [1] 3.1436</code></pre>
<p>We see the estimated value for <span class="math inline">\(\pi\)</span> using this Monte Carlo simulation is close to its true value.</p>
</div>
<div id="considerations-with-monte-carlo-methods" class="section level3" number="6.4.4">
<h3>
<span class="header-section-number">6.4.4</span> Considerations with Monte Carlo Methods<a class="anchor" aria-label="anchor" href="#considerations-with-monte-carlo-methods"><i class="fas fa-link"></i></a>
</h3>
<p>In the examples above, we compared estimated values using Monte Carlo methods with their real values, so we could see the methods work. However, if we do not know the real values, two questions will come to mind:</p>
<ol style="list-style-type: decimal">
<li><p>How many replicates do we need? We only know the estimated values converge to the true values as we increase the number of replicates. Using more replicates will make the simulation run longer on your computer.</p></li>
<li><p>Related to the previous question, how close is close enough? How do you know your estimated value from the simulation is close enough to the truth? There is no way of knowing if the true value is unknown.</p></li>
</ol>
<div id="set.seed-in-r" class="section level4" number="6.4.4.1">
<h4>
<span class="header-section-number">6.4.4.1</span> set.seed() in R<a class="anchor" aria-label="anchor" href="#set.seed-in-r"><i class="fas fa-link"></i></a>
</h4>
<p>You may have noticed that in the provided simulations, we use a function <code><a href="https://rdrr.io/r/base/Random.html">set.seed()</a></code> and input a number. This is to enable others to replicate the exact same results, if someone wants to verify the code.</p>
<p>With Monte Carlo simulations, we are generating numbers randomly. When we set the seed with <code><a href="https://rdrr.io/r/base/Random.html">set.seed()</a></code> with a certain number, we ensure the same random numbers are generated each time the code is run.</p>
<p>We will not go into the details of how R generates the random numbers, and random number generation is a whole field in itself.</p>
<p>In terms of running the examples, you can choose to copy the code and exclude the line with <code><a href="https://rdrr.io/r/base/Random.html">set.seed()</a></code>. You should still observe that the estimated values from the simulations are close to the true values.</p>

</div>
</div>
</div>
</div>
  <div class="chapter-nav">
<div class="prev"><a href="joint-distributions.html"><span class="header-section-number">5</span> Joint Distributions</a></div>
<div class="next"><a href="est.html"><span class="header-section-number">7</span> Estimation</a></div>
</div></main><div class="col-md-3 col-lg-2 d-none d-md-block sidebar sidebar-chapter">
    <nav id="toc" data-toggle="toc" aria-label="On this page"><h2>On this page</h2>
      <ul class="nav navbar-nav">
<li><a class="nav-link" href="#inequalities-limit-theorems-and-simulations"><span class="header-section-number">6</span> Inequalities, Limit Theorems, and Simulations</a></li>
<li>
<a class="nav-link" href="#introduction-2"><span class="header-section-number">6.1</span> Introduction</a><ul class="nav navbar-nav"><li><a class="nav-link" href="#module-roadmap-4"><span class="header-section-number">6.1.1</span> Module Roadmap</a></li></ul>
</li>
<li>
<a class="nav-link" href="#inequalities"><span class="header-section-number">6.2</span> Inequalities</a><ul class="nav navbar-nav">
<li><a class="nav-link" href="#cauchy-schwartz-inequality"><span class="header-section-number">6.2.1</span> Cauchy-Schwartz Inequality</a></li>
<li><a class="nav-link" href="#jensens-inequality"><span class="header-section-number">6.2.2</span> Jensen’s Inequality</a></li>
<li><a class="nav-link" href="#chebyshevs-inequality"><span class="header-section-number">6.2.3</span> Chebyshev’s Inequality</a></li>
</ul>
</li>
<li>
<a class="nav-link" href="#limits"><span class="header-section-number">6.3</span> Limit Theorems</a><ul class="nav navbar-nav">
<li><a class="nav-link" href="#law-of-large-numbers"><span class="header-section-number">6.3.1</span> Law of Large Numbers</a></li>
<li><a class="nav-link" href="#CLT"><span class="header-section-number">6.3.2</span> Central Limit Theorem</a></li>
</ul>
</li>
<li>
<a class="nav-link" href="#MCsims"><span class="header-section-number">6.4</span> Monte Carlo Simulations</a><ul class="nav navbar-nav">
<li><a class="nav-link" href="#monte-carlo-methods-for-expected-values"><span class="header-section-number">6.4.1</span> Monte Carlo Methods for Expected Values</a></li>
<li><a class="nav-link" href="#monte-carlo-methods-for-probabilities"><span class="header-section-number">6.4.2</span> Monte Carlo Methods for Probabilities</a></li>
<li><a class="nav-link" href="#monte-carlo-methods-for-other-purposes"><span class="header-section-number">6.4.3</span> Monte Carlo Methods for Other Purposes</a></li>
<li><a class="nav-link" href="#considerations-with-monte-carlo-methods"><span class="header-section-number">6.4.4</span> Considerations with Monte Carlo Methods</a></li>
</ul>
</li>
</ul>

      <div class="book-extra">
        <ul class="list-unstyled">
          
        </ul>
</div>
    </nav>
</div>

</div>
</div> <!-- .container -->

<footer class="bg-primary text-light mt-5"><div class="container"><div class="row">

  <div class="col-12 col-md-6 mt-3">
    <p>"<strong>Understanding Uncertainty Course Notes</strong>" was written by Jeffrey Woo. It was last built on 2025-07-31.</p>
  </div>

  <div class="col-12 col-md-6 mt-3">
    <p>This book was built by the <a class="text-light" href="https://bookdown.org">bookdown</a> R package.</p>
  </div>

</div></div>
</footer><!-- dynamically load mathjax for compatibility with self-contained --><script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.9/latest.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script><script type="text/x-mathjax-config">const popovers = document.querySelectorAll('a.footnote-ref[data-toggle="popover"]');
for (let popover of popovers) {
  const div = document.createElement('div');
  div.setAttribute('style', 'position: absolute; top: 0, left:0; width:0, height:0, overflow: hidden; visibility: hidden;');
  div.innerHTML = popover.getAttribute('data-content');

  var has_math = div.querySelector("span.math");
  if (has_math) {
    document.body.appendChild(div);
    MathJax.Hub.Queue(["Typeset", MathJax.Hub, div]);
    MathJax.Hub.Queue(function() {
      popover.setAttribute('data-content', div.innerHTML);
      document.body.removeChild(div);
    })
  }
}
</script>
</body>
</html>
