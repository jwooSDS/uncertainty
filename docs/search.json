[{"path":"index.html","id":"preface","chapter":"Preface","heading":"Preface","text":"examples preface based OpenIntro Statistics (Diez, Ceytinka-Rundel, Barr), Chapter 9.4 9.5, provide background information. can access book free https://www.openintro.org/book/os/main goal using data science understand data. Broadly speaking, involve building statistical model predicting, estimating response variable based one predictors. models used wide variety fields finance, medicine, public policy, sports, . look couple examples.","code":""},{"path":"index.html","id":"examples","chapter":"Preface","heading":"Examples","text":"","code":""},{"path":"index.html","id":"example-1-mario-kart-auction-prices","chapter":"Preface","heading":"Example 1: Mario Kart Auction Prices","text":"first example, look Ebay auctions video game called Mario Kart played Nintendo Wii. want predict price auction based whether game new , whether auction’s main photo stock photo, duration auction days, number Wii wheels included auction.model can use example linear regression model:Generally speaking, linear regression equation takes following form:\\[\n\\hat{y} = \\hat{\\beta}_0 + \\hat{\\beta}_1 x_1 + \\hat{\\beta}_2 x_2 + \\cdots + \\hat{\\beta}_k x_k\n\\]\\(\\hat{y}\\) denotes predicted value response variable, price action example, \\(x_1, x_2, \\cdots, x_k\\) denote values predictors. example, : \\(x_1\\) whether game new , \\(x_2\\) whether auction’s main photo stock photo, \\(x_3\\) duration auction days, \\(x_4\\) number Wii wheels included auction. \\(\\hat{\\beta}_0, \\hat{\\beta}_1, \\cdots, \\hat{\\beta}_k\\) represent estimated regression parameters. know values , can easily plug values predictors obtain predicted price auction.Fitting model R, obtain estimated regression parameters::\\[\n\\hat{y} = 43.5201 - 2.5816 x_1 - 6.7542 x_2 + 0.3788 x_3 + 9.9476 x_4\n\\]auction Mario Kart game used, uses stock photo, listed 2 days, comes 0 wheels, predicted price \\(\\hat{y} = 43.5201 - 2.5816 - 6.7542 + 0.3788 \\times 2 = 34.94\\) 35 dollars.","code":"\nlibrary(openintro)\n\nData<-mariokart\n##fit model\nresult<-lm(total_pr~cond+stock_photo+duration+wheels, data=Data)\n##get estimated regression parameters\nresult## \n## Call:\n## lm(formula = total_pr ~ cond + stock_photo + duration + wheels, \n##     data = Data)\n## \n## Coefficients:\n##    (Intercept)        condused  stock_photoyes        duration          wheels  \n##        43.5201         -2.5816         -6.7542          0.3788          9.9476"},{"path":"index.html","id":"example-2-job-application-callback-rates-eg2","chapter":"Preface","heading":"Example 2: Job Application Callback Rates {#eg2}","text":"example, look data experiment sought evaluate effect race gender job application callback rates. experiment, researchers created fake resumes job postings Boston Chicago see resumes resulted callback. fake resumes included relevant information applicant’s educational attainment, many year’s experience applicant well first last name. names fake resume meant imply applicant’s race gender. two races considered (Black White) two genders considered (Make Female) experiment.Prior experiment, researchers conducted surveys check racial gender associations names fake resumes; names passed certain threshold surveys included experiment.model can used example logistic regression modelGenerally speaking, logistic regression equation takes following form\\[\n\\log (\\frac{\\hat{\\pi}}{1-\\hat{\\pi}}) = \\hat{\\beta}_0 + \\hat{\\beta}_1 x_1 + \\hat{\\beta}_2 x_2 + \\cdots + \\hat{\\beta}_k x_k\n\\]\\(\\hat{\\pi}\\) denotes predicted probability applicant receives call back. \\(x_1, x_2, \\cdots, x_k\\) denote values predictors. example, : \\(x_1\\) city job posting located , \\(x_2\\) whether applicant college degree , \\(x_3\\) experience applicant, \\(x_4\\) associated race applicant, \\(x_5\\) associated gender applicant. Similar linear regression, \\(\\hat{\\beta}_0, \\hat{\\beta}_1, \\cdots, \\hat{\\beta}_k\\) represent estimated regression parameters. know values , can easily plug values predictors obtain predicted probability applicant characteristics receive callback.Fitting model R, obtain estimated regression parametersso \\[\n\\log (\\frac{\\hat{\\pi}}{1-\\hat{\\pi}}) = -2.63974 - 0.39206 x_1 - 0.0655 x_2 + 0.03152 x_3 + 0.44299 x_4 - 0.22814 x_5\n\\]applicant Boston, college degree, 10 years experience name associated Black male, logistic regression equation becomes \\(\\log (\\frac{\\hat{\\pi}}{1-\\hat{\\pi}}) = -2.63974 - 0.0655 + 0.03152 \\times 10 - 0.22814 = -2.61818\\). little bit algebra solve, get \\(\\hat{\\pi} = 0.06797751\\). applicant 6.8 percent chance receiving callback.","code":"\nData2<-resume\n##fit model\nresult2<-glm(received_callback~job_city + college_degree+years_experience+race+gender, family=\"binomial\", data=Data2)\n##get estimated regression parameters\nresult2## \n## Call:  glm(formula = received_callback ~ job_city + college_degree + \n##     years_experience + race + gender, family = \"binomial\", data = Data2)\n## \n## Coefficients:\n##      (Intercept)   job_cityChicago    college_degree  years_experience  \n##         -2.63974          -0.39206          -0.06550           0.03152  \n##        racewhite           genderm  \n##          0.44299          -0.22814  \n## \n## Degrees of Freedom: 4869 Total (i.e. Null);  4864 Residual\n## Null Deviance:       2727 \n## Residual Deviance: 2680  AIC: 2692"},{"path":"index.html","id":"how-were-estimated-parameters-calculated","chapter":"Preface","heading":"How were Estimated Parameters Calculated?","text":"two examples, notice used R functions, supplied names variables, R functions generated values estimated parameters? One thing learn functions actually calculate numbers. turns calculations based foundational concepts associated measures uncertainty, probability, expected values. learning concepts class.want know calculations performed? understand intuition logic behind models built. becomes lot easier work models understand logic (example, know models can used used, know steps take notice data certain characteristics, etc), instead memorizing bunch steps.presenting models data people, people may occasionally question methods models. trust model? trust numbers seem come black box?Notice used two different models, linear regression logistic regression, examples 1 2. use models? swapped type model used examples? answer actually . Knowing foundational concepts models help explain certain models can used certain situations.","code":""},{"path":"index.html","id":"the-course-understanding-uncertainty","chapter":"Preface","heading":"The Course: Understanding Uncertainty","text":"mentioned previous section, learning foundational concepts associated measures uncertainty, probability, expected values. concepts help explain intuition statistical models built.end course, apply concepts revisit linear regression model. Linear regression widely used models used data science, relatively easy understand explain. modern methods (learn future classes) decision trees neural networks can viewed extensions linear regression model.","code":""},{"path":"index.html","id":"background-needed-for-the-course","chapter":"Preface","heading":"Background Needed for the Course","text":"","code":""},{"path":"index.html","id":"mathematical-background","chapter":"Preface","heading":"Mathematical Background","text":"covering basic mathematics associated probability measures uncertainty, know measuring.knowledge basic calculus helpful (.e. idea behind differentiation relates optimizing function, idea integrating function corresponds area function), performing hand, occasional first order derivative simpler calculus class ’ve .knowledge basic calculus helpful (.e. idea behind differentiation relates optimizing function, idea integrating function corresponds area function), performing hand, occasional first order derivative simpler calculus class ’ve .comfortable matrix notation also helpful.comfortable matrix notation also helpful.also comfortable notation regarding sums products. example, know \\(\\sum_{=1}^n x_i\\) \\(\\prod_{=1}^n x_i\\) represent.also comfortable notation regarding sums products. example, know \\(\\sum_{=1}^n x_i\\) \\(\\prod_{=1}^n x_i\\) represent.","code":""},{"path":"index.html","id":"r-background","chapter":"Preface","heading":"R Background","text":"skills covered R Boot Camp programming proficiency test.Control structures, especially () loops.Data wrangling.Data visualization.Note primarily use base R functions perform data wrangling data visualization, although perform operations using tidyverse functions (dplyr ggplot2 functions part tidyverse).Practitioners data science comfortable working using base R tidyverse functions. highly discourage anyone learning one :likely work people, may need comfortable using either base R tidyverse functions.find perform tasks faster base R, faster tidyverse functions.Packages constantly added. Depending author package, package may work base R tidyverse functions. now, say packages work base R functions, may change future.code produce visualizations base R tends lot shorter ggplot2 functions. find base R provides good enough visualizations, especially needed basic visuals exploration, although `ggplot2 functions can produce visualizations complex beautiful.feel free use whatever approach, long performs needed tasks. can even use mix approaches choose .","code":""},{"path":"index.html","id":"reporting-issues-with-this-book","chapter":"Preface","heading":"Reporting issues with this book","text":"find issues (typos, formatting, etc), please report https://github.com/jwooSDS/uncertainty/issues. Please specific can, including providing specific section paragraph issue found.","code":""},{"path":"descriptive.html","id":"descriptive","chapter":"1 Descriptive Statistics","heading":"1 Descriptive Statistics","text":"module based OpenIntro Statistics (Diez, Ceytinka-Rundel, Barr), Chapter 2.1. can access book free https://www.openintro.org/book/os/ Please note cover additional topics, skip certain topics book.","code":""},{"path":"descriptive.html","id":"uncertainty-with-data","chapter":"1 Descriptive Statistics","heading":"1.1 Uncertainty with Data","text":"analyzing data, always going degree uncertainty, randomness lot phenomena observe world. event random individual outcomes event unpredictable. example, weight next baby born local hospital. Without knowing information biological parents, high degree uncertainty try predict baby’s weight. Even know detailed information biological parents (example tall), may feel confident predicting baby likely heavier average, certain prediction.end hand, event deterministic can predict individual outcomes event certainty. example, know length cube 2 inches, know sure volume \\(2^3 = 8\\) cubic inches, based rules mathematics. volume cube length 2 inches always going 8 cubic inches, volume deterministic.Thought question: think data see real life. Write . data random deterministic?explore tools help us quantify uncertainty data. module, explore fairly standard tools used describe data give us idea degree uncertainty data. describing data quantitative, usually describe following: shape distribution, average typical value, spread uncertainty.","code":""},{"path":"descriptive.html","id":"visualizing-data","chapter":"1 Descriptive Statistics","heading":"1.2 Visualizing Data","text":"Data visualization representation information form pictures. Imagine access weights newborn babies local hospital. Examining numerical value time consuming. instead, can use visualizations give us idea values weights. example, weights newborns common? proportion babies dangerously low weights (may indicate health risks)? Good data visualizations can give us information fairly quickly. Next, explore common visualizations used quantitative (numerical) variables.","code":""},{"path":"descriptive.html","id":"dot-plots","chapter":"1 Descriptive Statistics","heading":"1.2.1 Dot Plots","text":"start dot plot, basic visualization quantitative variable. use loan50 dataset openintro package. data originally consist thousands loans made Lending Club platform, randomly select 50 loans. Let us study interest rate loans 50 applicants received.simplicity, round numerical values interest rates nearest whole number:can create corresponding dot plot, per Figure 1.1:\nFigure 1.1: Dot Plot 50 Interest Rates (rounded)\nNotice 1 black dot corresponds interest rate 20 (presumably percent), one applicant rounded interest rate 20 percent. 8 black dots correspond interest rate 10 percent, 8 applicants rounded interest rate 10 percent. interest rates 10 percent much commonly occurring interest rate 20 percent. can use height, number dots, help us glean often value certain interest rate occurs. Based dotplot, interest rates 5 11 percent common, higher values less common.Note: get torn details code produce dot plot. chosen present dot plot way highlight use , without getting bogged details can produced. using dot plots class.","code":"\nlibrary(tidyverse)\nlibrary(openintro)\n\n##create object for data\nData<-loan50\n##round interest rate to whole number\nData<- Data%>%\n  mutate(r_int_rate = round(interest_rate))\n##dotplot\nggplot(Data,aes(x=r_int_rate))+\n  geom_dotplot(binwidth=1)+\n  theme(\n    axis.text.y = element_blank(),  # Remove y-axis labels\n    axis.title.y = element_blank(), # Remove y-axis title\n    axis.ticks.y = element_blank()  # Remove y-axis ticks\n  )+ \n  labs(x=\"Interest Rates (Rounded)\")"},{"path":"descriptive.html","id":"histograms","chapter":"1 Descriptive Statistics","heading":"1.2.2 Histograms","text":"turns dot plots often useful large data sets, provide general idea visualizations larger data sets work. height dots inform us frequency values occurring.visualization commonly used larger data sets histogram. Instead displaying common value variable exists, think values belonging bin values. example, can create bin contains interest rates 5 7.5 percent, another bin containing interest rates 7.5 10 percent, . things note histograms:convention, values lie exactly boundary bin belong lower bin. example, interest rate exactly 12.5 percent belong bin 10 12.5 percent, bin 12.5 15 percent.convention, values lie exactly boundary bin belong lower bin. example, interest rate exactly 12.5 percent belong bin 10 12.5 percent, bin 12.5 15 percent.bin width. example, width 2.5.bin width. example, width 2.5.create histogram (using original interest rates) , per Figure 1.2:\nFigure 1.2: Historgram 50 Interest Rates\nSimilar dot plot Figure 1.1, height histogram inform us values commonly occurring. can see histogram interest rates 5 10 percent common, much loans interest rates greater 20 percent. say certainty randomly selected loan applicant interest rate 5 10 percent interest rate greater 20 percent.","code":"\n##set up sequence to specify the bins\ns25<-seq(5,27.5,2.5)\n\nggplot(Data,aes(x=interest_rate))+\n  geom_histogram(breaks=s25,fill=\"blue\",color=\"orange\")+\n  labs(x=\"Interest Rate\", title=\"Histogram of Interest Rates\")"},{"path":"descriptive.html","id":"shapes-of-distribution","chapter":"1 Descriptive Statistics","heading":"1.2.2.1 Shapes of Distribution","text":"Histograms can also give us idea shape distribution interest rates. histogram Figure 1.2, loans less 15 percent, small number loans greater 20 percent. can say greater certainty loan interest rate less 15 percent. data tail right histogram, shape said right-skewed. variable said right-skewed, large values variable much less common small values variable; smaller values likely occur.histogram reverse characteristic, .e. data tail left instead, shape said left-skewed. implies small values variable much less common large values variable; larger values likely occur.histogram reverse characteristic, .e. data tail left instead, shape said left-skewed. implies small values variable much less common large values variable; larger values likely occur.Histograms tail similarly directions called symmetric. Large small values variable equally likely.Histograms tail similarly directions called symmetric. Large small values variable equally likely.Histograms peak middle, tail sides symmetric, also bell-shaped, normal distribution. Note: turns one assumptions linear regression response variable follow normal distribution. may seem restrictive, however, see later modules assumption particularly crucial circumstances.Histograms peak middle, tail sides symmetric, also bell-shaped, normal distribution. Note: turns one assumptions linear regression response variable follow normal distribution. may seem restrictive, however, see later modules assumption particularly crucial circumstances.Thought question: Can think real life variables symmetric, right-skewed, left-skewed distributions? Feel free search internet examples.","code":""},{"path":"descriptive.html","id":"considerations-with-histograms","chapter":"1 Descriptive Statistics","heading":"1.2.2.2 Considerations with Histograms","text":"interest rate example, may noticed made specific choice width bins created histograms. turns width bins can impact shape histogram, potentially, interpret histogram.Consider creating histogram bin width 0.5, instead 2.5, per Figure 1.3:\nFigure 1.3: Historgram 50 Interest Rates, Bin Width 0.5\nComparing Figure 1.3 Figure 1.2, note following:Visually, histogram looks jagged smaller bin width, whereas histogram looks smoother larger bin width.Visually, histogram looks jagged smaller bin width, whereas histogram looks smoother larger bin width.Smaller bin widths may preferred need information smaller ranges interest rates. However, can difficult write general trends.Smaller bin widths may preferred need information smaller ranges interest rates. However, can difficult write general trends.Larger bin widths may useful trying look general trends interest rates.Larger bin widths may useful trying look general trends interest rates.Thought question: happens create histogram bin width large?","code":"\n##set up sequence to specify the bins. width now 0.5\ns05<-seq(5,27.5,0.5)\n\nggplot(Data,aes(x=interest_rate))+\n  geom_histogram(breaks=s05,fill=\"blue\",color=\"orange\")+\n  labs(x=\"Interest Rate\", title=\"Histogram of Interest Rates\")"},{"path":"descriptive.html","id":"densplots","chapter":"1 Descriptive Statistics","heading":"1.2.3 Density Plots","text":"Another visualization quantitative variable density plot. density plot can viewed smoothed version histogram. can use heights inform us values common. create density plot interest rates Figure 1.4:\nFigure 1.4: Density Plot 50 Interest Rates\nBased Figure 1.4, see low interest rates (5 12.5 percent) much common high interest rates (higher 20 percent). things note interpreting density plots:area density plot always equal 1.find proportion interest rates two values, example 10 15 percent, integrate density plot range, .e. \\(\\int_{10}^{15} f(x) dx\\), \\(f(x)\\) mathematical equation describes density plot. learn equation detail later module.values vertical axis equal probabilities (common misconception).density plot found using method called kernel density estimation (KDE). details KDE Section 4.6.1 need cover quite bit material .","code":"\n##density plot\nplot(density(Data$interest_rate), main=\"Density Plot of Interest Rates\")"},{"path":"descriptive.html","id":"considerations-with-density-plots","chapter":"1 Descriptive Statistics","heading":"1.2.3.1 Considerations with Density Plots","text":"Similar bins histograms, density plots affected bandwidth. Larger bandwidths lead smoother density plots, smaller bandwidths lead jagged density plots. create density plot uses bandwidth twice default Figure 1.5 :\nFigure 1.5: Density Plot 50 Interest Rates Larger Bandwidth\nNotice Figure 1.5 little peak interest rates 15 20 (existed Figures 1.4 also 1.2) longer exists. Using bandwidths large can smooth peaks.Thought question: bin widths histograms bandwidths density plots related?","code":"\nplot(density(Data$interest_rate, adjust=2), main=\"Density Plot of Interest Rates, with Bandwidth Twice the Default\")"},{"path":"descriptive.html","id":"ordered-statistics","chapter":"1 Descriptive Statistics","heading":"1.3 Ordered Statistics","text":"idea behind ordered statistics pretty self-explanatory: take numerical variable, order values smallest largest. Going back example interest rates 50 loan applicants, let \\(X\\) denote interest rate. \\(x_{(1)}\\) denote interest rate smallest, \\(x_{(2)}\\) denotes second smallest interest rate, \\(x_{(50)}\\) denotes largest interest rate sample 50.","code":""},{"path":"descriptive.html","id":"quantiles","chapter":"1 Descriptive Statistics","heading":"1.3.1 Quantiles","text":"Quantiles partition range numerical data continuous intervals (groups) (nearly) equal proportions. Common quantiles names:Quartiles: 4 groupsPercentiles: 100 groupsWe go quartiles detail.","code":""},{"path":"descriptive.html","id":"quart","chapter":"1 Descriptive Statistics","heading":"1.3.1.1 Quartiles","text":"Quartiles divide data 4 groups, group (nearly) equal number observations. three quartiles, denoted \\(Q_1, Q_2, Q_3\\).first group values negative infinity \\(Q_1\\).second group values \\(Q_1\\) \\(Q_2\\).third group values \\(Q_2\\) \\(Q_3\\).fourth group values \\(Q_3\\) infinity.\\(Q_2\\), sometimes called second quartile, easiest value find. also called median data. Going back interest rates 50 loan applicants. Using ordered statistics, median middle observation. Since even number observations, two middle observations, \\(x_{(25)}\\) \\(x_{(26)}\\). situation, median average two middle observations. Using R, find median :roughly half interest rates less 9.93 percent, roughly half interest rates greater 9.93 percent. might also recognize another term median: 50th percentile, 50 percent interest rates less 9.93.find middle observation(s) based sample size \\(n\\):\\(n\\) even, 2 middle observations position \\(\\frac{n}{2}\\) \\(\\frac{n}{2} + 1\\) ordered statistics.\\(n\\) odd, middle observation position \\(\\frac{n}{2} + 0.5\\) ordered statistics.\\(Q_1\\) \\(Q_3\\) (also called first third quartiles) found together, finding \\(Q_2\\). Note \\(Q_2\\) divides data two groups. Using interest rates example, one group contains \\(x_{(1)}, \\cdots, x_{(25)}\\), another group contains \\(x_{(26)}, \\cdots, x_{(50)}\\). \\(Q_1\\) median first group, \\(Q_3\\) median second group. 50 loan applicants:\\(Q_1\\) \\(x_{(13)}\\), \\(Q_3\\) \\(x_{(38)}\\).find values R, type:\\(Q_1\\) 7.96 percent, \\(Q_3\\) 14.08 percent. turns \\(Q_1\\) also 25th percentile, \\(Q_3\\) also 75th percentile, definition.Remember wrote following earlier:first group values negative infinity \\(Q_1\\). quarter observations interest rates less 7.96 percent.second group values negative \\(Q_1\\) \\(Q_2\\). quarter observations interest rates 7.96 9.93 percent.third group values negative \\(Q_2\\) \\(Q_3\\). quarter observations interest rates 9.93 14.08 percent.fourth group values negative \\(Q_3\\) infinity. quarter observations interest rates 14.08 percent.Note: may notice used type = 1 inside quantile() function. Using type = 1 gives values first third quartiles based method just described. actually several ways find quantiles, may result slightly differing values, although generally meet definition \\(Q_1\\) 25th percentile, \\(Q_3\\) 75th percentile.","code":"\nmedian(Data$interest_rate)## [1] 9.93\nquantile(Data$interest_rate, prob=c(0.25,0.75), type = 1)##   25%   75% \n##  7.96 14.08"},{"path":"descriptive.html","id":"percentiles","chapter":"1 Descriptive Statistics","heading":"1.3.1.2 Percentiles","text":"Another common quantile percentile. general k-th percentile value data point \\(k\\) percent observations found. earlier example, said \\(Q_3\\) interest rates 14.08 percent, also 75th percentile. 75 percent interest rates less 14.08 percent.","code":""},{"path":"descriptive.html","id":"box-plots","chapter":"1 Descriptive Statistics","heading":"1.3.2 Box Plots","text":"Another visualization used summarize quantitative data box plot. box plot summarizes 5-number summary. 5 numbers minimum, \\(Q_1, Q_2, Q_3\\), maximum. Using interest rate data, box plot shown Figure 1.6:\nFigure 1.6: Box Plot Interest Rates\npeople call box plot box whisker plot.boundaries box represent \\(Q_1\\) \\(Q_3\\).thick line box represents median.two whiskers either side box extend minimum maximum, outliers exist. outliers exist, whiskers extend minimum maximum values outliers.Generally, one quantitative variable, outlier observation whose numerical value far away rest data. words, lot smaller larger relative rest data.50 loans, two loan applicants interest rates around 25 percent flagged lot larger rest loans, reasonable since loans lot smaller 20 percent.go details outliers determined box plots. interested, can read Chapter 2.1.5 OpenIntro Statistics (Diez, Ceytinka-Rundel, Barr). Generally, working one variable, outliers observations lot larger smaller rest observations.Notice much large values (\\(Q_3\\) maximum) median, compared distance small values (\\(Q_1\\) minimum) median. indicates distribution interest rates right-skewed. Compare boxplot interest rates Figure 1.6 corresponding histogram (Figure 1.2) density plot (Figure 1.4).Thought question: can sketch box plot represents variable left-skewed? variable symmetric?","code":"\n##box plot\nggplot(Data,aes(y=interest_rate))+\n  geom_boxplot()+\n  labs(y=\"Interest Rate\", title=\"Box Plot of Interest Rates\")"},{"path":"descriptive.html","id":"ecdf","chapter":"1 Descriptive Statistics","heading":"1.3.3 Empirical Cumulative Distribution Function","text":"previous sections, can see use histograms, density plots, box plots inform us proportion observations take certain values, values data correspond certain percentiles. However, limited quartiles percentile using box plots, need find areas density plot (using integration, trivial task), add frequencies histogram (can time consuming).plot can easily give us values variable correspond percentiles empirical cumulative distribution function (ECDF) plot.Let \\(X\\) denote random variable, observed \\(n\\) observations \\(X\\) denoted \\(x_1, \\cdots, x_n\\). Let \\(x_{(1)}, \\cdots x_{(n)}\\) denote ordered statistics \\(n\\) observations. ECDF, denoted \\(\\hat{F}_n(x)\\) proportion sample observations less equal value \\(x\\) random variable. Mathematically, ECDF :\\[\n\\hat{F}_n(x) =\n  \\begin{cases}\n   0, & \\text{} x < x_{(1)} \\\\\n   \\frac{k}{n},       & \\text{} x_{(k)} \\leq x < x_{(k+1)}, k = 1, \\cdots, n-1\\\\\n   1, & \\text{} x \\geq x_{(n)}.\n  \\end{cases}\n\\]\nshall use simple toy example illustrate ECDF constructed. Suppose ask 5 people many times go gym (least 20 minutes) typical work week. answers : 3, 0, 1, 5, 3. random variable \\(X\\) many times person goes gym least 20 minutes, ordered statistics \\(x_{(1)} = 0, x_{(2)} = 1, x_{(3)} = 3, x_{(4)} = 3, x_{(5)} = 5\\). Using mathematical definition ECDF, :\\(\\hat{F}_n(x) = 0\\) \\(x < x_{(1)} = 0\\).\\(\\hat{F}_n(x) = \\frac{1}{5}\\) \\(0 \\leq x < x_{(2)} = 1\\).\\(\\hat{F}_n(x) = \\frac{2}{5}\\) \\(1 \\leq x < x_{(3)} = 3\\).\\(\\hat{F}_n(x) = \\frac{4}{5}\\) \\(3 \\leq x < x_{(5)} = 5\\). value special example since two observations \\(x=3\\).\\(\\hat{F}_n(x) = 1\\) \\(x \\geq 5\\).corresponding ECDF plot shown Figure 1.7:\nFigure 1.7: ECDF Plot Toy Example\ncan easily find percentiles plot, example, 40th percentile equal 1, going gym week. 20 percent observations go gym less 1 time week. video explains construction ECDF:Next, create ECDF plot interest rates 50 loan applicants.\nFigure 1.8: ECDF Plot Interest Rates\noverlaid horizontal line 80th percentile, can read horizontal axis corresponds interest rate 17 percent. 80 percent loan applicants interest rate less 17 percent.Thought question: try using histogram density plot interest rates (Figures 1.2 1.4) find interest rate corresponds 80th percentile. easy perform?","code":"\n##toy data\ny<-c(3, 0, 1, 5, 3)\n##ECDF plot\nplot(ecdf(y), main = \"ECDF for Toy Example\")\nplot(ecdf(Data$interest_rate), main = \"ECDF Plot of Interest Rates\")\nabline(h=0.8)"},{"path":"descriptive.html","id":"measures-of-centrality","chapter":"1 Descriptive Statistics","heading":"1.4 Measures of Centrality","text":"far, used visualizations summarize shape distribution quantitative variable. Next, look common measures centrality. Loosely speaking, measures centrality measures describe average typical value quantitative variable. common measures centrality mean, median, mode.","code":""},{"path":"descriptive.html","id":"mean","chapter":"1 Descriptive Statistics","heading":"1.4.1 Mean","text":"sample mean simply average value variable sample. sample mean random variable \\(X\\) denoted \\(\\bar{x}\\), found :\\[\\begin{equation}\n\\bar{x} = \\frac{\\sum_{=1}^n x_i}{n}.\n\\tag{1.1}\n\\end{equation}\\], toy example 5 people often go gym week, sample mean \\(\\bar{x} = \\frac{3+0+1+5+3}{5} = 2.4\\).","code":""},{"path":"descriptive.html","id":"median","chapter":"1 Descriptive Statistics","heading":"1.4.2 Median","text":"went find median section 1.3.1.1. median value middle observation ordered statistics. also called \\(Q_2\\), second quartile, 50th percentile, approximately 50 percent observations values smaller median., toy example 5 people often go gym week, sample median \\(x_{(3)} = 3\\). 50 percent people went gym less 3 times week.","code":""},{"path":"descriptive.html","id":"mode","chapter":"1 Descriptive Statistics","heading":"1.4.3 Mode","text":"Another measure mode. Mathematically speaking, mode commonly occurring value data. toy example, mode 3, since 3 occurs twice occurs often data.","code":""},{"path":"descriptive.html","id":"considerations","chapter":"1 Descriptive Statistics","heading":"1.4.4 Considerations","text":"things consider using measures centrality:mean measure people comfortable , however, caution needs used variable skewed, extreme outliers drastically alter value mean. Using toy example gym, suppose person visits gym visits 50 times, instead 5. numerical value sample mean explodes, give good representation central value many visits gym person makes week. mean fine variable symmetric.mean measure people comfortable , however, caution needs used variable skewed, extreme outliers drastically alter value mean. Using toy example gym, suppose person visits gym visits 50 times, instead 5. numerical value sample mean explodes, give good representation central value many visits gym person makes week. mean fine variable symmetric.median measure recommended skewed distributions, since order associated ordered statistics influenced extreme outliers. Using gym example, previous bullet point, median unaffected.median measure recommended skewed distributions, since order associated ordered statistics influenced extreme outliers. Using gym example, previous bullet point, median unaffected.mean larger median indication distribution right-skewed. Using interest rate example, :mean larger median indication distribution right-skewed. Using interest rate example, :consistent right skew saw histogram density plot Figures 1.2 1.4. Conversely, left-skewed distribution usually mean smaller median. symmetric distribution typically similar values mean median.mean considered sensitive measure, since numerical value can drastically affected outliers. median considered robust measure, since numerical value resistant less affected outliers.mean considered sensitive measure, since numerical value can drastically affected outliers. median considered robust measure, since numerical value resistant less affected outliers.mathematical definition mode can difficult use variables continuous, since likely observations value variable continuous. instance, mode typically refers bin histogram tallest. , using histogram Figure 1.2 interest rates, mode 7.5 10 percent.mathematical definition mode can difficult use variables continuous, since likely observations value variable continuous. instance, mode typically refers bin histogram tallest. , using histogram Figure 1.2 interest rates, mode 7.5 10 percent.","code":"\nmean(Data$interest_rate)## [1] 11.5672\nmedian(Data$interest_rate)## [1] 9.93"},{"path":"descriptive.html","id":"measures-of-spread","chapter":"1 Descriptive Statistics","heading":"1.5 Measures of Spread","text":"previous sections, learned summarizing features quantitative variable, using visualizations summarize shape, using measures centrality describe average typical values variable. One feature can summarize spread, associated values quantitative variable. Measures spread considered way measure uncertainty. Data larger spread uncertainty.","code":""},{"path":"descriptive.html","id":"variance-and-standard-deviation","chapter":"1 Descriptive Statistics","heading":"1.5.1 Variance and Standard Deviation","text":"One measure spread variance. sample variance random variable \\(X\\) denoted \\(s^2\\), sometimes \\(s_x^2\\), found :\\[\\begin{equation}\ns^2 = \\frac{\\sum_{=1}^n (x_i-\\bar{x})^2}{n-1}.\n\\tag{1.2}\n\\end{equation}\\]variance can interpreted approximate average squared distance observations mean. formula equation (1.2) may look bit complicated, let us use toy example asked 5 people often go gym workweek. answers : 3, 0, 1, 5, 3, earlier found sample mean \\(\\bar{x} = 2.4\\). calculate sample variance:\\[\n\\begin{split}\ns^2 &= \\frac{\\sum_{=1}^n (x_i-\\bar{x})^2}{n-1}\\\\\n&= \\frac{(3-2.4)^2 + (0-2.4)^2 + (1-2.4)^2 + (5-2.4)^2 + (3-2.4)^2}{5-1} \\\\\n&= 3.8\n\\end{split}\n\\]Notice numerator equation (1.2): take difference observed value sample mean, square differences, add squared differences. divide \\(n-1\\), rather \\(n\\), hence sample variance approximate averaged squared distance observations mean. nuance mathematics divide \\(n-1\\) instead \\(n\\), may intuitive . turns dividing \\(n-1\\) makes sample variance unbiased estimator true variance population (denoted \\(\\sigma^2\\)) reliable divided \\(n\\). go detail later module covering additional concepts.video explains calculation sample variance:Larger values sample variance indicate observations generally away sample mean, indicating larger spread, higher degree uncertainty future values.Thought question: mean sample variance set observations 0? indicate little () uncertainty set observations?Another related measure sample standard deviation, square root sample variance. Similar variance, larger values indicated spread data.","code":""},{"path":"descriptive.html","id":"interquartile-range","chapter":"1 Descriptive Statistics","heading":"1.5.2 Interquartile Range","text":"Another measure spread interquartile range (IQR), difference third first quartiles,\\[\\begin{equation}\nIQR = Q_3 - Q_1.\n\\tag{1.3}\n\\end{equation}\\]IQR considered robust measure spread, sample variance standard deviations considered sensitive.","code":""},{"path":"probability.html","id":"probability","chapter":"2 Probability","heading":"2 Probability","text":"module based Introduction Probability (Blitzstein, Hwang), Chapters 1 2. can access book free https://stat110.hsites.harvard.edu/ (click Book). Please note cover additional topics, skip certain topics book. may skip: Sections 1.4, 1.5, Theorem 1.6.3, Examples 1.6.4, 2.4.5, 2.5.12, 2.7.3 book.","code":""},{"path":"probability.html","id":"introduction-to-probability","chapter":"2 Probability","heading":"2.1 Introduction to Probability","text":"way quantifying uncertainty probability. Think statements: “100% certain rain next hour” “50% certain rain next hour”. percentages used reflect degree certainty event happening. first statement reflects certainty; second reflects uncertainty statement implies belief equally likely rain . module, learn basic concepts probability.","code":""},{"path":"probability.html","id":"why-study-probability","chapter":"2 Probability","heading":"2.1.1 Why Study Probability?","text":"book (Section 1.1) lists 10 different applications probability, many applications. go far say anything deals data also deal probability.","code":""},{"path":"probability.html","id":"frequentiest-and-bayesian-view-of-probability","chapter":"2 Probability","heading":"2.1.2 Frequentiest and Bayesian View of Probability","text":"couple viewpoints interpret probability: frequentist Bayesian. Consider statement “flip fair coin, coin 50% chance landing heads”.frequentist viewpoint views probability relative frequency associated event repeated infinite number times. interpret 50% probability : flip coin many many times, 50% times result coin landing heads.frequentist viewpoint views probability relative frequency associated event repeated infinite number times. interpret 50% probability : flip coin many many times, 50% times result coin landing heads.Bayesian viewpoint views probability measure belief, certainty, event happen. interpret 50% probability : heads tails equally likely occur coin flip.Bayesian viewpoint views probability measure belief, certainty, event happen. interpret 50% probability : heads tails equally likely occur coin flip.coin flip example, interpretations reasonable. However, instances, frequentist interpretation may interpretable repeat event many times. example, earlier statement rain: “50% certain rain next hour”. Whether rain next hour repeatable event, frequentist interpretation makes less sense .","code":""},{"path":"probability.html","id":"module-roadmap","chapter":"2 Probability","heading":"2.1.3 Module Roadmap","text":"Section 2.2 introduces key terms, concepts, rules probability.Section 2.3 introduces idea conditional probability. frequentist framework, conditional probabilities probabilities certain “conditions” fulfilled. Bayesian frameowrk, conditional probabilities represent measure belief updated seeing data.Section 2.4 covers common misconceptions comes interpreting conditional probabilities real life (much misconceptions names wikipedia pages).","code":""},{"path":"probability.html","id":"probconcepts","chapter":"2 Probability","heading":"2.2 Key Concepts in Probability","text":"section, cover basic terminology foundational ideas probability.","code":""},{"path":"probability.html","id":"sample-space","chapter":"2 Probability","heading":"2.2.1 Sample Space","text":"sample space experiment, denoted \\(S\\), set possible outcomes experiment.rest module, use following example: consider standard deck 52 cards, draw one card random. card drawn? sample space experiment can viewed list 52 cards, per Figure 2.1 .\nFigure 2.1: Sample Space Drawing One Card Standard Deck. Picture https://en.wikipedia.org/wiki/Standard_52-card_deck\ndefinition sample space may appear elementary, writing sample space almost always first step performing probability calculations.","code":""},{"path":"probability.html","id":"event","chapter":"2 Probability","heading":"2.2.2 Event","text":"event subset sample space, usually denoted upper case letter. example, let \\(\\) denote event draw card black suit (spades clubs), let \\(B\\) denote event draw picture card (Jack, Queen, King). Events \\(\\) \\(B\\) shown Figures Figure 2.2 Figure 2.3 .\nFigure 2.2: Event \\(\\) (Blue)\n\nFigure 2.3: Event \\(B\\) (gold)\nsample space experiment can finite infinite. card example, sample space finite since can actually write possible outcomes. number possible outcomes infinite (.e. write entire list possible outcomes), sample space infinite.assign probability event. probability event \\(\\) happening \\(P()\\). outcome sample space equally likely finite sample space, probability event number outcomes belonging event divided number outcomes sample space. Using card example, \\(P() = \\frac{26}{52} = \\frac{1}{2}\\) \\(P(B) = \\frac{12}{52} = \\frac{3}{13}\\).","code":""},{"path":"probability.html","id":"complements","chapter":"2 Probability","heading":"2.2.3 Complements","text":"complement event set outcomes belong event. example, complement \\(\\), denoted \\(^c\\), drawing card red suit (hearts diamonds). One way think complements complement event event happening. Looking Figure 2.2, cards outlined blue. example, \\(P(^c) = \\frac{26}{52} = \\frac{1}{2}\\).Thought question: probability drawing non picture card?examples, might realize probability associated complement event can found subtracting probability event 1, .e.\\[\\begin{equation}\nP(^c) = 1 - P().\n\\tag{2.1}\n\\end{equation}\\]Sometimes, calculation probability complement event much less tedious probability event. instance, equation (2.1) useful.","code":""},{"path":"probability.html","id":"unions","chapter":"2 Probability","heading":"2.2.4 Unions","text":"union events least one events happen. example, union events \\(\\) \\(B\\), denoted \\(\\cup B\\), event card drawn either black suit, picture card, black suit picture card. reflected Figure 2.4.\nFigure 2.4: Union , B (blue gold, blue gold)\nfind \\(P(\\cup B)\\), can refer Figure 2.4 just count number outcomes belong either event \\(\\) (black suit) event \\(B\\) (picture card), find \\(\\frac{32}{52}\\).union \\(\\) \\(B\\) can viewed event either event \\(\\) \\(B\\) () happens.","code":""},{"path":"probability.html","id":"intersections","chapter":"2 Probability","heading":"2.2.5 Intersections","text":"intersection events events happen. Using example, intersection events \\(\\) \\(B\\) denoted \\(\\cap B\\), event card drawn black suit picture card. Using Figure 2.4, outcomes belonging \\(\\cap B\\) cards outlined blue gold. probability \\(P(\\cap B) = \\frac{6}{52}\\).","code":""},{"path":"probability.html","id":"addition-rule","chapter":"2 Probability","heading":"2.2.6 Addition Rule","text":"common mistake can made calculating \\(P(\\cup B)\\) just add probabilities individual event, mistake say probability \\(\\frac{26}{52} + \\frac{12}{52} = \\frac{38}{52}\\). problem approach outcomes belong events (black picture cards) get counted twice, want count . leads following formula calculating probabilities involving unions two events, sometimes called addition rule probability:\\[\\begin{equation}\nP(\\cup B) = P() + P(B) - P(\\cap B).\n\\tag{2.2}\n\\end{equation}\\]Using equation (2.2), \\(P(\\cup B) = \\frac{26}{52} + \\frac{12}{52} - \\frac{6}{32} = \\frac{32}{52}\\).video explains addition rule example bit detail:","code":""},{"path":"probability.html","id":"disjoint-or-mutually-exclusive-events","chapter":"2 Probability","heading":"2.2.7 Disjoint or Mutually Exclusive Events","text":"previous discussion leads idea disjoint, mutually exclusive events. Events disjoint happen simultaneously. card example, events \\(\\) \\(B\\) disjoint, since \\(\\) \\(B\\) can happen simultaneously, since card drawn can black picture card, e.g. draw king spades.Using Figure 2.4 visual example, can see events \\(\\) \\(B\\) disjoint since outcomes blue overlap outcomes gold.Suppose define another event, \\(C\\), denote card drawn Ace. events \\(B\\) \\(C\\) disjoint since card drawn picture card ace. definition disjoint events leads following: events disjoint, probability intersection 0.Using Figure 2.5 visual example, can see events \\(B\\) \\(C\\) disjoint since outcomes gold pink overlap.\nFigure 2.5: Events B, C (gold pink respectively)\nApplying idea equation (2.2), following disjoint events: disjoint events, probability least one event happening sum probabilities event.","code":""},{"path":"probability.html","id":"axioms-of-probability","chapter":"2 Probability","heading":"2.2.8 Axioms of Probability","text":"following called axioms probability, considered foundation properties associated probability:probability event, \\(E\\), non negative, .e. \\(P(E) \\geq 0\\).probability least one outcome sample space occurs 1, .e.\\(P(S) = 1\\).\\(A_1, A_2, \\cdots\\) disjoint events, \\[\nP(\\bigcup\\limits_{=1}^{\\infty} A_{}) = \\sum_{=1}^{\\infty} P(A_i).\n\\]\nwords, disjoint events, probability least one event happens sum individual probabilities.Note: writers list three axioms. book combines first two axioms 1, write two axioms.can easily see equations (2.1) (2.2) can derived axioms. Note equations axioms apply circumstances, regardless whether sample space finite .","code":""},{"path":"probability.html","id":"condprob","chapter":"2 Probability","heading":"2.3 Conditional Probability","text":"concept conditional probability appears almost statistical data science models. statistical models logistic regression, trying use observable data (called predictors, input variables, etc) model probabilities associated different values outcome random (called response variable, output variable, etc). observable data predictive outcome, probabilities associated outcome indicate greater certainty, observable data. Conditional probabilities allows us incorporate observable data, evidence, evaluating uncertainty random outcomes.Consider headed lunch, need decide want bring umbrella (assuming bring umbrella think going rain). working windowless basement internet, high degree uncertainty evaluating rain . However, look outside observe current weather conditions heading , likely higher degree certainty evaluating rain . Conditional probabilities allow us incorporate see prediction random event.use language probability denote example, let \\(R\\) denote event rain go lunch. working windowless basement internet, calculating \\(P(R)\\), probability rain go lunch. able incorporate current weather conditions, probability denoted \\(P(R|data)\\), data denotes current observe weather conditions. \\(P(R|data)\\) can read probability rain go lunch, given observed weather. example, can see \\(P(R)\\) \\(P(R|data)\\) different, since update probability given useful information. Notice \\(|\\) symbol inside probability. symbol implies working conditional probability, given observed information listed \\(|\\).","code":""},{"path":"probability.html","id":"def","chapter":"2 Probability","heading":"2.3.1 Definition","text":"\\(X\\) \\(Y\\) events, \\(P(X)>0\\), conditional probability \\(Y\\) given \\(X\\), denoted \\(P(Y|X)\\), \\[\\begin{equation}\nP(Y|X) = \\frac{P(Y \\cap X)}{P(X)}.\n\\tag{2.3}\n\\end{equation}\\]definition, want update probability \\(Y\\) happening, given observed \\(X\\). \\(X\\) can viewed observable data evidence want incorporate.Bayesian viewpoint probability, \\(P(Y)\\) called prior probability \\(Y\\) since reflects belief \\(Y\\) observing data. \\(P(Y|X)\\) called posterior probability \\(Y\\), reflects update belief \\(Y\\) incorporating observed data.Let us go back standard deck cards example. Let us find \\(P(B|)\\), probability card picture card, given know card black suit. Visually, can use definition conditional probability using Figure 2.6 .\nFigure 2.6: Events , given B\ntold card black suit, 26 possible outcomes consider, red cards eliminated crossed Figure 2.6. 26 outcomes, many picture cards? probability \\(P(B|)\\) \\(\\frac{6}{26}\\). Figure 2.6 represents frequentist viewpoint conditional probability: \\(P(B|)\\) represents long run proportion picture cards among cards black suits.can also apply equation (2.3): \\(P(B|) = \\frac{\\frac{6}{52}}{\\frac{1}{2}} = \\frac{6}{26}\\) gives answer.video explains conditional probability example bit detail:Thought question: work probability card drawn black suit, given know card picture card.can see example general \\(P(Y|X) \\neq P(X|Y)\\). informs us need extremely careful writing conditional probabilities interpreting , knowing one matters analysis. example, probability feel unwell given flu close 1, probability flu given feel unwell close 1 (since many things can make feel unwell). confusion regarding conditional probabilities sometimes called confusion inverse prosecutor’s fallacy. fallacy wrongly assumes probability fingerprint match given person innocent small, means probability person innocent given fingerprint match must also small. going fallacy detail, need cover concepts.","code":""},{"path":"probability.html","id":"multiplication-rule","chapter":"2 Probability","heading":"2.3.2 Multiplication Rule","text":"equation (2.3), multiplication rule probability:\\[\\begin{equation}\nP(Y \\cap X) = P(Y|X) \\times P(X) = P(X|Y) \\times P(Y).\n\\tag{2.4}\n\\end{equation}\\]multiplication rule useful finding probability multiple events happening, especially events happen sequentially. example, consider drawing two cards, without replacement, standard deck cards. Without replacement means drawing first card, returned deck, 51 cards remaining first draw. Let \\(D_1\\) \\(D_2\\) denote events first draw diamond suit second draw diamond suit respectively. want find probability cards drawn diamond suits. probability can written \\(P(D_1 \\cap D_2) = P(D_1) \\times P(D_2|D_1) = \\frac{13}{52} \\times \\frac{12}{51} = \\frac{156}{2652}\\).","code":""},{"path":"probability.html","id":"independent-events","chapter":"2 Probability","heading":"2.3.3 Independent Events","text":"Events independent knowledge whether one event happens change probability event happening. implies \\(X\\) \\(Y\\) independent events, definition conditional probability simplifies \\(P(Y|X) = P(Y)\\). Likewise \\(P(X|Y) = P(X)\\). Applying multiplication rule, following multiplication rule independent events\\[\\begin{equation}\nP(Y \\cap X) = P(Y) \\times P(X).\n\\tag{2.5}\n\\end{equation}\\]probability events happening just product probabilities individual event, events independent.Going back example standard deck cards, \\(\\) denotes event draw card black suit (spades clubs), \\(B\\) denotes event draw picture card (Jack, Queen, King). earlier found \\(P(B) = \\frac{12}{52}\\) \\(P(B|) = \\frac{6}{26}\\). Notice two probabilities numerically equal, informs us events independent. Knowing whether card black suit change probability card picture card. makes sense intuitively since proportion cars picture black red suits.","code":""},{"path":"probability.html","id":"bayes-rule","chapter":"2 Probability","heading":"2.3.4 Bayes’ Rule","text":"definition conditional probability equation (2.3) multiplication rule equation (2.4) give us Bayes’ rule\\[\\begin{equation}\nP(Y|X) = \\frac{P(X|Y)P(Y)}{P(X)}.\n\\tag{2.6}\n\\end{equation}\\]Bayes’ rule useful want find \\(P(Y|X)\\) information regarding \\(P(X|Y)\\) available. fairly popular model called linear discriminant analysis, models conditional probability using Bayes’ rule.","code":""},{"path":"probability.html","id":"odds","chapter":"2 Probability","heading":"2.3.5 Odds","text":"odds event \\(Y\\) \\[\\begin{equation}\nodds(Y) = \\frac{P(Y)}{P(Y^c)}.\n\\tag{2.7}\n\\end{equation}\\]may realize left hand side equation (2.7) equal left hand side logistic regression equation saw Section ??.Using equation (2.7), can switch odds probability easily\\[\\begin{equation}\nP(Y) = \\frac{odds(Y)}{1 + odds(Y)}.\n\\tag{2.8}\n\\end{equation}\\]","code":""},{"path":"probability.html","id":"odds-form-of-bayes-rule","chapter":"2 Probability","heading":"2.3.6 Odds Form of Bayes’ Rule","text":"Using Bayes’ rule equation (2.6) definition odds equation (2.7), odds form Bayes’ rule\\[\\begin{equation}\n\\frac{P(Y|X)}{P(Y^c|X)} = \\frac{P(X|Y)}{P(X|Y^c)} \\frac{P(Y)}{P(Y^c)}.\n\\tag{2.9}\n\\end{equation}\\]","code":""},{"path":"probability.html","id":"law-of-total-probability","chapter":"2 Probability","heading":"2.3.7 Law of Total Probability","text":"Let \\(Y_1, Y_2, \\cdots, Y_n\\) partition sample space (\\(Y_1, Y_2, \\cdots, Y_n\\) disjoint union sample space, \\(P(Y_i) > 0\\)) \\(\\). \\[\\begin{equation}\n\\begin{split}\nP(X) &= \\sum_{=1}^n P(X|Y_i) \\times P(Y_i)\\\\\n    &= P(X|Y_1) \\times P(Y_1) + P(X|Y_2) \\times P(Y_2) + \\cdots + P(X|Y_n) \\times P(Y_n).\n\\end{split}\n\\tag{2.10}\n\\end{equation}\\]law total probability informs us way find probability \\(X\\). can divide sample space disjoint sets \\(Y_i\\), find conditional probability \\(X\\) within set, take weighted sum conditional probabilities, weighted \\(P(Y_i)\\). useful conditional probability set easy obtain.law total probability equation (2.10) can applied denominator Bayes’ rule equation (2.6) following variation Bayes’ rule:\\[\\begin{equation}\nP(Y|X) = \\frac{P(X|Y)P(Y)}{\\sum_{=1}^n P(X|Y_i) \\times P(Y_i)}.\n\\tag{2.11}\n\\end{equation}\\]","code":""},{"path":"probability.html","id":"worked-example","chapter":"2 Probability","heading":"2.3.8 Worked Example","text":"consider worked example apply Bayes’ rule law total probability. Suppose email can divided three categories: \\(E_1\\) denotes spam email, \\(E_2\\) denotes important email, \\(E_3\\) denotes important email. email must belong one categories. Let \\(F\\) denote event email contains word “free”. past data, following information:\\(P(E_1) = 0.2, P(E_2) = 0.5, P(E_3) = 0.3\\).word “free” appears 99% spam email, \\(P(F|E_1) = 0.99\\).word “free” appears 10% important email, \\(P(F|E_2) = 0.1\\).word “free” appears 5% important email, \\(P(F|E_3) = 0.05\\).receive email word free. probability spam? want find \\(P(E_1|F)\\).","code":""},{"path":"probability.html","id":"approach-1-using-bayes-rule","chapter":"2 Probability","heading":"2.3.8.1 Approach 1: Using Bayes’ Rule","text":"Using equation (2.11), \\[\n\\begin{split}\nP(E_1|F) &= \\frac{P(E_1 \\cap F)}{P(F)}\\\\\n&= \\frac{P(F|E_1) \\times P(E_1)}{P(F|E_1) \\times P(E_1) + P(F|E_2) \\times P(E_2) + P(F|E_3) \\times P(E_3)} \\\\\n&= \\frac{0.99 \\times 0.2}{0.99 \\times 0.2 + 0.1 \\times 0.5 + 0.05 \\times 0.3}\\\\\n&= 0.7528517\n\\end{split}\n\\]video goes approach little bit detail:","code":""},{"path":"probability.html","id":"approach-2-using-tree-diagrams","chapter":"2 Probability","heading":"2.3.8.2 Approach 2: Using Tree Diagrams","text":"tree diagram useful finding conditional probabilities probabilities involving intersections. visual way displaying information hand, conditional probabilities disjoint sets probabilities disjoint set. toy example, disjoint sets type email receive, \\(E_1, E_2, E_3\\), conditional probabilities disjoint sets, .e. \\(P(F|E_1), P(F|E_2)\\) \\(P(F|E_3)\\). can put information visual first splitting sample space disjoint sets \\(E_1, E_2, E_3\\), splitting disjoint set whether email word “free” (\\(F\\)) (\\(F^c\\)). information displayed tree diagram Figure 2.7.\nFigure 2.7: Tree Diagram Email Example\nsplit represented branch, write corresponding probability branch. want find probability received email spam given contains word “free”, \\(P(E_1|F)\\), using definition conditional probability equation (2.3)\\[\nP(E_1|F) = \\frac{P(E_1 \\cap F)}{P(F)}.\n\\]Looking tree diagram Figure 2.7, can label branches lead numerator \\(P(E_1 \\cap F)\\), probability email spam contains word free. shown tree diagram Figure 2.8 highlighting corresponding branches blue.\nFigure 2.8: Tree Diagram Email Example, Branch Numerator Blue\n\\(P(E_1 \\cap F) = 0.2 \\times 0.99 = 0.198\\). need find denominator \\(P(F)\\). Looking Figure 2.7, can see three branches lead email containing word free: \\(P(E_1 \\cap F)\\) \\(P(E_2 \\cap F)\\) \\(P(E_3 \\cap F)\\). shown tree diagram Figure 2.9 highlighting corresponding branches gold.\nFigure 2.9: Tree Diagram Email Example, Branches Denominator Gold\nknow probability branch, add obtain denominator \\(P(F) = 0.2 \\times 0.99 + 0.5 \\times 0.1 + 0.3 \\times 0.05 = 0.263.\\) Putting pieces together, \\[\nP(E_1|F) = \\frac{P(E_1 \\cap F)}{P(F)} = \\frac{0.198}{0.263} = 0.7528517.\n\\]Note: compare intermediate calculations approach 2, end using calculations approach 1, without referring associated equations.video goes tree diagrams little bit detail:","code":""},{"path":"probability.html","id":"confusion","chapter":"2 Probability","heading":"2.4 Confusion of the Inverse","text":"now ready talk prosecutor’s fallacy, confusion inverse, earlier mention section 2.3.1. essence, confusion happens falsely equate \\(P(X|Y)\\) equal \\(P(Y|X)\\). fact, large value \\(P(X|Y)\\) necessarily imply \\(P(Y|X)\\) also large. term prosecutor’s fallacy confusion applied criminal trial, e.g. probability abusive relationship ends murder small, probability abuse relationship ended murder lot higher.go examples based real life.","code":""},{"path":"probability.html","id":"disease-diagnostics","chapter":"2 Probability","heading":"2.4.1 Disease Diagnostics","text":"Suppose testing patient rare disease, estimated prevalent 0.5% people. Suppose medical test disease accurate. can number definitions accuracy. disease diagnostics, couple measures sensitivity, proportion people disease test positive, specificity, proportion people without disease test negative. positive test indicates person disease. Suppose sensitivity specificity high: 0.95 0.9 respectively. Suppose patient tests positive, probability patient actually disease? Assume test always indicates positive negative.example, let \\(D\\) denote event patient disease, let + denote event patient tests positive test, - denote event patient tests negative test. Given information, \\(P(D) = 0.005\\).\\(P(+|D) = 0.95\\).\\(P(-|D^c) = 0.9\\).wish find \\(P(D|+)\\). Using Bayes rule Law Total probability, \\[\n\\begin{split}\nP(D|+) &= \\frac{P(D \\cap +)}{P(+)}\\\\\n&= \\frac{P(+|D) \\times P(D)}{P(+|D) \\times P(D) + P(+|D^c) \\times P(D^c)} \\\\\n&= \\frac{0.95 \\times 0.005}{0.95 \\times 0.005 + 0.1 \\times 0.995 }\\\\\n&= 0.04556355\n\\end{split}\n\\]small probability, patient highly unlikely actually rare disease. test high sensitivity \\(P(+|D) = 0.95\\), imply patient tests positive actually disease, since \\(P(D|+)\\) low. implication rare disease, positive test imply high probability disease, even test accurate.result make sense? Essentially, large proportion small population still numerically much smaller small proportion large population. disease rare, small population people disease, almost detected test. also extremely large population people without disease, even small proportion erroneously test positive still fairly large number. among positive tests, people disease. consider following table based population 20 thousand people, Table 2.1 :Table 2.1:  Hypothetical Table Based 20,000 PeopleLook first column Table 2.1, shows number people test positive. see large proportion diseased people detected, since relatively people disease, number small, 95. small proportion people disease test positive disease, small proportion large population results relatively larger number, 1990. people test positive, \\(95 + 1990 = 2085\\) actually disease. Therefore \\(P(D|+) = \\frac{95}{2085} = 0.04556355\\).can also explain result Bayes’ viewpoint probability. Without knowing information results test, prior probability \\(P(D) = 0.005\\). However, upon seeing person positive, updated posterior probability \\(P(D|+) = 0.04556355\\), increase 0.005 knew knowing. updated posterior probability 9 times prior. believe person likely disease upon viewing positive test, knew nothing test result. posterior probability still small since value depends two pieces information: prior \\(P(D)\\) sensitivity \\(P(+|D)\\). product values belong numerator calculating \\(P(D|+)\\). denominator \\(P(+|D) \\times P(D) + P(+|D^c) \\times P(D^c)\\). prior \\(P(D)\\) extremely low, \\(P(D^c)\\) extremely close 1, since person either disease disease. \\(P(D)\\) belong extremely low, numerator close 0, value denominator close \\(P(+|D^c) \\times P(D^c)\\), therefore \\(P(D|+)\\) small.Notice talking rare diseases? confusion inverse, thinking high sensitivity implies person likely disease test positive, applies rare diseases. disease prevalent, high sensitivity likely imply person disease test positive.take tests rare diseases? ? go test . turns test positive twice rare disease, probability disease increases lot tested tested positive.perform calculation, use odds form Bayes’ rule, per equation (2.9)\\[\n\\begin{split}\n\\frac{P(D|T_1 \\cap T_2)}{P(D^c|T_1 \\cap T_2)} &= \\frac{P(T_1 \\cap T_2 | D)}{P(T_1 \\cap T_2 | D^c)} \\frac{P(D)}{P(D^c)}\\\\\n&= \\frac{0.95^2}{0.1^2} \\frac{0.005}{0.995} \\\\\n&= 0.4535176\n\\end{split}\n\\]\\(T_1\\) \\(T_2\\) denote events person test positive first test second test respectively. also assume results test independent previous tests.odds disease given person positive twice 0.4535176. Therefore, using equation (2.8), corresponding probability disease given person tested positive twice \\(P(D|T_1 \\cap T_2) = \\frac{0.4535176}{1+0.4535176} = 0.3120138\\). See posterior probability increased two positive tests, 1 positive test.Thought question: perform calculations show posterior probability person disease person tests positive 3 tests 0.8116199.Thought question: notice certain pattern emerging performing calculations person undergoes tests? write either mathematical equation, even function R, allows us quickly compute probability person disease given person tested positive \\(k\\) times, \\(k\\) can denote non negative integer?","code":""},{"path":"probability.html","id":"prosecutors-fallacy","chapter":"2 Probability","heading":"2.4.2 Prosecutor’s Fallacy","text":"confusion inverse also called prosecutor’s fallacy (sometimes also called defense attorney’s fallacy depending side making mistake) occurs legal setting. Generally, confusion comes equating P(evidence|innocent) P(innocent|evidence).book provides discussion Section 2.8, examples 2.8.1 2.8.2.","code":""},{"path":"discrete-random-variables.html","id":"discrete-random-variables","chapter":"3 Discrete Random Variables","heading":"3 Discrete Random Variables","text":"module based Introduction Probability (Blitzstein, Hwang), Chapters 3 4. can access book free https://stat110.hsites.harvard.edu/ (click Book). Please note cover additional topics, skip certain topics book. may skip Sections 3.4, 3.9, Example 4.2.3, Section 4.3, Example 4.4.6, 4.4.7, Theorem 4.4.8, Example 4.4.9, 4.6.4, 4.7.4, 4.7.7, Section 4.9 book.","code":""},{"path":"discrete-random-variables.html","id":"random-variables","chapter":"3 Discrete Random Variables","heading":"3.1 Random Variables","text":"idea behind random variables simplify notation regarding probability, enable us summarize results experiments, make easier quantify uncertainty.","code":""},{"path":"discrete-random-variables.html","id":"example","chapter":"3 Discrete Random Variables","heading":"3.1.1 Example","text":"Consider flipping coin three times recording lands heads tails time. sample space experiment \\(S = \\{HHH, HHT, HTH, THH, HTT, THT, TTH, TTT\\}\\). Given outcome equally likely, probability associated outcome \\(\\frac{1}{8}\\).Suppose want find probability get exactly 2 heads 3 flips. express :\\(P(\\text{two heads three flips})\\), \\(P(HHT \\cup HTH \\cup THH)\\), \\(P()\\) \\(\\) denotes event getting two heads three flips.Another way define random variable \\(X\\) expresses event bit efficiently. Let \\(X\\) denote number heads three flips, another way write \\(P(X=2\\)). idea behind random variables: assign events number.","code":""},{"path":"discrete-random-variables.html","id":"definition","chapter":"3 Discrete Random Variables","heading":"3.1.2 Definition","text":"random variable (RV) function sample space real numbers.convention, denote random variables capital letters. Using 3 coin flip example, \\(X\\) 0, 1, 2, 3. assign number possible outcome sample space.Random variables provide numerical summaries experiment. can useful especially sample space complicated. Random variables can also used non numeric outcomes.","code":""},{"path":"discrete-random-variables.html","id":"discrete-vs-continuous","chapter":"3 Discrete Random Variables","heading":"3.1.3 Discrete Vs Continuous","text":"One key distinctions make random variables determine discrete continuous. way express probabilities random variables depends whether random variable discrete continuous.discrete random variable can take countable (finite infinite) number values.number heads 3 coin flips, \\(X\\) countable finite, since can actually list values can take \\(\\{0,1,2,3 \\}\\) 4 values. \\(X\\) must take one 4 numerical values; number outside list. discrete.random variable countable infinite can list values can take, list end. example, number people using crosswalk 10 year period take values \\(\\{0, 1, 2, 3, \\cdots \\}\\). number take infinite number values, values whole numbers occur. number people using crosswalk 10 year period discrete random variable.continuous random variable can take uncountable number values interval real numbers.example, height American adult continuous random variable, height can take value interval interval, say 40 100 inches. values 40 100 possible.module, focus discrete random variables.support discrete random variable \\(X\\) set values \\(X\\) can take \\(P(X = x) > 0\\), .e. set values non zero probability happening. Using 3 coin flips example, \\(X\\) number heads 3 coin slips, support \\(\\{0,1,2,3 \\}\\). support discrete random variables usually integers.Thought question: Can come examples discrete continuous random variables ? Feel free search internet examples well.","code":""},{"path":"discrete-random-variables.html","id":"module-roadmap-1","chapter":"3 Discrete Random Variables","heading":"3.1.4 Module Roadmap","text":"Section 3.2 introduces PMFs, way mapping outcome discrete random variable corresponding probability. PMFs always used define behavior discrete random variables.Section 3.3 introduces CDFs, another way defining behavior random variables.Section 3.4 introduces notion (mathematical) expectation, long-run average random variable.Section 3.5 introduces commonly used discrete random variables. random variables basis many statistical models.Section 3.6 goes use R commonly used discrete random variables.","code":""},{"path":"discrete-random-variables.html","id":"PMFs","chapter":"3 Discrete Random Variables","heading":"3.2 Probability Mass Functions (PMFs)","text":"use probability describe behavior random variables. called distribution random variable. distribution random variable specifies probabilities events associated random variable.discrete random variables, distribution specified probability mass function (PMF). PMF discrete random variable \\(X\\) function \\(P_X(x) = P(X=x)\\). positive \\(x\\) support \\(X\\), 0 otherwise.Note: notation random variables, capital letters \\(X\\) denote random variables, lower case letters \\(x\\) denote actual numerical values. want find probability 2 heads 3 coin flips, write \\(P(X=2)\\), \\(x\\) 2 example.Going back example record number heads 3 coin flips, can write PMF random variable \\(X\\):\\(P_X(0) = P(X=0) = P(TTT) = \\frac{1}{8}\\),\\(P_X(1) = P(X=1) = P(HTT \\cup THT \\cup TTH) = \\frac{3}{8}\\),\\(P_X(2) = P(X=2) = P(HHT \\cup THH \\cup HTH) = \\frac{3}{8}\\),\\(P_X(3) = P(X=3) = P(HHH) = \\frac{1}{8}\\).Fairly often, PMF discrete random variable presented simple table like Table 3.1 :Table 3.1: PMF XOr PMF can represented using simple plot like one Figure 3.1:\nFigure 3.1: PMF X\nPMF provides list possible values random variable corresponding probabilities. words, PMF describes distribution relative frequencies outcome. experiment, observing 1 2 heads equally likely, occur three times often observing 0 3 heads. Observing 0 3 heads also equally likely.","code":"\n##support\nx<-0:3\n## PMF for each value in the support. \nPMFs<-c(1/8, 3/8, 3/8, 1/8)\n## create plot of PMF vs each value in support\nplot(x, PMFs, type=\"h\", main = \"PMF for X\", xlab=\"# of heads\", ylab=\"Probability\", ylim=c(0,1))"},{"path":"discrete-random-variables.html","id":"valid-pmfs","chapter":"3 Discrete Random Variables","heading":"3.2.1 Valid PMFs","text":"Consider discrete random variable \\(X\\) support \\(x_1, x_2, \\cdots\\). PMF \\(P_X(x)\\) \\(X\\) must satisfy:\\(P_X(x) > 0\\) \\(x = x_j\\), \\(P_X(x) = 0\\) otherwise.\\(\\sum_{j=1}^{\\infty} P_X(x_j) = 1\\).words, probabilities associated support greater 0, sum probabilities across whole support must add 1.Thought question: based Table 3.1, can see PMF \\(X\\) valid?","code":""},{"path":"discrete-random-variables.html","id":"pmfhist","chapter":"3 Discrete Random Variables","heading":"3.2.2 PMFs and Histograms","text":"Recall frequentist viewpoint probability, represents relative frequency associated event repeated infinite number times.Consider experiment flip coin 3 times count number heads. support random variable \\(X\\), number heads, \\(\\{0,1,2,3 \\}\\). Imagine performing experiment large number times. time perform experiment, record number heads. performed experiment one million times, recorded one million values number heads, value must support \\(X\\). create histogram one million values number heads, shape histogram close shape plot PMF Figure 3.1. Figure 3.2 shows resulting histogram performing experiment 1 million times.\nFigure 3.2: Histogram Experiment Performed 1 Million Times\ngeneral, PMF random variable match histogram long run.Note: just done use simulations repeat experiment large number times.","code":""},{"path":"discrete-random-variables.html","id":"CDFs","chapter":"3 Discrete Random Variables","heading":"3.3 Cumulative Distribution Functions (CDFs)","text":"Another function used describe distribution discrete random variables cumulative distribution function (CDF). CDF random variable \\(X\\) \\(F_X(x) = P(X \\leq x)\\). Notice unlike PMF, definition CDF applies discrete continuous random variables.Going back example record number heads 3 coin flips, can write CDF random variable \\(X\\):\\(F_X(0) = P(X \\leq 0) = P(X=0) = \\frac{1}{8}\\),\\(F_X(1) = P(X \\leq 1) = P(X=0) + P(X=1) = \\frac{1}{8} + \\frac{3}{8} = \\frac{1}{2}\\),\\(F_X(2) = P(X \\leq 2) = P(X=0) + P(X=1) + P(X=2) = \\frac{1}{2} + \\frac{3}{8} = \\frac{7}{8}\\),\\(F_X(3) = P(X \\leq 3) = P(X=0) + P(X=1) + P(X=2) + P(X=3) = \\frac{7}{8} + \\frac{1}{8} = 1\\).Notice calculations based PMF. find \\(P(X \\leq x)\\), summed PDF values support less equal \\(x\\). Therefore, another way write CDF discrete random variable \\[\\begin{equation}\nF_X(x) = P(X \\leq x) = \\sum_{x_j \\leq x} P(X=x_j).\n\\tag{3.1}\n\\end{equation}\\]Fairly often, CDF discrete random variable presented simple table like Table 3.2 :Table 3.2: CDF XOr simple plot like Figure 3.3 :\nFigure 3.3: CDF X\nCDF discrete random variables always look like step function, increases discrete jumps value support. height jump corresponds PMF value support.Thought question: see similarities CDF empirical cumulative density function (ECDF) section 1.3.3?","code":""},{"path":"discrete-random-variables.html","id":"valid-cdfs","chapter":"3 Discrete Random Variables","heading":"3.3.1 Valid CDFs","text":"CDF \\(F_X(x)\\) \\(X\\) must:non decreasing. means \\(x\\) gets larger, CDF either stays increases. Visually, graph CDF never decreases \\(x\\) increases.approach 1 \\(x\\) approaches infinity approach 0 \\(x\\) approaches negative infinity. Visually, graph CDF equal close 1 large values x, equal close 0 small values x.Thought question: Look CDF example Figure 3.3, see satisfies criteria listed valid CDF.","code":""},{"path":"discrete-random-variables.html","id":"expectations","chapter":"3 Discrete Random Variables","heading":"3.4 Expectations","text":"previous section, see PMFs CDFs can used describe distribution random variable. PMF can viewed long-run version histogram, gives us idea shape distribution. Similar Section 1, also interested measures centrality spread random variables.measure centrality random variables expectation, expected value. expectation random variable can interpreted long-run mean random variable, .e. able repeat experiment infinite number times, expectation random variable average result among experiments.discrete random variable \\(X\\) support \\(x_1, x_2, \\cdots,\\), expected value, denoted \\(E(X)\\), \\[\\begin{equation}\nE(X) = \\sum_{j=1}^{\\infty} x_j P(X=x_j).\n\\tag{3.2}\n\\end{equation}\\]can use Table 3.1 example. find expected number heads 3 coin flips, using equation (3.2),\\[\n\\begin{split}\nE(X) &= 0 \\times \\frac{1}{8} + 1 \\times \\frac{3}{8} + 2 \\times \\frac{3}{8} + 3 \\times \\frac{1}{8}\\\\\n       &= 1.5\n\\end{split}\n\\]take product value support random variable corresponding probability, add products.can see another interpretation expected value random variable calculation: weighted average values random variable, weighted probabilities.Intuitively, expected value 1.5 make sense. flip coin 3 times, coin fair, expect half flips land heads, 1.5 flips land heads.View video detailed explanation calculate expected values:","code":""},{"path":"discrete-random-variables.html","id":"linearity-of-expectations","chapter":"3 Discrete Random Variables","heading":"3.4.1 Linearity of Expectations","text":"seen calculate expected value random variable \\(X\\) using equation (3.2). need PMF \\(X\\). Sometimes random variable can viewed sum (difference) random variables, involve multiplication / adding constant random variable. Consider scenarios:Suppose friend fisherman. Let \\(Y\\) random variable describing number fish catch workday, let \\(W\\) random variable describing number fish friend catches workday. can let \\(T = Y+W\\) random variable describing total number fish catch workday.Suppose friend fisherman. Let \\(Y\\) random variable describing number fish catch workday, let \\(W\\) random variable describing number fish friend catches workday. can let \\(T = Y+W\\) random variable describing total number fish catch workday.Suppose sell fish $10 friend sells fish $15. can let \\(R = 10Y + 15W\\) random variable describes revenue generate workday.Suppose sell fish $10 friend sells fish $15. can let \\(R = 10Y + 15W\\) random variable describes revenue generate workday.Suppose friend rent space market sell fish, costs $5 day rent space. can let \\(G = 10Y + 15W - 5\\) random variable describes gross income day.Suppose friend rent space market sell fish, costs $5 day rent space. can let \\(G = 10Y + 15W - 5\\) random variable describes gross income day.examples involve new random variables, \\(T, R, G\\) can based previously defined random variables, \\(Y, W\\). turns find expectations new random variables, need expectations previously defined random variables. need find PMFs \\(T, R\\) \\(S\\) apply equation (3.2).can done linearity expectations: Let \\(X\\) \\(Y\\) denote random variables, \\(,b,c\\) denote constants, \\[\\begin{equation}\nE(aX + + c) = aE(X) + (Y) + c.\n\\tag{3.3}\n\\end{equation}\\]Applying equation (3.3) fishing examples:\\(E(T) = R(Y + W) = E(Y) + E(W)\\),\\(E(R) = E(10Y + 15W) = 10E(Y) + 15E(W)\\),\\(E(G) = E(10Y + 15W - 5) = 10E(Y) + 15E(W) - 5\\).need find expected values total number fish, revenue generated, gross income expected values number fish us caught. need PMFs \\(T,R,G\\).","code":""},{"path":"discrete-random-variables.html","id":"visual-explanation","chapter":"3 Discrete Random Variables","heading":"3.4.1.1 Visual Explanation","text":"visual explanation equation (3.3) makes sense, go back previous example \\(X\\) denotes number heads 3 coin flips. Figure 3.1 displays PMF random variable. Let us create PMF new random variable \\(Y=2X\\) display Figure 3.4 :\nFigure 3.4: PMF X Y=2X\nNote red vertical lines represent expected value random variable, since PMFs symmetric, expected value lies right middle support. Comparing PMFs Figure 3.4, get \\(Y\\) multiplying \\(X\\) 2. support \\(Y\\) now \\(\\{0,2,4,6\\}\\) associated probabilities unchanged, heights probabilities vertical axis unchanged. Therefore, center, expected value, multiplied constant.Consider another random variable \\(W = X+3\\). create PMF \\(W\\) display Figure 3.5 :\nFigure 3.5: PMF X W=X+3\nNotice PMFs \\(X\\) \\(W\\) look almost exactly . difference every value support \\(X\\) shifted 3 units. probabilities stay , heights PMFs unchanged. every value shifted 3 units, expected value, long-run average, also gets shifted 3 units. Adding constant random variable shifts expected value accordingly.","code":"\n##support of X\nx<-0:3\n## PMF for each value in the support. \nPMFs<-c(1/8, 3/8, 3/8, 1/8)\nEX<-1.5\n\n##support of Y\ny<-2*x\n## PMF for each value in the support. \nPMFs<-c(1/8, 3/8, 3/8, 1/8)\nEY<-2*EX\n\npar(mfrow=c(2,1))\n\n## create plot of PMF vs each value in support\nplot(x, PMFs, type=\"h\", main = \"PMF for X\", xlab=\"X\", ylab=\"Probability\", ylim=c(0,1))\n##overlay a line representing EX in red\nabline(v=EX, col=\"red\")\n\n## create plot of PMF vs each value in support\nplot(y, PMFs, type=\"h\", main = \"PMF for Y\", xlab=\"Y\", ylab=\"Probability\", ylim=c(0,1))\n##overlay a line representing EY in red\nabline(v=EY, col=\"red\")\n##support of X\nx<-0:3\n## PMF for each value in the support. \nPMFs<-c(1/8, 3/8, 3/8, 1/8)\nEX<-1.5\n\n##support of W\nw<-x+3\n## PMF for each value in the support. \nPMFs<-c(1/8, 3/8, 3/8, 1/8)\nEW<-EX+3\n\npar(mfrow=c(2,1))\n\n## create plot of PMF vs each value in support\nplot(x, PMFs, type=\"h\", main = \"PMF for X\", xlab=\"X\", ylab=\"Probability\", ylim=c(0,1))\n##overlay a line representing EX in red\nabline(v=EX, col=\"red\")\n\n## create plot of PMF vs each value in support\nplot(w, PMFs, type=\"h\", main = \"PMF for w\", xlab=\"W\", ylab=\"Probability\", ylim=c(0,1))\n##overlay a line representing EW in red\nabline(v=EW, col=\"red\")"},{"path":"discrete-random-variables.html","id":"one-more-example","chapter":"3 Discrete Random Variables","heading":"3.4.1.2 One More Example","text":"look one example illustrate usefulness linearity expectations. Consider drunk man walk one-dimensional number line starts 0 position. step drunk man takes, either moves forward, backward, stays spot. steps forward probability \\(p_f\\), backward probability \\(p_b\\), stays spot probability \\(p_s\\), \\(p_f + p_b+p_s = 1\\). Let \\(Y\\) position number line drunk man 2 steps. expected position drunk man two steps, .e. \\(E(Y)\\)? Assume step independent.Using brute force, can find PMF \\(Y\\), find \\(E(Y)\\) using equation (3.2). First, need find sample space \\(Y\\). two steps, sample space \\(\\{-2,-1,0,1,2\\}\\). Next, need find probabilities associated outcome sample space.\\(Y=-2\\), man must move backward step. probability \\(P(Y=-2) = p_b^2\\).Likewise, \\(Y=2\\), man must move forward step. probability \\(P(Y=2) = p_f^2\\).\\(Y=-1\\), man stay first step, move back second, move back first step, stay second. probability \\(P(Y=-1) = p_s p_b + p_b p_s = 2p_b p_s\\).Likewise, \\(Y=1\\), man stay first step, move forward second, move forward first step, stay second. probability \\(P(Y=1) = p_s p_f + p_f p_s = 2p_f p_s\\).\\(Y=0\\), man move forward, backward, move backward forward, stay steps. \\(P(Y=0) = p_f p_b + p_b p_f + p_s^2 = p_s^2 + 2 p_b p_f\\).Using equation (3.2),\\[\n\\begin{split}\nE(Y) &= -2 \\times p_b^2 + -1 \\times 2p_b p_s + 0 \\times p_s^2 + 2 p_b p_f + 1 \\times 2p_f p_s + 2 \\times p_f^2 \\\\\n       &= 2 (p_f - p_b)\n\\end{split}\n\\]Note: skipped lot messy algebra get end result. Even skipping messy algebra, setting PMF quite bit work.Suppose use linearity expectations equation (3.3). Let \\(Y_1, Y_2\\) denote distance man moves step 1 2 respectively. \\(Y = Y_1 + Y_2\\). sample \\(Y_1\\) \\(Y_2\\) : \\(\\{-1,0,1\\}\\). \\(Y_1\\) \\(Y_2\\) following PMF:\\(P(Y_i = -1) = p_b\\)\\(P(Y_i = 0) = p_s\\)\\(P(Y_i = 1) = p_f\\)using equation (3.2),\\[\n\\begin{split}\nE(Y_i) &= -1 \\times p_b + 0 \\times p_s + 1 \\times p_f \\\\\n       &= p_f - p_b\n\\end{split}\n\\]therefore \\(E(Y) = E(Y_1 + Y_2) = E(Y_1) + E(Y_2) = 2(p_f - p_b)\\). approaches resulted answer, notice much simpler obtain solution using linearity expectations. Imagine wanted find expected position 500 steps? Writing sample space 500 steps extremely long.View video detailed explanation worked example:","code":""},{"path":"discrete-random-variables.html","id":"law-of-the-unconscious-statistician","chapter":"3 Discrete Random Variables","heading":"3.4.2 Law of the Unconscious Statistician","text":"Suppose PMF random variable \\(X\\), want find \\(E(g(X))\\), \\(g\\) function \\(X\\) (can think \\(g\\) transformation performed \\(X\\)). One idea find PMF \\(g(X)\\) use definition expectation equation (3.2). seen previous subsection finding sample space transforming random variable can challenging. turns can find \\(E(g(X))\\) based PMF \\(X\\), without find PMF \\(g(X)\\).done Law Unconscious Statistician (LOTUS). Let \\(X\\) discrete random variable support \\(\\{x_1, x_2, \\cdots \\}\\), \\(g\\) function applied \\(X\\), \\[\\begin{equation}\nE(g(X)) = \\sum_{=j}^{\\infty} g(x_j) P(X=x_j).\n\\tag{3.4}\n\\end{equation}\\]application LOTUS finding variance discrete random variable.","code":""},{"path":"discrete-random-variables.html","id":"variance","chapter":"3 Discrete Random Variables","heading":"3.4.3 Variance","text":"talked shape distribution discrete random variable, expected value. One measure interested spread associated distribution. One common measure variance random variable.variance random variable \\(X\\) \\[\\begin{equation}\nVar(X) = E[(X - EX)^2]\n\\tag{3.5}\n\\end{equation}\\]standard deviation random variable \\(X\\) square root variance\\[\\begin{equation}\nSD(X) = \\sqrt{Var(X)}.\n\\tag{3.6}\n\\end{equation}\\]Looking equation (3.5) little closely, can see natural interpretation variance random variable: average squared distance random variable mean, long-run. equivalent formula variance random variable \\[\\begin{equation}\nVar(X) = E(X^2) - (EX)^2.\n\\tag{3.7}\n\\end{equation}\\]Equation (3.7) easier work equation (3.5) performing actual calculations.Let us now go back original example, \\(X\\) denotes number heads 3 coin flips. Earlier, found PMF random variable, per Table 3.1, found expectation 1.5. find variance \\(X\\) using equation (3.7), find \\(E(X^2)\\) first using LOTUS equation (3.4)\\[\n\\begin{split}\nE(X^2) &= 0^2 \\times \\frac{1}{8} + 1^2 \\times \\frac{3}{8} + 2^2 \\times \\frac{3}{8} + 3^2 \\times \\frac{1}{8} \\\\\n       &= 3\n\\end{split}\n\\]\\(Var(x) = 3 - 1.5^2 = \\frac{3}{4}\\).Thought question: Try find \\(Var(X)\\) using equation (3.5) LOTUS. arrive answer steps may bit complicated.View video detailed explanation calculate variance discrete random variables using equations (3.7) (3.5):","code":""},{"path":"discrete-random-variables.html","id":"var-prop","chapter":"3 Discrete Random Variables","heading":"3.4.3.1 Properties of Variance","text":"Variance following properties:\\(Var(X+c) = Var(X)\\), \\(c\\) constant. make sense, since add constant random variable, shift \\(c\\) units. shown earlier Figure 3.5, expected value also gets shifted \\(c\\) units. Variance measures average squared distance variable mean. distance, squared distance, \\(X\\) mean unchanged.\\(Var(cX) = c^2 Var(X)\\). Look Figure 3.4, notice distance value support expected value gets multiplied 2 (since \\(Y=2X\\)). multiply random variable \\(c\\), distance value support expected value multiplied \\(c\\). Since variance measures squared distance, variance gets multiplied \\(c^2\\).\\(X\\) \\(Y\\) independent random variables, \\(Var(X+Y) = Var(X) + Var(Y)\\).","code":""},{"path":"discrete-random-variables.html","id":"commonDisRVs","chapter":"3 Discrete Random Variables","heading":"3.5 Common Discrete Random Variables","text":"Next, introduce commonly used distributions may used discrete random variables. number common statistical models (example, logistic regression, Poisson regression) based distributions.","code":""},{"path":"discrete-random-variables.html","id":"bernoulli","chapter":"3 Discrete Random Variables","heading":"3.5.1 Bernoulli","text":"Bernoulli distribution might simplest discrete random variable. support random variable \\(\\{0,1\\}\\). words, value random variable follows Bernoulli distribution either 0 1. Bernoulli distribution also described parameter \\(p\\), probability random variable takes value 1.formally, random variable \\(X\\) follows Bernoulli distribution parameter \\(p\\) \\(P(X=1) = p\\) \\(P(X=0) = 1-p\\), \\(0<p<1\\). Using mathematical notation, can write \\(X \\sim Bern(p)\\) express random variable \\(X\\) distributed Bernoulli parameter \\(p\\). PMF Bernoulli distribution written \\[\\begin{equation}\nP(X=k) = p^k (1-p)^{1-k}\n\\tag{3.8}\n\\end{equation}\\]\\(k=0, 1\\).enough specify random variable follows Bernoulli distribution. need also clearly specify value parameter \\(p\\). Consider following two examples describe two different experiments:Suppose flip fair coin . Let \\(Y=1\\) coin lands heads, \\(Y=0\\) coin lands tails. \\(Y \\sim Bern(\\frac{1}{2})\\) example since coin fair.Suppose flip fair coin . Let \\(Y=1\\) coin lands heads, \\(Y=0\\) coin lands tails. \\(Y \\sim Bern(\\frac{1}{2})\\) example since coin fair.Suppose asked question given 5 multiple choices, 1 correct answer. idea topic, multiple choices help, guess. Let \\(W=1\\) answer correctly, \\(W=0\\) answer incorrectly. \\(W \\sim Bern(\\frac{1}{5})\\).Suppose asked question given 5 multiple choices, 1 correct answer. idea topic, multiple choices help, guess. Let \\(W=1\\) answer correctly, \\(W=0\\) answer incorrectly. \\(W \\sim Bern(\\frac{1}{5})\\).\\(P(Y=1)\\) \\(P(W=1)\\) examples.Fairly often, Bernoulli random variable, event results random variable coded 1 called success, event results random variable coded 0 called failure. setting, parameter \\(p\\) called success probability Bernoulli distribution. experiment Bernoulli distribution can called Bernoulli trial.go back second example section ??, modeling whether job applicant receives callback . example, let \\(V\\) random variable applicant receives callback, \\(V=1\\) denoting applicant received callback, \\(V=0\\) applicant receive callback. used logistic regression example. turns logistic regression used variable interest follows Bernoulli distribution.","code":""},{"path":"discrete-random-variables.html","id":"bernprop","chapter":"3 Discrete Random Variables","heading":"3.5.1.1 Properties of Bernoulli","text":"Consider \\(X\\) Bernoulli distribution parameter \\(p\\). expectation Bernoulli distribution \\[\\begin{equation}\nE(X) = p\n\\tag{3.9}\n\\end{equation}\\]variance \\[\\begin{equation}\nVar(X) = p(1-p).\n\\tag{3.10}\n\\end{equation}\\]Thought question: Use definition expectations discrete random variables, equation (3.2), PMF Bernoulli random variable, LOTUS prove equations (3.9) (3.10).expected value equal \\(p\\) Bernoulli distribution make sense. Remember support random variable 0 1, \\(P(X=1) = p\\). Using frequentist viewpoint, flip coin record heads tails, repeat coin flipping many times, record bunch 0s 1s represent result coin flips. average bunch 0s 1s just proportion 1s.equation variance Bernoulli distribution exhibits interesting intuitive behavior. Figure 3.6 shows variance behaves vary value \\(p\\):\nFigure 3.6: Variance Bernoulli\nNotice variance maximum \\(p=0.5\\), variance minimum (fact 0) \\(p=0\\) \\(p=1\\). biased coin always lands heads, every coin flip land heads exception. variability result, utmost certainty result coin flip. hand, coin fair \\(p=0.5\\), least certainty result coin flip, variance maximum coin fair.Another application property election results (assuming 2 candidates, idea applies candidates). swing states race closer (\\(p\\) closer half), projections winner uncertainty need get data wait longer projections. states primarily vote one candidate (\\(p\\) closer 0 1), projections happen lot quicker projections less uncertainty.","code":"\np<-seq(0,1,by = 0.001) ##sequence of values for p\nBern_var<-p*(1-p) ##variance of Bernoulli\n##plot variance against p\nplot(p, Bern_var, ylab=\"Variance\", main=\"Variance of Bernoulli as p is Varied\")"},{"path":"discrete-random-variables.html","id":"binomial","chapter":"3 Discrete Random Variables","heading":"3.5.2 Binomial","text":"Suppose experiment follows Bernoulli distribution, perform experiment \\(n\\) times (sometimes called trials), time success probability \\(p\\). experiments independent . Let \\(X\\) denote number successes \\(n\\) trials. \\(X\\) follows binomial distribution parameters \\(n\\) \\(p\\) (number trials success probability). write \\(X \\sim Bin(n,p)\\) express \\(X\\) follows binomial distribution parameters \\(n\\) \\(p\\), \\(n>0\\) \\(0<p<1\\). PMF Binomial distribution written \\[\\begin{equation}\nP(X=k) = \\binom{n}{k} p^k (1-p)^{n-k}\n\\tag{3.11}\n\\end{equation}\\]\\(k=0,1,2, \\cdots, n\\), also support binomial distribution.equation (3.11), \\(\\binom{n}{k}\\) called binomial coefficient, number combinations result \\(k\\) successes \\(n\\) trials. binomial coefficient can found using\\[\\begin{equation}\n\\binom{n}{k} =  \\frac{n!}{k! (n-k)!}.\n\\tag{3.12}\n\\end{equation}\\]\\(n!\\) called n-factorial, product positive integers less equal n. \\(n! = n \\times (n-1) \\times (n-2) \\times \\cdots \\times 1.\\) example \\(5! = 5 \\times 4 \\times 3 \\times 2 \\times 1 = 120\\), using R:Note: fairly common model, logistic regression model aggregated data, based binomial distribution. mentioned logistic regression earlier. difference two (without aggregated data) based structure data frame. interested differences, please read https://www.r-bloggers.com/2021/02/--run-logistic-regression--aggregate-data--r/.go back first example counting number heads three coin flips follows binomial distribution.coin flip either heads tails. two outcomes flip.success probability, probability heads, 0.5 flip. parameter fixed flip.result flip independent flips since flips affect outcome.number trials (flips example) \\(n=3\\) specified fixed value.Since four conditions met, number heads 3 coin flips can modeled using binomial distribution. let \\(x\\) denote number heads 3 coin clips, write \\(X \\sim Bin(3,0.5)\\).Suppose want calculate \\(P(X=2)\\) using equation (3.11):\\[\n\\begin{split}\nP(X=2) &= \\binom{3}{2} (0.5)^2 (0.5)^1\\\\\n       &= \\frac{3!}{2! 1!} (0.5)^2 (0.5)^1 \\\\\n       &= 3 \\times \\frac{1}{8} \\\\\n       &= \\frac{3}{8}.\n\\end{split}\n\\]example, binomial coefficient equals 3. indicates 3 combinations obtain 2 heads 3 coin flips. \\(P(X=2)\\) can written \\(P(HHT \\cup HTH \\cup THH)\\). Solving \\(P(HHT \\cup HTH \\cup THH)\\), \\[\n\\begin{split}\nP(HHT \\cup HTH \\cup THH) &= P(HHT) + P(HTH) + P(THH)\\\\\n       &= 0.5^3 + 0.5^3 + 0.5^3 \\\\\n       &= 3 \\times \\frac{1}{8} \\\\\n       &= \\frac{3}{8}.\n\\end{split}\n\\]\nsolved using basic probability rules previous module, without using PMF binomial distribution equation (3.11). course, PMF binomial distribution gets lot convenient \\(n\\) gets larger, number combinations sample space get lot larger.can also use R find \\(P(X=2)\\):","code":"\nfactorial(5)## [1] 120\ndbinom(2,3,0.5) ##specify values of k, n, p in this order## [1] 0.375"},{"path":"discrete-random-variables.html","id":"relationship-between-binomial-and-bernoulli","chapter":"3 Discrete Random Variables","heading":"3.5.2.1 Relationship Between Binomial and Bernoulli","text":"Looking description Bernoulli binomial distributions, may notice Bernoulli random variable special case binomial random variable \\(n=1\\), .e. 1 trial.binomial random variable also sometimes viewed sum \\(n\\) independent Bernoulli random variables, value \\(p\\).","code":""},{"path":"discrete-random-variables.html","id":"properties-of-binomial","chapter":"3 Discrete Random Variables","heading":"3.5.2.2 Properties of Binomial","text":"\\(X \\sim Bin(n,p)\\), \\[\\begin{equation}\nE(X) = np\n\\tag{3.13}\n\\end{equation}\\]\\[\\begin{equation}\nVar(X) = np(1-p).\n\\tag{3.14}\n\\end{equation}\\]results make sense note relationship binomial random variable Bernoulli random variable. Suppose random variables \\(Y_1, Y_2, \\cdots, Y_n\\) Bernoulli random variables parameter \\(p\\) independent. \\(Y = Y_1 + Y_2 + \\cdots + Y_n \\sim Bin(n,p)\\). Therefore, using linearity expectations equation (3.3), \\(E(Y) = E(Y_1) + E(Y_2) + \\cdots + E(Y_n) = np\\). Since \\(Y_1, Y_2, \\cdots, Y_n\\) independent, \\(Var(Y) = Var(Y_1) + Var(Y_2) + \\cdots + Var(Y_n) = np(1-p)\\).","code":""},{"path":"discrete-random-variables.html","id":"pmfs-of-binomial","chapter":"3 Discrete Random Variables","heading":"3.5.2.3 PMFs of Binomial","text":"take look PMFs binomials, \\(n=10\\) vary \\(p\\) 0.2, 0.5, 0.9, Figure 3.7:\nFigure 3.7: PMF X, n=10, p varied\nfigure 3.7, can see distribution binomial symmetric \\(p=0.5\\), middle values \\(k\\) higher probabilities, probabilities decrease go away middle. \\(p \\neq 0.5\\), see distribution gets skewed. success probability small, smaller number successes likelier, success probability large, larger number successes likelier, intuitive. probability success small, expect outcomes failures.","code":""},{"path":"discrete-random-variables.html","id":"poisson","chapter":"3 Discrete Random Variables","heading":"3.5.3 Poisson","text":"One common distribution used discrete random variables Poisson distribution. often used variable interest call count data (support non negative integers), example, number cars cross intersection day.random variable \\(X\\) follows Poisson distribution parameter \\(\\lambda\\), \\(\\lambda>0\\). Using mathematical notation, can write \\(X \\sim Pois(\\lambda)\\) express random variable \\(X\\) distributed Poisson parameter \\(p\\). PMF Poisson distribution written \\[\\begin{equation}\nP(X=k) = \\frac{e^{-\\lambda}\\lambda^k}{k!}\n\\tag{3.15}\n\\end{equation}\\]\\(k=0,1,2,\\cdots\\). \\(\\lambda\\) sometimes called rate parameter, related rate arrivals, example, number cross intersection period time.","code":""},{"path":"discrete-random-variables.html","id":"properties-of-poisson","chapter":"3 Discrete Random Variables","heading":"3.5.3.1 Properties of Poisson","text":"\\(X \\sim Pois(\\lambda)\\), \\[\\begin{equation}\nE(X) = \\lambda\n\\tag{3.16}\n\\end{equation}\\]\\[\\begin{equation}\nVar(X) = \\lambda.\n\\tag{3.17}\n\\end{equation}\\]imply larger values Poisson random variable associated larger variances. common feature count data. Consider number cars cross intersection one-hour time period. Consider average number cars rush hour, say 5 6pm. average number large, number lot smaller due inclement weather, number get lot larger convention occurring nearby. hand, consider average number cars 3 4am. average number small, likely small time, regardless weather conditions whether special events happening.Another interesting property Poisson distribution skewed \\(\\lambda\\) small, approaches bell-shaped distribution \\(\\lambda\\) gets bigger. Figure 3.8 displays density plots Poisson distributions \\(\\lambda\\) varied:\nFigure 3.8: PMF Poissons Rate Parameter Varied\n","code":"\n##calculate probability of Poisson with these values on the support\nx<-0:20\nlambda<-c(0.5, 1, 4, 10) ##try 4 different values of lambda\n\n##create PMFs of these 4 Poissons with different lambdas\npar(mfrow=c(2,2))\nfor (i in 1:4)\n  \n{\n  dens<-dpois(x,lambda[i])\n  plot(x, dens, type=\"l\", main=paste(\"Lambda is\", lambda[i]))\n}"},{"path":"discrete-random-variables.html","id":"poisson-approximation-to-binomial","chapter":"3 Discrete Random Variables","heading":"3.5.3.2 Poisson Approximation to Binomial","text":"\\(X \\sim Bin(n,p)\\), \\(n\\) large \\(p\\) small, PMF \\(X\\) can approximated Poisson distribution rate parameter \\(\\lambda = np\\). words, approximation works better \\(n\\) gets larger \\(np\\) gets smaller.several rules thumbs exist guide large \\(n\\) small \\(np\\) . National Institute Standards Technology (NIST) suggest \\(n \\geq 20\\) \\(p \\leq 0.05\\), \\(n \\geq 100\\), \\(np \\leq 10\\).One main using approximation, instead directly using binomial distribution, binomial coefficient can become computationally expensive compute \\(n\\) large.Consider example: company manufactures computer chips, 2 percent chips defective. quality control manager randomly samples 100 chips coming assembly line. probability 3 chips defective?Let \\(Y\\) denote number chips defective 100 chips.chip either defective . two outcomes chip.“success” probability 0.02 chip. probability assumed fixed chip.assume chip independent.number chips fixed \\(n=100\\).can model \\(Y \\sim Bin(100,0.02)\\), long assume chips independent. find \\(P(Y \\leq 3)\\), can:use binomial distribution, orapproximate using \\(Pois(2)\\), \\(\\lambda = np = 100 \\times 0.02\\).Notice values close .","code":"\n##set up binomial\nn<-100 \np<-0.02 \ny<-0:3 ##we want P(Y=0), P(Y=1), P(Y=2), P(Y=3)\nsum(dbinom(y,n,p))## [1] 0.8589616\n##Use Poisson to approx binomial\nlambda<-n*p\nsum(dpois(y,lambda))## [1] 0.8571235"},{"path":"discrete-random-variables.html","id":"Rdis","chapter":"3 Discrete Random Variables","heading":"3.6 Using R","text":"R built functions compute PMF, CDF, percentiles, well simulate data common distributions. start using random variable \\(Y\\) follows binomial distribution, \\(n=5, p = 0.3\\) example first. Note example support \\(Y\\) \\(\\{0,1,2,3,4,5 \\}\\).find \\(P(Y=2)\\), use:probability \\(Y\\) equal 2 0.3087.find \\(P(Y \\leq 2)\\), use:probability \\(Y\\) less equal 2 0.83692.find value support corresponds median (50th percentile), use:median binomial distribution 5 trials success probability 0.3 1.simulate 10 realizations (replications) \\(Y\\), use:outputs vector length 10. value represents result rep. first time ran binomial distribution \\(n=5, p=0.3\\), 1 5 success. second time run, 2 5 success, .Notice functions ended binom. just added different letter first, depending whether want PMF, CDF, percentile, random draw. letters d, p, q, r respectively.idea works distribution. example, find probability Poisson distribution rate parameter 2 equal 1, type:Thought questions: Suppose \\(Y \\sim Pois(1)\\).Find \\(P(Y \\leq 2)\\).Find 75th percentile \\(Y\\).Simulate 10,000 reps Y, find sample mean. sample mean close expected value?","code":"\ndbinom(2, 5, 0.3) ##supply the value of Y you want, then the parameters n and p in this order## [1] 0.3087\npbinom(2, 5, 0.3) ##supply the value of Y you want, then the parameters n and p in this order## [1] 0.83692\nqbinom(0.5, 5, 0.3) ##supply the value of the percentile you need, then the parameters n and p in this order## [1] 1\nset.seed(2) ##use set.seed() so we get the same random numbers each time the code is run\nrbinom(10, 5, 0.3) ##supply the number of simulated data you need, then the parameters n and p##  [1] 1 2 2 0 3 3 0 2 1 2\ndpois(1, 2) ##supply value of k, then parameter## [1] 0.2706706"},{"path":"continuous-random-variables.html","id":"continuous-random-variables","chapter":"4 Continuous Random Variables","heading":"4 Continuous Random Variables","text":"module based Introduction Probability (Blitzstein, Hwang), Chapters 5 6. can access book free https://stat110.hsites.harvard.edu/ (click Book). Please note cover additional topics, skip certain topics book. may skip Examples 5.1.6, 5.1.7, Proposition 5.2.3, Example 5.2.4, Sections 5.2.6, 5.2.7, Definition 5.3.7, Theorem 5.3.8, Example 5.4.7, Sections 5.5, 5.6, 5.7, Proposition 6.2.5, 6.2.6, Theorem 6.3.4, Sections 6.4 6.7 book.","code":""},{"path":"continuous-random-variables.html","id":"introduction","chapter":"4 Continuous Random Variables","heading":"4.1 Introduction","text":"previous module, learned discrete random variables. learned distributions can described PMFs CDFs, find expected values variances, well common distributions discrete random variables. learn counterparts dealing continuous random variables. concepts similar, computed can quite different.reminder:discrete random variable can take countable (finite infinite) number values.continuous random variable can take uncountable number values interval real numbers.example, height American adult continuous random variable, height can take value interval 40 100 inches. values 40 100 possible. list possible real numbers range list never ending.sample space associated continuous random variable difficult list, since takes uncountable number values. Using example heights American adults, real number 40 100 inches possible.different discrete random variable list sample space, support, find probability associated value support.Similar discrete random variables, want describe shape distribution, centrality, spread continuous random distribution idea probabilities associated different ranges values random variable.","code":""},{"path":"continuous-random-variables.html","id":"module-roadmap-2","chapter":"4 Continuous Random Variables","heading":"4.1.1 Module Roadmap","text":"Section 4.2 covers CDFs used continuous random variables. Note similarities differences used discrete random variables.Section 4.3 introduces PDFs, analogous PMFs discrete random variables.Section 4.4 introduces common measures used summarize distribution continuous random variables.Section 4.5 introduces commonly used continuous random variables. random variables basis many statistical models.Section 4.6 goes use R continuous random variables.","code":""},{"path":"continuous-random-variables.html","id":"contCDFs","chapter":"4 Continuous Random Variables","heading":"4.2 Cumulative Distribution Functions (CDFs)","text":"start talking cumulative distribution function, definition applies discrete continuous random variables. CDF random variable \\(X\\) \\(F_X(x) = P(X \\leq x)\\). difference lies CDF looks visually.Take look CDF discrete random variable CDF continuous random variable Figure 4.1:\nFigure 4.1: CDF Discrete RV vs CDF Continuous RV\nmentioned previous module, CDF discrete random variable called step function, jumps value support. hand, CDF continuous random variable increases smoothly sample space infinite.height CDF informs us percentile associated value random variable. Looking CDF continuous random variable Figure 4.1, height 0.5 random variable 0, value 0 corresponds 50th percentile distribution.technical definition continuous random variable : random variable continuous distribution CDF differentiable.discrete random variable fails definition since derivative undefined jumps.","code":""},{"path":"continuous-random-variables.html","id":"valid-cdfs-1","chapter":"4 Continuous Random Variables","heading":"4.2.1 Valid CDFs","text":"criteria valid CDF , matter random variable discrete continuous:non decreasing. means \\(x\\) gets larger, CDF either stays increases. Visually, graph CDF never decreases \\(x\\) increases.approach 1 \\(x\\) approaches infinity approach 0 \\(x\\) approaches negative infinity. Visually, graph CDF equal close 1 large values x, equal close 0 small values x.Thought question: Look CDFs example Figure 4.1, see satisfy criteria listed valid CDF.","code":""},{"path":"continuous-random-variables.html","id":"PDFs","chapter":"4 Continuous Random Variables","heading":"4.3 Probability Density Functions (PDFs)","text":"probability density function (PDF) continuous random variable analogous PMF discrete random variable.definition PDF continuous random variables following: continuous random variable \\(X\\) CDF \\(F_X(x)\\), PDF \\(X\\), \\(f_X(x)\\), derivative CDF, words, \\(f_X(x) = F_X^{\\prime}(x)\\). support \\(X\\) set \\(x\\) \\(f_X(x) >0\\).relationship PDF CDF continuous random variable \\(X\\) can expressed \\[\\begin{equation}\nF_X(x) = P(X \\leq x) = \\int_{-\\infty}^{x} f_X(x) dx.\n\\tag{4.1}\n\\end{equation}\\]take look example . Suppose continuous random variable \\(X\\) CDF PDF displayed , want find \\(P(X \\leq 1)\\):\nFigure 4.2: Probabilities CDF PDF\ncan find \\(P(X \\leq 1)\\) two different ways:CDF, find value 1 horizontal axis, read corresponding value vertical axis (blue lines). tells us \\(P(X \\leq 1) = 0.84\\).PDF, find area PDF \\(X \\leq 1\\). area corresponds shaded region blue, equal 0.84 performed integration per equation (4.1).Compare equation (4.1) equation (3.1) note similarities differences CDFs continuous discrete random variables. discrete CDFs, sum PMF values less equal \\(x\\), whereas continuous CDFs, integrate, accumulate area, PDF values less equal \\(x\\). people view integral continuous version summation.equation (4.1), can generalize way find probability \\(P(<X<b)\\) continuous random variable \\(X\\):\\[\\begin{equation}\nP(<X<b) = F_X(b) - F_X() = \\int_{}^{b} f_X(x) dx.\n\\tag{4.2}\n\\end{equation}\\]words, find probability range values \\(X\\), just find area PDF range values. Going back example, want find \\(P(0<X<1)\\), find area PDF \\(0<X<1\\), like Figure 4.3 :\nFigure 4.3: Probabilities PDF\nmentioned, PDF continuous random variable analogous, exactly , PMF discrete random variable. One common misconception PDF tells us probability, example, value \\(f_X(2) = P(X=2)\\), \\(X\\) continuous. correct \\(X\\) discrete. fact, look equation (4.2) little closely, \\(P(X=c) = 0\\) \\(X\\) continuous \\(c\\) constant, since area PDF 0.","code":""},{"path":"continuous-random-variables.html","id":"valid-pdfs","chapter":"4 Continuous Random Variables","heading":"4.3.1 Valid PDFs","text":"PDF continuous random variable must satisfy following criteria:Non negative: \\(f_X(x) \\geq 0\\),Integrates 1: \\(\\int_{-\\infty}^{\\infty}f_X(x) dx = 1\\).","code":""},{"path":"continuous-random-variables.html","id":"pdfs-and-density-plots","chapter":"4 Continuous Random Variables","heading":"4.3.2 PDFs and Density Plots","text":"Recall Section 3.2.2, learned discrete random variables, PMF histogram related. PMF represents long-run proportion, histogram represents relative frequency based data. sample size gets larger, PMF match histogram.Similarly continuous random variables, PDF density plot related. PDF associated distribution known random variable, density plot estimated data, data follows known random variable, PDF match density plot sample size gets larger.go details density plots created end module, Section 4.6.1, still need cover concepts.","code":""},{"path":"continuous-random-variables.html","id":"summsDist","chapter":"4 Continuous Random Variables","heading":"4.4 Summaries of a Distribution","text":"Next, talk common summaries associated distribution. involve measures centrality variance, covered . also talk couple measures: skewness kurtosis.","code":""},{"path":"continuous-random-variables.html","id":"expectations-1","chapter":"4 Continuous Random Variables","heading":"4.4.1 Expectations","text":"expected value continuous random variable \\(X\\) \\[\\begin{equation}\nE(X) = \\int_{-\\infty}^{\\infty} x f_X(x) dx.\n\\tag{4.3}\n\\end{equation}\\]Another common notation \\(E(X)\\) \\(\\mu\\), sometimes \\(\\mu_X\\) show writing mean random variable \\(X\\).compare equation (4.3) equation (3.2), notice use integral instead summation now working continuous random variables.interpretation expected values still : expectation random variable can interpreted long-run mean random variable, .e. able repeat experiment infinite number times, expectation random variable average result among experiments. still measure centrality random variable.linearity expectations still hold way, per equation (3.3). matter random variable discrete continuous.Law Unconscious Statistician (LOTUS) also still applies. continuous random variable \\(X\\), (unsurprisingly):\\[\\begin{equation}\nE(g(X)) = \\int_{-\\infty}^{\\infty} g(x) f_X(x) dx.\n\\tag{4.4}\n\\end{equation}\\]Notice compare equation (4.4) discrete counterpart equation (3.4): just replaced summation integral.Thought question: Can guess write equation variance continuous random variable? Hint: variance discrete random variable given equation (3.5).","code":""},{"path":"continuous-random-variables.html","id":"median-1","chapter":"4 Continuous Random Variables","heading":"4.4.1.1 Median","text":"value \\(m\\) median random variable \\(X\\) \\(P(X \\leq c) \\geq \\frac{1}{2}\\) \\(P(X \\geq c) \\geq \\frac{1}{2}\\).Intuitively, median value \\(m\\) splits area PDF half (close half possible random variable discrete). Half area left \\(m\\), half area right \\(m\\).","code":""},{"path":"continuous-random-variables.html","id":"mode-1","chapter":"4 Continuous Random Variables","heading":"4.4.1.2 Mode","text":"continuous random variable \\(X\\), mode value \\(c\\) maximizes PDF: \\(f_X(c) \\geq f_X(x)\\) \\(x\\).discrete random variable \\(X\\), mode value \\(c\\) maximizes PMF: \\(P(X=c) \\geq P(X=x)\\) \\(x\\). Intuitively, mode commonly occurring value discrete random variable","code":""},{"path":"continuous-random-variables.html","id":"lossfunc","chapter":"4 Continuous Random Variables","heading":"4.4.1.3 Loss Functions","text":"goal statistical modeling use model make predictions. want able quantify quality prediction, prediction error. Suppose experiment can described random variable \\(X\\), want predict value next experiment. mean median natural guesses value next experiment.turns several ways quantify prediction error. usually called loss functions. Suppose predicted value denoted \\(x_{pred}\\). couple common loss functions :Mean squared error (MSE): \\(E(X-x_{pred})^2\\),Mean absolute error (MAE): \\(E|X-x_{pred}|\\).turns expected value \\(E(X)\\) minimizes MSE, median minimizes MAE. depending loss function suits analysis, use either mean median predictions. cover ideas detail later module (indeed later courses program).","code":""},{"path":"continuous-random-variables.html","id":"variance-1","chapter":"4 Continuous Random Variables","heading":"4.4.2 Variance","text":"variance continuous random variable \\(X\\) \\[\\begin{equation}\nVar(X) = \\int_{-\\infty}^{\\infty} (x-\\mu)^2 f_X(x) dx.\n\\tag{4.5}\n\\end{equation}\\]properties variance still Section 3.4.3.1. matter random variable discrete continuous. common notation used variance \\(\\sigma^2\\), sometimes \\(\\sigma_X^2\\) show variance random variable \\(X\\).","code":""},{"path":"continuous-random-variables.html","id":"moments","chapter":"4 Continuous Random Variables","heading":"4.4.3 Moments","text":"talking measures used describe continuous distributions, cover terminology used measures. Suppose random variable \\(X\\).\\(k\\)th moment \\(X\\) \\(E(X^k)\\). expected value, mean, sometimes called first moment.\\(k\\)th central moment \\(X\\) \\(E((X-\\mu)^k)\\). variance sometimes called second central moment.\\(k\\) standardized moment \\(X\\) \\(E(\\frac{(X-\\mu)^k}{\\sigma})\\).","code":""},{"path":"continuous-random-variables.html","id":"skewness","chapter":"4 Continuous Random Variables","heading":"4.4.4 Skewness","text":"One measure used describe shape distribution skewness, measure symmetry (measure skewness). skew random variable \\(X\\) third standardized moment:\\[\\begin{equation}\nSkew(X) = E \\left(\\frac{(X-\\mu)^3}{\\sigma} \\right)\n\\tag{4.6}\n\\end{equation}\\]random variable \\(X\\) symmetric distribution mean \\(X - \\mu\\) distribution \\(\\mu - X\\). Fairly often, people just say \\(X\\) symmetric; almost always assumed symmetry mean.Intuitively, symmetry means PDF \\(X\\) left mean mirror image PDF \\(X\\) right mean. look couple examples Figure 4.4:\nFigure 4.4: PDFs Symmetric RV vs Skewed RV\nblue vertical lines indicate mean distributions. Notice mirror image first plot, second plot.distribution symmetric, can say distribution asymmetric, skewed. values \\(Skew(X)\\) associated different shapes :\\(Skew(X) = 0\\): \\(X\\) symmetric.\\(Skew(X) > 0\\): \\(X\\) right (positively) skewed.\\(Skew(X) < 0\\): \\(X\\) left (negatively) skewed.","code":""},{"path":"continuous-random-variables.html","id":"excess-kurtosis","chapter":"4 Continuous Random Variables","heading":"4.4.5 (Excess) Kurtosis","text":"One measure deals tail behavior distribution. Visually, tails PDF associated probabilities extreme values random variable. distribution heavy tailed means extreme values (ends) likely occur. Tail behavior important consideration risk management finance: e.g. heavy left tail PDF mean financial crisis. Figure 4.5 shows example heavy tailed distribution (blue), compared Gaussian distribution (black). talk Gaussian distribution next subsection.\nFigure 4.5: PDF Heavy Tailed Distribution\ncommon measure tail behavior excess kurtosis. excess kurtosis random variable \\(X\\) shifted fourth standardized moment:\\[\\begin{equation}\nKurt(X) = E \\left(\\frac{(X-\\mu)^4}{\\sigma} \\right) - 3.\n\\tag{4.7}\n\\end{equation}\\]reason subtracting (shifting ) 3 Gaussian distribution (commonly used distribution continuous random variables) excess kurtosis 0.Note: authors write kurtosis instead, subtract 3 equation (4.7), compare kurtosis 3, instead comparing excess kurtosis 0.Note: authors switch terms kurtosis excess kurtosis, need careful reading kurtosis excess kurtosis.values \\(Kurt(X)\\) associated tail behaviors :\\(Kurt(X) = 0\\): \\(X\\) similar tails Gaussian distribution.\\(Kurt(X) > 0\\): \\(X\\) heavier tails compared Gaussian distribution (extreme values likely).\\(Kurt(X) < 0\\): \\(X\\) smaller tails compared Gaussian distribution (extreme values less likely).","code":""},{"path":"continuous-random-variables.html","id":"commContRVs","chapter":"4 Continuous Random Variables","heading":"4.5 Common Continuous Random Variables","text":"Next, introduce commonly used distributions may used continuous random variables. number common statistical models (example, linear regression) based distributions.","code":""},{"path":"continuous-random-variables.html","id":"uniform","chapter":"4 Continuous Random Variables","heading":"4.5.1 Uniform","text":"random variable follows uniform distribution interval \\((,b)\\) completely random number \\(\\) \\(b\\). Notionally, upper case \\(U\\) usually used denote uniform random variable. \\(U\\) said uniform distribution interval \\((,b)\\), denoted \\(U \\sim(,b)\\), PDF \\[\\begin{equation}\nf_X(x) = \\begin{cases}\n  \\frac{1}{b-} & \\text{} <x<b \\\\\n  0 & \\text{otherwise }.\n\\end{cases}\n\\tag{4.8}\n\\end{equation}\\]Note parameters \\(,b\\) also help define support uniform distribution. Figure 4.6 displays plot PDF \\(U(,b)\\):\nFigure 4.6: PDF U(,b). Picture https://en.wikipedia.org/wiki/Continuous_uniform_distribution\nThought question: Can verify valid PDF?Figure 4.7 displays plot CDF \\(U(,b)\\):\nFigure 4.7: CDF U(,b). Picture https://en.wikipedia.org/wiki/Continuous_uniform_distribution\nproperties uniform distribution:mean \\(E(U) = \\frac{+b}{2}\\).variance \\(Var(U) = \\frac{(b-)^2}{12}\\).skewness 0, symmetric.excess kurtosis -\\(\\frac{6}{5}\\), tails heavy compared Gaussian distribution.Thought question: Can see uniform distribution symmetric? Can see tails heavy?support uniform distribution 0 1, standard uniform distribution. talk importance standard uniform distribution next subsection.","code":""},{"path":"continuous-random-variables.html","id":"universality-of-uniform","chapter":"4 Continuous Random Variables","heading":"4.5.1.1 Universality of Uniform","text":"turns can construct random variable continuous distribution based standard uniform distribution. fact used simulate random numbers continuous distributions, called Universality Uniform. Let \\(F_X(x)\\) denote CDF continuous random variable \\(X\\), :Let \\(U \\sim U(0,1)\\) \\(X = F^{-1}(U)\\). \\(X\\) random variable CDF \\(F_X(x)\\).\\(F_X(X) \\sim U(0,1)\\).give insight means, look example. Another continuous distribution called standard logistic distribution, denote \\(X\\). CDF \\[\nF_X(x) = \\frac{e^x}{1+e^x}.\n\\]\nLet \\(U \\sim U(0,1)\\). first part universality uniform informs us inverse CDF standard logistic \\(F_X^{-1}(U) \\sim X\\), invert \\(F_X(x)\\) get inverse \\(F_X^{-1}(x)\\). done setting CDF \\(X\\) equal \\(u\\), .e. let \\(u = \\frac{e^x}{1+e^x}\\), solving \\(x\\):\\[\n\\begin{split}\nu + u e^x &= e^x\\\\\n\\implies u &= e^x (1-u) \\\\\n\\implies e^x &= \\frac{u}{1-u} \\\\\n\\implies x &= \\log (\\frac{u}{1-u}).\n\\end{split}\n\\]Therefore \\(F^{-1}(u) = \\log (\\frac{u}{1-u})\\) \\(F^{-1}(U) = \\log (\\frac{U}{1-U})\\). Therefore \\(\\log (\\frac{U}{1-U})\\) follows standard logistic distribution.Let us use simulations show going . First, simulate 10,000 reps standard uniform distribution, invert values using \\(\\log (\\frac{u}{1-u})\\), create density plot \\(\\log (\\frac{u}{1-u})\\). steps shown Figure 4.8 :\nFigure 4.8: Uniform Logistic\nFigure 4.8:first plot shows density plot 10,000 reps standard normal. close PDF standard uniform.second plot shows density plot inverting 10,000 reps standard normal, .e. \\(F^{-1}(u) = \\log (\\frac{u}{1-u})\\).third plot shows PDF standard logistic. Notice similar looks second plot.see \\(\\log (\\frac{U}{1-U})\\) follows standard logistic distribution.View video detailed explanation example:second part universality uniform informs us \\(X\\) follows standard logistic distribution, \\(F(X) = \\frac{e^X}{1 + e^X} \\sim U(0,1)\\)., can see purpose universality uniform:part 1, can simulate reps distribution, long know CDF. software use may able simulate reps particular distribution, can write code simulate reps distribution based standard uniform.part 2, can convert random variable unknown distribution one known: standard uniform.","code":"\nset.seed(4)\n\nreps<-10000 ##number of reps\nu<-runif(reps) ##simulate standard uniform\ninvert<- log(u/(1-u)) ##invert based on F inverse. These should now follow standard logistic\n\npar(mfrow=c(1,3))\nplot(density(u), main=\"Density Plot from 10,000 U's\")\nplot(density(invert), main=\"Density Plot after Inverting\", xlim=c(-6,6))\ncurve(dlogis, from = -7, to = 7, main = \"PDF for Logistic\", ylab=\"Density\", xlab=\"\")"},{"path":"continuous-random-variables.html","id":"normdist","chapter":"4 Continuous Random Variables","heading":"4.5.2 Normal","text":"Another widely used distribution continuous random variables normal, Gaussian distribution. distribution symmetric bell-shaped. probably important distribution statistics data science due Central Limit Theorem. define theorem later module, loosely speaking, says take average bunch random variables, average approximate normal distribution, even random variables individually normal.lot questions wish answer based averages. exampleDoes implementation certain technologies class improve test scores students, average?male Gentoo penguins heavier female counterparts, average?replacing traffic lights roundabout reduce number traffic accidents, average?central limit theorem implies even test scores, weights Gentoo penguins, number traffic accidents follow normal distribution, average values approximate normal distribution.","code":""},{"path":"continuous-random-variables.html","id":"standard-normal","chapter":"4 Continuous Random Variables","heading":"4.5.2.1 Standard Normal","text":"First, talk standard normal distribution, normal distributions can viewed variations standard normal. standard normal distribution mean 0 variance 1. usually denoted \\(Z\\). can also write \\(Z \\sim N(0,1)\\) say \\(Z\\) normally distributed mean 0 variance 1. PDF standard normal distribution :\\[\\begin{equation}\n\\phi(z) = \\frac{1}{\\sqrt{2 \\pi}} e^{-z^2/2}.\n\\tag{4.9}\n\\end{equation}\\]Notice constant \\(\\frac{1}{\\sqrt{2 \\pi}}\\) equation (4.9). presence needed make PDF valid, since PDF must integrate 1. constants called normalizing constants.Figure 4.9 displays PDF:\nFigure 4.9: PDF Standard Normal\nFigure 4.9, can see following properties standard normal distribution (apply normal distribution):PDF symmetric mean. Figure 4.9, PDF symmetric 0, .e. \\(\\phi(-z) = \\phi(z)\\).implies tail areas also symmetric. example, \\(P(Z \\leq -2) = P(Z \\geq 2)\\).skew 0, since symmetric.actually closed-formed equation CDF standard normal (normal distribution). write \\(\\Phi(z) = P(Z \\leq z) = \\int_{\\infty}^z \\phi(z) dz\\) express CDF standard normal.Notice special letters \\(Z, \\phi, \\Phi\\) denote standard normal distribution. indication often used warrant notation.","code":"\ncurve(dnorm, from = -4, to = 4, main = \"PDF for Z\", ylab=\"Density\", xlab=\"\")"},{"path":"continuous-random-variables.html","id":"norm","chapter":"4 Continuous Random Variables","heading":"4.5.2.2 From Standard Normal to Other Normals","text":"\\(Z \\sim N(0,1)\\), \\(X = \\mu + \\sigma Z \\sim N(\\mu, \\sigma^2)\\). words, \\(Z\\) standard normal, \\(X = \\mu + \\sigma Z\\) follows normal distribution mean \\(\\mu\\) variance \\(\\sigma^2\\). parameters normal distribution mean \\(\\mu\\) variance \\(\\sigma^2\\).Note authors say parameters mean \\(\\mu\\) standard deviation \\(\\sigma\\) instead, careful reading notation associated normal distributions various sources. example, \\(N(0,2)\\) class book means normal distribution mean 0 variance 2, authors, \\(N(0,2)\\) means normal distribution mean 0 standard deviation 2. Indeed, functions R use alternate parameterization, need careful.Thought question: Can use linearity expectations explain \\(X\\) mean \\(\\mu\\)? Can use properties variance Section 3.4.3.1 explain \\(X\\) variance \\(\\sigma^2\\)?Notice started standard normal \\(Z\\), transformed \\(Z\\) multiplying \\(\\sigma\\) adding \\(\\mu\\) get normal distribution. transformation called location-scale transformation, shifting scaling. scale changes since multiply constant \\(\\sigma\\); location transformed since mean changes 0 \\(\\mu\\).can also reverse transformation state following: \\(X \\sim N(\\mu, \\sigma^2)\\), \\(Z = \\frac{X-\\mu}{\\sigma} \\sim N(0,1)\\). start \\(X \\sim N(\\mu, \\sigma^2)\\), can transform \\(X\\) subtracting , dividing \\(\\sigma\\), obtain \\(Z\\). particular transformation called standardization:\\[\\begin{equation}\nZ = \\frac{X-\\mu}{\\sigma}.\n\\tag{4.10}\n\\end{equation}\\]PDF normal distribution \\(X \\sim N(\\mu, \\sigma^2)\\) \\[\\begin{equation}\nf_X(x) = \\frac{1}{\\sigma \\sqrt{2 \\pi}} \\exp \\left(-\\frac{(x - \\mu)^2}{2 \\sigma^2} \\right).\n\\tag{4.11}\n\\end{equation}\\]Thought question: Compare equations (4.11) (4.9). Can see equation (4.9) can derived equation (4.11)?","code":""},{"path":"continuous-random-variables.html","id":"rulenorm","chapter":"4 Continuous Random Variables","heading":"4.5.2.3 68-95-99.7% Rule","text":"following property holds normal distribution, often called 68-99-99.7% rule. normal distribution \\(X \\sim N(\\mu, \\sigma^2)\\):\\(P(\\mu - \\sigma < X < \\mu + \\sigma) \\approx 0.68\\),\\(P(\\mu - 2\\sigma < X < \\mu + 2\\sigma) \\approx 0.95\\),\\(P(\\mu - 3\\sigma < X < \\mu + 3\\sigma) \\approx 0.997\\).mean normal distribution:68% observed values fall within 1 standard deviation mean,95% observed values fall within 2 standard deviations mean, andAbout 99.7% observed values fall within 3 standard deviations mean.last statement basis term six sigma used manufacturing, since virtually data points fall within range six sigma wide (assuming follow normal distribution). Visually, rule shown Figure 4.10 applied standard normal:\nFigure 4.10: 68-95-99.7 Rule\nwork first statement, 68% observed values fall within 1 standard deviation mean normal distribution. use R help us verify rule standard normal:Thought question: tweak code verify two statements associated 68-95-99.7% rule?","code":"\nupper1<-pnorm(1) ## what is percentile associated with Z=1 (i.e. 1 standard deviation above mean)\nlower1<-pnorm(-1) ## what is percentile associated with Z=-1 (i.e. 1 standard deviation below mean)\nupper1-lower1 ## find proportion in between 1 SD above and below mean.## [1] 0.6826895"},{"path":"continuous-random-variables.html","id":"RCont","chapter":"4 Continuous Random Variables","heading":"4.6 Using R","text":"R built functions compute density, CDF, percentiles, well simulate data common distributions. start random variable \\(Y \\sim N(1, 9)\\) example.find \\(f_Y(2)\\), use:density \\(f_Y(2)\\) 0.1257944. Note: R, normal distribution parameterized mean standard deviation, different set notes book, uses mean variance.find \\(P(Y \\leq 2)\\), use:probability \\(Y\\) less equal 2 0.6305587.Alternatively, can standardize normal distribution, use standard normal. standardization, per equation (4.10), gives us\\[\nz = \\frac{2-1}{3} = \\frac{1}{3},\n\\]\\[\n\\begin{split}\nP(Y \\leq 2) &= P(\\frac{Y-\\mu}{\\sigma} \\leq \\frac{2-1}{3}) \\\\\n            &= P(Z \\leq \\frac{1}{3}) \\\\\n            &= \\Phi(\\frac{1}{3})\n\\end{split}\n\\]can found usingwhich gives answer pnorm(2,1,3).View video detailed explanation example:find value support corresponds 90th percentile, use:90th percentile \\(Y \\sim N(1,9)\\) 4.844655.want use standard normal, find 90th percentile:apply location scale transformationwhich answer qnorm(0.9,1,3).simulate 10 draws (replicates) \\(Y\\), use:outputs vector length 10. value represents result rep. first value drawn \\(Y \\sim N(1,9)\\) -1.6907436, second value drawn 1.5545476 .Just like Section 3.6, notice functions ended norm. just added different letter first, depending whether want density (analogous PDF), CDF, percentile, random draw. letters d, p, q, r respectively.One thing note: supply mean standard deviation, example type rnorm(10), R assume want use standard normal distribution, rnorm(10) draw 10 random numbers standard normal.","code":"\ndnorm(2, 1, 3) ##supply the value of Y you want, then the parameters mu and sigma## [1] 0.1257944\npnorm(2, 1, 3) ##supply the value of Y you want, then the parameters mu and sigma## [1] 0.6305587\npnorm(1/3) ##don't supply mu and sigma means you want to use standard normal## [1] 0.6305587\nqnorm(0.9, 1, 3) ##supply the value of the percentile you need, then the parameters mu and sigma## [1] 4.844655\nqnorm(0.9)## [1] 1.281552\nqnorm(0.9)*3 + 1 ##multiply by sigma, then add mu## [1] 4.844655\nset.seed(2) ##use set.seed() so we get the same random numbers each time the code is run\nrnorm(10, 1, 3) ##supply the number of simulated data you need, then the parameters mu and sigma##  [1] -1.6907436  1.5545476  5.7635360 -2.3911270  0.7592447  1.3972609\n##  [7]  3.1238642  0.2809059  6.9534218  0.5836390"},{"path":"continuous-random-variables.html","id":"KDE","chapter":"4 Continuous Random Variables","heading":"4.6.1 Density Plots and Kernel Density Estimation","text":"now ready talk density plots, like ones Figure 4.8 created. Recall difference density plots PDFs:plot PDF describes distribution known random variable.density plot based data, used describe distribution data. data may may follow commonly known random variable. , plot PDF density plot match gather data.Proportions found way, finding area PDF density plot appropriate range support.Suppose \\(n\\) observed values unknown random variable \\(X\\): \\(x_1, x_2, \\cdots, x_n\\).density \\(f\\) \\(X\\) unknown want estimate data. estimate density \\(f\\), use kernel density estimator:\\[\\begin{equation}\n\\hat{f}_h(x) = \\frac{1}{nh} \\sum_{=1}^n K \\left( \\frac{x-x_i}{h}\\right ),\n\\tag{4.12}\n\\end{equation}\\]\\(K\\) kernel \\(h\\) smoothing parameter, often called bandwidth. Looking equation (4.12), KDE can viewed weighted average relative likelihood observing particular value.kernel can viewed weighting function, weights following shape distribution user specifies (usually symmetric). Common kernel functions shapes displayed Figure 4.11:\nFigure 4.11: Common Kernals. Picture adapted https://tgstewart.cloud/compprob/kde.html\nhorizontal axis kernel can viewed distance value data point specific value support, mid point horizontal axis represents distance 0.Looking normal kernel, nearest values receive highest weight, values away receive less weight.uniform kernel, values within certain distance receive weight, values beyond certain distance receive weight.Epanechnikov (parabolic) kernel mix : values beyond certain distance receive weight, values within certain distance receive weight roughly inversely proportional distance.\\(h\\) smoothing parameter analogous bin width histograms. Larger values result smoother looking density plots.Let us go back old example. use loan50 dataset openintro package. data originally consist thousands loans made Lending Club platform, randomly select 50 loans. Let us study interest rate loans 50 applicants received.\nFigure 4.12: Density Plot 50 Interest Rates\nuses KDE default settings: kernel normal, bandwidth based Silverman’s rule thumb.change , add kernel adjust argument using density() function, example, use Epanechnikov kernel twice default bandwidth:\nFigure 4.13: Density Plot 50 Interest Rates, Epanechnikov Kernel, Twice Bandwidth\ndensity plot Figure 4.13 looks smoother density plot Figure 4.12.","code":"\nlibrary(openintro)\n\n##create object for data\nData<-loan50\n\n##create density plot using default\nplot(density(Data$interest_rate), main=\"Density Plot of Interest Rates\")\n##create density plot using different settings\nplot(density(Data$interest_rate, kernel = \"epanechnikov\", adjust = 2), \n     main=\"Density Plot of Interest Rates\")"},{"path":"continuous-random-variables.html","id":"density-plots-and-histograms","chapter":"4 Continuous Random Variables","heading":"4.6.2 Density Plots and Histograms","text":"Section 1.2.3, mentioned density plots can viewed smoothed versions histogram. create histogram interest rates, overlay density plot blue, per Figure 4.14 :\nFigure 4.14: Histogram Density Plot 50 Interest Rates\nNotice density plot approximates histogram.","code":"\nhist(Data$interest_rate, prob = TRUE, main = \"Histogram with Density Plot\", xlab=\"Interest Rates\")\n\n##create density plot using default\nlines(density(Data$interest_rate), col=\"blue\")"},{"path":"continuous-random-variables.html","id":"numerical-summaries","chapter":"4 Continuous Random Variables","heading":"4.6.3 Numerical Summaries","text":"Equations (4.3), (4.5), (4.6), (4.7) used obtain mean, variance, skewness, excess kurtosis known distribution random variable. calculate quantities based sample observed data, \\(x_1, x_2, \\cdots, x_n\\), use:\\[\\begin{equation}\n\\bar{x} =  \\frac{1}{n} \\sum_{=1}^n x_i,\n\\tag{4.13}\n\\end{equation}\\]\\[\\begin{equation}\ns_X^2 =  \\frac{1}{n-1} \\sum_{=1}^n (x_i - \\bar{x})^2,\n\\tag{4.14}\n\\end{equation}\\]\\(\\bar{x}\\) \\(s_x^2\\) denote sample mean variance respectively. sample skewness sample excess kurtosis \\[\\begin{equation}\n\\text{sample skewness } =  \\frac{\\frac{1}{n} \\sum_{=1}^n (x_i - \\bar{x})^3}{s_X^3},\n\\tag{4.15}\n\\end{equation}\\]\\[\\begin{equation}\n\\text{sample excess kurtosis } =  \\frac{\\frac{1}{n} \\sum_{=1}^n (x_i - \\bar{x})^4}{s_X^4} - 3.\n\\tag{4.16}\n\\end{equation}\\]functions mean(), var(), skewness(), kurtosis() compute quantities R. latter two functions come moments package sure install load prior using .data also right skewed heavy tailed. Note kurtosis() function moments package reports kurtosis, subtract 3 reported value obtain excess kurtosis.","code":"\nmean(Data$interest_rate) ##mean## [1] 11.5672\nvar(Data$interest_rate) ##variance## [1] 25.52387\nlibrary(moments)\nmoments::skewness(Data$interest_rate) ##greater than 0## [1] 1.102193\nmoments::kurtosis(Data$interest_rate) - 3 ##greater than 0## [1] 0.6516311"},{"path":"joint-distributions.html","id":"joint-distributions","chapter":"5 Joint Distributions","heading":"5 Joint Distributions","text":"module based Introduction Probability (Blitzstein, Hwang), Chapters 7 9. can access book free https://stat110.hsites.harvard.edu/ (click Book). Please note cover additional topics, skip certain topics book. may skip Story 7.1.9, Theorems 7.1.10 7.1.12, Examples 7.1.23 7.1.26, Section 7.2, Examples 7.3.6 7.3.8, 7.4.8 (parts d f ), Definition 7.5.6, Examples 9.1.8 9.1.10, Example 9.2.5, Theorem 9.3.2, Example 9.3.3, Theorems 9.3.7 9.3.9, Sections 9.4 9.6 book.","code":""},{"path":"joint-distributions.html","id":"introduction-1","chapter":"5 Joint Distributions","heading":"5.1 Introduction","text":"previous two modules, learned summarize distribution individual random variables. now ready extend concepts modules apply slightly different setting, analyzing multiple variables related . extremely common want analyze relationship least two variables. book lists examples, :Public policy: increasing expenditure infrastructure impact economic development?Education: smaller class sizes higher teacher pay impact student learning outcomes?Marketing: design website influence probability customer purchasing item?module consider variables jointly, words, relate . lot concepts CDF, PDF, PMF, expectations, variances, analogous versions considering variables jointly.","code":""},{"path":"joint-distributions.html","id":"module-roadmap-3","chapter":"5 Joint Distributions","heading":"5.1.1 Module Roadmap","text":"Section 5.2 explore concepts variables discrete. concepts easier learn variables discrete.Section 5.3 introduce concepts variables continuous.Section 5.4 covers couple common ways quantify strength linear relationship two quantitative variables.Section 5.5 covers notion conditional expectation random variable.Section 5.6 introduces common multivariate distributions.","code":""},{"path":"joint-distributions.html","id":"jointDis","chapter":"5 Joint Distributions","heading":"5.2 Joint Distributions for Discrete RVs","text":"start discrete random variables, move continuous random variables. keep things simple, use two random variables explain concepts. concepts can generalized number random variables.Recall single discrete random variable \\(X\\), use PMF inform us support \\(X\\) probability associated value support. said PMF informs us distribution random variable \\(X\\).now two discrete random variables, \\(X\\) \\(Y\\). joint distribution \\(X\\) \\(Y\\) provides probability associated possible combination \\(X\\) \\(Y\\). joint PMF \\(X\\) \\(Y\\) \\[\\begin{equation}\np_{X,Y}(x,y) = P(X=x, Y=y).\n\\tag{5.1}\n\\end{equation}\\]Equation (5.1) can read probability random variables \\(X\\) \\(Y\\) equal \\(x\\) \\(y\\) respectively. Recall upper case letters usually used denote random variables, lower case letters usually used placeholder actual numerical value random variable take.Joint distributions sometimes called multivariate distributions. looking distribution one random variable, distribution can called univariate distribution.Joint PMFs can displayed via table, like Table 5.1 . example, consider study time, \\(X\\), related grades, \\(Y\\), \\(X=1\\) studying 0 5 hours week,\\(X=2\\) studying 6 10 hours week, \\(X=3\\) studying 10 hours week.\\(Y=1\\) denotes getting ,\\(Y=2\\) denotes getting B, \\(Y=3\\) denotes getting C lower.Table 5.1:  Example Joint PMF Study Time (\\(X\\)) Grades (\\(Y\\))also write joint PMF :\\(P(X=1, Y=1) = 0.05\\)\\(P(X=1, Y=2) = 0.05\\)\\(P(X=1, Y=3) = 0.10\\)\\(P(X=2, Y=1) = 0.15\\)\\(P(X=2, Y=2) = 0.20\\)\\(P(X=2, Y=3) = 0.05\\)\\(P(X=3, Y=1) = 0.30\\)\\(P(X=3, Y=2) = 0.10\\)\\(P(X=3, Y=3) = 0\\)Just like PMFs single discrete random variable must sum 1 PMF must non negative, joint PMFs discrete random variables must sum 1 individual PMF must non negative valid.Thought question: Can verify joint PMF Table 5.1 valid?joint CDF discrete random variables \\(X\\) \\(Y\\) \\[\\begin{equation}\nF_{X,Y}(x,y) = P(X \\leq x, Y \\leq y).\n\\tag{5.2}\n\\end{equation}\\]Thought question: Compare equation (5.2) univariate counterpart equation (3.1). Can see similarities differences?","code":""},{"path":"joint-distributions.html","id":"marginal-distributions-for-discrete-rvs","chapter":"5 Joint Distributions","heading":"5.2.1 Marginal Distributions for Discrete RVs","text":"joint distribution \\(X\\) \\(Y\\), can get distribution individual random variable. call marginal distribution, unconditional distribution, \\(X\\) \\(Y\\). marginal distribution \\(X\\) gives us information distribution \\(X\\), without taking \\(Y\\) consideration. get marginal PMF \\(X\\) joint PMF \\(X\\) \\(Y\\):\\[\\begin{equation}\nP(X=x) = \\sum_y P(X=x, Y=y).\n\\tag{5.3}\n\\end{equation}\\]Note summation performed support \\(Y\\). go back Table 5.1 example. Suppose want find marginal distribution study times, \\(X\\). Applying equation (5.3):\\[\n\\begin{split}\nP(X=1) &= \\sum_y P(X=1, Y=y)\\\\\n&= P(X=1, Y=1) + P(X=1, Y=2) + P(X=1, Y=3) \\\\\n&= 0.05 + 0.05 + 0.10\\\\\n&= 0.2,\n\\end{split}\n\\]\\[\n\\begin{split}\nP(X=2) &= \\sum_y P(X=2, Y=y)\\\\\n&= P(X=2, Y=1) + P(X=2, Y=2) + P(X=2, Y=3) \\\\\n&= 0.15 + 0.20 + 0.05\\\\\n&= 0.4,\n\\end{split}\n\\]\\[\n\\begin{split}\nP(X=3) &= \\sum_y P(X=3, Y=y)\\\\\n&= P(X=3, Y=1) + P(X=3, Y=2) + P(X=3, Y=3) \\\\\n&= 0.30 + 0.10 + 0\\\\\n&= 0.4.\n\\end{split}\n\\]can add information Table 5.1, create Table 5.2Table 5.2:  Example Joint PMF Study Time (\\(X\\)) Grades (\\(Y\\)), Marginal PMF Study TimeNotice just added probabilities column get marginal PMF \\(X\\), write probabilities margin table (hence term marginal PMF).may notice marginal PMF \\(X\\) ends just PMF \\(X\\). term marginal used imply PMF derived joint PMF, even information .Thought question: Can see equation (5.3) based Law Total Probability equation (2.10)?View video detailed explanation deriving marginal PMF \\(X\\):Likewise, obtain marginal PMF \\(Y\\) joint PMF \\(X\\) \\(Y\\):\\[\\begin{equation}\nP(X=x) = \\sum_x P(X=x, Y=y).\n\\tag{5.4}\n\\end{equation}\\]summation now performed support \\(X\\).Thought question: Can verify marginal PMF grades displayed Table 5.3 ?Table 5.3:  Example Joint PMF Study Time (\\(X\\)) Grades (\\(Y\\)), Marginal PMF Study Time Study Time","code":""},{"path":"joint-distributions.html","id":"conddist","chapter":"5 Joint Distributions","heading":"5.2.2 Conditional Distributions for Discrete RVs","text":"may need update distribution one variables based observed value variable, need distribution one variables based specific value variable. leads conditional PMF.Suppose want update distribution \\(Y\\) based observed value \\(X=x\\), want distribution \\(Y\\) observations \\(X=x\\) (words, \\(X\\) equal specific value \\(x\\)). \\(X\\) \\(Y\\) discrete, conditional PMF \\(Y\\) given \\(X=x\\) :\\[\\begin{equation}\nP(Y=y|X=x) = \\frac{P(X=x, Y=y)}{P(X=x)}.\n\\tag{5.5}\n\\end{equation}\\]conditional PMF \\(Y\\) given \\(X=x\\) essentially joint PMF \\(X\\) \\(Y\\) divided marginal PMF \\(X\\). Note conditional PMF \\(Y\\) given \\(X=x\\) viewed function value \\(x\\) fixed.go back Table 5.1 example find conditional PMFs. Suppose want find distribution grades students study little (0 5 hours per week). want conditional PMF \\(Y\\) given \\(X=1\\). Applying equation (5.5) Table 5.3, \\[\n\\begin{split}\nP(Y=1|X=1) &= \\frac{P(X=1, Y=1)}{P(X=1)}\\\\\n&= \\frac{0.05}{0.2} \\\\\n&= 0.25,\n\\end{split}\n\\]\\[\n\\begin{split}\nP(Y=2|X=1) &= \\frac{P(X=1, Y=2)}{P(X=1)}\\\\\n&= \\frac{0.05}{0.2} \\\\\n&= 0.25,\n\\end{split}\n\\]\\[\n\\begin{split}\nP(Y=3|X=1) &= \\frac{P(X=1, Y=3)}{P(X=1)}\\\\\n&= \\frac{0.10}{0.2} \\\\\n&= 0.5.\n\\end{split}\n\\]\nfrequentist interpretation values among students studied little, 50% chance getting C lower, 25% chance getting B, 25% chance getting .Bayesian interpretation values know student studied little, student 50% chance getting C lower, 25% chance getting B, 25% chance getting .Thought question: Can show conditional PMF \\(Y\\) given \\(X=3\\) based Table 5.3 \\(P(Y=1|X=3) = 0.75, P(Y=2|X=3) = 0.25, P(Y=3|X=3) = 0\\)?find conditional PMF \\(X\\) given \\(Y=y\\):\\[\\begin{equation}\nP(X=x|Y=y) = \\frac{P(X=x, Y=y)}{P(Y=y)}.\n\\tag{5.6}\n\\end{equation}\\]Thought question: Can show conditional PMF \\(X\\) given \\(Y=1\\) based Table 5.3 \\(P(X=1|Y=1) = 0.1, P(X=2|Y=1) = 0.3, P(X=3|Y=1) = 0.6\\)?","code":""},{"path":"joint-distributions.html","id":"bayes-rule-1","chapter":"5 Joint Distributions","heading":"5.2.3 Bayes’ Rule","text":"can apply Bayes’ Rule alternative way finding conditional PMF \\(Y\\) given \\(X=x\\). Equation (5.5) can written :\\[\\begin{equation}\nP(Y=y|X=x) = \\frac{P(X=x|Y=y) P(Y=y)}{P(X=x)}.\n\\tag{5.7}\n\\end{equation}\\]","code":""},{"path":"joint-distributions.html","id":"law-of-total-probability-1","chapter":"5 Joint Distributions","heading":"5.2.4 Law of Total Probability","text":"can apply law total probability denominator equations (5.5) (5.7), .e. \\(P(X=x) = \\sum_y P(X=x|Y=y) P(Y=y)\\), conditional PMF \\(Y\\) given \\(X=x\\) can also written \\[\\begin{equation}\nP(Y=y|X=x) = \\frac{P(X=x|Y=y) P(Y=y)}{\\sum_y P(X=x|Y=y) P(Y=y)}.\n\\tag{5.8}\n\\end{equation}\\]","code":""},{"path":"joint-distributions.html","id":"indepdence-of-discrete-rvs","chapter":"5 Joint Distributions","heading":"5.2.5 Indepdence of Discrete RVs","text":"notion whether two random variables independent (also called dependent) whether random variables association, words, changing value one random variable affect distribution ?\\(X\\) \\(Y\\) discrete random variables, independent , values support \\(X\\) \\(Y\\):\\[\\begin{equation}\nP(X=x, Y=y) = P(X=x) P(Y=y).\n\\tag{5.9}\n\\end{equation}\\]equivalent condition values support \\(X\\) \\(Y\\):\\[\\begin{equation}\nP(Y=y | X=x) = P(Y=y),\n\\tag{5.10}\n\\end{equation}\\]\\[\\begin{equation}\nP(X=x | Y=y) = P(X=x),\n\\tag{5.11}\n\\end{equation}\\]show \\(X\\) \\(Y\\) independent, need show one equations (5.9), (5.10), (5.11) true values support \\(X\\) \\(Y\\). show \\(X\\) \\(Y\\) dependent, need show one equations (5.9), (5.10), (5.11) false just one value \\(X\\) \\(Y\\).Equations (5.10) (5.11) pretty intuitive. equations say conditional distribution one variable, given , marginal distribution variable. means distribution variable influenced knowledge variable.first equation (5.9) informs us discrete variables independent, joint PMF equal product marginal PMFs.go back study time grades example shown Table 5.3. Study time grades dependent (independent) since \\(P(Y=1|X=1) = 0.25\\) \\(P(Y=1) = 0.5\\). equal study time grades independent. usually easier prove condition met providing counterexample: find one specific example condition false.study time grades independent, needed show \\(P(Y=1|X=x) = P(Y=1)\\) \\(X=1,2,3\\), \\(P(Y=2|X=x) = P(Y=2)\\) \\(X=1,2,3\\), \\(P(Y=3|X=x) = P(Y=3)\\) \\(X=1,2,3\\). usually tedious prove condition met show condition met circumstances.often, knowing context random variables helps. Since expect students study get better grades, expect variables dependent, know just need provide counterexample.","code":""},{"path":"joint-distributions.html","id":"jointCont","chapter":"5 Joint Distributions","heading":"5.3 Joint, Marginal, Conditional Distributions for Continuous RVs","text":"Recall previous modules CDFs PDFs continuous random variable similar CDFs PMFs discrete random variables. continuous versions generally found swapping summations integrals. general idea applies joint distributions random variables continuous.Now suppose \\(X\\) \\(Y\\) denote random variables continuous. required joint CDF \\(F_{X,Y}(x,y) = P(X \\leq x, Y \\leq y)\\) differentiable respect \\(x\\) \\(y\\). joint PDF partial derivative joint CDF respect \\(x\\) \\(y\\): \\(f_{X,Y}(x,y) = \\frac{\\partial^2}{\\partial x \\partial y} F_{X,Y}(x,y)\\).Similar univariate PDFs, joint PDFs valid, require :\\(f_{X,Y}(x,y) \\geq 0\\) \\(\\int_{-\\infty}^\\infty \\int_{-\\infty}^\\infty f_{X,Y}(x,y) dx dy = 1\\).find probabilities, example \\(P(<X<b, c<Y<d)\\), integrate joint PDF two-dimensional region, .e. \\(\\int_{c}^d \\int_{}^b f_{X,Y}(x,y) dx dy\\).marginal PDF \\(X\\) can found integrating joint PDF support \\(Y\\):\\[\\begin{equation}\nf_X(x) = \\int_{-\\infty}^\\infty f_{X,Y}(x,y) dy.\n\\tag{5.12}\n\\end{equation}\\]conditional PDF \\(Y\\) given \\(X=x\\) \\[\\begin{equation}\nf_{Y|X}(y|x) = \\frac{f_{X,Y}(x,y)}{f_X(x)}\n\\tag{5.13}\n\\end{equation}\\]Bayes’ rule continuous random variables \\[\\begin{equation}\nf_{Y|X}(y|x) = \\frac{f_{X|Y}(x|y) f_Y(y)}{f_X(x)}\n\\tag{5.14}\n\\end{equation}\\]law total probability \\[\\begin{equation}\nf_X(x) = \\int_{-\\infty}^\\infty f_{X|Y}(x|y) f_Y(y) dy.\n\\tag{5.15}\n\\end{equation}\\]Continuous random variables \\(X\\) \\(Y\\) independent values \\(x\\) \\(y\\):\\[\\begin{equation}\nF_{X,Y} (x,y) = F_X(x) F_Y(y)\n\\tag{5.16}\n\\end{equation}\\]\\[\\begin{equation}\nf_{X,Y} (x,y) = f_X(x) f_Y(y)\n\\tag{5.17}\n\\end{equation}\\]\\[\\begin{equation}\nf_{Y|X} (y|x) = f_Y(y)\n\\tag{5.18}\n\\end{equation}\\]\\[\\begin{equation}\nf_{X|Y} (x|y) = f_X(x).\n\\tag{5.19}\n\\end{equation}\\]","code":""},{"path":"joint-distributions.html","id":"covcorr","chapter":"5 Joint Distributions","heading":"5.4 Covariance and Correlation","text":"previous modules, used summaries mean, variance, skewness, kurtosis provide insight distribution single random variable. multiple random variables, one question random variables related . Summaries used quantify linear relationship two quantitative random variables covariance correlation.Generally speaking, two random variables positive covariance correlation increase decrease together, .e. \\(X\\) increases, \\(Y\\) also generally increases; \\(X\\) decreases, \\(Y\\) also generally decreases.Two random variables negative covariance correlation move opposite direction, .e. \\(X\\) increases, \\(Y\\) generally decreases; \\(X\\) decreases, \\(Y\\) generally increases. Figure 5.1 displays ideas visually scatter plots. scatter plot left shows example pair random variables positive covariance, scatter plot right shows example pair random variables negative covariance.\nFigure 5.1: Positive Covariance (Left), Negative Covariance (Right)\nOne thing note: covariance correlations can calculated random variables long quantitative, least one categorical. concept increasing random variable categorical make intuitive sense, example, random variable denotes color eyes, increasing color eyes mean?","code":""},{"path":"joint-distributions.html","id":"covariance","chapter":"5 Joint Distributions","heading":"5.4.1 Covariance","text":"now define covariance. covariance random variables \\(X\\) \\(Y\\) \\[\\begin{equation}\nCov(X,Y) = E\\left[(X- \\mu_X)(Y - \\mu_Y) \\right].\n\\tag{5.20}\n\\end{equation}\\]Looking equation (5.20), see \\(X\\) \\(Y\\) generally move direction, \\(X - \\mu_x\\) \\(Y - \\mu_y\\) either positive negative, therefore product positive, average. \\(X\\) \\(Y\\) generally move opposite directions, \\(X - \\mu_x\\) \\(Y - \\mu_y\\) opposite signs, therefore product negative, average.key properties covariance:\\(Cov(X,X) = Var(X)\\).covariance random variable variance.\\(Cov(X,Y) = Cov(Y,X)\\). covariance \\(X\\) \\(Y\\) covariance \\(Y\\) \\(X\\).\\(Cov(X,c) = 0\\) constant \\(c\\). Since constant move, relationship \\(X\\).\\(Cov(aX,Y) = Cov(X,Y)\\) constant \\(\\). implies covariance affected units \\(X\\) \\(Y\\).\\(Var(X + Y) = Var(X) + Var(Y) + 2 Cov(X,Y)\\).\\(X\\) \\(Y\\) independent, \\(Cov(X,Y) = 0\\).However, \\(Cov(X,Y) = 0\\) mean \\(X\\) \\(Y\\) independent. common misconception. Remember covariance measures linear relationship. relationship \\(X\\) \\(Y\\) non linear, instances, covariance used. Figure 5.2 provides example . figure, \\(X\\) \\(Y\\) quadratic relationship, clearly independent, yet covariance virtually 0.\nFigure 5.2: Covariance Non Linear Relationship\nSuppose two vectors observed data, size \\(n\\): \\(X = (x_1, x_2, \\cdots, x_n)\\) \\(Y = (y_1, y_2, \\cdots, y_n)\\). sample covariance \\[\\begin{equation}\ns_{x,y} = \\frac{\\sum_{=1}^n (x_i - \\bar{x})(y_i - \\bar{y})}{n-1}\n\\tag{5.21}\n\\end{equation}\\]noted earlier covariance affected units variables. Suppose one variable measured meters, convert become centimeters. value covariance get multiplied 100. People find easier interpret measure depend units. correlation comes : unitless version covariance.View video visual explanation sample covariance positive linear relationship positive:","code":"\nx<-seq(-1,1, by=0.01)\ny<-x^2\n\n##note from plot that X & Y do not have a linear relationship\nplot(x,y, xlab=\"X\", ylab=\"Y\")\ncov(x,y) ##covariance is virtually 0## [1] 1.19967e-17"},{"path":"joint-distributions.html","id":"corr","chapter":"5 Joint Distributions","heading":"5.4.2 Correlation","text":"correlation random variables \\(X\\) \\(Y\\) \\[\\begin{equation}\n\\rho = Corr(X,Y) = \\frac{Cov(X,Y)}{\\sqrt{Var(X) Var(Y)}}.\n\\tag{5.22}\n\\end{equation}\\]sample correlation two vectors observed data, size \\(n\\): \\(X = (x_1, x_2, \\cdots, x_n)\\) \\(Y = (y_1, y_2, \\cdots, y_n)\\), \\[\\begin{equation}\nr = \\frac{\\sum_{=1}^n (x_i - \\bar{x})(y_i - \\bar{y})}{\\sqrt{\\sum_{=1}^n (x_i - \\bar{x})^2 \\sum_{=1}^n (y_i - \\bar{y})^2}}\n\\tag{5.23}\n\\end{equation}\\]key properties correlation:bounded -1 1.Values closer -1 1 indicate stronger linear relationship.Values closer 0 indicate weaker linear relationship.numerical value unchanged location / scale changes.\\(X\\) \\(Y\\) independent, \\(Corr(X,Y) = 0\\).However, \\(Corr(X,Y) = 0\\) mean \\(X\\) \\(Y\\) independent.Correlation used relationship \\(X\\) \\(Y\\) linear.Figure 5.3 shows examples scatterplots sample correlations. left plot, data points fall close straight line, negative, correlation close -1. middle plot linear relationship, see trend one variable increasing decreasing variable increases. right plot shows data points fairly close straight line, close left plot), correlation close 1 (-1).\nFigure 5.3: Strong Negative Correlation (Left), Correlation (Middle), Moderate Positive Correlation (Right)\n","code":""},{"path":"joint-distributions.html","id":"condexp","chapter":"5 Joint Distributions","heading":"5.5 Conditional Expectation","text":"Section 2.3 5.2.2, explored notion conditional probabilities conditional distributions, used :Updating probability distribution random variable \\(Y\\), observing certain outcome another random variable \\(X\\), orRestricting probability distribution random variable \\(Y\\) certain value another random variable.represent Bayesian frequentist viewpoints conditional probability conditional distribution.turns similar idea applies expected value random variable. Recall expected value random variable long-run average, words, average value observing random variable infinite number times.conditional expectation random variable long run-average:observing certain outcome another variable event, orafter restricting attention cases another random variable fixed equal specific value.Fairly often, use statistical models predict response variable \\(Y\\) based predictor \\(X\\). Predictions values \\(Y\\) based observed values \\(X\\) usually use conditional expectation \\(Y\\) given \\(X\\). Given see predictor, long run average \\(Y\\) ends used predicted value response variable. basis statistical models.two slightly different notions conditional expectations:Conditional expectation random variable \\(Y\\) given event \\(\\). \\(\\) happened, expected value \\(Y\\)?Conditional expectation random variable \\(Y\\) given another random variable \\(X\\). fix value random variable \\(X\\) value support, expected value \\(Y\\)?second notion one usually used statistical models, cover first notion easier understand, help us understand second notion.","code":""},{"path":"joint-distributions.html","id":"conditional-expectation-given-event","chapter":"5 Joint Distributions","heading":"5.5.1 Conditional Expectation Given Event","text":"Recall expectation \\(E(Y)\\) random variable \\(Y\\) long-run average. \\(Y\\) discrete, take weighted average involving probabilities PMF \\(P(Y=y)\\). calculation conditional expectation \\(E(Y|)\\) \\(\\) event occurred simply replaces probabilities \\(P(Y=y)\\) conditional probabilities \\(P(Y=y|)\\). Therefore, discrete random variable \\(Y\\),\\[\\begin{equation}\nE(Y|) = \\sum_y y P(Y=y|)\n\\tag{5.24}\n\\end{equation}\\]sum support \\(Y\\). Notice summing product support corresponding conditional probability, whereas find \\(E(Y)\\), sum product support corresponding unconditional probability.\\(Y\\) continuous, use conditional PDF instead:\\[\\begin{equation}\nE(Y|) = \\int_{-\\infty}^{\\infty} y f(y|) dy.\n\\tag{5.25}\n\\end{equation}\\]key understand intuition behind conditional expectations, use simulation approximate (approximation works better use simulated data). Simulation represents frequentist viewpoint conditional expectation. code following:Generate 100 values \\(X\\) uniformly support \\(\\{1,2,3,4\\}\\).Simulate \\(Y\\) using \\(Y = 10 + X + \\epsilon\\) \\(\\epsilon \\sim N(0,1)\\).Represent values scatter plot, also overlay line represents sample mean \\(Y\\), estimates \\(E(Y)\\). simply average value y-axis 100 data points. plot left Figure 5.4 .Represent values scatter plot, use blue denote event \\(\\) \\(X=1\\). line represents sample mean \\(Y\\), blue data points (.e. \\(X=1\\)), overlaid. value estimates \\(E(Y|)\\) \\(E(Y|X=1)\\). plot right Figure 5.4 . calculating sample mean, completed disregarded black data points \\(X\\) 1.\nFigure 5.4: Comparison E(Y) E(Y|X=1)\n, can interpret conditional expectation \\(E(Y|)\\) long-run average \\(Y\\) () \\(\\) happened. long-run average \\(Y\\) certain condition met.View video detailed explanation simulation:","code":"\nset.seed(40)\nn<-100 ##100 data points\n\n##generate X\nx<-c(rep(1,n/4), rep(2,n/4), rep(3,n/4), rep(4,n/4)) \n\n##simulate Y\ny<- 10 + x + rnorm(n)\n\npar(mfrow=c(1,2))\nplot(x,y, main=\"Estimated E(Y) Overlaid\")\nabline(h=mean(y)) ##add line to represent est E(Y)\n\nplot(x,y, col = ifelse(x == 1,'blue', 'black'), pch = 19, main=\"Estimated E(Y|X=1) Overlaid\" )\nabline(h=mean(y[x=1]), col=\"blue\") ##add line to represent est E(Y|X=1)"},{"path":"joint-distributions.html","id":"conditional-expectation-given-random-variable","chapter":"5 Joint Distributions","heading":"5.5.2 Conditional Expectation Given Random Variable","text":"conditional expectation \\(Y\\) given random variable \\(X\\) slightly different. simulated example previous subsection, set \\(X\\) specific value. Now, consider long-run average \\(Y\\) value, instead specific value, support \\(X\\).One way think consider \\(E(Y|X=x)\\), \\(x\\) value support \\(X\\). \\(Y\\) discrete, conditional expectation :\\[\\begin{equation}\nE(Y|) = \\sum_y y P(Y=y|X=x)\n\\tag{5.26}\n\\end{equation}\\]sum support \\(Y\\).\\(Y\\) continuous:\\[\\begin{equation}\nE(Y|) = \\int_{-\\infty}^{\\infty} y f(y|x) dy.\n\\tag{5.27}\n\\end{equation}\\]go back simulated example previous subsection explain \\(E(Y|X=x)\\) represents. Recall support \\(X\\) \\(\\{1,2,3,4\\}\\) \\(Y = 10 + X + \\epsilon\\) \\(\\epsilon \\sim N(0,1)\\). \\[\n\\begin{split}\nE(Y|X=x) &= E(10 + X + \\epsilon | X=x)\\\\\n&= E(10 + x + \\epsilon) \\\\\n&= E(10) + E(x) + E(\\epsilon) \\\\\n&= 10 + x + 0 \\\\\n&= 10 + x.\n\\end{split}\n\\]brief explanation step:go line 1 line 2, subbed \\(x\\) \\(X\\), since setting \\(X=x\\).go line 2 line 3, apply linearity expectations.go line 3 4, \\(E(c)=c\\) constant. case, fixing \\(x\\) value support constant, \\(E(\\epsilon) = 0\\) since \\(\\epsilon \\sim N(0,1)\\).\\(E(Y|X) = 10 + X\\). means :\\(X=1\\), \\(E(Y|X=1) = 11\\),\\(X=2\\), \\(E(Y|X=1) = 12\\),\\(X=3\\), \\(E(Y|X=1) = 13\\), andWhen \\(X=4\\), \\(E(Y|X=1) = 14\\).Note: set \\(Y = 10 + X + \\epsilon\\) \\(\\epsilon \\sim N(0,1)\\) simulation. follows framework linear regression sets \\(Y = \\beta_0 + \\beta_1 X + \\epsilon\\) \\(\\epsilon \\sim N(0,\\sigma^2)\\), .e. \\(\\epsilon\\) normal mean 0 variance fixed value. conditional expectation given \\(X\\) ends prediction \\(Y\\) minimizes mean squared error linear regression.View video detailed explanation example:","code":""},{"path":"joint-distributions.html","id":"commMultDist","chapter":"5 Joint Distributions","heading":"5.6 Common Multivariate Distributions","text":"now cover two common multivariate distributions: multinomial distribution multivariate normal distribution discrete continuous random variables respectively.","code":""},{"path":"joint-distributions.html","id":"multinomial","chapter":"5 Joint Distributions","heading":"5.6.1 Multinomial","text":"multinomial distribution can viewed generalization binomial distribution higher dimensions. Recall binomial distribution, carry \\(n\\) trials, trial record whether success failure, words, two outcomes trial. multinomial distribution differs can two outcomes trial. example, randomly select \\(n\\) adults ask political affiliation. affiliation Democrat, Republican, party, affiliation, four possible outcomes categories person.set multinomial distribution follows:\\(n\\) independent trials, trial belongs one \\(k\\) categories.trial belongs category \\(j\\) probability \\(p_j\\), \\(p_j\\) non negative \\(\\sum_{j=1}^k p_j = 1\\), .e. sum one.Let \\(X_1\\) denote number trials belonging category 1, \\(X_2\\) denote number trials belonging category 2, . \\(X_1 + \\cdots X_k = n\\).say \\(\\boldsymbol{X} = (X_1, \\cdots, X_k)\\) said multinomial distribution parameters \\(n\\) \\(\\boldsymbol{p} = (p_1, \\cdots, p_k)\\). can written \\(\\boldsymbol{X} \\sim Mult_k(n, \\boldsymbol{p})\\).Note vectors \\(\\boldsymbol{X}\\) \\(\\boldsymbol{p}\\) written bold. Vectors matrices commonly written using bold distinguish scalars, bold. \\(\\boldsymbol{X}\\) example call random vector, vector random variables \\(X_1, \\cdots, X_k\\).\\(\\boldsymbol{X} \\sim Mult_k(n, \\boldsymbol{p})\\), PMF \\[\\begin{equation}\nP(X_1 = n_1, \\cdots, X_k = n_k) = \\frac{n!}{n_{1}! \\cdots n_{k}!} p_1^{n_1} \\cdots p_k^{n_k},\n\\tag{5.28}\n\\end{equation}\\]\\(n_1 + \\cdots + n_k = n\\).Let us use toy example. Going back political affiliations. Suppose among American voters, 28% identify Democrats, 29% identify Republicans, 10% identify affiliations, 33% independents. Let \\(X_1, X_2, X_3, X_4\\) denote number Democrats, Republicans, others, independents. joint distribution \\(X_1, X_2, X_3, X_4\\) \\(\\boldsymbol{X} = (X_1, X_2, X_3, X_4) \\sim Mult_4(0.28, 0.29, 0.1, 0.33)\\).Suppose want find probability sample 10 voters, 2 Democrats, 3 Republicans, 1 another affiliation, 4 Independents:\\[\n\\begin{split}\nP(X_1 = 2, X_2 = 3, X_3 = 1, X_4 = 4) &= \\frac{10!}{2! 3! 1!4!} 0.28^{2} 0.29^{3} 0.1^1 0.33^4\\\\\n&= 0.02857172.\n\\end{split}\n\\]use","code":"\ndmultinom(c(2,3,1,4), prob=c(0.28,0.29,0.1,0.33)) ##specify X1, X2, X3, X4, then p1,p2,p3, p4## [1] 0.02857172"},{"path":"joint-distributions.html","id":"multinomial-marginals","chapter":"5 Joint Distributions","heading":"5.6.1.1 Multinomial Marginals","text":"marginals multinomial binomial. \\(\\boldsymbol{X} \\sim Mult_k(n, \\boldsymbol{p})\\), \\(X_j \\sim Bin(n, p_j)\\).Going back toy example American voters, means \\(X_1 \\sim Bin(n,0.28), X_2 \\sim Bin(n,0.29), X_3 \\sim Bin(n, 0.1), X_4 \\sim Bin(n,0.33)\\). Hopefully example makes sense. look \\(X_1,\\) looking number voters democrats . proportion Democrats still remains , proportion Republicans, affiliations, independents sum individual proportions, 1 minus proportion Democrats.","code":""},{"path":"joint-distributions.html","id":"multinomial-lumping","chapter":"5 Joint Distributions","heading":"5.6.1.2 Multinomial Lumping","text":"discrete categorical variables, can common want lump (merge, collapse, combine) categories together. \\(\\boldsymbol{X} \\sim Mult_k(n, \\boldsymbol{p})\\), \\(X_i + X_j \\sim Bin(n, p_i + p_j)\\). decide merge categories 1 2, \\((X_1 + X_2, X_3, \\cdots, X_k) \\sim Mult_{k-1}(n, (p_1 + p_2, p_3, \\cdots, p_k))\\).go back toy example. Suppose consider Democrats Republicans major parties, may wish combine everyone else one category: affiliations independents. can define using new random variable \\(\\boldsymbol{Y} = (X_1, X_2, X_3+X_4) \\sim Mult_3(n,(0.29,0.29,0.43)\\). Note now 3 categories instead 4. proportion lumped category sum individual proportions.","code":""},{"path":"joint-distributions.html","id":"multinomial-covariance","chapter":"5 Joint Distributions","heading":"5.6.1.3 Multinomial Covariance","text":"\\(\\boldsymbol{X} \\sim Mult_k(n, \\boldsymbol{p})\\) \\(\\boldsymbol{p} = (p_1,p_2, \\cdots, p_k)\\). covariance two distinct components \\(X_i\\) \\(X_j\\) \\[\\begin{equation}\nCov(X_i, X_j) = -n p_i p_j,\n\\tag{5.29}\n\\end{equation}\\]\\(\\neq j\\). book provides nice proof, Theorem 7.4.6, interested.Looking (5.29), notice covariance two distinct components negative (since probabilities non negative). means numerical values \\(X_i\\) \\(X_j\\) go opposite directions. make intuitive sense since \\(n = X_1 + \\cdots + X_k\\) fixed, \\(X_i\\) large, \\(X_j\\) small since \\(n\\) fixed. extreme example \\(X_i = n\\), \\(X_j\\) must 0.go back toy example. Suppose want find correlation \\(X_1\\) \\(X_2\\), number Democrats Republicans sample size \\(n\\). Note \\(X_1 \\sim Bin(n,0.28), X_2 \\sim Bin(n,0.29)\\), \\(Cov(X_1,X_2) = -n \\times 0.28 \\times 0.29 = 0.0812n\\),\\[\n\\begin{split}\nCorr(X_1,X_2) &= \\frac{Cov(X_1,X_2)}{\\sqrt{Var(X_1) Var(X_2)}}\\\\\n&= \\frac{-n p_1 p_2}{\\sqrt{n p_1 (1-p_1) n p_2 (1-p_2)}} \\\\\n&= -\\sqrt{\\frac{p_1 p_2}{(1-p_1)(1-p_2)}} \\\\\n&= -\\sqrt{\\frac{0.28 \\times 0.29}{(1-0.28)(1-0.29)}} \\\\\n&= -0.3985498.\n\\end{split}\n\\]","code":""},{"path":"joint-distributions.html","id":"conditional-multinomial","chapter":"5 Joint Distributions","heading":"5.6.1.4 Conditional Multinomial","text":"Sometimes, observed data multinomial distribution, wish update distribution. Suppose \\(\\boldsymbol{X} \\sim Mult_k(n, \\boldsymbol{p})\\), observed \\(X_1 = n_1\\), \\((X_2, \\cdots, X_k)|X_1 = n_1 \\sim Mult_{k-1}(n-n_1, (p_2^{\\prime}, \\cdots, p_k^{\\prime}))\\) \\(p_j^{\\prime} = \\frac{p_j}{p_2 + \\cdots + p_k}\\).","code":""},{"path":"joint-distributions.html","id":"multivariate-normal","chapter":"5 Joint Distributions","heading":"5.6.2 Multivariate Normal","text":"multivariate normal (MVN) distribution can viewed generalization normal distribution higher dimensions. Just like univariate normal distribution, central limit theorem also applies higher dimensions.\\(k\\)-dimensional random vector \\(\\boldsymbol{X} = (X_1, \\cdots, X_k)\\) said MVN distribution every linear combination \\(X_j\\) normal. means \\(t_1 X_1 + \\cdots + t_k X_k\\) normally distributed constants \\(t_1, \\cdots, t_k\\). \\(k=2\\), MVN often called bivariate normal.Section 4.5.2.2, mentioned parameters normal distribution mean \\(\\mu\\) variance \\(\\sigma^2\\). idea generalized MVN \\(\\boldsymbol{X} = (X_1, \\cdots, X_k)\\). parameters :mean vector \\((\\mu_1, \\cdots, \\mu_k)\\) \\(\\mu_j = E(X_j)\\). vector length \\(k\\) entry expected value component.mean vector \\((\\mu_1, \\cdots, \\mu_k)\\) \\(\\mu_j = E(X_j)\\). vector length \\(k\\) entry expected value component.covariance matrix. \\(k \\times k\\) matrix \\((,j)\\)th entry (.e. row \\(\\), column \\(j\\)) covariance \\(X_i\\) \\(X_j\\). implies diagonal entries give variance component (since \\(Cov(X_i, X_i) = Var(X_i)\\)), covariance matrix symmetric (since \\(Cov(X_i, X_j) = Cov(X_j, X_i)\\)).covariance matrix. \\(k \\times k\\) matrix \\((,j)\\)th entry (.e. row \\(\\), column \\(j\\)) covariance \\(X_i\\) \\(X_j\\). implies diagonal entries give variance component (since \\(Cov(X_i, X_i) = Var(X_i)\\)), covariance matrix symmetric (since \\(Cov(X_i, X_j) = Cov(X_j, X_i)\\)).example, suppose \\(\\boldsymbol{X} = (X_1, X_2, X_3)\\) MVN mean vector \\((5, 2, 8)\\) covariance matrix\\[\n\\begin{pmatrix}\n3 & 1.5 & 2.5\\\\\n1.5 & 2 & 4.2 \\\\\n2.5 & 4.2 & 1\n\\end{pmatrix},\n\\]\\(E(X_1) = 5, E(X_2) = 2, E(X_3) = 8\\),\\(Var(X_1) = 3, Var(X_2) = 2, Var(X_3) = 1\\),\\(Cov(X_1, X_2) = Cov(X_2, X_1) = 1.5\\),\\(Cov(X_1, X_3) = Cov(X_3, X_1) = 2.5\\), \\(Cov(X_2, X_3) = Cov(X_3, X_2) = 4.2\\).properties MVN distribution:\\(\\boldsymbol{X} = (X_1, \\cdots, X_k)\\) MVN, marginal distribution \\(X_j\\) normal, can set \\(t_j =1\\) constants 0.\\(\\boldsymbol{X} = (X_1, \\cdots, X_k)\\) MVN, marginal distribution \\(X_j\\) normal, can set \\(t_j =1\\) constants 0.However, converse necessarily true. \\(X_1, \\cdots, X_k\\) normal, \\((X_1, \\cdots, X_k)\\) necessarily MVN.However, converse necessarily true. \\(X_1, \\cdots, X_k\\) normal, \\((X_1, \\cdots, X_k)\\) necessarily MVN.\\((X_1, \\cdots, X_k)\\) MVN, subvector, e.g. \\((X_i, X_j)\\) bivariate normal.\\((X_1, \\cdots, X_k)\\) MVN, subvector, e.g. \\((X_i, X_j)\\) bivariate normal.\\(\\boldsymbol{X} = (X_1, \\cdots, X_k)\\) \\(\\boldsymbol{Y} = (Y_1, \\cdots, Y_m)\\) MVN \\(\\boldsymbol{X}\\) independent \\(\\boldsymbol{Y}\\), \\(\\boldsymbol{W} = (X_1, \\cdots, X_k, Y_1, \\cdots, Y_m)\\) MVN.\\(\\boldsymbol{X} = (X_1, \\cdots, X_k)\\) \\(\\boldsymbol{Y} = (Y_1, \\cdots, Y_m)\\) MVN \\(\\boldsymbol{X}\\) independent \\(\\boldsymbol{Y}\\), \\(\\boldsymbol{W} = (X_1, \\cdots, X_k, Y_1, \\cdots, Y_m)\\) MVN.Within MVN random vector, uncorrelated implies independence. \\(\\boldsymbol{X}\\) MVN \\(\\boldsymbol{X} = (\\boldsymbol{X_1, X_2})\\) \\(\\boldsymbol{X_1}\\) \\(\\boldsymbol{X_2}\\) subvectors, every component \\(\\boldsymbol{X_1}\\) uncorrelated every component \\(\\boldsymbol{X_2}\\), \\(\\boldsymbol{X_1}\\) \\(\\boldsymbol{X_2}\\) independent.Within MVN random vector, uncorrelated implies independence. \\(\\boldsymbol{X}\\) MVN \\(\\boldsymbol{X} = (\\boldsymbol{X_1, X_2})\\) \\(\\boldsymbol{X_1}\\) \\(\\boldsymbol{X_2}\\) subvectors, every component \\(\\boldsymbol{X_1}\\) uncorrelated every component \\(\\boldsymbol{X_2}\\), \\(\\boldsymbol{X_1}\\) \\(\\boldsymbol{X_2}\\) independent.","code":""},{"path":"joint-distributions.html","id":"SW1","chapter":"5 Joint Distributions","heading":"5.6.2.1 Simulations","text":"can use simulations verify first property. simulation, following:Simulate 5000 draws MVN distribution mean vector \\((1,2,5)\\) covariance matrix\\[\n\\begin{pmatrix}\n1 & 0.5 & 0.6\\\\\n0.5 & 2 & 0.2 \\\\\n0.6 & 0.2 & 4\n\\end{pmatrix}.\n\\]Assess component \\(X_1, X_2, X_3\\) normally distributed using Shapiro-Wilk test normality.\nnull hypothesis variable follows normal distribution, alternative hypothesis variable follow normal distribution.\nrejecting null hypothesis means variable inconsistent normal distribution, rejecting means evidence variable inconsistent normal distribution.\nrecord p-value test \\(X_1, X_2, X_3\\).\nAssess component \\(X_1, X_2, X_3\\) normally distributed using Shapiro-Wilk test normality.null hypothesis variable follows normal distribution, alternative hypothesis variable follow normal distribution.rejecting null hypothesis means variable inconsistent normal distribution, rejecting means evidence variable inconsistent normal distribution.record p-value test \\(X_1, X_2, X_3\\).Repeat previous 2 steps total 10 thousand reps.Repeat previous 2 steps total 10 thousand reps.Count proportion reps Shapiro-Wilk test rejected null hypothesis significance level 0.05 \\(X_1, X_2, X_3\\).\nproperty correct, expect close 5% p-values (wrongly) reject null hypothesis, since tests conducted 0.05 significance level.\nCount proportion reps Shapiro-Wilk test rejected null hypothesis significance level 0.05 \\(X_1, X_2, X_3\\).property correct, expect close 5% p-values (wrongly) reject null hypothesis, since tests conducted 0.05 significance level.Since close 5% hypothesis test rejected null hypothesis, appears component consistent normal distribution. (accurately, evidence say component normal.) appear \\(\\boldsymbol{X} = (X_1, \\cdots, X_k)\\) MVN, marginal distribution \\(X_j\\) normal. simulation provide evidence property.Note: done called Monte Carlo simulation, often used research verify theorems. may involved research, writing code run simulations good way understand theorems applied. cover Monte Carlo simulations detail later module.","code":"\nlibrary(mvtnorm) ##package to simulate from MVN\n\nreps<-1000 ## how many reps\npvalsx1<-pvalsx2<-pvalsx3<-array(0,reps) ##initialize an array to store the pvalues from each test at each rep\nsiglevel<-0.05 ##sig level\nn<-5000 ##number of draws for each rep\n\nmu_vector<-c(1,2,5) ##mean vector\n\n##set up covariance matrix\nsig12<-0.5\nsig13<-0.6\nsig23<-0.2\ncov_mat<-matrix(c(1,sig12,sig13,sig12,2,sig23,sig13,sig23,4), nrow=3, ncol=3)\n\n##set.seed so you can replicate my result.\nset.seed(30)\n\n##run steps 1 and 2 for 10 000 times\nfor (i in 1:reps)\n  \n{\n\n  data<-rmvnorm(n, mu_vector, cov_mat)\n  \n  x1<-data[,1] ##extract X1\n  x2<-data[,2] ##extract X2\n  x3<-data[,3] ##extract X3\n  \n  ##store pvalue from Shapiro-Wilk test from each component\n  pvalsx1[i]<-shapiro.test(x1)$p.value \n  pvalsx2[i]<-shapiro.test(x2)$p.value\n  pvalsx3[i]<-shapiro.test(x3)$p.value \n  \n}\n\n##proportion of tests that wrongly reject the null\nsum(pvalsx1<siglevel)/reps ##for X1## [1] 0.054\nsum(pvalsx2<siglevel)/reps ##for X2## [1] 0.037\nsum(pvalsx3<siglevel)/reps ##for X3## [1] 0.047"},{"path":"inequalities-limit-theorems-and-simulations.html","id":"inequalities-limit-theorems-and-simulations","chapter":"6 Inequalities, Limit Theorems, and Simulations","heading":"6 Inequalities, Limit Theorems, and Simulations","text":"module based Introduction Probability (Blitzstein, Hwang), Chapter 10. can access book free https://stat110.hsites.harvard.edu/ (click Book). Please note cover additional topics, skip certain topics book. may skip Example 10.1.3, 10.1.4, 10.1.7 10.1.9, Theorem 10.1.12, Example 10.2.5, 10.2.6, 10.3.7, Section 10.4 book.","code":""},{"path":"inequalities-limit-theorems-and-simulations.html","id":"introduction-2","chapter":"6 Inequalities, Limit Theorems, and Simulations","heading":"6.1 Introduction","text":"can difficult calculate probabilities expected values, example, PDF distribution unknown, integral difficult work . may notice used simulations approximate probabilities expected values examples previous modules. improvement computing capabilities, simulations can now performed faster tool used . tools calculate difficult probabilities expected values include using inequalities bound probabilities (e.g. probability greater less certain value), approximating using known theorems. ’ll look three tools module.","code":""},{"path":"inequalities-limit-theorems-and-simulations.html","id":"module-roadmap-4","chapter":"6 Inequalities, Limit Theorems, and Simulations","heading":"6.1.1 Module Roadmap","text":"Section 6.2 covers 3 famous inequalities used probability (Cauchy-Schwartz, Jensen, Chebyshev), well implications.Section 6.3 covers 2 consequential theorems probability, Law Large Numbers Central Limit Theorem. invoking 2 theorems number remaining modules .Section 6.4 covers Monte Carlo simulations, computational tool used estimate probabilities, especially may difficult derive “hand”. using Monte Carlo simulations extensively rest class.","code":""},{"path":"inequalities-limit-theorems-and-simulations.html","id":"inequalities","chapter":"6 Inequalities, Limit Theorems, and Simulations","heading":"6.2 Inequalities","text":"probability expected value difficult calculate, may easier find bound via inequality. usually means can guarantee certain probability expected value within certain range values, narrows possible values exact answer. example, instead able calculate probability certain event, may able show probability 0.1, know event unlikely happen. cover couple well-known inequalities probability.","code":""},{"path":"inequalities-limit-theorems-and-simulations.html","id":"cauchy-schwartz-inequality","chapter":"6 Inequalities, Limit Theorems, and Simulations","heading":"6.2.1 Cauchy-Schwartz Inequality","text":"Cauchy-Schwarz inequality one famous inequalities mathematics many applications. context probability, written : random variables \\(X\\) \\(Y\\) finite variances\\[\\begin{equation}\n|E(XY)| \\leq \\sqrt{E(X^2)E(Y^2)}.\n\\tag{6.1}\n\\end{equation}\\]Next, use Cauchy-Schwartz inequality prove couple properties stated earlier modules:Cauchy-Schwartz inequality can used show correlation two random variables finite variances must -1 1. quick proof follows: apply equation (6.1) centered random variables \\(X - \\mu_X\\) \\(Y - \\mu_Y\\):\\[\n\\begin{split}\n|E[(X - \\mu_X)(Y - \\mu_Y)]| & \\leq \\sqrt{E[(X - \\mu_X)^2] E[(Y - \\mu_Y)^2]} \\\\\n\\implies |Cov(X,Y)| & \\leq \\sqrt{Var(X) Var(Y)} \\\\\n\\implies |Corr(X,Y)| & \\leq 1.\n\\end{split}\n\\]View video detailed explanation proof:Cauchy-Schwarz inequality can also used show variance random variable non negative. quick proof follows: apply equation (6.1) random variable \\(X\\) constant 1:\\[\n\\begin{split}\n|E(X)| & \\leq \\sqrt{E(X^2)E(1^2)}. \\\\\n\\implies |E(X)| & \\leq \\sqrt{E(X^2)} \\\\\n\\implies E(X)^2 & \\leq E(X^2) \\\\\n\\implies 0 & \\leq E(X^2) - E(X)^2 = Var(X).\n\\end{split}\n\\]\nView video detailed explanation proof:Note: One place may seen Cauchy-Schwarz inequality proof triangle inequality geometry.","code":""},{"path":"inequalities-limit-theorems-and-simulations.html","id":"jensens-inequality","chapter":"6 Inequalities, Limit Theorems, and Simulations","heading":"6.2.2 Jensen’s Inequality","text":"may noticed previous modules, written transforming random variable. One way transforming random variable scale change, words, value random variable multiplied constant. can happen change units variable. example want convert random variable based weight kilograms pounds. \\(X\\) \\(Y\\) denote weight kilograms pounds respectively, can write \\(Y = 2.2X\\). know expected value \\(X\\), can easily find expected value \\(Y\\) multiplying \\(E(X)\\) 2.2. fairly intuitive based linearity expectations using equation (3.3).stating Jensen’s inequality, cover couple concepts: linear vs non linear transformations, convex vs concave functions.","code":""},{"path":"inequalities-limit-theorems-and-simulations.html","id":"linnonlin","chapter":"6 Inequalities, Limit Theorems, and Simulations","heading":"6.2.2.1 Linear and Non Linear Transformations","text":"way think transformations write \\(Y = g(X)\\), \\(g\\) function describes transformation. kilograms pounds example, \\(g\\) exactly 2.2, \\(Y = 2.2X\\). transformation linear transformation since graph \\(Y = 2.2X\\) straight line. example, \\(E(Y) = E(2.2X) = 2.2E(X)\\).use non linear transformation? popular non linear transformation log transformation. used random variable right skewed (happens pretty often real data, wages, since people make really high wages vast majority people wages lower end). Expected values often used statistical models predictions; however, know mean may best measure centrality skewed data. One way transform right skewed data become less skewed log transform data. example, \\(Y = \\log(X)\\), \\(g(x) = \\log(x)\\). know expected value original variable, \\(E(X)\\), can easily find expected value \\(Y\\)? Can write \\(E(Y) = E(\\log(X)) = \\log E(X)\\)? actually incorrect. turns operations work non linear transformations, .e. \\(g\\) non linear, \\(E(g(X))\\) necessarily equal \\(g(E(X))\\). log transformation linear since graph \\(Y = \\log(X)\\) straight line.Let us use toy example show . Suppose roll fair six-sided die, let \\(X\\) denote number dots die shows. game, get win money based result roll, specifically twice result. Let \\(D\\) denote winnings game, \\(D = 2X\\). Since know \\(E(X) = 3.5\\), means expected winnings game \\(E(D) = E(2X) = 2E(X) = 7\\), since linear transformation . code verifies :Now suppose winnings now defined squared number dots die shows. Let \\(T\\) denote new winnings, \\(T = X^2\\). Since non linear transformation, \\(E(T) = E(X^2)\\) may equal \\(E(X)^2\\):example, see \\(E(T) > E(X)^2\\), words, \\(E(g(X)) > g(E(X))\\), \\(g(x) = x^2\\). \\(E(g(X)) > g(E(X))\\) always non linear function \\(g\\)? turns always case.summarize:\\(g\\) linear, \\(E(g(X)) = g(E(X))\\), can use linearity expectations.\\(g\\) non linear, \\(E(g(X)) \\neq g(E(X))\\).","code":"\nX<-c(1,2,3,4,5,6) ##support for X\n\nD<-2* X ##winnings\n\nmean(X) ##EX since die is fair## [1] 3.5\nmean(D) ##Expected winnings. This is equal to 2 times mean(X)## [1] 7\nX<-c(1,2,3,4,5,6) ##support for X\n\nT<-X^2 ##winnings\n\nmean(T) ##Expected winnings. ## [1] 15.16667\nmean(X)^2 ##not equal## [1] 12.25"},{"path":"inequalities-limit-theorems-and-simulations.html","id":"convex-and-concave-functions","chapter":"6 Inequalities, Limit Theorems, and Simulations","heading":"6.2.2.2 Convex and Concave Functions","text":"example , instance \\(E(g(X)) \\neq g(E(X))\\). direction inequality depends whether function \\(g\\) convex concave. couple ways decide function convex concave:Using derivatives:\nfunction \\(g(x)\\) convex second derivative non negative, .e. \\(g^{\\prime \\prime}(x) \\geq 0\\) domain. domain set values \\(x\\) \\(g(x)\\) defined.\nfunction \\(g(x)\\) concave second derivative non positive, .e. \\(g^{\\prime \\prime}(x) \\leq 0\\) domain.\nfunction \\(g(x)\\) convex second derivative non negative, .e. \\(g^{\\prime \\prime}(x) \\geq 0\\) domain. domain set values \\(x\\) \\(g(x)\\) defined.function \\(g(x)\\) concave second derivative non positive, .e. \\(g^{\\prime \\prime}(x) \\leq 0\\) domain.Using visuals:\nfunction \\(g(x)\\) convex every line segment joining two points graph never graph.\nfunction \\(g(x)\\) concave every line segment joining two points graph never graph.\nfunction \\(g(x)\\) convex every line segment joining two points graph never graph.function \\(g(x)\\) concave every line segment joining two points graph never graph.now look couple functions see convex concave:\\(g(x) = \\log(x)\\) concave function.\nsecond derivative \\(g^{\\prime \\prime}(x) = -\\frac{1}{x^2}\\). Note domain \\(\\log(x)\\) positive real numbers (undefined \\(x \\leq 0\\)), second derivative always negative.\ncan also look graph \\(y = \\log(x)\\), draw line segments join two points graph. lines never graph. Figure 6.1 shows example one line segment, can see line segment joins two points graph never graph.\nsecond derivative \\(g^{\\prime \\prime}(x) = -\\frac{1}{x^2}\\). Note domain \\(\\log(x)\\) positive real numbers (undefined \\(x \\leq 0\\)), second derivative always negative.can also look graph \\(y = \\log(x)\\), draw line segments join two points graph. lines never graph. Figure 6.1 shows example one line segment, can see line segment joins two points graph never graph.\nFigure 6.1: Example Concave Function\n\\(g(x) = x^2\\) convex function.\nsecond derivative \\(g^{\\prime \\prime}(x) = 2\\), always positive.\ncan also look graph \\(y = x^2\\), draw line segments join two points graph. lines never graph. Figure 6.2 shows example one line segment, can see line segment joins two points graph never graph.\nsecond derivative \\(g^{\\prime \\prime}(x) = 2\\), always positive.can also look graph \\(y = x^2\\), draw line segments join two points graph. lines never graph. Figure 6.2 shows example one line segment, can see line segment joins two points graph never graph.\nFigure 6.2: Example Convex Function\nThought question: Consider function \\(g(x) = \\frac{1}{x}\\), .e. inverse function. Can explain function convex \\(x>0\\) concave \\(x<0\\)?","code":""},{"path":"inequalities-limit-theorems-and-simulations.html","id":"jensens-inequality-1","chapter":"6 Inequalities, Limit Theorems, and Simulations","heading":"6.2.2.3 Jensen’s Inequality","text":"now ready state Jensen’s inequality. Let \\(X\\) denote random variable. \\(g\\) convex, \\(E(g(X)) \\geq g(E(X))\\). \\(g\\) concave, \\(E(g(X)) \\leq g(E(X))\\).equality holds \\(g\\) linear function. turns linear functions convex concave. book goes simple proof Jensen’s inequality worth reading. Next, apply Jensen’s inequality examples:apply Jensen’s inequality toy example Section 6.2.2.1. reminder, suppose roll fair six-sided die, let \\(X\\) denote number dots die shows. winnings defined squared number dots die shows. Let \\(T\\) denote new winnings, \\(T = X^2\\), \\(g(x) = x^2\\) function representing non linear transformation. established quadratic function convex, Jensen’s inequality tells us \\(E(g(X)) \\geq g(E(X))\\), .e. \\(E(T) > E(X)^2\\) showed code.apply Jensen’s inequality toy example Section 6.2.2.1. reminder, suppose roll fair six-sided die, let \\(X\\) denote number dots die shows. winnings defined squared number dots die shows. Let \\(T\\) denote new winnings, \\(T = X^2\\), \\(g(x) = x^2\\) function representing non linear transformation. established quadratic function convex, Jensen’s inequality tells us \\(E(g(X)) \\geq g(E(X))\\), .e. \\(E(T) > E(X)^2\\) showed code.mentioned Section 6.2.2.1, log transformation often applied make data right skewed less skewed, popular methods linear regression, tree based methods, \\(K\\) nearest neighbors can used (methods can sensitive outliers since based conditional expectations conditional means). often happens log transformation applied variable interest, model fit, prediction made log transformed variable using conditional expectations, exponential applied predicted value convert back original variable. Jensen’s inequality tells us exponential average log variable greater average variable, model estimates.mentioned Section 6.2.2.1, log transformation often applied make data right skewed less skewed, popular methods linear regression, tree based methods, \\(K\\) nearest neighbors can used (methods can sensitive outliers since based conditional expectations conditional means). often happens log transformation applied variable interest, model fit, prediction made log transformed variable using conditional expectations, exponential applied predicted value convert back original variable. Jensen’s inequality tells us exponential average log variable greater average variable, model estimates.Jensen’s inequality can also used show sample standard deviation biased estimator population standard deviation, appears counter intuitive, since sample variance unbiased estimator population variance, .e. \\(E(s^2) = \\sigma^2\\), \\(E(s) \\neq \\sigma\\). quick proof isJensen’s inequality can also used show sample standard deviation biased estimator population standard deviation, appears counter intuitive, since sample variance unbiased estimator population variance, .e. \\(E(s^2) = \\sigma^2\\), \\(E(s) \\neq \\sigma\\). quick proof \\[\nE(s) = E(\\sqrt{s^2}) \\leq \\sqrt{E(s^2)} = \\sigma.\n\\]sample standard deviation underestimates population standard deviation. However, bias tends small sample size large. cover ideas relating unbiased estimators future module detail.","code":""},{"path":"inequalities-limit-theorems-and-simulations.html","id":"chebyshevs-inequality","chapter":"6 Inequalities, Limit Theorems, and Simulations","heading":"6.2.3 Chebyshev’s Inequality","text":"common inequality used probability Chebyshev’s inequality. provides upper bound probability random variable least certain distance mean. Let \\(X\\) random variable mean \\(\\mu\\) variance \\(\\sigma^2\\). \\(>0\\),\\[\\begin{equation}\nP(|X-\\mu| \\geq ) \\leq \\frac{\\sigma^2}{^2}.\n\\tag{6.2}\n\\end{equation}\\]alternative way expressing Chebyshev’s inequality let \\(= c \\sigma\\) equation (6.2), can interpreted providing upper bound probability random variable least \\(c\\) standard deviations mean:\\[\\begin{equation}\nP(|X-\\mu| \\geq c \\sigma) \\leq \\frac{\\sigma^2}{c^2 \\sigma^2} = \\frac{1}{c^2}.\n\\tag{6.3}\n\\end{equation}\\]Using equation (6.3), can say following upper bond probability random variable least 1, 2, 3 standard deviations mean:\\(c=1\\),\\[\nP(|X-\\mu| \\geq \\sigma) \\leq \\frac{1}{1^2} = 1.\n\\]\ninforms us probability random variable least one standard deviation mean 1. upper bound informative setting since know probabilities greater 1.\\(c=2\\),\\[\nP(|X-\\mu| \\geq 2\\sigma) \\leq \\frac{1}{2^2} = 0.25.\n\\]informs us probability random variable least two standard deviations mean 0.25. words, 25% chance random variable least 2 standard deviations mean, less 75% chance random variable within 2 standard deviations mean, since \\(P(|X-\\mu| \\leq 2\\sigma)\\) complement \\(P(|X-\\mu| \\geq 2\\sigma)\\).\\(c=3\\),\\[\nP(|X-\\mu| \\geq 3\\sigma) \\leq \\frac{1}{3^2} = \\frac{1}{9}.\n\\]11.11% chance random variable least 3 standard deviations mean, less 88.89% chance random variable within 3 standard deviations mean.Thought question: Can explain results consistent 68-99-99.7% rule normal distributions, stated Section 4.5.2.3?Notice Chebyshev’s inequality can applied distribution, can used provide bounds data can spread . flexible 68-99-99.7% rule normal distributions can applied distribution, bounds exact inequality. can trade-relaxing assumptions accuracy results.","code":""},{"path":"inequalities-limit-theorems-and-simulations.html","id":"limits","chapter":"6 Inequalities, Limit Theorems, and Simulations","heading":"6.3 Limit Theorems","text":"previous subsection, used inequalities provide bounds probabilities expectations may difficult calculate. Another way handing difficult calculations use approximations distribution random variable, instead exact distribution random variable. Generally speaking, approximations work better data (.e. sample size larger). approximations covered two important limit theorems: Law Large Numbers Central Limit Theorem. theorems approximate distribution sample mean ..d. (independent identically distributed) random variables sample size gets larger.Note: idea ..d. random variables implies observed value random variable independent , observed value come random variable. example, let \\(X\\) denote number dots roll 6-sided fair die, let \\(X_1, X_2\\) denote value first second roll respectively. \\(X_1\\) \\(X_2\\) ..d. since outcomes first second roll influence , independent. \\(X_1\\) \\(X_2\\) identically distributed follow distribution, \\(Mult_6(1, (1/6, 1/6, 1/6, 1/6, 1/6, 1/6))\\).rest section, Section 6.3, assume ..d. \\(X_1, \\cdots, X_n\\) finite mean \\(\\mu\\) finite variance \\(\\sigma^2\\). positive integers \\(n\\) (.e. possible sample size), define sample mean \\(\\bar{X}_n = \\frac{X_1 + \\cdots + X_n}{n}\\). can easily derive expected value variance sample mean using properties expectations variances. expected value \\[\\begin{equation}\n\\begin{split}\nE(\\bar{X}_n) &= E(\\frac{X_1 + \\cdots + X_n}{n}) \\\\\n             &= \\frac{1}{n}E(X_1 + \\cdots + X_n) \\\\\n             &= \\frac{1}{n} (E(X_1) + \\cdots + E(X_n)) \\\\\n             &= \\frac{1}{n} (\\mu + \\cdots + \\mu) \\\\\n             &= \\mu.\n\\end{split}\n\\tag{6.4}\n\\end{equation}\\]variance \\[\\begin{equation}\n\\begin{split}\nVar(\\bar{X}_n) &= Var(\\frac{X_1 + \\cdots + X_n}{n}) \\\\\n             &= \\frac{1}{n^2}Var(X_1 + \\cdots + X_n) \\\\\n             &= \\frac{1}{n^2} (Var(X_1) + \\cdots + Var(X_n)) \\\\\n             &= \\frac{1}{n^2} (\\sigma^2 + \\cdots + \\sigma^2) \\\\\n             &= \\frac{\\sigma^2}{n}.\n\\end{split}\n\\tag{6.5}\n\\end{equation}\\]View video detailed explanation results:Equation (6.4) informs us long-run average sample means equal population mean. can imagine taken different random samples size \\(n\\) population, random sample find sample mean, average sample means. average equals population mean \\(\\mu\\). code provides demonstration steps:simulate random sample \\(X_1, \\cdots, X_{500}\\) ..d. standard normal.Compute sample mean store .Repeat previous steps total 10 thousand reps.Find average 10 thousand sample means.Equation (6.5) informs us calculate variance sample means. can imagine taken different random samples size \\(n\\) population, random sample find sample mean, find variance sample means. variance original random variable divided \\(n\\). means sample size gets larger, variance sample means get smaller, words, sample means tend get closer population mean. re run code also find variance sample means.","code":"\nreps<- 10000 ##take 10000 random samples. This value should be large\nn<-500 ##sample size for each random sample\nxbar<-array(0,reps) ##store the sample mean for each random sample\n\nset.seed(90)\n\nfor (i in 1:reps)\n  \n{\n  \n  xbar[i]<-mean(rnorm(n)) ##find and store sample mean for each random sample\n  \n}\n\nmean(xbar) ##average the sample means. This should be close to 0. ## [1] -0.0001034368\nvar(xbar) ##variance of sample means. This should be close to 1/500, since n=500. ## [1] 0.001979948"},{"path":"inequalities-limit-theorems-and-simulations.html","id":"law-of-large-numbers","chapter":"6 Inequalities, Limit Theorems, and Simulations","heading":"6.3.1 Law of Large Numbers","text":"Law Large Numbers (LLN) states \\(n\\) gets larger approaches infinity, sample mean \\(\\bar{X}_n\\) converges true mean \\(\\mu\\). implies sample mean tends get closer population mean larger sample sizes. key word tends , guarantee sample mean always gets closer population mean whenever \\(n\\) gets larger, generally . explains tend trust results larger sample sizes.Another implication LLN can use simulations verify theoretical results, since results usually require us simulate data based large number independent replications.LLN product equations (6.4) (6.5). Equation (6.5) informs us \\(n\\) gets larger, variance sample mean gets smaller. Equation (6.4) informs us sample mean unbiased, .e. long run average equal true mean. Collectively, inform us \\(n\\) gets larger, sample mean likely closer true mean.use example illustrate LLN, comes flipping fair coin. Let \\(X\\) denote whether coin lands heads tails, let \\(X=1\\) heads \\(X=0\\) tails. can say \\(X \\sim Bern(0.5)\\) since coin fair. Imagine flipping coin \\(n\\) times, record outcome flip, \\(X_1, \\cdots, X_n\\) denote outcome flip. know \\(E(X) = 0.5\\) since \\(X \\sim Bern(0.5)\\). LLN informs us \\(\\bar{X}_1, \\cdots, \\bar{X}_n\\) usually get closer 0.5 \\(n\\) increases. words, value sample proportion flip get usually closer 0.5 flips. code simulates example \\(n=500\\), Figure 6.3 shows sample proportions get closer 0.5, general, \\(n\\) increases.\nFigure 6.3: LLN Example 2\nView video detailed explanation code:Note: set.seed() used can reproduce results exactly. However, observation sample mean tends get closer true mean \\(n\\) increases happen regardless set.seed() used, even set.seed() used.Note: LLN actually comes two versions, Weak Law Large Numbers (WLLN), Strong Law Large Numbers (SLLN). book goes detail definitions differences. written gives intuitive explanation LLN implies.","code":"\nn<-500 ##make this big, but not too big otherwise picture is difficult to see\n\nset.seed(23)\n\nX<-rbinom(n,1,0.5) ##simulate 500 flips of fair coin\n\ntotals<-cumsum(X) ##count total number of heads after each flip\nindex<-1:n\nprops<-totals/index ##find proportion of heads after each flip\n\n##create visual. LLN says that as n gets larger, the value of the sample proportion tends to get closer to 0.5\nplot(props, type=\"l\", main=\"Prop vs Sample Size\", ylab=\"Proportion\", xlab=\"n\")\nabline(h=0.5, col=\"blue\") ##overlay 0.5 for easy comparison"},{"path":"inequalities-limit-theorems-and-simulations.html","id":"misconceptions-with-lln","chapter":"6 Inequalities, Limit Theorems, and Simulations","heading":"6.3.1.1 Misconceptions with LLN","text":"One key idea LLN sample mean tends get closer true mean \\(n\\) gets larger. key words “tends” “\\(n\\) gets larger”.misunderstanding LLN gambler’s fallacy, erroneously believes sample mean must “self correct” get closer population mean small increments \\(n\\).Using example flipping fair coin. gambler’s fallacy erroneously thinks :proportion heads close 0.5, even small \\(n\\).results subsequent flips self correct, .e. proportion heads get closer 0.5 next flip. example, first 5 flips heads, next flip “due” tails since proportion get closer 0.5 next flip.convergence 0.5 comes flipping coin many times.","code":""},{"path":"inequalities-limit-theorems-and-simulations.html","id":"CLT","chapter":"6 Inequalities, Limit Theorems, and Simulations","heading":"6.3.2 Central Limit Theorem","text":"LLN informs us sample mean converges true mean. Statistical theory informs us expected value variance sample mean equations (6.4) (6.5). inform us distribution \\(\\bar{X}_n\\). Central Limit Theorem (CLT) comes .CLT states sample size gets larger tends infinity, distribution \\(\\bar{X}_n\\) standardization approaches standard normal distribution, .e.\\[\\begin{equation}\n\\sqrt{n} \\left(\\frac{\\bar{X}_n \\ - \\mu}{\\sigma} \\right) \\N(0,1).\n\\tag{6.6}\n\\end{equation}\\]CLT called asymptotic result, informs us limiting distribution \\(\\bar{X}_n\\) \\(n\\) gets larger tends infinity. CLT implies approximation \\(n\\) large enough. large \\(n\\), distribution \\(\\bar{X}_n\\) approximately \\(N(\\mu, \\frac{\\sigma^2}{n})\\).implication CLT even data follow normal distribution, average value data can approximated normal distribution sample size large enough. mentioned Section 4.5.2, lot questions research deal averages.consider hypothetical situation. Suppose waiting time customers calling customer service lunch time known mean 600 seconds standard deviation 30 seconds. company decides cut costs reduces staffing call center, claims wait times affected negatively. customers convinced otherwise. researchers obtains wait times 500 customers call lunch time staffing reduced. sample mean wait times customers 700 seconds. Can data used counter company’s claim wait times affected?One possible calculation assume company correct, wait times changed, average. , sample means approximately normal, mean 600 variance \\(\\frac{30^2}{500}\\), .e. \\(\\bar{X}_{500} \\sim N(600, \\frac{30^2}{500})\\). calculate \\(P(\\bar{X}_{500} \\geq 700)\\), probability sample mean equal greater 700 seconds. Using R, probability virtually 0, small, indicating data inconsistent company’s claim.CLT traditionally associated distribution sample mean \\(\\bar{X}_n\\). can applied sum well, due properties expectations variances. Let \\(T_n = X_1 + \\cdots + X_n = n \\bar{X}_n\\) denote sum \\(n\\) ..d. random variables. CLT says , large \\(n\\), distribution \\(T_n\\) approximately \\(N(n\\mu, n\\sigma^2)\\).","code":"\n1-pnorm(700, 600, 30/sqrt(500))## [1] 0"},{"path":"inequalities-limit-theorems-and-simulations.html","id":"considerCLT","chapter":"6 Inequalities, Limit Theorems, and Simulations","heading":"6.3.2.1 Considerations with CLT","text":"One question raised CLT used large sample size \\(n\\) approximation accurate? suggestions plentiful (usually along lines sample size least 25 30), fixed answer question. depends distribution \\(X\\). general, skewed \\(X\\) , \\(n\\) needs larger approximation work. hand, \\(X\\) already normal, distribution \\(\\bar{X}_n\\) exactly \\(N(\\mu, \\frac{\\sigma^2}{n})\\) sample size \\(n\\). look couple examples based different distributions.\\(X\\) standard normal. code following:simulate \\(n\\) draws \\(X\\) \\(n = 1\\).obtain distribution \\(\\bar{X}_n\\) value \\(n\\), repeat previous step total 10 thousand reps, produce histogram 10 thousand values \\(\\bar{X}_n\\).Repeat previous two steps, different values \\(n\\). use \\(n= 5, 30, 100\\) wellWe expect histograms \\(\\bar{X}_n\\) look normal values \\(n\\) used.\nFigure 6.4: Distribution Sample Means X N(0,1), n varied\nFigure 6.4 displays histograms simulation, matches expect CLT. Since \\(X\\) normal, \\(\\bar{X}_n\\) follows normal distribution value \\(n\\).\\(X\\) Poisson parameter 1. skewed distribution. use code mimics previous example, difference data simulated \\(Pois(1)\\) instead standard normal. \\(n\\) small, expect distribution \\(\\bar{X}_n\\) look normal. \\(n\\) gets larger, expect distribution \\(\\bar{X}_n\\) look normal.\nFigure 6.5: Distribution Sample Means X Pois(1), n varied\nFigure 6.5 displays histograms simulation, matches expect. \\(n\\) 1 5, histograms clearly normal, CLT approximation work well. \\(n=30\\) histogram looks approximately normal, \\(n=100\\), histogram looks even closer normal distribution.","code":""},{"path":"inequalities-limit-theorems-and-simulations.html","id":"MCsims","chapter":"6 Inequalities, Limit Theorems, and Simulations","heading":"6.4 Monte Carlo Simulations","text":"may noticed used simulations earlier sections module (previous modules) help explain certain concepts. simulations called Monte Carlo methods, Monte Carlo simulations. idea behind Monte Carlo methods used repeated random sampling (repeated, mean repeated large number times) estimate features data, usually probabilities expected values. Monte Carlo methods used following purposes:probability expectation complicated work hand. Recall finding probabilities expectations hand involve summations integrals, becomes obvious working summations especially integrals can get onerous.probability expectation complicated work hand. Recall finding probabilities expectations hand involve summations integrals, becomes obvious working summations especially integrals can get onerous.verify theoretical results involving probability expectations. lot theory proved using mathematics, academic papers include Monte Carlo simulations verify theoretical results. done verify LLN CLT previous subsection (circumstances).verify theoretical results involving probability expectations. lot theory proved using mathematics, academic papers include Monte Carlo simulations verify theoretical results. done verify LLN CLT previous subsection (circumstances).help confirm understand meaning theoretical results. way code matches theory understand theory.help confirm understand meaning theoretical results. way code matches theory understand theory.","code":""},{"path":"inequalities-limit-theorems-and-simulations.html","id":"monte-carlo-methods-for-expected-values","chapter":"6 Inequalities, Limit Theorems, and Simulations","heading":"6.4.1 Monte Carlo Methods for Expected Values","text":"Suppose want find expectation continuous random variable \\(X\\), \\(E(g(X))\\), \\(g\\) function. LOTUS says need use equation (4.4), .e. \\(E(g(X)) = \\int_{-\\infty}^{\\infty} g(x) f_X(x).\\) Monte Carlo methods avoid integration simulating \\(X_1, \\cdots, X_M\\) \\(X\\) estimate \\(E(g(X))\\) sample mean \\(g(X)\\), \\(\\frac{1}{M} \\sum_{=1}^M g(X_i)\\). LLN tells us \\(M\\) gets larger, sample mean converges \\(E(g(X))\\).Monte Carlo methods replace integral (summation) simulating random variable repeatedly many times. use simple example illustrate idea.Let \\(X\\) standard normal distribution. Suppose want find value \\(E(X^2)\\). try find using LOTUS, need find \\(\\int_{-\\infty}^{\\infty} x^2 \\frac{1}{\\sqrt{2 \\pi}} e^{-x^2/2} dx\\). Instead working integral hand, carry Monte Carlo simulation steps:Simulate \\(M\\) random values standard normal, \\(M\\) large.Calculate \\(X_1^2, \\cdots, X_M^2\\).Find sample average \\(X_1^2, \\cdots, X_M^2\\).Since \\(X\\) standard normal, know \\(E(X) = 0\\) \\(Var(X) = E(X^2) - E(X)^2 = E(X^2) = 1\\). see simulation estimated value \\(E(X^2)\\) pretty close theoretical value.","code":"\nset.seed(5)\n\nreps<-10000 ## this is M\n\nXs<-rnorm(reps) ##generate M values of X\n\nsquared.values<-Xs^2 ##square each X\n\nmean(squared.values) ##sample average of squared values. ## [1] 1.024546\n##when reps is large, this sample mean should be close to the true E(X^2), which is 1"},{"path":"inequalities-limit-theorems-and-simulations.html","id":"monte-carlo-methods-for-probabilities","chapter":"6 Inequalities, Limit Theorems, and Simulations","heading":"6.4.2 Monte Carlo Methods for Probabilities","text":"Suppose want find probability random variable satisfies event \\(E\\), \\(P(E)\\). perform summation integral find probability, estimate probability using Monte Carlo methods. simulate \\(X_1, \\cdots, X_M\\) \\(X\\), \\(M\\) large, words, simulate large number replicates \\(X\\). count many \\(X_i\\)s correspond event \\(E\\) happening, divide number \\(M\\), number replicates. use example illustrate idea.Let \\(X\\) standard normal distribution. Suppose want find probability \\(P(X^2 > 1)\\). carry Monte Carlo simulation steps:Simulate \\(M\\) random values standard normal, \\(M\\) large.Calculate \\(X_1^2, \\cdots, X_M^2\\).Count number times \\(X_i^2\\) greater 1.Divide number \\(M\\) estimate probability, since probability can interpreted long-run proportion.see estimated probability \\(P(X^2 > 1)\\) close theoretical probability.","code":"\nset.seed(5)\n\nreps<-10000 ## this is M\n\nXs<-rnorm(reps) ##generate M values of X\n\nsquared.values<-Xs^2 ##square each X\n\nsum(squared.values>1)/reps ##count the number of times X^2 is greater than 1, and divide by M## [1] 0.3178\n##when reps is large, this proportion should be close to\n1-pchisq(1, df=1)## [1] 0.3173105\n##it turns out that squaring a standard normal gives a chi-squared distribution with 1 df."},{"path":"inequalities-limit-theorems-and-simulations.html","id":"monte-carlo-methods-for-other-purposes","chapter":"6 Inequalities, Limit Theorems, and Simulations","heading":"6.4.3 Monte Carlo Methods for Other Purposes","text":"Monte Carlo methods exclusively used estimating expected values probabilities. versatile can used number purposes, long need repeated random sampling.fun example pretty famous uses Monte Carlo simulations estimate value \\(\\pi\\). can consider following hypothetical dart throwing experiment , based figure :\nFigure 6.6: Board Dart Throwing Experiment\nexperiment works way:dart always land square, equal probability landing spot square.\ncan let \\(X \\sim U(-1,1)\\) represent position dart x-axis circle Figure 6.6.\ncan let \\(Y \\sim U(-1,1)\\) represent position dart y-axis circle Figure 6.6.\ncan let \\(X \\sim U(-1,1)\\) represent position dart x-axis circle Figure 6.6.can let \\(Y \\sim U(-1,1)\\) represent position dart y-axis circle Figure 6.6.throw large number darts. dart, see lands circle .\nassess dart lies circle, assess whether \\(x_i^2 + y_i^2 \\leq 1\\) dart \\(\\). condition met, know dart \\(\\) lies circle, , lies outside circle.\nassess dart lies circle, assess whether \\(x_i^2 + y_i^2 \\leq 1\\) dart \\(\\). condition met, know dart \\(\\) lies circle, , lies outside circle.stands reason \\(\\frac{\\text{Area circle}}{\\text{square}} = \\frac{\\pi}{4} \\approx \\frac{\\text{Number darts landing circle}}{\\text{Number darts thrown}}\\).Therefore, throwing large number darts, \\(\\pi \\approx 4 \\times \\frac{\\text{Number darts landing circle}}{\\text{Number darts thrown}}.\\)code carries experiment 10 thousand reps (10 thousand dart throws):see estimated value \\(\\pi\\) using Monte Carlo simulation close true value.","code":"\nreps<-10000 ##number of dart throws\n\ncount<-0 ##counter that keeps track of number of throws inside circle\n\nset.seed(222)\n\nfor (i in 1:reps) {\n\nx<-runif(1,min=-1, max=1) ##simulate landing spot on x axis\ny<-runif(1,min=-1, max=1) ##simulate landing spot on y axis\n\n  if (x^2 + y^2 <= 1){\n    count <- count+1 ##counter adds 1 if dart lands in circle\n  }\n\n}\n\n##estimate pi. should be close to real value of pi. Gets closer if we throw more darts\ncount/reps * 4 ## [1] 3.1436"},{"path":"inequalities-limit-theorems-and-simulations.html","id":"considerations-with-monte-carlo-methods","chapter":"6 Inequalities, Limit Theorems, and Simulations","heading":"6.4.4 Considerations with Monte Carlo Methods","text":"examples , compared estimated values using Monte Carlo methods real values, see methods work. However, know real values, two questions come mind:many replicates need? know estimated values converge true values increase number replicates. Using replicates make simulation run longer computer.many replicates need? know estimated values converge true values increase number replicates. Using replicates make simulation run longer computer.Related previous question, close close enough? know estimated value simulation close enough truth? way knowing true value unknown.Related previous question, close close enough? know estimated value simulation close enough truth? way knowing true value unknown.","code":""},{"path":"inequalities-limit-theorems-and-simulations.html","id":"set.seed-in-r","chapter":"6 Inequalities, Limit Theorems, and Simulations","heading":"6.4.4.1 set.seed() in R","text":"may noticed provided simulations, use function set.seed() input number. enable others replicate exact results, someone wants verify code.Monte Carlo simulations, generating numbers randomly. set seed set.seed() certain number, ensure random numbers generated time code run.go details R generates random numbers, random number generation whole field .terms running examples, can choose copy code exclude line set.seed(). still observe estimated values simulations close true values.","code":""},{"path":"est.html","id":"est","chapter":"7 Estimation","heading":"7 Estimation","text":"module based Introduction Probability Data Science (Chan), Chapter 8.1 8.2. can access book free https://probability4datascience.com. Please note cover additional topics, skip certain topics book. may skip Section 8.1.3, 8.1.4, 8.1.6 book.","code":""},{"path":"est.html","id":"introduction-3","chapter":"7 Estimation","heading":"7.1 Introduction","text":"consider building models based data . Many models based distribution, example, linear regression model based normal distribution, logistic regression model based Bernoulli distribution. Recall distributions specified parameters: mean \\(\\mu\\) variance \\(\\sigma^2\\) normal distribution, success probability \\(p\\) Bernoulli distribution. value parameters almost always unknown real life. module deals estimation: estimate values parameters, well quantify level uncertainty estimated values, given data .","code":""},{"path":"est.html","id":"big-picture-idea-with-estimation","chapter":"7 Estimation","heading":"7.1.1 Big Picture Idea with Estimation","text":"Consider simple scenario. want find distribution associated systolic blood pressure American adults. able achieve goal, get systolic blood pressure every single American adult. usually feasible researchers unlikely time money interview every single American adult. Instead, representative sample American adults obtained, example, 750 randomly selected American adults interviewed. can create density plots, histograms, compute mean, median, variance, skewness, summaries may interest, based 750 American adults.","code":""},{"path":"est.html","id":"population-vs-sample","chapter":"7 Estimation","heading":"7.1.1.1 Population Vs Sample","text":"scenario illustrates concepts terms fundamental estimation. study, must clear population interest, sample.population (sometimes called population interest) entire set individuals, objects, events study interested . scenario described , population () American adults.sample set individuals, objects, events data . scenario described , sample 750 randomly selected American adults.Ideally, sample representative population. representative sample often achieved simple random sample, unit population chance selected sample. module, assume representative sample. Note: may feel obtaining simple random sample may difficult. get discussion sampling (sometimes called survey sampling), field statistics handles obtain representative samples, calculations adjusted sample representative. still lot research done survey sampling.","code":""},{"path":"est.html","id":"variables-observations","chapter":"7 Estimation","heading":"7.1.1.2 Variables & Observations","text":"variable characteristic attribute individuals, objects, events make population sample. scenario, variable systolic blood pressure American adults. can use notation random variables describe variables. example, can let \\(X\\) denote systolic blood pressure American adult, writing \\(P(X>200)\\) means want find probability American adult systolic blood pressure greater 200 mm Hg .observation individual person, object event collect data . scenario, observation single American adult sample 750.One way think variables observations spreadsheet. Typically, row represents observation column represents variable. Figure 7.1 displays example, based described scenario. row represents observation, .e. single American adult sample, column represents variable, systolic blood pressure.\nFigure 7.1: Example Data Spreadsheet\n","code":""},{"path":"est.html","id":"parameter-vs-estimator","chapter":"7 Estimation","heading":"7.1.1.3 Parameter Vs Estimator","text":"Now made distinction population sample, ready define parameters estimators.parameter numerical summary associated population. scenario described , example population parameter population mean systolic blood pressure American adults.estimator numerical summary associated samples. estimator typically used estimate parameter. scenario described , estimator population mean systolic blood pressure American adults average systolic blood pressure sample. sample mean estimator population mean.estimated value, estimate, actual value estimator based sample. scenario described , suppose average systolic blood pressure 750 American adults 140 mm Hg. say estimated value mean systolic blood pressure American adults 140 mm Hg.parameter number associated population, estimator number associated sample. differences parameters estimators:value parameters unknown, can actually calculate numerical values estimators.value parameters considered fixed (one population), numerical values estimators can vary obtain multiple random samples sample size. Using scenario , suppose obtain second representative sample 750 American adults. average systolic blood pressure second sample likely different average systolic blood pressure first sample. illustrates variance, uncertainty, associated estimators due random sampling. uncertainty focusing section.Whenever propose estimator parameter, want assess “good” estimator . situations, obvious choice estimator, example, using sample mean, \\(\\bar{x} = \\frac{\\sum x_i}{n}\\) estimate population mean. instances, choice may obvious. example, use sample variance \\(s^2 = \\frac{\\sum (x_i - \\bar{x})^2}{n-1}\\) estimator population variance, \\(\\frac{\\sum (x_i - \\bar{x})^2}{n}\\)? cover measures used assess estimator: bias, variance, mean-squared error.also cover couple methods estimating parameters: method moments, method maximum likelihood. notice use probability rules methods.sum estimation: use data sample estimate unknown characteristics population, can answer questions regarding variables population, well provide measure uncertainty answers.","code":""},{"path":"est.html","id":"module-roadmap-5","chapter":"7 Estimation","heading":"7.1.2 Module Roadmap","text":"Section 7.2 covers estimation using method moments. method fairly intuitive, although commonly used lacks certain ideal properties.Section 7.3 covers estimation using method maximum likelihood. method workhorse statistics probability many models estimated using method.Section 7.4 covers measures used evaluate “good” estimator .Section 7.5 provides final comments estimation.","code":""},{"path":"est.html","id":"MOM","chapter":"7 Estimation","heading":"7.2 Method of Moments Estimation","text":"cover couple methods estimation. first method method moments. intuitive method, although lacks certain ideal properties. defining method, recall define terms.Section 4.4.3, defined moments. reminder, random variable \\(X\\), \\(k\\)th moment \\(E(X^k)\\), can found using LOTUS: \\(\\int_{-\\infty}^{\\infty} x^k f_X(x) dx\\).Suppose observe random sample \\(x_1, \\cdots, x_n\\) comes \\(X\\). \\(k\\)th sample moment \\(M_k = \\frac{1}{n} \\sum_{=1}^n x_i^k\\).Using definitions,1st moment \\(E(X) = \\mu_x\\), population mean \\(X\\).1st sample moment \\(M_1 = \\frac{1}{n} \\sum_{=1}^n x_i = \\bar{x}\\), sample mean.2nd moment \\(E(X^2)\\).2nd sample moment \\(M_2 = \\frac{1}{n} \\sum_{=1}^n x_i^2\\)..method moments estimation : Let \\(X\\) random variable distribution depending parameters \\(\\theta_1, \\cdots, \\theta_m\\). method moments (MOM) estimates \\(\\hat{\\theta}_1, \\cdots, \\hat{\\theta}_m\\) found equating first \\(m\\) sample moments corresponding first \\(m\\) moments solving \\(\\theta_1, \\cdots, \\theta_m\\).might noticed method moments based Law Large Numbers.Note: convention, parameters typically denoted Greek letters, estimators denoted hat symbol corresponding letter.Let us look couple examples:Suppose coin know fair . two outcomes flip, heads tails. flip independent flips. Let \\(X_i\\) denote whether \\(\\)th flip lands heads, \\(X_i = 1\\) heads \\(X_i = 0\\) tails. can see \\(X_i \\sim Bern(p)\\), \\(p\\) probability lands heads. Derive MOM estimate \\(p\\).Bernoulli distribution 1 parameter, \\(p\\), using method moments, need equate first sample moment first moment.first moment \\(E(X_i) = p\\), since \\(X_i \\sim Bern(p)\\).first sample moment \\(M_1 = \\frac{1}{n} \\sum_{=1}^n x_i = \\bar{x}\\).Set \\(E(X_i) = M_1\\), .e. \\(\\hat{p} = \\bar{x}\\). Since \\(X_i = 1\\) heads \\(X_i = 0\\) tails, \\(\\frac{1}{n} \\sum_{=1}^n x_i = \\bar{x}\\) actually represents proportion flips land heads, based \\(n\\) flips. actually sample proportion.MOM estimate problem \\(\\hat{p}\\), proportion \\(n\\) flips land heads. result fairly intuitive. flip coin large number times, 70% flips land heads, sample proportion \\(\\hat{p} = 0.7\\), estimated value \\(p\\), success probability 0.7.Birth weights newborn babies typically follow normal distribution. data births Baystate Medical Center Springfield, MA, 1986. Assuming births hospital representative births New England 1986, derive MOM estimates \\(\\mu\\) \\(\\sigma^2\\), mean variance distribution birth weights New England 1986.normal distribution 2 parameters, \\(\\mu\\) \\(\\sigma^2\\), need equate first two sample moments first two moments. Let \\(X\\) denote birth weights New England 1986, \\(X \\sim N(\\mu, \\sigma^2)\\).first moment \\(E(X) = \\mu\\), since \\(X \\sim N(\\mu, \\sigma^2)\\).first sample moment \\(M_1 = \\frac{1}{n} \\sum_{=1}^n x_i = \\bar{x}\\).set \\(E(X) = M_1\\), .e. \\(\\hat{\\mu} = \\bar{x}\\). just sample average birth weights Baystate Medical Center 1986.second moment \\(E(X^2)\\). know since \\(X \\sim N(\\mu, \\sigma^2)\\).\\[\n\\begin{split}\nVar(X) &= E(X^2) - E(X)^2\\\\\n\\implies E(X^2) &= Var(X) + E(X)^2 \\\\\n\\implies E(X^2) &= \\sigma^2 + \\mu^2\n\\end{split}\n\\]second sample moment \\(M_2 = \\frac{1}{n} \\sum_{=1}^n x_i^2\\).set \\(E(X^2) = M_2\\), .e. \\(\\hat{\\sigma^2} + \\hat{\\mu}^2 = \\frac{1}{n} \\sum_{=1}^n x_i^2\\). Since earlier found \\(\\hat{\\mu} = \\bar{x}\\), get \\(\\hat{\\sigma^2} = \\frac{1}{n} \\sum_{=1}^n x_i^2 - \\bar{x}^2 = \\frac{1}{n} \\sum_{=1}^n (x_i - \\bar{x})^2\\).Therefore, MOM estimates \\(\\mu\\) \\(\\sigma^2\\) \\(\\hat{\\mu} = \\bar{x}\\) \\(\\hat{\\sigma^2} = \\frac{1}{n} \\sum_{=1}^n (x_i - \\bar{x})^2\\) respectively.View video detailed explanation deriving MOM estimates normal distribution :now use MOM estimates data set birth weights Baystate Medical Center 1986. well established literature birth weights babies follow normal distribution. quick check Shapiro-Wilk test normality shows contradiction, proceed finding estimates \\(\\mu\\) \\(\\sigma^2\\). produce density plot birth weights, overlay curve corresponds normal distribution \\(\\hat{\\mu} = \\bar{x}\\) \\(\\hat{\\sigma^2} = \\frac{1}{n} \\sum_{=1}^n (x_i - \\bar{x})^2\\). normal curve pretty close density plot, appears reasonable say birth weights follow normal distribution mean 2944.587 (grams) variance 528940 (grams-squared).\nFigure 7.2: Density Plot Birth Weights. Normal Curve (Blue) Parameters Estimated MOM Overlaid\n","code":"\nlibrary(MASS)\ndata<-MASS::birthwt ##dataset comes from MASS package\n\nshapiro.test(data$bwt) ##check for normality## \n##  Shapiro-Wilk normality test\n## \n## data:  data$bwt\n## W = 0.99244, p-value = 0.4353\nmu<-mean(data$bwt) ##MOM estimate for mu\nmu## [1] 2944.587\nsigma2<-mean((data$bwt-mu)^2) ##MOM estimate for sigma2\nsigma2## [1] 528940\n##create density plot for data, and overlay Normal curve with parameters estimated by MOM\nplot(density(data$bwt), main=\"\", ylim=c(0,6e-04))\ncurve(dnorm(x, mean=mu, sd=sqrt(sigma2)), \n      col=\"blue\", lwd=2, add=TRUE)"},{"path":"est.html","id":"alternative-form-of-method-of-moments-estimation","chapter":"7 Estimation","heading":"7.2.1 Alternative Form of Method of Moments Estimation","text":"Section 4.4.3, defined central moments. reminder, random variable \\(X\\), \\(k\\)th central moment \\(E((X-\\mu)^k)\\).Suppose observe random sample \\(x_1, \\cdots, x_n\\) comes \\(X\\). \\(k\\)th sample central moment \\(M_k^* = \\frac{1}{n} \\sum_{=1}^n (x_i - \\bar{x})^k\\).alternative form method moments estimation : Let \\(X\\) random variable distribution depending parameters \\(\\theta_1, \\cdots, \\theta_m\\). method moments (MOM) estimates \\(\\hat{\\theta}_1, \\cdots, \\hat{\\theta}_m\\) found equating first sample moment first moments, equating subsequent sample central moments corresponding central moments, solving \\(\\theta_1, \\cdots, \\theta_m\\).alternative form often easier work , since 2nd central moment actually variance \\(X\\).go back 2nd example previous subsection, trying find estimates \\(\\mu\\) \\(\\sigma^2\\) normal distribution.first moment \\(E(X) = \\mu\\), since \\(X \\sim N(\\mu, \\sigma^2)\\).first sample moment \\(M_1 = \\frac{1}{n} \\sum_{=1}^n x_i = \\bar{x}\\).set \\(E(X) = M_1\\), .e. \\(\\hat{\\mu} = \\bar{x}\\). just sample average birth weights Baystate Medical Center 1986.second central moment \\(Var(x) = E[(X-\\mu)^2] = \\sigma^2\\).second sample central moment \\(M_2^* = \\frac{1}{n} \\sum_{=1}^n (x_i - \\bar{x})^2\\).set \\(Var(X) = M_2^*\\) .e. \\(\\hat{\\sigma^2} = \\frac{1}{n} \\sum_{=1}^n (x_i - \\bar{x})^2\\). compare solution solution previous subsection, exactly .idea behind method moments estimation fairly intuitive; however, drawbacks talk introducing another method estimation next section, method maximum likelihood.","code":""},{"path":"est.html","id":"MLE","chapter":"7 Estimation","heading":"7.3 Method of Maximum Likelihood Estimation","text":"method maximum likelihood workhorse statistics data science since widely used estimating models. preferred method moments built upon stronger theoretical framework, estimators tend desirable properties. guaranteed see method maximum likelihood future.name suggests, method estimating parameters maximizing likelihood. go idea behind likelihood next.","code":""},{"path":"est.html","id":"likelihood-function","chapter":"7 Estimation","heading":"7.3.1 Likelihood Function","text":"Suppose \\(n\\) observations, denoted vector \\(\\boldsymbol{x} = (x_1, \\cdots, x_n)^{T}\\). can use PDF generalize distribution observations, \\(f_{\\boldsymbol{X}}(\\boldsymbol{x})\\). joint PDF variables.seen PDFs (hence joint PDFs) always described parameters (e.g. Normal distribution \\(\\mu, \\sigma^2\\), Bernoulli \\(p\\)). section, let \\(\\boldsymbol{\\theta}\\) denote parameters PDF. example, working multivariate normal distribution, \\(\\boldsymbol{\\theta} = (\\boldsymbol{\\mu}, \\boldsymbol{\\Sigma})\\), mean vector covariance matrix.write \\(f_{\\boldsymbol{X}}(\\boldsymbol{x}; \\boldsymbol{\\theta})\\)\nexpress PDF random vector \\(\\boldsymbol{X}\\) parameter \\(\\boldsymbol{\\theta}\\). PDF function two items:first item vector \\(\\boldsymbol{x} = (x_1, \\cdots, x_n)^{T}\\), basically vector observed data. previous modules, expressed PDFs function observed data, since calculate PDF \\(\\boldsymbol{X} = \\boldsymbol{x}\\). estimation, vector observed data actually fixed something given data set.first item vector \\(\\boldsymbol{x} = (x_1, \\cdots, x_n)^{T}\\), basically vector observed data. previous modules, expressed PDFs function observed data, since calculate PDF \\(\\boldsymbol{X} = \\boldsymbol{x}\\). estimation, vector observed data actually fixed something given data set.second item parameter \\(\\boldsymbol{\\theta}\\). Estimating parameter focus estimation. general idea maximum likelihood find value \\(\\boldsymbol{\\theta}\\) “best explains” “consistent” observed values data \\(\\boldsymbol{x}\\). maximize \\(f_{\\boldsymbol{X}}(\\boldsymbol{x}; \\boldsymbol{\\theta})\\) achieve goal.second item parameter \\(\\boldsymbol{\\theta}\\). Estimating parameter focus estimation. general idea maximum likelihood find value \\(\\boldsymbol{\\theta}\\) “best explains” “consistent” observed values data \\(\\boldsymbol{x}\\). maximize \\(f_{\\boldsymbol{X}}(\\boldsymbol{x}; \\boldsymbol{\\theta})\\) achieve goal.likelihood function PDF, written way shifts emphasis parameters. likelihood function denoted \\(L(\\boldsymbol{\\theta} | \\boldsymbol{x})\\) defined \\(f_{\\boldsymbol{X}}(\\boldsymbol{x})\\).Note: likelihood function viewed function \\(\\boldsymbol{\\theta}\\), shape changes depending values observed data \\(\\boldsymbol{x}\\).simplify calculations involving likelihood function, make assumption observations \\(\\boldsymbol{x}\\) independent come identical distribution PDF \\(f_X(x)\\), words, observations ..d. (independent identically distributed).Given ..d. random variables \\(X_1, \\cdots, X_n\\), PDF \\(f_X(x)\\), likelihood function \\[\\begin{equation}\nL(\\boldsymbol{\\theta} | \\boldsymbol{x} ) = \\prod_i^n f_X(x; \\boldsymbol{\\theta}).\n\\tag{7.1}\n\\end{equation}\\]maximizing likelihood function, often log transform likelihood function first, maximize log transformed likelihood function. log transformed likelihood function called log-likelihood function, \\[\\begin{equation}\n\\ell(\\boldsymbol{\\theta} | \\boldsymbol{x}) = \\log L(\\boldsymbol{\\theta} | \\boldsymbol{x}) = \\sum_{=1}^n \\log f_X(x; \\boldsymbol{\\theta}).\n\\tag{7.2}\n\\end{equation}\\]turns maximizing log-likelihood function often easier computationally maximizing likelihood function.logarithm monotonic increasing function (never decreases), maximizing log transformed function equivalent maximizing original function. Next, look write likelihood log-likelihood functions couple examples.","code":""},{"path":"est.html","id":"example-1-bernoulli","chapter":"7 Estimation","heading":"7.3.1.1 Example 1: Bernoulli","text":"Let \\(X_1, \\cdots, X_n\\) ..d. \\(Bern(p)\\). Find corresponding likelihood log-likelihood functions.equation (3.8), PMF \\(X \\sim Bern(p)\\) \\(f_X(x) = p^x (1-p)^{1-x}\\), support \\(x\\) \\(\\{0,1\\}\\).likelihood function, per equation (7.1), becomes\\[\nL(p | \\boldsymbol{x} ) = \\prod_{=1}^n f_X(x_i; p) = \\prod_{=1}^n p^{x_i} (1-p)^{1-x_i}.\n\\]log-likelihood function, per equation (7.2), becomes\\[\n\\begin{split}\n\\ell (p | \\boldsymbol{x}) &= \\sum_{=1}^n \\log f_X(x_i;p) \\\\\n                          &= \\sum_{=1}^n \\log \\left( p^{x_i} (1-p)^{1-x_i} \\right) \\\\\n                          &= \\sum_{=1}^n x_i \\log p + (1-x_i) \\log (1-p) \\\\\n                          &= \\log p \\left(\\sum_{=1}^n x_i \\right) + \\log (1-p) \\left( n - \\sum_{=1}^n x_i \\right).\n\\end{split}\n\\]","code":""},{"path":"est.html","id":"example-1-continued-visualizing-likelihood-and-log-likelihood-functions","chapter":"7 Estimation","heading":"7.3.1.2 Example 1 Continued: Visualizing Likelihood and Log-Likelihood Functions","text":"mentioned likelihood log-likelihood functions, \\(L(\\boldsymbol{\\theta} | \\boldsymbol{x} )\\) \\(\\ell(\\boldsymbol{\\theta} | \\boldsymbol{x} )\\), functions parameters \\(\\boldsymbol{\\theta}\\) observed data \\(\\boldsymbol{x}\\). typically view functions observing data \\(\\boldsymbol{x}\\).Suppose trying estimate proportion college students use passphrases university email account. randomly select 20 students ask use passphrases university email account. Let \\(x_i\\) denote response student \\(\\), \\(x_i = 1\\) student \\(\\) uses passphrases \\(x_i=0\\) otherwise. can say \\(X \\sim Bern(p)\\) \\(p\\) denotes proportion college students use passphrases university email account. sample 20 students, 7 said use passphrases university email account. example 1, know likelihood function now \\[\n\\begin{split}\nL(p | \\boldsymbol{x} ) &= \\prod_{=1}^n p^{x_i} (1-p)^{1-x_i} \\\\\n                       &= p^7 (1-p)^{13}\n\\end{split}\n\\]log-likelihood function becomes\\[\n\\begin{split}\n\\ell (p | \\boldsymbol{x}) &= \\log p \\left(\\sum_{=1}^n x_i \\right) + \\log (1-p) \\left( n - \\sum_{=1}^n x_i \\right)\\\\\n                          &= 7 \\log p + 13 \\log(1-p).\n\\end{split}\n\\]\ncan create plots \\(L(p | \\boldsymbol{x} )\\) \\(\\ell(p | \\boldsymbol{x} )\\) based observed data, vary value \\(p\\) 0 1 (support \\(p\\)). plots displayed Figure 7.3 \nFigure 7.3: Likelihood (left) Log-Likelihood (right) Functions Bernoulli, n=20, 7 Yeses\nplots Figure 7.3 show us likelihood log-likelihood functions behave, given data (.e. 7 20 students said use passphrases), vary value parameter \\(p\\). note value \\(p\\) maximized likelihood log-likelihood functions 0.35. idea behind method maximum likelihood estimation: value parameter maximizes likelihood log-likelihood functions? regular language, value parameter best explains data?Note: value 0.35 corresponds sample proportion students use passphrases. make intuitive sense 7 20 students sample say use passphrases, say best estimate proportion college students use passphrases university email 0.35.View video provides bit detail finding visualizing likelihood log-likelihood functions Bernoulli distribution:","code":"\n##function to compute likelihood function \nbern_like<-function(x, p) ##supply vector of data, and value of success probability\n  \n{\n  \n  n<-length(x) ##sample size\n  S<-sum(x) \n  \n  likelihood<-p^S * (1-p)^(n-S) ##formula for likelihood function from Example 1\n  return(likelihood)\n  \n}\n\n##function to compute loglikelihood function\n\nbern_loglike<-function(x,p)\n  \n{\n  \n  n<-length(x) ##sample size\n  S<-sum(x) \n  \n  loglike<- S*log(p) + (n-S)*log(1-p) ##formula for log-likelihood function from Example 1\n  return(loglike) \n  \n}\n\n##our \"data\" according to described scenario\ndata<-c(rep(1,7), rep(0,13)) \n##vary the value of p, from 0 to 1, in increments of 0.01\nprops<-seq(0,1,by=0.01) \n\npar(mfrow=c(1,2))\n##plot the likelihood function on y axis, against the value of p\nplot(props, bern_like(data, props), type=\"l\", xlab=\"p\", ylab=\"Likelihood Function\")\n##overlay line on x-axis that corresponds to max value for likelihood function\nabline(v=props[which.max(bern_like(data, props))], col=\"blue\")\n##plot the loglikelihood function on y axis, against the value of p\nplot(props, bern_loglike(data, props), type=\"l\", xlab=\"p\", ylab=\"Log-Likelihood Function\")\n##overlay line on x-axis that corresponds to max value for loglikelihood function\nabline(v=props[which.max(bern_loglike(data, props))], col=\"blue\")\n## what value of p had maximum value of likelihood\nprops[which.max(bern_like(data, props))]## [1] 0.35\n## what value of p had maximum value of loglikelihood\nprops[which.max(bern_loglike(data, props))]## [1] 0.35"},{"path":"est.html","id":"example-2-normal","chapter":"7 Estimation","heading":"7.3.1.3 Example 2: Normal","text":"Let \\(X_1, \\cdots, X_n\\) ..d. \\(N(\\mu, \\sigma^2)\\). Find corresponding likelihood log-likelihood functions.equation (4.11), PDF \\(X \\sim N(\\mu, \\sigma^2)\\) \\(f_X(x) = \\frac{1}{\\sigma \\sqrt{2 \\pi}} \\exp \\left(-\\frac{(x - \\mu)^2}{2 \\sigma^2} \\right)\\), support \\(x\\) real numbers.likelihood function, per equation (7.1), becomes\\[\nL(\\mu, \\sigma^2 | \\boldsymbol{x} ) = \\prod_{=1}^n f_X(x_i; \\mu, \\sigma^2) = \\prod_{=1}^n \\frac{1}{\\sigma \\sqrt{2 \\pi}} \\exp \\left(-\\frac{(x_i - \\mu)^2}{2 \\sigma^2} \\right).\n\\]log-likelihood function, per equation (7.2), becomes\\[\n\\begin{split}\n\\ell (\\mu, \\sigma^2 | \\boldsymbol{x}) &= \\sum_{=1}^n \\log f_X(x_i; \\boldsymbol{\\theta}) \\\\\n                          &= \\sum_{=1}^n \\log \\left\\{ \\frac{1}{\\sigma \\sqrt{2 \\pi}} \\exp \\left(-\\frac{(x_i - \\mu)^2}{2 \\sigma^2} \\right) \\right\\} \\\\\n                          &= \\sum_{=1}^n \\left\\{-\\frac{1}{2} \\log (2 \\pi \\sigma^2) -\\frac{(x_i - \\mu)^2}{2 \\sigma^2} \\right\\} \\\\\n                          &= -\\frac{n}{2} \\log (2 \\pi \\sigma^2) - \\frac{1}{2 \\sigma^2} \\sum_{=1}^n (x_i - \\mu)^2.\n\\end{split}\n\\]View video provides bit detail finding visualizing likelihood log-likelihood functions normal distribution:","code":""},{"path":"est.html","id":"maximum-likelihood-estimation","chapter":"7 Estimation","heading":"7.3.2 Maximum Likelihood Estimation","text":"now ready formally define method maximum likelihood estimation. maximum likelihood (ML) estimates \\(\\hat{\\theta}_1, \\cdots, \\hat{\\theta}_m\\) parameters \\(\\theta_1, \\cdots, \\theta_m\\) found maximizing likelihood function \\(L(\\boldsymbol{\\theta} | \\boldsymbol{x} )\\).Remember following finding ML estimates:values \\(\\boldsymbol{x}\\) considered fixed given data.varying values parameters \\(\\boldsymbol{\\theta}\\) finding specific values \\(\\boldsymbol{\\theta}\\) maximize likelihood function.choose maximize log-likelihood function instead. often easier work log-likelihood function, likelihood functions often exponents (powers) , makes maximizing complicated. solution .re-visit Examples 1 2 previous subsection.","code":""},{"path":"est.html","id":"example-1-bernoulli-1","chapter":"7 Estimation","heading":"7.3.2.1 Example 1: Bernoulli","text":"Let \\(X_1, \\cdots, X_n\\) ..d. \\(Bern(p)\\). Find ML estimate \\(p\\).work log-likelihood function, found \\[\n\\ell (p | \\boldsymbol{x}) = \\log p \\left(\\sum_{=1}^n x_i \\right) + \\log (1-p) \\left( n - \\sum_{=1}^n x_i \\right).\n\\]find ML estimate \\(p\\), need find value \\(p\\) maximizes \\(\\ell (p | \\boldsymbol{x})\\). previous subsection, provided visual way represent log-likelihood function \\(p\\) varied, find value \\(p\\) corresponded peak graph. visual approach works specific scenario 7 20 students say yes. Can generalize ML estimate Bernoulli distribution?can easily maximize function taking first derivative setting 0. take first derivative \\(\\ell (p | \\boldsymbol{x})\\) respect parameter \\(p\\):\\[\n\\begin{split}\n\\frac{d}{d p}\\ell (p | \\boldsymbol{x}) &= \\frac{d}{d p} \\left\\{ \\log p \\left(\\sum_{=1}^n x_i \\right) + \\log (1-p) \\left( n - \\sum_{=1}^n x_i \\right) \\right\\}\\\\\n                          &= \\frac{\\sum_{=1}^n x_i}{p} - \\frac{n - \\sum_{=1}^n x_i}{1-p}\n\\end{split}\n\\]\nset 0:\\[\n\\begin{split}\n\\frac{\\sum_{=1}^n x_i}{p} - \\frac{n - \\sum_{=1}^n x_i}{1-p} &= 0\\\\\n                                               \\implies p     &= \\frac{\\sum_{=1}^n x_i}{n}\n\\end{split}\n\\]ML estimate \\(p\\) \\(\\hat{p}_{ML} = \\frac{\\sum_{=1}^n x_i}{n}\\). just sample proportion observed data \\(x_i = 1\\). result means data comes Bernoulli distribution, sample proportion data “success” ML estimate success probability \\(p\\).go back example 7 20 students say use passphrases, ML estimate \\(p\\) \\(\\hat{p}_{ML} = \\frac{7}{20} = 0.35\\), matches result obtained viewed log-likelihood function visually Figure 7.3.View video provides bit detail deriving ML estimates Bernoulli distribution:Thought question: Play around code produced Figure 7.3. Change vector data (anything ’d like). find value \\(p\\) maximizes likelihood log-likelihood functions always \\(\\hat{p}_{ML} = \\frac{\\sum_{=1}^n x_i}{n}\\), sample proportion “success” sample.","code":""},{"path":"est.html","id":"example-2-normal-1","chapter":"7 Estimation","heading":"7.3.2.2 Example 2: Normal","text":"Let \\(X_1, \\cdots, X_n\\) ..d. \\(N(\\mu, \\sigma^2)\\). Find ML estimates \\(\\mu\\) \\(\\sigma^2\\)., work log-likelihood function, found \\[\n\\ell (\\mu, \\sigma^2 | \\boldsymbol{x}) = -\\frac{n}{2} \\log (2 \\pi \\sigma^2) - \\frac{1}{2 \\sigma^2} \\sum_{=1}^n (x_i - \\mu)^2.\n\\]Notice two parameters \\(\\mu\\) \\(\\sigma^2\\), take partial derivatives \\(\\ell (\\mu, \\sigma^2 | \\boldsymbol{x})\\) respect parameter:Partial derivative respect \\(\\mu\\):\\[\n\\begin{split}\n\\frac{d}{d \\mu}\\ell (\\mu, \\sigma^2 | \\boldsymbol{x}) &= \\frac{d}{d \\mu} \\left\\{ -\\frac{n}{2} \\log (2 \\pi \\sigma^2) - \\frac{1}{2 \\sigma^2} \\sum_{=1}^n (x_i - \\mu)^2 \\right \\} \\\\\n                                                     &= \\frac{1}{\\sigma^2} \\sum_{=1}^n (x_i - \\mu).\n\\end{split}\n\\]Partial derivative respect \\(\\sigma^2\\):\\[\n\\begin{split}\n\\frac{d}{d \\sigma^2}\\ell (\\mu, \\sigma^2 | \\boldsymbol{x}) &= \\frac{d}{d \\sigma^2} \\left\\{ -\\frac{n}{2} \\log (2 \\pi \\sigma^2) - \\frac{1}{2 \\sigma^2} \\sum_{=1}^n (x_i - \\mu)^2 \\right \\} \\\\\n                                                     &= -\\frac{n}{2}\\frac{2\\pi}{2 \\pi \\sigma^2} + \\frac{1}{2 \\sigma^4} \\sum_{=1}^n (x_i - \\mu)^2 \\\\\n                                                     &= -\\frac{n}{2 \\sigma^2} + \\frac{1}{2 \\sigma^4} \\sum_{=1}^n (x_i - \\mu)^2\n\\end{split}\n\\]\nset partial derivatives 0:\\[\n\\begin{split}\n\\frac{d}{d \\mu}\\ell (\\mu, \\sigma^2 | \\boldsymbol{x}) &= 0 \\\\\n\\implies \\frac{1}{\\sigma^2} \\sum_{=1}^n (x_i - \\mu) &= 0 \\\\\n\\implies \\mu  &= \\frac{\\sum_{=1}^n x_i}{n} = \\bar{x}.\n\\end{split}\n\\]\nML estimate \\(\\mu\\) \\(\\hat{\\mu_{ML}} = \\bar{x}\\), .e. sample mean.\\[\n\\begin{split}\n\\frac{d}{d \\sigma^2}\\ell (\\mu, \\sigma^2 | \\boldsymbol{x}) &= 0 \\\\\n\\implies -\\frac{n}{2 \\sigma^2} + \\frac{1}{2 \\sigma^4} \\sum_{=1}^n (x_i - \\mu)^2 &= 0 \\\\\n\\implies \\sigma^2 &= \\frac{\\sum_{=1}^n (x_i - \\mu)^2}{n}.\n\\end{split}\n\\]ML estimate \\(\\sigma^2\\) \\(\\hat{\\sigma_{ML}^2} = \\frac{\\sum_{=1}^n (x_i - \\mu)^2}{n}\\). Note normally calculate sample variance, per equation (1.2): \\(\\frac{\\sum_{=1}^n (x_i-\\bar{x})^2}{n-1}\\).View video provides bit detail deriving ML estimates normal distribution:","code":""},{"path":"est.html","id":"calculating-maximum-likelihood-estimates","chapter":"7 Estimation","heading":"7.3.2.3 Calculating Maximum Likelihood Estimates","text":"examples worked , found values maximum likelihood estimates using couple approaches, one using plot likelihood function different values parameter, based observed data, another using calculus, setting derivative log-likelihood function 0.times approaches may feasible, example, closed form solution first derivative. instances, numerical methods used. Numerical methods typically algorithms perform complex computations approximate mathematical result. go details algorithms, numerical methods still active area research.","code":""},{"path":"est.html","id":"estprops","chapter":"7 Estimation","heading":"7.4 Properties of Estimators","text":"learned couple different methods estimate parameters. two methods common, methods estimate parameters. One question assess estimates “good” ? define concepts used assessment.","code":""},{"path":"est.html","id":"estimators-vs-estimates","chapter":"7 Estimation","heading":"7.4.1 Estimators VS Estimates","text":"wrote estimators estimates earlier module, quick reminder :estimator numerical summary associated samples.estimator numerical summary associated samples.estimated value, estimate, actual value estimator based sample.estimated value, estimate, actual value estimator based sample.previously said ML estimate found maximizing likelihood function, .e. \\(\\hat{\\theta}_{ML}(\\boldsymbol{x})\\) value \\(\\theta\\) maximizes \\(L(\\theta| \\boldsymbol{x})\\). write \\(\\hat{\\theta}_{ML}(\\boldsymbol{x})\\) emphasize ML estimate function observed data \\(\\boldsymbol{x} = (x_1,\\cdots, x_n)^T\\). , ML estimate sample mean, write \\(\\hat{\\theta}_{ML}(\\boldsymbol{x}) = \\frac{\\sum_{=1}^n x_i}{n}\\). value calculated based observed data.\ncan also view sample mean random variable, especially want analyze uncertainty associated sample mean. words, distribution sample mean, obtained many different random samples calculated sample mean random sample? viewing sample mean random variable, write \\(\\hat{\\Theta}_{ML}(\\boldsymbol{X}) = \\frac{\\sum_{=1}^n X_i}{n}\\), call \\(\\hat{\\Theta}_{ML}\\) ML estimator parameter \\(\\theta\\).Note: consider one parameter \\(\\theta\\) subsection, simplify notation introduction \\(\\Theta\\). ideas can applied number parameters.ML estimators just one kind estimators. can find estimators ways (method moments, method). estimator function uses data calculates number data can denoted \\(\\hat{\\Theta}(\\boldsymbol{X})\\). call \\(\\hat{\\Theta}\\) estimator \\(\\theta\\).Consider \\(X_1, \\cdots, X_n\\) ..d. Normal unknown mean \\(\\mu\\) variance \\(\\sigma^2\\) known. can define two estimators \\(\\mu\\) \\(\\hat{\\Theta}_1(\\boldsymbol{X}) = \\frac{\\sum_{=1}^n X_i}{n}\\) \\(\\hat{\\Theta}_2(\\boldsymbol{X}) = X_1 + 2\\). first estimator takes average value \\(n\\) data points. second estimator uses first data point adds 2 . first estimator ML estimator, second estimator .can see free define estimators various ways. question now evaluate whether estimator “good” . metrics evaluate estimators.","code":""},{"path":"est.html","id":"bias","chapter":"7 Estimation","heading":"7.4.2 Bias","text":"One metric used evaluate estimators consider long-run average estimator. estimator unbiased long run average estimator equal true value parameter. Mathematically, estimator \\(\\hat{\\Theta}\\) unbiased \\(E(\\hat{\\Theta}) = \\theta\\). definition unbiased estimator, definition bias estimator:\\[\\begin{equation}\nBias(\\hat{\\Theta}) = E(\\hat{\\Theta}) - \\theta.\n\\tag{7.3}\n\\end{equation}\\], value estimator vary sample sample, estimator unbiased equal true parameter, average long-run.go back example previous subsection. Consider \\(X_1, \\cdots, X_n\\) ..d. Normal unknown mean \\(\\mu\\) variance \\(\\sigma^2\\) known. can define two estimators \\(\\mu\\) \\(\\hat{\\Theta}_1(\\boldsymbol{X}) = \\frac{\\sum_{=1}^n X_i}{n}\\) \\(\\hat{\\Theta}_2(\\boldsymbol{X}) = X_1 + 2\\). Assess whether \\(\\hat{\\Theta}_1(\\boldsymbol{X})\\) \\(\\hat{\\Theta}_2(\\boldsymbol{X})\\) unbiased .mathematical way answering question evaluate expected value estimators:\n\\(E(\\hat{\\Theta}_1) = \\frac{1}{n}E(\\sum_{=1}^n X_i) = \\frac{1}{n}\\sum_{=1}^n E(X_i) = \\frac{1}{n}(\\mu + \\cdots + \\mu) = \\mu\\), unbiased.\n\\(E(\\hat{\\Theta}_2) = E(X_1 + 2) = E(X_1) + 2 = \\mu +2\\) equal \\(\\mu\\), biased.\nmathematical way answering question evaluate expected value estimators:\\(E(\\hat{\\Theta}_1) = \\frac{1}{n}E(\\sum_{=1}^n X_i) = \\frac{1}{n}\\sum_{=1}^n E(X_i) = \\frac{1}{n}(\\mu + \\cdots + \\mu) = \\mu\\), unbiased.\\(E(\\hat{\\Theta}_1) = \\frac{1}{n}E(\\sum_{=1}^n X_i) = \\frac{1}{n}\\sum_{=1}^n E(X_i) = \\frac{1}{n}(\\mu + \\cdots + \\mu) = \\mu\\), unbiased.\\(E(\\hat{\\Theta}_2) = E(X_1 + 2) = E(X_1) + 2 = \\mu +2\\) equal \\(\\mu\\), biased.\\(E(\\hat{\\Theta}_2) = E(X_1 + 2) = E(X_1) + 2 = \\mu +2\\) equal \\(\\mu\\), biased.can also use Monte Carlo simulations show results. simulation following:\ncode simulates \\(X_1, \\cdots, X_{n}\\) ..d. known distribution, \\(n\\) fixed. code , used standard normal, \\(n=100\\).\nGenerate large number replicates (used 10 thousand replicates) \\(X_1, \\cdots, X_{n}\\).\nreplicate \\(X_1, \\cdots, X_{n}\\), calculate \\(\\hat{\\Theta}_1(\\boldsymbol{X}) = \\frac{\\sum_{=1}^n X_i}{n}\\) \\(\\hat{\\Theta}_2(\\boldsymbol{X}) = X_1 + 2\\).\nfind average \\(\\hat{\\Theta}_1(\\boldsymbol{X})\\) \\(\\hat{\\Theta}_2(\\boldsymbol{X})\\) across replicates.\ncan also use Monte Carlo simulations show results. simulation following:code simulates \\(X_1, \\cdots, X_{n}\\) ..d. known distribution, \\(n\\) fixed. code , used standard normal, \\(n=100\\).Generate large number replicates (used 10 thousand replicates) \\(X_1, \\cdots, X_{n}\\).replicate \\(X_1, \\cdots, X_{n}\\), calculate \\(\\hat{\\Theta}_1(\\boldsymbol{X}) = \\frac{\\sum_{=1}^n X_i}{n}\\) \\(\\hat{\\Theta}_2(\\boldsymbol{X}) = X_1 + 2\\).find average \\(\\hat{\\Theta}_1(\\boldsymbol{X})\\) \\(\\hat{\\Theta}_2(\\boldsymbol{X})\\) across replicates.estimator unbiased, average Monte Carlo simulation close 0, since true mean standard normal 0.\nFigure 7.4: Dist Theta1 (left) Theta2 (right)\nFigure 7.4 displays distribution 10 thousand \\(\\hat{\\Theta}_1(\\boldsymbol{X})\\)s \\(\\hat{\\Theta}_2(\\boldsymbol{X})\\)s. Visually, estimator unbiased “middle” equal value parameter, 0. can see clearly case \\(\\hat{\\Theta}_2(\\boldsymbol{X})\\) biased.results MC simulation matches math. shown \\(\\hat{\\Theta}_1(\\boldsymbol{X}) = \\frac{\\sum_{=1}^n X_i}{n}\\) unbiased estimator \\(\\mu\\), whereas \\(\\hat{\\Theta}_2(\\boldsymbol{X}) = X_1 + 2\\) biased estimator., based bias, sample mean better estimator population mean using first data point adding 2 .bias deals centrality estimator, .e. estimator equal true parameter, average. seen previous modules, concerned measures centrality, also measures uncertainty. example, likely observe random sample estimated value far true parameter?","code":"\nreps<-10000\n\nest1<-est2<-array(0,reps) ##array to store est1 & est2 from each rep\n\nn<-100\n\nset.seed(7)\n\nfor (i in 1:reps)\n  \n{\n  \n  X<-rnorm(n) ##Xi iid N(0,1) with n=100\n  \n  est1[i]<-mean(X) ##est 1\n  est2[i]<-X[1] + 2 ##est 2\n  \n}\n\nmean(est1) ##should be close to 0, indicating sample mean is unbiased## [1] -0.0009093132\nmean(est2) ##should not be close to 0, indicating first obs + 2 is biased## [1] 2.003234\n##create density plots to show distribution for est1 and est 2, and overlay line to show true value of mu\npar(mfrow=c(1,2))\nplot(density(est1), main=\"Density Plot for Theta1\")\nabline(v=0, col=\"blue\")\nplot(density(est2), main=\"Density Plot for Theta2\")\nabline(v=0, col=\"blue\")"},{"path":"est.html","id":"standard-error-and-variance","chapter":"7 Estimation","heading":"7.4.3 Standard Error and Variance","text":"turns concept variance can also applied estimators, can viewed random variables, just individual data points. Estimators smaller variances smaller degree uncertainty: value estimators change much random sample random sample.can go back example previous subsection. Consider \\(X_1, \\cdots, X_n\\) ..d. Normal unknown mean \\(\\mu\\) variance \\(\\sigma^2\\) known. defined two estimators \\(\\mu\\) \\(\\hat{\\Theta}_1(\\boldsymbol{X}) = \\frac{\\sum_{=1}^n X_i}{n}\\) \\(\\hat{\\Theta}_2(\\boldsymbol{X}) = X_1 + 2\\). Derive variance estimators.mathematical way answering question evaluate variance estimators:\\(Var(\\hat{\\Theta}_1) = \\frac{1}{n^2}Var(\\sum_{=1}^n X_i) = \\frac{1}{n^2} \\sum_{=1}^n Var(X_i) = \\frac{1}{n^2} (\\sigma^2 + \\cdots + \\sigma^2) = \\frac{\\sigma^2}{n}\\).\\(Var(\\hat{\\Theta}_1) = \\frac{1}{n^2}Var(\\sum_{=1}^n X_i) = \\frac{1}{n^2} \\sum_{=1}^n Var(X_i) = \\frac{1}{n^2} (\\sigma^2 + \\cdots + \\sigma^2) = \\frac{\\sigma^2}{n}\\).\\(Var(\\hat{\\Theta}_2) = Var(X_1 + 2) = Var(X_1) = \\sigma^2\\).\\(Var(\\hat{\\Theta}_2) = Var(X_1 + 2) = Var(X_1) = \\sigma^2\\).also separate term used evaluating variance estimator: standard error estimator. essentially standard deviation estimator, .e. \\(SE(\\hat{\\Theta}) = \\sqrt{Var(\\hat{\\Theta})}\\). Going back example: \\(SE(\\hat{\\Theta}_1) = \\frac{\\sigma}{\\sqrt{n}}\\) \\(SE(\\hat{\\Theta}_2) = \\sigma\\).Note: term standard error applied estimators. used finding standard deviation data points.can also use Monte Carlo simulation previous subsection estimate values:results reflected Figure 7.4. Notice scale x-axis plot \\(\\hat{\\Theta}_2\\) (right) much larger, \\(\\hat{\\Theta}_2\\) larger variability \\(\\hat{\\Theta}_1\\), .e. uncertain value \\(\\hat{\\Theta}_2\\), since deviate .clear now estimators smaller standard errors (variances) desired. , based standard error, sample mean better estimator population mean using first data point adding 2 .","code":"\nvar(est1) ##should be close to 1/100, since n=1000## [1] 0.009956095\nvar(est2) ##should be close to 1## [1] 0.9988228\nsd(est1) ##should be close to 1/10## [1] 0.09978023\nsd(est2) ##should be close to 1## [1] 0.9994112"},{"path":"est.html","id":"consistency","chapter":"7 Estimation","heading":"7.4.3.1 Consistency","text":"associated concept variance standard errors estimators consistency. definition consistency estimators fairly technical, give broad overview concept.Notice denoted estimator \\(\\hat{\\Theta}(\\boldsymbol{X})\\), \\(\\boldsymbol{X} = (X_1, \\cdots, X_n)^T\\). stands reason behavior \\(\\hat{\\Theta}(\\boldsymbol{X})\\) may change number data points \\(n\\) changes. use notation \\(\\hat{\\Theta}_n\\) denote estimator based \\(n\\) data points, emphasize focusing estimator changes \\(n\\) changes.estimator consistent \\(\\hat{\\Theta}_n\\) gets closer approaches true value \\(\\theta\\) \\(n\\) gets larger approaches infinity. means sample size \\(n\\) gets larger, estimator tends get closer true value parameter.go back example previous subsection. Consider \\(X_1, \\cdots, X_n\\) ..d. Normal unknown mean \\(\\mu\\) variance \\(\\sigma^2\\) known. defined two estimators \\(\\mu\\) \\(\\hat{\\Theta}_1(\\boldsymbol{X}) = \\frac{\\sum_{=1}^n X_i}{n}\\) \\(\\hat{\\Theta}_2(\\boldsymbol{X}) = X_1 + 2\\). Assess whether \\(\\hat{\\Theta}_1(\\boldsymbol{X})\\) \\(\\hat{\\Theta}_2(\\boldsymbol{X})\\) consistent .mathematical way answering question involves mathematical concepts get . general way assessing estimator consistent see variance shrinks towards zero \\(n\\) gets larger goes infinity.\nearlier showed \\(Var(\\hat{\\Theta}_1) = \\frac{\\sigma^2}{n}\\), shrinks towards zero \\(n\\) gets larger, consistent.\nearlier showed \\(Var(\\hat{\\Theta}_2) = \\sigma^2\\), shrink towards zero \\(n\\) gets larger, consistent.\nmathematical way answering question involves mathematical concepts get . general way assessing estimator consistent see variance shrinks towards zero \\(n\\) gets larger goes infinity.earlier showed \\(Var(\\hat{\\Theta}_1) = \\frac{\\sigma^2}{n}\\), shrinks towards zero \\(n\\) gets larger, consistent.earlier showed \\(Var(\\hat{\\Theta}_1) = \\frac{\\sigma^2}{n}\\), shrinks towards zero \\(n\\) gets larger, consistent.earlier showed \\(Var(\\hat{\\Theta}_2) = \\sigma^2\\), shrink towards zero \\(n\\) gets larger, consistent.earlier showed \\(Var(\\hat{\\Theta}_2) = \\sigma^2\\), shrink towards zero \\(n\\) gets larger, consistent.can also use Monte Carlo simulations show results:\ncode simulates \\(X_1, \\cdots, X_{n}\\) ..d. known distribution, \\(n\\) varied small values large values. code , used standard normal, \\(n=10, 100, 1000\\).\nvalue \\(n\\), generate large number replicates (used 10 thousand replicates) \\(X_1, \\cdots, X_{n}\\).\nreplicate \\(X_1, \\cdots, X_{n}\\), calculate \\(\\hat{\\Theta}_1(\\boldsymbol{X}) = \\frac{\\sum_{=1}^n X_i}{n}\\) \\(\\hat{\\Theta}_2(\\boldsymbol{X}) = X_1 + 2\\).\nproduce density plots \\(\\hat{\\Theta}_1\\) \\(\\hat{\\Theta}_2\\) \\(n=10, 100, 1000\\).\ncan also use Monte Carlo simulations show results:code simulates \\(X_1, \\cdots, X_{n}\\) ..d. known distribution, \\(n\\) varied small values large values. code , used standard normal, \\(n=10, 100, 1000\\).value \\(n\\), generate large number replicates (used 10 thousand replicates) \\(X_1, \\cdots, X_{n}\\).replicate \\(X_1, \\cdots, X_{n}\\), calculate \\(\\hat{\\Theta}_1(\\boldsymbol{X}) = \\frac{\\sum_{=1}^n X_i}{n}\\) \\(\\hat{\\Theta}_2(\\boldsymbol{X}) = X_1 + 2\\).produce density plots \\(\\hat{\\Theta}_1\\) \\(\\hat{\\Theta}_2\\) \\(n=10, 100, 1000\\).estimator consistent, notice spread density plot get narrower \\(n\\) gets larger.\nFigure 7.5: Dist Theta1 (left) Theta2 (right) n Varied\nFigure 7.5 displays distribution 10 thousand \\(\\hat{\\Theta}_1(\\boldsymbol{X})\\)s \\(\\hat{\\Theta}_2(\\boldsymbol{X})\\)s \\(n=10, 100, 1000\\). Visually, estimator consistent density plot becomes narrower \\(n\\) gets larger. see left plot, \\(\\hat{\\Theta}_1(\\boldsymbol{X})\\) consistent, see right plot, \\(\\hat{\\Theta}_2(\\boldsymbol{X})\\) consistent.Note unbiased estimators consistent estimators two different concepts. estimator unbiased inconsistent, biased consistent.Thought question: Suppose \\(X_1, \\cdots, X_n\\) ..d. standard normal. Consider estimator \\(\\mu\\), \\(\\hat{\\Theta}_3 = X_1\\), estimator \\(\\sigma^2\\), \\(\\hat{\\Theta}_4 = \\frac{1}{n} \\sum_{=1}^n (X_i - \\mu)^2\\). book says \\(\\hat{\\Theta}_3\\) unbiased inconsistent estimator \\(\\mu\\), \\(\\hat{\\Theta}_4\\) biased consistent estimator \\(\\sigma^2\\). Can use Monte Carlo simulations verify claims?","code":"\nn<-c(10,100,1000)\n\nreps<-10000\n\nest1<-est2<-array(0,c(length(n),reps)) ##arrays to store est 1 and est 2 as n changes\n\nset.seed(50)\n\nfor (i in 1:length(n))\n  \n{\n  \n  for (j in 1:reps)\n    \n  {\n    \n    X<-rnorm(n[i])\n    est1[i,j]<-mean(X) ##est 1\n    est2[i,j]<-X[1]+2 ##est 2\n    \n  }\n  \n}\n\npar(mfrow=c(1,2))\n\n##find max value of density plots for est 1 so plots all show up complete\nmax_y1 <- max(density(est1[1,])$y, density(est1[2,])$y, density(est1[3,])$y) \n\nplot(density(est1[1,]), ylim=c(0, max_y1), main=\"Density Plot of Est1 with n Varied\")\nlines(density(est1[2,]), col=\"blue\")\nlines(density(est1[3,]), col=\"red\")\nlegend(\"topright\", legend = c(\"n=10\", \"n=100\", \"n=1000\"), col = c(\"black\",\"blue\", \"red\"), lty = 1, cex=0.7)\n\n##find max value of density plots for est 2 so plots all show up complete\nmax_y2 <- max(density(est2[1,])$y, density(est2[2,])$y, density(est2[3,])$y)\n\nplot(density(est2[1,]), ylim=c(0, max_y2), main=\"Density Plot of Est2 with n Varied\")\nlines(density(est2[2,]), col=\"blue\")\nlines(density(est2[3,]), col=\"red\")\nlegend(\"topright\", legend = c(\"n=10\", \"n=100\", \"n=1000\"), col = c(\"black\",\"blue\", \"red\"), lty = 1, cex=0.7)"},{"path":"est.html","id":"sampling-distribution","chapter":"7 Estimation","heading":"7.4.4 Sampling Distribution","text":"plots Figure 7.4 Figure 7.5 give rise idea distribution associated estimator. term , called sampling distribution estimator.estimators known distributions, example sample mean, \\(\\bar{X}\\). common estimators also known distributions, sample proportion estimator population proportion, sample variance estimator population variance, well ML estimators linear regression logistic regression models.sampling distribution estimator known follows well known distributions, can easily perform probability calculations involving estimators, example, likely observe sample mean 2 standard errors away true mean?However, estimators may known distributions, example, sample median estimator population median known sampling distribution. mean unable calculate probabilities associated estimators? turns exist methods called resampling methods allow us approximate calculations. cover bootstrap, commonly used resampling method, future module.","code":""},{"path":"est.html","id":"sampdistmean","chapter":"7 Estimation","heading":"7.4.4.1 Sampling Distribution of Sample Mean","text":"state sampling distribution sample mean, \\(\\bar{X}_n\\). couple conditions consider:\\(X_1, \\cdots, X_n\\) ..d. normal distribution finite mean \\(\\mu\\) finite variance \\(\\sigma^2\\). \\(\\bar{X}_n \\sim N(\\mu, \\frac{\\sigma^2}{n})\\). result based fact sum independent normals result normal distribution.means data originally normally distributed, sample mean normally distributed, regardless sample size.\\(X_1, \\cdots, X_n\\) ..d. distribution finite mean \\(\\mu\\) finite variance \\(\\sigma^2\\), \\(n\\) large enough, \\(\\bar{X}_n\\) approximately \\(N(\\mu, \\frac{\\sigma^2}{n})\\). based CLT section 6.3.2.means even data originally normally distributed, sample mean approximately normally distributed sample size large enough.apply sampling distribution sample mean left plot Figure 7.5, visually displays distribution sample mean \\(n=10, 100, 1000\\). data ..d. standard normal, meet first scenario know sample means follow normal distribution:\\(n=10\\), \\(\\bar{X}_n \\sim N(0, \\frac{1}{10}) = N(0, 0.1)\\).\\(n=100\\), \\(\\bar{X}_n \\sim N(0, \\frac{1}{100}) = N(0,0.01)\\).\\(n=1000\\), \\(\\bar{X}_n \\sim N(0, \\frac{1}{1000}) = N(0,0.001)\\).see sample size gets larger, variance sample mean decreases, less spread resulting density plot concentrated around mean. matches see left plot Figure 7.5.","code":""},{"path":"est.html","id":"estMSE","chapter":"7 Estimation","heading":"7.4.5 Mean-Squared Error","text":"introduced mean-squared error (MSE) context evaluating prediction errors Section 4.4.1.3. MSE can also used evaluate estimator. context, MSE estimator \\(\\hat{\\Theta}\\) \\[\\begin{equation}\nMSE(\\hat{\\Theta}) = E\\left[(\\hat{\\Theta} - \\theta)^2 \\right].\n\\tag{7.4}\n\\end{equation}\\]MSE estimator can interpreted average squared difference estimator value parameter. turns MSE estimator related two metrics: bias estimator variance estimator:\\[\\begin{equation}\nMSE(\\hat{\\Theta}) = Var(\\hat{\\Theta}) + Bias(\\hat{\\Theta})^2.\n\\tag{7.5}\n\\end{equation}\\]words, MSE estimator equal variance estimator plus squared bias estimator. Equation (7.5) often called bias-variance decomposition MSE. estimator unbiased, equation Equation (7.5) tells us MSE estimator equal variance.MSE estimator suggests need consider bias variance estimator. People often think unbiased estimator “best”. However, unbiased estimator high variance. estimator high MSE. setting, may worth considering another estimator may biased, much smaller variance, resulting lower MSE. example can happen linear regression. Classical methods linear regression yield unbiased estimators, specific scenarios, estimators high variances. Another model, called ridge regression model, considers biased estimators may smaller variances, can result lower MSEs specific scenarios. learn models greater detail future class.","code":""},{"path":"est.html","id":"estcomments","chapter":"7 Estimation","heading":"7.5 Final Comments on Estimation","text":"covered method moments method maximum likelihood estimating parameters. methods also called parametric methods involve making assumption data follow well-known distribution unknown parameters, using data, assumed distribution, estimate numerical value unknown parameters.exist nonparametric methods estimation. actually seen one method (without calling nonparametric), kernel density estimation (KDE) Section 4.6.1. KDE used estimate PDF random variable can visualize distribution. look back KDE, notice made assumption distribution random variable. one fundamental differences parametric nonparametric estimation: former assumes distribution data, latter .","code":""},{"path":"est.html","id":"why-ml-estimators","chapter":"7 Estimation","heading":"7.5.1 Why ML Estimators?","text":"mentioned earlier ML estimators considered workhorse statistics data science likely common type estimator used. ML estimators properties:ML estimators consistent.ML estimators asymptotically Normal, .e. \\(\\frac{\\hat{\\Theta} - \\theta}{SE(\\hat{\\Theta})}\\) approximately standard normal \\(n\\) approaches infinity. also implies ML estimators asymptotically unbiased, .e. large \\(n\\), bias shrinks towards 0.ML estimators efficient. \\(n\\) approaches infinity, ML estimators lowest variance among unbiased estimators. However, smaller sample sizes, ML estimators may biased unbiased estimators smaller variances.ML estimators equivariant. \\(\\hat{\\Theta}\\) ML estimator \\(\\theta\\), \\(g(\\hat{\\Theta})\\) ML estimator \\(g(\\theta)\\).properties imply sample size large enough, estimates ML estimators highly likely close value parameter, ML estimators virtually unbiased, approximately normally distributed, smallest variance among possible unbiased estimators. properties exist sample size small.MOM estimators necessarily properties.properties require called regularity conditions. definitions get fairly technical beyond scope class. One conditions data ..d..","code":""},{"path":"confidence-intervals.html","id":"confidence-intervals","chapter":"8 Confidence Intervals","heading":"8 Confidence Intervals","text":"module based Introduction Probability Data Science (Chan), Chapter 9.1 9.2. can access book free https://probability4datascience.com. Please note cover additional topics, skip certain topics book.","code":""},{"path":"confidence-intervals.html","id":"introduction-4","chapter":"8 Confidence Intervals","heading":"8.1 Introduction","text":"Section 7, use data sample estimate parameters population. example, use sample mean systolic blood pressure 750 randomly selected American adults estimate mean systolic blood pressure American adults. also established estimators sample mean randomness . obtain another random sample 750 American adults, sample mean blood pressure sample likely different original random sample. uncertainty estimator due random sampling. also learned ways measure “well” estimator estimating parameter, bias, variance, standard error, mean-squared error estimator.section, introduce confidence intervals. Confidence intervals build ideas Section 7: estimators random can quantify uncertainty. purpose confidence interval provide range plausible values unknown population parameter, based sample. confidence interval provides estimated value parameter, also measure uncertainty associated estimation.first cover confidence intervals mean confidence intervals proportion, two basic confidence intervals. intervals based fact distribution corresponding estimators, sample mean sample proportion, known long certain conditions met. notice general ideas finding confidence intervals pretty similar; confidence intervals estimators known distributions constructed similarly. last subsection module, learn bootstrap, used distribution estimator unknown.","code":""},{"path":"confidence-intervals.html","id":"module-roadmap-6","chapter":"8 Confidence Intervals","heading":"8.1.1 Module Roadmap","text":"Section 8.2 covers key concepts confidence intervals apply mean.Section 8.3 shows concepts previous section carry confidence interval proportion.Section 8.4 goes method create confidence intervals estimators unknown sampling distributions.","code":""},{"path":"confidence-intervals.html","id":"CImean","chapter":"8 Confidence Intervals","heading":"8.2 Confidence Interval for the Mean","text":"","code":""},{"path":"confidence-intervals.html","id":"randomness-of-estimators","chapter":"8 Confidence Intervals","heading":"8.2.1 Randomness of Estimators","text":"Suppose trying estimate mean systolic blood pressure American adults, using sample mean 750 randomly selected American adults. sample mean estimator population mean. want able report value estimator, well uncertainty estimator. way measure uncertainty estimator variance standard error estimator. Larger values indicate higher degree uncertainty, estimator larger variance means value estimator likely different among random samples.Monte Carlo simulations Section 7.4 show distribution associated estimator. start sample mean, since distribution known (see Section 7.4.4.1). talk confidence interval mean first, generalizing ideas estimators known distributions.","code":""},{"path":"confidence-intervals.html","id":"randomness-of-confidence-intervals","chapter":"8 Confidence Intervals","heading":"8.2.2 Randomness of Confidence Intervals","text":"confidence interval probability applied estimator \\(\\bar{X}_n\\). Instead focusing estimated value sample mean variance, construct confidence interval mean takes form:\\[\\begin{equation}\n= \\left(\\bar{X}_n - \\epsilon, \\bar{X}_n + \\epsilon \\right).\n\\tag{8.1}\n\\end{equation}\\]terminology associated intervals form equation (8.1):\\(\\epsilon\\) often called margin error. (Yes margin error often see reported elections polls). value function standard error estimator, gives measure uncertainty estimated value.\nRemember uncertainty measured uncertainty due random sampling, due sources uncertainty getting representative sample, people lying, etc. mentioned earlier modules, methods used handle issues belong field survey sampling, interesting active area research. get issues class.\n\\(\\epsilon\\) often called margin error. (Yes margin error often see reported elections polls). value function standard error estimator, gives measure uncertainty estimated value.Remember uncertainty measured uncertainty due random sampling, due sources uncertainty getting representative sample, people lying, etc. mentioned earlier modules, methods used handle issues belong field survey sampling, interesting active area research. get issues class.value \\(\\bar{X}_n - \\epsilon\\) often called lower bound confidence interval.value \\(\\bar{X}_n - \\epsilon\\) often called lower bound confidence interval.value \\(\\bar{X}_n + \\epsilon\\) often called upper bound confidence interval.value \\(\\bar{X}_n + \\epsilon\\) often called upper bound confidence interval.value \\(\\bar{X}_n\\) often called point estimate population mean.value \\(\\bar{X}_n\\) often called point estimate population mean.Equation (8.1) sometimes expressed \\[\\begin{equation}\n\\text{point estimate } \\pm \\text{ margin error}.\n\\tag{8.2}\n\\end{equation}\\]Indeed, lot confidence intervals take form expressed equation (8.2), long sampling distribution estimator symmetric. Given interval mean expressed equation (8.1), ask probability interval \\(\\) includes true value parameter \\(\\mu\\), .e. want evaluate\\[\\begin{equation}\nP(\\mu \\ ) = P(\\bar{X}_n - \\epsilon \\leq \\mu \\leq \\bar{X}_n + \\epsilon).\n\\tag{8.3}\n\\end{equation}\\]important bear mind since estimator, sample mean \\(\\bar{X}_n\\) random variable, also randomness interval \\(\\). numerical values lower upper bounds change different random sample, since value \\(\\bar{X}_n\\) change.idea interval \\(\\) random represented Figure 8.1 :\nFigure 8.1: Randomness Confidence Interval. Picture https://en.wikipedia.org/wiki/Confidence_interval\ndensity curve top Figure 8.1 represents PDF random variable, represents distribution variable population wish study.row dots represents values 10 randomly sampled data points PDF.colored lines row represent lower upper bounds 50% confidence interval calculated sampled data points row.colored dot center confidence interval represents \\(\\bar{x}\\) sampled data points row.intervals blue represent confidence intervals contain value \\(\\mu\\), intervals red represent confidence intervals contain value \\(\\mu\\).Figure 8.1, note 50% confidence intervals capture value \\(\\mu\\), probability per equation (8.3) 0.5. matches theory since confidence interval Figure 8.1 computed 50% confidence.create 95% confidence intervals row Figure 8.1, upper lower bounds intervals adjusted expect 19 20 intervals contain value \\(\\mu\\).illustration gives us interpretation probability associated confidence interval per equation (8.3): construct 95% confidence interval, 95% chance random interval \\(\\) contain true value parameter. words, 100 random samples construct 95% confidence intervals based sample, expect 95 intervals contain value parameter.idea probability random interval \\(\\) captures true parameter gives rise confidence level. confidence level confidence interval denoted \\(1-\\alpha\\), .e. construct interval 95% confidence, \\(\\alpha=0.05\\). Equation (8.3) can written \\[\\begin{equation}\nP(\\bar{X}_n - \\epsilon \\leq \\mu \\leq \\bar{X}_n + \\epsilon) = 1 - \\alpha.\n\\tag{8.4}\n\\end{equation}\\]say \\(\\) \\((1-\\alpha) \\times 100\\%\\) confidence interval, \\(\\) confidence interval confidence level \\((1-\\alpha) \\times 100\\%\\).Now established confidence intervals random definition confidence level, ready go details constructing confidence interval mean.","code":""},{"path":"confidence-intervals.html","id":"constructing-confidence-interval-for-the-mean","chapter":"8 Confidence Intervals","heading":"8.2.3 Constructing Confidence Interval for the Mean","text":"remind sampling distribution sample mean, \\(\\bar{X}_n\\), Section 7.4.4.1. couple conditions consider:\\(X_1, \\cdots, X_n\\) ..d. normal distribution finite mean \\(\\mu\\) finite variance \\(\\sigma^2\\). \\(\\bar{X}_n \\sim N(\\mu, \\frac{\\sigma^2}{n})\\).\\(X_1, \\cdots, X_n\\) ..d. normal distribution finite mean \\(\\mu\\) finite variance \\(\\sigma^2\\). \\(\\bar{X}_n \\sim N(\\mu, \\frac{\\sigma^2}{n})\\).\\(X_1, \\cdots, X_n\\) ..d. distribution finite mean \\(\\mu\\) finite variance \\(\\sigma^2\\), \\(n\\) large enough, \\(\\bar{X}_n\\) approximately \\(N(\\mu, \\frac{\\sigma^2}{n})\\).\\(X_1, \\cdots, X_n\\) ..d. distribution finite mean \\(\\mu\\) finite variance \\(\\sigma^2\\), \\(n\\) large enough, \\(\\bar{X}_n\\) approximately \\(N(\\mu, \\frac{\\sigma^2}{n})\\).either conditions met, distribution \\(\\bar{X}_n\\) standardization either standard normal approaches standard normal distribution, \\(\\frac{\\bar{X}_n - \\mu}{\\frac{\\sigma}{\\sqrt{n}}} = \\frac{\\bar{X}_n - \\mu}{\\sqrt{Var(\\bar{X})}} = \\frac{\\bar{X}_n - \\mu}{SE(\\bar{X}_n)}\\) either standard normal approximately standard normal \\(n\\) large enough.simplify notation pertains confidence interval mean, let \\(\\hat{Z} = \\frac{\\bar{X}_n - \\mu}{SE(\\bar{X})}\\), \\(\\hat{Z}\\) standard normal approximately standard normal. \\(\\hat{Z}\\) can called standardized version sample mean standardized score.","code":""},{"path":"confidence-intervals.html","id":"critical-value","chapter":"8 Confidence Intervals","heading":"8.2.3.1 Critical Value","text":"perform math operations equation (8.4) see can construct confidence interval mean:\\[\\begin{equation}\n\\begin{split}\nP(\\bar{X}_n - \\epsilon \\leq \\mu \\leq \\bar{X}_n + \\epsilon) &= 1 - \\alpha \\\\\n\\implies P(|\\bar{X}_n - \\mu| \\leq \\epsilon) &= 1 - \\alpha \\\\\n\\implies P \\left(|\\hat{Z}| = |\\frac{\\bar{X}_n - \\mu}{SE(\\bar{X}_n)}|  \\leq \\frac{\\epsilon}{SE(\\bar{X}_n)} = z^{*} \\right) &= 1 - \\alpha \\\\\n\\implies P(|\\hat{Z}| \\leq z^{*}) &= 1 - \\alpha \\\\\n\\implies P(-z^{*} \\leq \\hat{Z} \\leq z^{*}) &= 1 - \\alpha.\n\\end{split}\n\\tag{8.5}\n\\end{equation}\\]equation (8.5), \\(z^*\\) called critical value. can see related margin error, \\(\\epsilon\\): margin error critical value multiplied standard error estimator (case standard error sample mean since constructing confidence interval mean).words, equation (8.5) says want find critical value \\(z^*\\) probability standardized score \\(-z^*\\) \\(z^*\\) \\(1 - \\alpha\\). Visually, probability displayed Figure 8.2 \\(\\alpha=0.05\\). want find values horizontal axis blue shaded area corresponds value 0.95 (recall area PDF represents probability).\nFigure 8.2: Finding Critical Value 95% Confidence\ncontinue working equation (8.5) see obtain value \\(z^*\\), long either two conditions sampling distribution \\(\\bar{X}_n\\) known met:\\[\\begin{equation}\n\\begin{split}\nP(-z^{*} \\leq \\hat{Z} \\leq z^{*}) &= P(\\hat{Z} \\leq z^{*}) - P(\\hat{Z} \\leq -z^{*}) \\\\\n                                  &= \\Phi(z^{*}) - \\Phi(-z^{*}) = 1 - \\alpha.\n\\end{split}\n\\tag{8.6}\n\\end{equation}\\]\\(\\Phi(z) = P(\\hat{Z} \\leq z)\\) CDF standard normal. Due symmetry standard normal, \\(\\Phi(-z^{*}) = 1- \\Phi(z^{*})\\), sub equation (8.6) continue working solve \\(z^*\\):\\[\\begin{equation}\n\\begin{split}\nP(-z^{*} \\leq \\hat{Z} \\leq z^{*}) &= 2 \\Phi(z^*) - 1 = 1 - \\alpha \\\\\n\\implies \\Phi(z^*) &= 1 - \\frac{\\alpha}{2} \\\\\n\\implies z^* &= \\Phi^{-1} \\left(1 - \\frac{\\alpha}{2} \\right)\n\\end{split}\n\\tag{8.7}\n\\end{equation}\\]\\(z^*\\) found inverting CDF standard normal evaluated \\(1 - \\frac{\\alpha}{2}\\). quantity can easily found using R. example, 95% confidence, \\(\\alpha = 0.05\\), type:tells us critical value 1.96 95% confidence.Note: qnorm() function introduced bit detail Section 4.6, feel free go back review.View video explains steps bit detail:Thought question: Can show critical value 96% confidence 2.054? Can show critical value 98% confidence 2.326?","code":"\nalpha<-0.05\nqnorm(1-alpha/2)## [1] 1.959964"},{"path":"confidence-intervals.html","id":"confidence-interval-for-the-mean","chapter":"8 Confidence Intervals","heading":"8.2.3.2 Confidence Interval for the Mean","text":"now ready put pieces together work confidence interval mean:\\[\\begin{equation}\n\\begin{split}\nP(-z^{*} \\leq \\hat{Z} \\leq z^{*}) &= P(-z^{*} \\leq \\frac{\\bar{X}_n - \\mu}{SE(\\bar{X}_n)} \\leq z^{*}) \\\\\n                                  &= P\\left(-z^{*}SE(\\bar{X}_n) \\leq \\bar{X}_n - \\mu \\leq z^{*}SE(\\bar{X}_n)\\right) \\\\\n                                  &= P\\left(\\bar{X}_n - z^{*}SE(\\bar{X}_n) \\leq \\mu \\leq \\bar{X}_n + z^{*}SE(\\bar{X}_n)\\right) \\\\\n                                  &= P\\left(\\bar{X}_n - z^{*} \\frac{\\sigma}{\\sqrt{n}} \\leq \\mu \\leq \\bar{X}_n + z^{*} \\frac{\\sigma}{\\sqrt{n}}\\right).\n\\end{split}\n\\tag{8.8}\n\\end{equation}\\]Therefore, \\((1-\\alpha) \\times 100\\%\\) confidence interval mean \\[\\begin{equation}\n\\left( \\bar{x}_n - z^{*} \\frac{\\sigma}{\\sqrt{n}}, \\bar{x}_n + z^{*} \\frac{\\sigma}{\\sqrt{n}} \\right).\n\\tag{8.9}\n\\end{equation}\\]View video explains steps bit detail:, formula equation (8.9) valid either two conditions Section 7.4.4.1 met, .e. either data originally normal, sample size large enough.several rules thumb exist assess “sample size large enough” (usually least 25 30 suggested). However, mentioned Section 6.3.2.1, fixed answer question. depends distribution data. general, skewed data , \\(n\\) needs larger approximation work.","code":""},{"path":"confidence-intervals.html","id":"CP","chapter":"8 Confidence Intervals","heading":"8.2.3.3 Coverage Probability","text":"Figure 8.1 illustrates concept coverage probability. 20 random samples drawn, corresponding 50% confidence intervals constructed, find 10 20 intervals contained true value parameter. coverage probability 50%, since 10 20 intervals contained true value value parameter. matches confidence level 50%. coverage probability match confidence level, , distribution used sampling distribution estimator probably incorrect.general, confidence intervals constructed \\((1-\\alpha) \\times 100\\%\\) confidence, coverage probability confidence intervals \\((1-\\alpha) \\times 100\\%\\).run Monte Carlo simulation show coverage probability confidence intervals mean using equation (8.9). code following:Simulate \\(X_1, \\cdots, X_{10}\\) ..d. standard normal.Calculate 95% confidence interval mean using (8.9).Assess calculated confidence interval contains 0, since true mean 0 (simulated data standard normal)Repeat steps total 10 thousand replicates.Count number confidence intervals contain 0, divide number replicates. value estimates coverage probability.\nthings correctly (correct use formula, correct distribution estimator), estimated coverage probability close confidence level 95%.\nthings correctly (correct use formula, correct distribution estimator), estimated coverage probability close confidence level 95%.estimated coverage probability based 10 thousand replicates 94.93%, close confidence level 95%. informs us distribution assumed sample mean correct.View video explains Monte Carlo simulation bit detail:","code":"\nn<-10 ## sample size of each random sample\nalpha<-0.05\nreps<-10000 \n\nCIs<-array(0, c(reps,2)) ##store lower and upper bounds of CI\ncontain<-array(0, reps) ##store assessment if the true mean is contained within bounds of CI\n\nset.seed(100)\nfor (i in 1:reps)\n  \n{\n  \n  X<-rnorm(n) ##draw n data points from standard normal\n  ##calculate elements needed for CI of mean\n  xbar<-mean(X) ##sample mean\n  SE<-1/sqrt(n) ##SE of sample mean\n  crit<-qnorm(1-alpha/2) ##critical value\n  ME<-crit*SE ##margin of error\n  CIs[i,1]<-xbar-ME ##lower bound of CI\n  CIs[i,2]<-xbar+ME ##upper bound of CI\n  contain[i]<-CIs[i,1]<0 & CIs[i,2]>0 ##assess if CI contains 0, the true mean\n  \n}\n\n##find proportion of CIs from random samples that contain 0, should be close to 1 - alpha\nsum(contain)/reps ## [1] 0.9493"},{"path":"confidence-intervals.html","id":"worked-example-1","chapter":"8 Confidence Intervals","heading":"8.2.3.4 Worked Example","text":"basis extensive tests, yield point particular type mild steel reinforcing bar known normally distributed \\(\\sigma=100\\) pounds. composition bar slightly modified, modification believed affected either normality value \\(\\sigma\\). random sample 25 modified bars resulted sample average yield point 8439 pounds, compute 90% CI true average yield point modified bars.question, summarize information :\\(n = 25\\),\\(\\bar{x} = 8439\\),\\(\\sigma = 100\\),\\(\\alpha = 0.1\\), \\(z^*\\) found using qnorm(1-0.1/2) 1.644854.Since assuming distribution yield points normally distribution, sample means normally distributed regardless sample size, can proceed computing confidence interval true average yield point using equation (8.9):\\[\n\\left( 8439 - 1.644854 \\frac{100}{\\sqrt{25}} , 8439 + 1.644854 \\frac{100}{\\sqrt{25}} \\right).\n\\]\nWorking everything , get (8406.103, 8471.891).Interpreting CI: 90% probability random interval (8406.103, 8471.891) include true average yield point modified bars.else can say confidence interval?Values outside confidence interval considered “ruled ” plausible values parameter. wanted assess average yield point modified bars 8000 pounds, interval support claim, since value 8000 lies outside interval. can say data support claim average yield point modified bars 8000 pounds.Values outside confidence interval considered “ruled ” plausible values parameter. wanted assess average yield point modified bars 8000 pounds, interval support claim, since value 8000 lies outside interval. can say data support claim average yield point modified bars 8000 pounds.Values inside confidence interval considered plausible values parameter. value inside interval considered plausible. common mistake specify certain value interval, conclude parameter equal specific value. example, mistake say since value 8410 lies inside interval, interval supports claim average yield point modified bars 8410 pounds. values interval still considered plausible.\nsituation, say data support claim average yield point modified bars different 8410 pounds, since 8410 lies inside interval. rule value 8410.\nValues inside confidence interval considered plausible values parameter. value inside interval considered plausible. common mistake specify certain value interval, conclude parameter equal specific value. example, mistake say since value 8410 lies inside interval, interval supports claim average yield point modified bars 8410 pounds. values interval still considered plausible.situation, say data support claim average yield point modified bars different 8410 pounds, since 8410 lies inside interval. rule value 8410.","code":""},{"path":"confidence-intervals.html","id":"CImeant","chapter":"8 Confidence Intervals","heading":"8.2.4 Confidence Interval for the Mean Using t Distribution","text":"may noticed calculate confidence interval mean using equation (8.9), involves knowing value \\(\\sigma^2\\), variance variable population, parameter. However, mentioned whole purpose estimation confidence intervals estimate value unknown parameters quantify uncertainty associated estimate. numerical value parameters rarely known! actually use equation (8.9) real life?solution fairly intuitive, use sample variance \\(s^2 = \\frac{1}{n-1} \\sum_{=1}^n (x_i - \\bar{x})^2\\) estimate \\(\\sigma^2\\). less intuitive distribution standardized version sample mean changes.earlier mentioned certain conditions met, \\(\\hat{Z} = \\frac{\\bar{X}_n - \\mu}{\\frac{\\sigma}{\\sqrt{n}}}\\) either standard normal approximately standard normal. estimate \\(\\sigma^2\\) \\(s^2\\) replace \\(\\sigma\\) \\(s\\) \\(\\hat{Z}\\), get new random variable\\[\\begin{equation}\nT =  \\frac{\\bar{X}_n - \\mu}{\\frac{s}{\\sqrt{n}}} = \\frac{\\bar{X}_n - \\mu}{SE(\\bar{X}_n)},\n\\tag{8.10}\n\\end{equation}\\]\\(SE(\\bar{X}_n)\\) now \\(\\frac{s}{\\sqrt{n}}\\). turns \\(T\\) follows another well-known distribution, called \\(t\\) distribution \\(n-1\\) degrees freedom. PDF \\(t\\) distribution pretty long needed course (can look ), take look plot PDF compare plot PDF standard normal, Figure 8.3 .\nFigure 8.3: Plot PDF Z t Distributions\nFigure 8.3, note things \\(t\\) distribution:centered 0, just like standard normal.also symmetric bell-shaped, just like standard normal.heavier tails standard normal. words, extreme values (large small) slightly higher probabilities occurring \\(t\\) distribution standard normal.degree freedom increases, \\(t\\) distribution gets closer standard normal. Notice blue curve closer standard normal curve black, red curve black curve. fact, one can show mathematically PDF \\(t\\) distribution converges PDF standard normal degree freedom increases infinity.fact working \\(t\\) distribution instead standard normal affect calculate confidence interval mean? equation (8.9) change? replace \\(\\sigma^2\\) \\(s^2\\) \\(z^*\\) \\(t^*\\). critical value now denoted \\(t^*\\) emphasize based \\(t\\) distribution.example find critical value \\(t\\) distribution 10 degrees freedom 95% confidence:Therefore, \\((1-\\alpha) \\times 100\\%\\) confidence interval mean , \\(\\sigma^2\\) unknown, \\[\\begin{equation}\n\\left( \\bar{x}_n - t^{*} \\frac{s}{\\sqrt{n}}, \\bar{x}_n + t^{*} \\frac{s}{\\sqrt{n}} \\right).\n\\tag{8.11}\n\\end{equation}\\], formula equation (8.11) valid either two conditions Section 7.4.4.1 met, .e. either data originally normal, sample size large enough.","code":"\n##plot PDF from -5 to 5\nx <- seq(-5, 5, length.out = 100)\n\n##plot the standard normal \ncurve(dnorm(x), from = -5, to = 5, lwd = 2,\n      ylab = \"Density\", xlab = \"x\", \n      main = \"Pdf of Standard Normal and t-Distribution\")\n\n##overlay the t-distribution with 1 and 10 degree of freedom\ncurve(dt(x, df = 1), from = -5, to = 5, col = \"red\", lwd = 2, add = TRUE)\ncurve(dt(x, df = 10), from = -5, to = 5, col = \"blue\", lwd = 2, add = TRUE)\nlegend(\"topright\", legend = c(\"Standard Normal\", \"t-Distribution (df=1)\", \"t-Distribution (df=10)\"),\n       col = c(\"black\", \"red\", \"blue\"), lty = 1, lwd = 2)\nalpha<-0.05\nqt(1-alpha/2, 10) ##supply percentile first, then value of df## [1] 2.228139"},{"path":"confidence-intervals.html","id":"worked-example-2","chapter":"8 Confidence Intervals","heading":"8.2.4.1 Worked Example","text":"sample 66 obese adults put low-carbohydrate diet year. average weight loss 11 lb \nstandard deviation 19 lb. Calculate 99% confidence interval true average weight loss. confidence interval provide support claim mean weight loss positive?question, summarize information :\\(n = 66\\),\\(df = n-1 = 65\\)\\(\\bar{x} = 11\\),\\(s = 19\\),\\(\\alpha = 0.01\\), \\(t^*\\) found using qt(1-0.01/2, 65) 2.653604.information distribution data. However, sample size 66, usually large enough use CLT, can use equation (8.11):\\[\n\\left( 11 - 2.653604 \\frac{19}{\\sqrt{66}} , 11 + 2.653604 \\frac{19}{\\sqrt{66}} \\right)\n\\]\ngives (4.793914, 17.20609). Since entire confidence interval lies 0, data support claim mean weight loss positive.","code":""},{"path":"confidence-intervals.html","id":"confidence-interval-for-mean-using-t-vs-using-z","chapter":"8 Confidence Intervals","heading":"8.2.4.2 Confidence Interval for Mean Using \\(t\\) VS Using \\(z\\)","text":"couple interesting things note critical values associated \\(t\\) distribution:level confidence, \\(t^*\\) never smaller \\(z^*\\).Thought question: Can give intuitive explanation ? Using Figure 8.3 may helpful.value \\(t^*\\) gets closer value \\(z^*\\) degree freedom gets larger. implies \\(t^*\\) approximately equal \\(z^*\\) large sample sizes.Recall margin error critical value multiplied standard error estimator. margin errors tend larger \\(\\sigma^2\\) unknown. make intuitive sense higher degree uncertainty since additional parameter estimate.Since margin error tends larger \\(\\sigma^2\\) unknown, means width confidence interval mean tends wider \\(\\sigma^2\\) unknown. width confidence interval difference upper lower bound, twice margin error.Thought question: Try running Monte Carlo simulation Section 8.2.3.3, now use sample variance instead population variance. Find coverage probabilities confidence interval use correct formula equation (8.11), use \\(z^*\\) critical value sample variance.","code":""},{"path":"confidence-intervals.html","id":"df","chapter":"8 Confidence Intervals","heading":"8.2.4.3 Degrees of Freedom","text":"intuitive explanation degrees freedom number independent pieces information can take numerical value, estimating parameter.Generally speaking, lose 1 degree freedom every equation must satisfied. context estimating population mean using sample mean, must always satisfy equation \\(\\bar{x} = \\frac{\\sum_{=1}^n x_i}{n}\\), lose 1 degree freedom original set \\(n\\) observations, degree freedom \\(n-1\\) calculating CI mean.View video explains idea behind degrees freedom toy example:","code":""},{"path":"confidence-intervals.html","id":"factors-affecting-precision-of-confidence-intervals","chapter":"8 Confidence Intervals","heading":"8.2.5 Factors Affecting Precision of Confidence Intervals","text":"width confidence interval used measure precision. wider width indicates less precision higher degree uncertainty estimate. Narrower widths preferred, able narrow range plausible values unknown parameter ruling values. factors (within researcher’s control) affecting width confidence interval items used calculating margin error:level confidence, \\(1 - \\alpha\\): width increases level confidence increases.Though question: Can use Figure 8.2 help explain width increases level confidence increases?sample size, \\(n\\): \\(n\\) increases, width decreases. implies precision, lower degree uncertainty, larger sample sizes.factors whether \\(\\sigma^2\\) unknown , value variance, usually controllable researcher.","code":""},{"path":"confidence-intervals.html","id":"CIprop","chapter":"8 Confidence Intervals","heading":"8.3 Confidence Interval for the Proportion","text":"Next, go confidence interval proportion. another common confidence interval. sample proportion estimator population proportion.Proportions used summarize categorical variables, whereas means used summarize quantitative variables. One way decide variable categorical quantitative ask whether arithmetic operations make sense performed variable. operations make sense, variable quantitative, , variable categorical. Consider following two research questions:measure systolic blood pressure sample American adults. variable quantitative, since answer numeric value arithmetic operations can applied. calculate average systolic blood pressure American adults. can work confidence interval mean, using equation (8.11).measure systolic blood pressure sample American adults. variable quantitative, since answer numeric value arithmetic operations can applied. calculate average systolic blood pressure American adults. can work confidence interval mean, using equation (8.11).ask sample voters whether support particular candidate. variable categorical, since answer yes , apply arithmetic operations answer. calculate proportion voters support candidate. need confidence interval proportion.ask sample voters whether support particular candidate. variable categorical, since answer yes , apply arithmetic operations answer. calculate proportion voters support candidate. need confidence interval proportion.","code":""},{"path":"confidence-intervals.html","id":"sampdistprops","chapter":"8 Confidence Intervals","heading":"8.3.1 Sampling Distribution of Sample Proportions","text":"can use Central Limit Theorem (CLT) approximate sampling distribution sample proportions. sketch derive sampling distribution follows:Let \\(X_1, \\cdots, X_n\\) ..d. Bernoulli success probability p. Using equations (3.9) (3.10), know \\(E(X_i) = p\\) \\(Var(X_i) = p(1-p)\\).Let \\(X_1, \\cdots, X_n\\) ..d. Bernoulli success probability p. Using equations (3.9) (3.10), know \\(E(X_i) = p\\) \\(Var(X_i) = p(1-p)\\).Let \\(S = X_1 + \\cdots + X_n = \\sum_{=1}^n X_i\\) denote number successes sample size \\(n\\). Using properties expectations variances, know \\(E(S) = np\\) \\(Var(S) = np(1-p)\\).Let \\(S = X_1 + \\cdots + X_n = \\sum_{=1}^n X_i\\) denote number successes sample size \\(n\\). Using properties expectations variances, know \\(E(S) = np\\) \\(Var(S) = np(1-p)\\).sample proportion, \\(\\hat{p}\\), just number successes divided sample size, \\(\\hat{p} = \\frac{S}{n} = \\frac{\\sum_{=1}^n X_i}{n} = \\bar{X}_n\\). Therefore, using property expectations variances, know \\(E(\\hat{p}) = p\\) \\(Var(\\hat{p}) = \\frac{p(1-p)}{n}\\).sample proportion, \\(\\hat{p}\\), just number successes divided sample size, \\(\\hat{p} = \\frac{S}{n} = \\frac{\\sum_{=1}^n X_i}{n} = \\bar{X}_n\\). Therefore, using property expectations variances, know \\(E(\\hat{p}) = p\\) \\(Var(\\hat{p}) = \\frac{p(1-p)}{n}\\).CLT informs us \\(n\\) large enough, \\(\\hat{p}\\) approximately \\(N\\left(p, \\frac{p(1-p)}{n}\\right)\\). Therefore, distribution \\(\\hat{p}\\) standardization approximately standard normal, .e. standardized score \\(\\hat{Z} = \\frac{\\hat{p}-p}{\\sqrt{\\frac{p(1-p)}{n}}}\\) approximately standard normal.CLT informs us \\(n\\) large enough, \\(\\hat{p}\\) approximately \\(N\\left(p, \\frac{p(1-p)}{n}\\right)\\). Therefore, distribution \\(\\hat{p}\\) standardization approximately standard normal, .e. standardized score \\(\\hat{Z} = \\frac{\\hat{p}-p}{\\sqrt{\\frac{p(1-p)}{n}}}\\) approximately standard normal.View video detailed explanation sampling distribution sample proportions:","code":""},{"path":"confidence-intervals.html","id":"constructing-confidence-interval-for-the-proportion","chapter":"8 Confidence Intervals","heading":"8.3.2 Constructing Confidence Interval for the Proportion","text":"lot concepts math confidence interval mean carry confidence interval proportion. skip math confidence interval proportion (although able derive result following steps adjusting).\\((1-\\alpha) \\times 100\\%\\) confidence interval proportion \\[\\begin{equation}\n\\left( \\hat{p} - z^{*} \\sqrt{\\frac{\\hat{p}(1-\\hat{p})}{n}}, \\hat{p} + z^{*} \\sqrt{\\frac{\\hat{p}(1-\\hat{p})}{n}} \\right).\n\\tag{8.12}\n\\end{equation}\\]equation (8.12), can see thatThe point estimate \\(\\hat{p}\\).standard error \\(\\hat{p}\\) \\(SE(\\hat{p}) = \\sqrt{\\frac{\\hat{p}(1-\\hat{p})}{n}}\\).margin error \\(z^{*} \\sqrt{\\frac{\\hat{p}(1-\\hat{p})}{n}}\\).framework confidence intervals usually takes form given equation (8.2): add subtract margin error point estimate. margin error critical value multiplied standard error estimator. see confidence intervals mean proportion take framework, confidence intervals estimators, long sampling distribution estimator symmetric.Note: equation (8.12) based proportions, percentages. calculating confidence interval proportion, reporters often convert values percentages multiplying hundred.equation (8.12), can also see factors affect width confidence interval proportion confidence interval mean.","code":""},{"path":"confidence-intervals.html","id":"conditions-for-confidence-interval-for-the-proportion","chapter":"8 Confidence Intervals","heading":"8.3.2.1 Conditions for Confidence Interval for the Proportion","text":"wrote \\(n\\) large enough, CLT informs us sample proportion \\(\\hat{p}\\) can approximated normal distribution. large large enough? , various rules thumb recommended, usually follow along lines needing least certain number successes, \\(n\\hat{p}\\), failures, \\(n(1-\\hat{p})\\) sample. Values least 5 10 usually recommended. Just bear mind approximation works better number successes failures, \\(n\\hat{p}\\) \\(n(1-\\hat{p})\\), increases.","code":""},{"path":"confidence-intervals.html","id":"worked-example-3","chapter":"8 Confidence Intervals","heading":"8.3.2.2 Worked Example","text":"Texas College Tobacco Project survey administered 2016 found sample 5767 undergraduates ages 18–25, 525 said used electronic cigarettes least previous 30 days. Find 95% confidence interval proportion students population sampled used e-cigarettes previous 30 days. Also report margin error. data support claim 5% students population used e-cigarettes previous 30 days?question, summarize information :\\(n = 5767\\),\\(\\hat{p} = \\frac{525}{5767}\\),\\(\\alpha = 0.05\\), \\(z^*\\) found using qnorm(1-0.05/2) 1.959964.number “successes” 525 number “failures” \\(5767-525 = 5242\\). lot larger 10, can work sampling distribution \\(\\hat{p}\\) calculate confidence interval using equation (8.12):\\[\n\\left( \\frac{525}{5767} - 1.959964 \\sqrt{\\frac{\\frac{525}{5767}(1-\\frac{525}{5767})}{5767}} , \\frac{525}{5767} + 1.959964 \\sqrt{\\frac{\\frac{525}{5767}(1-\\frac{525}{5767})}{5767}} \\right)\n\\]gives (0.08361097, 0.09845943), margin error 0.007424228. data support claim 5% students population used e-cigarettes previous 30 days, since entire interval lies 0.05.95% probability random interval (0.08361097, 0.09845943) contains true proportion students population used e-cigarettes previous 30 days.","code":""},{"path":"confidence-intervals.html","id":"minimum-sample-size","chapter":"8 Confidence Intervals","heading":"8.3.3 Minimum Sample Size","text":"Fairly often, researchers want know sample size needed order guarantee margin error specific value, want guarantee certain level precision report. Mathematically, want set \\(z^{*} \\sqrt{\\frac{\\hat{p}(1-\\hat{p})}{n}} \\leq M\\) \\(M\\) denotes value margin error needs guaranteed.know value \\(\\hat{p}\\) collecting data, mentioned Section 3.5.1.1 value \\(p(1-p)\\) maximized \\(p=\\frac{1}{2}\\). implies \\(SE(\\hat{p})\\) maximized \\(\\hat{p} = \\frac{1}{2}\\). long \\(M\\) satisfied \\(\\hat{p} = \\frac{1}{2}\\), \\(M\\) satisfied value \\(\\hat{p}\\). subbing \\(\\hat{p} = \\frac{1}{2}\\) \\(z^{*} \\sqrt{\\frac{\\hat{p}(1-\\hat{p})}{n}} \\leq M\\), \\[\\begin{equation}\n\\begin{split}\nz^{*} \\sqrt{\\frac{\\hat{p}(1-\\hat{p})}{n}} & \\leq M \\\\\n\\implies z^* \\sqrt{\\frac{\\frac{1}{2}\\frac{1}{2}}{n}}  & \\leq M \\\\\n\\implies \\left(\\frac{z^*}{2M}\\right)^2 & \\leq n.\n\\end{split}\n\\tag{8.13}\n\\end{equation}\\]Note: always sure round nearest whole number, working LHS inequality (8.13), guarantee margin error \\(M\\).","code":""},{"path":"confidence-intervals.html","id":"worked-example-4","chapter":"8 Confidence Intervals","heading":"8.3.3.1 Worked Example","text":"state legislator wishes survey residents district see proportion electorate aware position using state funds pay abortions. sample size necessary 95% CI p margin error 3 percentage points?Using inequality (8.13),\\[\n\\begin{split}\n\\left(\\frac{1.959964}{2 \\times 0.03}\\right)^2 \\leq n \\\\\n1067.072 \\leq n.\n\\end{split}\n\\]\nneed sample least 1068 residents guarantee margin error 3 percentage points.","code":""},{"path":"confidence-intervals.html","id":"bootstrap","chapter":"8 Confidence Intervals","heading":"8.4 The Bootstrap","text":"previous subsections, work confidence interval estimator using framework equation (8.2): add subtract margin error point estimate. margin error critical value multiplied standard error estimator. critical value based sampling distribution estimator. framework requires us know sampling distribution standard error estimator.However, estimators whose sampling distributions standard errors unknown, example, sample median. can construct confidence intervals median? can quantify uncertainty sample median estimator population median?One idea collect many random samples, compute sample median sample, create density plot like Monte Carlo simulations Figure 7.4. However, feasible real life rarely time money go around obtaining many random samples. able work one random sample .method work around issues bootstrap. bootstrap enable us :Estimate standard error variance estimators known formulas .Estimate standard error variance estimators known formulas .Construct confidence intervals estimators unknown sampling distributions unknown formulas standard errors.Construct confidence intervals estimators unknown sampling distributions unknown formulas standard errors.can without collect data many random samples. going bootstrap works, remind Monte Carlo simulations work terms finding sampling distribution standard errors estimators.","code":""},{"path":"confidence-intervals.html","id":"estimating-sampling-distributions-using-many-random-samples","chapter":"8 Confidence Intervals","heading":"8.4.1 Estimating Sampling Distributions Using Many Random Samples","text":"using Monte Carlo simulations estimate sampling distribution estimators (see code generated Figures 7.4 7.5 examples), typically following:Assume data follow known distribution PDF \\(f_X\\) CDF \\(F_X\\).Assume data follow known distribution PDF \\(f_X\\) CDF \\(F_X\\).Simulate \\(n\\) data points assumed distribution \\(K\\) times, \\(K\\) large, obtain \\(K\\) replicate datasets size \\(n\\) denoted \\(\\mathcal{X}^{1}, \\cdots, \\mathcal{X}^{K}\\). (lot code \\(K\\) called reps).Simulate \\(n\\) data points assumed distribution \\(K\\) times, \\(K\\) large, obtain \\(K\\) replicate datasets size \\(n\\) denoted \\(\\mathcal{X}^{1}, \\cdots, \\mathcal{X}^{K}\\). (lot code \\(K\\) called reps).dataset, calculate value estimator \\(\\hat{\\Theta}\\). Since \\(K\\) datasets, \\(K\\) values estimators, denoted \\(\\hat{\\Theta}^{1}, \\cdots, \\hat{\\Theta}^{K}\\).dataset, calculate value estimator \\(\\hat{\\Theta}\\). Since \\(K\\) datasets, \\(K\\) values estimators, denoted \\(\\hat{\\Theta}^{1}, \\cdots, \\hat{\\Theta}^{K}\\).estimate true mean \\(\\hat{\\Theta}\\) taking sample mean \\(\\hat{\\Theta}^{1}, \\cdots, \\hat{\\Theta}^{K}\\).estimate true mean \\(\\hat{\\Theta}\\) taking sample mean \\(\\hat{\\Theta}^{1}, \\cdots, \\hat{\\Theta}^{K}\\).estimate true variance \\(\\hat{\\Theta}\\) taking sample variance \\(\\hat{\\Theta}^{1}, \\cdots, \\hat{\\Theta}^{K}\\). Square-rooting value gives estimate standard error \\(\\hat{\\Theta}\\).estimate true variance \\(\\hat{\\Theta}\\) taking sample variance \\(\\hat{\\Theta}^{1}, \\cdots, \\hat{\\Theta}^{K}\\). Square-rooting value gives estimate standard error \\(\\hat{\\Theta}\\).Note: step bit different described book. book, author uses population variance, instead sample variance. population variance divides sum squared deviations mean \\(K\\), sample variance divides \\(K-1\\). seen authors divide \\(K\\), others divide \\(K-1\\). var() function R calculates sample variance, admit level convenience use sample variance can use R function. \\(K\\) needs large bootstrap work well, end result numerical values close .create density plot \\(\\hat{\\Theta}^{1}, \\cdots, \\hat{\\Theta}^{K}\\) estimate sampling distribution \\(\\hat{\\Theta}\\).written earlier, problem applying idea estimator unknown sampling distribution know associated PDF \\(f_X\\) CDF \\(F_X\\). carry steps listed. bootstrap mimic steps, access data one random sample. Next, describe steps carried bootstrap. Pay attention similarities differences steps steps .","code":""},{"path":"confidence-intervals.html","id":"the-bootstrap-algorithm","chapter":"8 Confidence Intervals","heading":"8.4.2 The Bootstrap Algorithm","text":"Instead simulating \\(K\\) replicate datasets assumed distribution, simulate \\(K\\) replicate datasets sampling, replacement, \\(n\\) data points original dataset. “replacement” term key: means data point sampled, returned original dataset can sampled .See video visual representation sampling replacement:Let \\(\\mathcal{X}\\) denote original dataset \\(n\\) observations. Let \\(\\hat{\\theta}\\) denote value estimator original dataset.Synthesize \\(K\\) bootstrapped datasets \\(\\mathcal{Y}^{(1)}, \\cdots, \\mathcal{Y}^{(K)}\\) bootstrapped dataset derived sampling \\(n\\) data points original dataset, replacement.Synthesize \\(K\\) bootstrapped datasets \\(\\mathcal{Y}^{(1)}, \\cdots, \\mathcal{Y}^{(K)}\\) bootstrapped dataset derived sampling \\(n\\) data points original dataset, replacement.dataset, calculate value estimator \\(\\hat{\\Theta}\\). Since \\(K\\) bootstrapped datasets, \\(K\\) values estimators, denoted \\(\\hat{\\Theta}^{(1)}, \\cdots, \\hat{\\Theta}^{(K)}\\).dataset, calculate value estimator \\(\\hat{\\Theta}\\). Since \\(K\\) bootstrapped datasets, \\(K\\) values estimators, denoted \\(\\hat{\\Theta}^{(1)}, \\cdots, \\hat{\\Theta}^{(K)}\\).estimate true mean \\(\\hat{\\Theta}\\) taking sample mean \\(\\hat{\\Theta}^{(1)}, \\cdots, \\hat{\\Theta}^{(K)}\\). value denoted \\(M_b(\\hat{\\Theta})\\).estimate true mean \\(\\hat{\\Theta}\\) taking sample mean \\(\\hat{\\Theta}^{(1)}, \\cdots, \\hat{\\Theta}^{(K)}\\). value denoted \\(M_b(\\hat{\\Theta})\\).estimate true variance \\(\\hat{\\Theta}\\) taking sample variance \\(\\hat{\\Theta}^{(1)}, \\cdots, \\hat{\\Theta}^{(K)}\\). value denoted \\(V_b(\\hat{\\Theta})\\), call bootstrap variance estimator. Square-rooting \\(V_b(\\hat{\\Theta})\\) gives estimate standard error \\(\\hat{\\Theta}\\), denoted \\(SE_b(\\Theta)\\). \\(K\\) large enough, \\(V_b(\\hat{\\Theta})\\) approximate true variance estimator.estimate true variance \\(\\hat{\\Theta}\\) taking sample variance \\(\\hat{\\Theta}^{(1)}, \\cdots, \\hat{\\Theta}^{(K)}\\). value denoted \\(V_b(\\hat{\\Theta})\\), call bootstrap variance estimator. Square-rooting \\(V_b(\\hat{\\Theta})\\) gives estimate standard error \\(\\hat{\\Theta}\\), denoted \\(SE_b(\\Theta)\\). \\(K\\) large enough, \\(V_b(\\hat{\\Theta})\\) approximate true variance estimator.Note: see note step 5 previous subsection. note applies .can create density plot \\(\\hat{\\Theta}^{(1)}, \\cdots, \\hat{\\Theta}^{(K)}\\) estimate sampling distribution \\(\\hat{\\Theta}\\). \\(K\\) large enough, density plot approximate true PDF \\(f_X\\) well.Comparing algorithm Monte Carlo simulations, main differences :bootstrap, assume distribution original data. Monte Carlo simulations, make assumption distribution original data.bootstrap, assume distribution original data. Monte Carlo simulations, make assumption distribution original data.\\(K\\) datasets produced slightly differently. bootstrap, synthesized sampling replacement \\(n\\) data points original dataset size \\(n\\). Monte Carlo simulations, draw \\(n\\) data points assumed distribution.\\(K\\) datasets produced slightly differently. bootstrap, synthesized sampling replacement \\(n\\) data points original dataset size \\(n\\). Monte Carlo simulations, draw \\(n\\) data points assumed distribution.math proves bootstrap works beyond level class, although freely available online.bootstrap variance estimator, \\(V_b(\\hat{\\Theta})\\), tends work better \\(K\\) increases \\(n\\) increases.","code":""},{"path":"confidence-intervals.html","id":"confidence-intervals-using-the-bootstrap","chapter":"8 Confidence Intervals","heading":"8.4.3 Confidence Intervals using the Bootstrap","text":"bootstrap \\((1-\\alpha) \\times 100\\%\\) confidence interval \\(\\theta\\) \\[\\begin{equation}\n\\left( 2 \\hat{\\theta} - \\hat{\\theta}_{1-\\alpha/2}^*, 2 \\hat{\\theta} - \\hat{\\theta}_{\\alpha/2}^* \\right),\n\\tag{8.14}\n\\end{equation}\\]\\(\\hat{\\theta}_{\\beta}^*\\) \\(\\beta \\times 100\\)th percentile \\(\\hat{\\Theta}^{(1)}, \\cdots, \\hat{\\Theta}^{(K)}\\), \\(K\\) numerical values estimator bootstrapped sample.may notice confidence interval equation (8.14) fit general framework given equation (8.2). sampling distribution estimator may symmetric.math derives confidence interval beyond level class, although freely available online.bootstrap confidence interval tends work better \\(K\\) increases \\(n\\) increases.","code":""},{"path":"confidence-intervals.html","id":"worked-example-5","chapter":"8 Confidence Intervals","heading":"8.4.4 Worked Example","text":"data 400 credit card customers. density plot credit limits shown :\nFigure 8.4: Density Plot Credit Limits\ndensity plot Figure 8.4 informs us values credit limits skewed, median better measure centrality credit limits. wish estimate population median credit limit, give measure uncertainty, well report 95% confidence interval median credit limit. Since sampling distribution sample median unknown, use bootstrap:report following:sample median $4622.5.standard error sample median $152.0525.95% confidence interval median credit limit (4380.000, 4948.525). 95% probability true population median credit limit lies random interval $4380 $4948.525.View video explains functions used code:","code":"\nlibrary(ISLR2)\nData<-ISLR2::Credit$Limit\nplot(density(Data), xlab=\"Credit Limit\", main=\"Density Plot of Credit Limits\")\n##get sample size\nn<-length(Data)\n\nK<-1000 ##Number of bootstrap samples\nset.seed(77)\n\n##the sample function, with replace=TRUE, samples with replacement. \n##The replicate function repeats the sample function K times\nboot.samples<-replicate(K,sample(Data,n,replace=TRUE))\n##Each column in boot.samples represents each bootstrap sample\n\n##find the sample median of each column, since each column represents a bootstrap sample\nboot.medians<-apply(boot.samples,2,median)\n\n##bootstrap SE of the median is just the SD of the sample medians from each bootstrap sample\nsd(boot.medians)## [1] 152.0525\n##bootstrap variance of the median is just the variance of the sample medians from each bootstrap sample\nvar(boot.medians)## [1] 23119.95\n##find bootstrap CI\nalpha<-0.05\n\n##median from original sample\norig.median<-median(Data)\norig.median## [1] 4622.5\n##lower and upper bound of the CI\n2*orig.median-quantile(boot.medians,c(1-alpha/2,alpha/2))##    97.5%     2.5% \n## 4380.000 4948.525"},{"path":"hypothesis-testing.html","id":"hypothesis-testing","chapter":"9 Hypothesis Testing","heading":"9 Hypothesis Testing","text":"module based Introduction Probability Data Science (Chan), Chapter 9.3. can access book free https://probability4datascience.com. Please note cover additional topics.","code":""},{"path":"hypothesis-testing.html","id":"introduction-5","chapter":"9 Hypothesis Testing","heading":"9.1 Introduction","text":"Consider scenario: conducting clinical trials assess whether new vaccine effective existing vaccines. Suppose efficacy existing vaccines certain disease 80%, conduct clinical trials 30 patients, new vaccine effective 25 patients. strong result trying prove new vaccine effective existing vaccines? Questions like lead hypothesis testing.previous modules, established fact estimators likely deviate value corresponding parameter, just due random sampling. Confidence intervals allow us provide measure uncertainty estimator well range plausible values parameter, using concepts regarding sampling distribution estimators. concepts can also used assess whether observed value estimator different enough potential value parameter random sampling alone unlikely reason difference. assessment done, want make conclusion decision unknown parameter. big picture idea behind hypothesis testing. Hypothesis testing method making systematic decision statistical guarantees.Hypothesis testing typically following steps:State hypothesis, based research question.Assume hypothesis true, compute metric measures much observed data deviates assumed hypothesis.Compare metric sort threshold assess observed data deviates “far enough” assumed hypothesis, make decision.ease exposition, introduce ideas framework hypothesis test mean. show framework applies two situations: hypothesis test proportion, goodness fit tests used assess data consistent certain distribution .hypothesis statement assess whether true using observed data. framework hypothesis testing, state two competing hypotheses:first hypothesis null hypothesis. normally “status quo”.first hypothesis null hypothesis. normally “status quo”.hypothesis alternative hypothesis. can viewed statement “opposite” null hypothesis.hypothesis alternative hypothesis. can viewed statement “opposite” null hypothesis.book presents hypotheses hypothesis testing using court trial analogy. Defendants assumed “innocent proven guilty”. “null hypothesis” status quo setting defendant innocent, “alternative hypothesis” defendant guilty. prosecutor needs present evidence contradicts null hypothesis order prove guilt. jurors, come trial assuming defendant innocent, assess strong evidence presented: strength beyond certain threshold (.e. beyond “reasonable doubt”) evidence reject null hypothesis decide defendant guilty. However, evidence strong enough, enough evidence reject null hypothesis jury renders guilty verdict.","code":""},{"path":"hypothesis-testing.html","id":"module-roadmap-7","chapter":"9 Hypothesis Testing","heading":"9.1.1 Module Roadmap","text":"Section 9.2 covers key concepts hypothesis tests apply mean.Section 9.3 shows concepts previous section carry hypothesis test proportion.Section 9.4 provides comments things mindful performing hypothesis tests real life.Section 9.5 introduces another class hypothesis tests: goodness fit tests, used assess consistent data assumed distribution model.","code":""},{"path":"hypothesis-testing.html","id":"testmean","chapter":"9 Hypothesis Testing","heading":"9.2 Hypothesis Test for the Mean","text":"testing mean, parameter population mean. happens variable measuring quantitative. Consider example:term “Freshman 15” expression says college students gain 15 pounds (average), first year college. Researchers claim better education regarding healthier lifestyle habits, gain less 15 pounds, average.Next, cover first step hypothesis testing: write null alternative hypothesis statements.","code":""},{"path":"hypothesis-testing.html","id":"null-and-alternative-hypotheses","chapter":"9 Hypothesis Testing","heading":"9.2.1 Null and Alternative Hypotheses","text":"Null alternative hypotheses statements regarding value parameter.Freshman 15 example, null hypothesis average weight gain first year college students 15 pounds. denoted \\(H_0: \\mu = 15\\), \\(H_0\\) denotes null hypothesis. alternative hypothesis average weight gain first year college students less 15 pounds. denoted \\(H_a: \\mu < 15\\), \\(H_a\\) denotes alternative hypothesis.","code":""},{"path":"hypothesis-testing.html","id":"features-of-null-and-alternative-hypotheses","chapter":"9 Hypothesis Testing","heading":"9.2.1.1 Features of Null and Alternative Hypotheses","text":"Null alternative hypotheses always population parameter, sample estimator. reason want make claim population, based data sample. Notice Freshman 15 example, null alternative hypotheses population mean \\(\\mu\\) sample mean \\(\\bar{x}\\).Null alternative hypotheses always population parameter, sample estimator. reason want make claim population, based data sample. Notice Freshman 15 example, null alternative hypotheses population mean \\(\\mu\\) sample mean \\(\\bar{x}\\).null hypothesis statement says parameter equal specific value. Let \\(\\mu_0\\) denote specific value. write \\(H_0: \\mu = \\mu_0\\). Freshman 15 example, \\(\\mu_0 = 15\\).null hypothesis statement says parameter equal specific value. Let \\(\\mu_0\\) denote specific value. write \\(H_0: \\mu = \\mu_0\\). Freshman 15 example, \\(\\mu_0 = 15\\).alternative hypothesis statement null hypothesis. hypothesis test mean, possible alternative hypotheses. one use driven research question. alternative hypothesis say parameter :\nDifferent specific value, .e. \\(H_a: \\mu \\neq \\mu_0\\). called two-sided alternative since parameter greater less specific value.\nGreater specific value, .e. \\(H_a: \\mu > \\mu_0\\). called one-sided alternative since parameter greater specific value.\nLess specific value, .e. \\(H_a: \\mu < \\mu_0\\). also called one-sided alternative since parameter less specific value.\nalternative hypothesis statement null hypothesis. hypothesis test mean, possible alternative hypotheses. one use driven research question. alternative hypothesis say parameter :Different specific value, .e. \\(H_a: \\mu \\neq \\mu_0\\). called two-sided alternative since parameter greater less specific value.Different specific value, .e. \\(H_a: \\mu \\neq \\mu_0\\). called two-sided alternative since parameter greater less specific value.Greater specific value, .e. \\(H_a: \\mu > \\mu_0\\). called one-sided alternative since parameter greater specific value.Greater specific value, .e. \\(H_a: \\mu > \\mu_0\\). called one-sided alternative since parameter greater specific value.Less specific value, .e. \\(H_a: \\mu < \\mu_0\\). also called one-sided alternative since parameter less specific value.Less specific value, .e. \\(H_a: \\mu < \\mu_0\\). also called one-sided alternative since parameter less specific value.hypothesis test mean, 3 options alternative hypothesis.Freshman 15 example, researchers claim better education, average weight gain less 15 pounds, write \\(H_a: \\mu < 15\\).","code":""},{"path":"hypothesis-testing.html","id":"some-comments-about-the-null-hypothesis-with-a-one-sided-alternative","chapter":"9 Hypothesis Testing","heading":"9.2.1.2 Some Comments about the Null Hypothesis with a One-sided Alternative","text":"number textbooks (including one using!) take view null alternative hypotheses opposites, write \\(H_a: \\mu < \\mu_0\\), must write \\(H_0: \\mu \\geq \\mu_0\\). way writing null hypothesis different wrote earlier: null hypothesis says parameter equal \\(\\mu_0\\). couple reasons say null hypothesis statement involving equality, rather inequality:calculations performed assess evidence data provides done assuming null hypothesis true. realize later subsections one can perform calculations null hypothesis says parameter equal specific value.calculations performed assess evidence data provides done assuming null hypothesis true. realize later subsections one can perform calculations null hypothesis says parameter equal specific value.seen many people get confused null alternative hypotheses insist null alternative must opposite one-sided alternative. Indeed, look Practice Exercise 9.3 9.4 book, author gotten confused proposed solutions.seen many people get confused null alternative hypotheses insist null alternative must opposite one-sided alternative. Indeed, look Practice Exercise 9.3 9.4 book, author gotten confused proposed solutions.","code":""},{"path":"hypothesis-testing.html","id":"test-statistic","chapter":"9 Hypothesis Testing","heading":"9.2.2 Test Statistic","text":"writing null alternative hypotheses, next step compute value measures far sample data expected value null hypothesis true. value called test statistic. test statistic calculated based sampling distribution estimator used.remind sampling distribution sample mean, \\(\\bar{X}_n\\), Section 7.4.4.1. couple conditions consider:\\(X_1, \\cdots, X_n\\) ..d. normal distribution finite mean \\(\\mu\\) finite variance \\(\\sigma^2\\). \\(\\bar{X}_n \\sim N(\\mu, \\frac{\\sigma^2}{n})\\).\\(X_1, \\cdots, X_n\\) ..d. normal distribution finite mean \\(\\mu\\) finite variance \\(\\sigma^2\\). \\(\\bar{X}_n \\sim N(\\mu, \\frac{\\sigma^2}{n})\\).\\(X_1, \\cdots, X_n\\) ..d. distribution finite mean \\(\\mu\\) finite variance \\(\\sigma^2\\), \\(n\\) large enough, \\(\\bar{X}_n\\) approximately \\(N(\\mu, \\frac{\\sigma^2}{n})\\).\\(X_1, \\cdots, X_n\\) ..d. distribution finite mean \\(\\mu\\) finite variance \\(\\sigma^2\\), \\(n\\) large enough, \\(\\bar{X}_n\\) approximately \\(N(\\mu, \\frac{\\sigma^2}{n})\\).either conditions met, distribution \\(\\bar{X}_n\\) standardization either standard normal approaches standard normal distribution, \\(\\frac{\\bar{X}_n - \\mu}{\\frac{\\sigma}{\\sqrt{n}}} = \\frac{\\bar{X}_n - \\mu}{\\sqrt{Var(\\bar{X})}} = \\frac{\\bar{X}_n - \\mu}{SE(\\bar{X}_n)}\\) either standard normal approximately standard normal \\(n\\) large enough.learned Section 8.2.4 since value population variance \\(\\sigma^2\\) almost always unknown real life, use sample variance \\(s^2\\) instead. work \\(\\frac{\\bar{X}_n - \\mu}{\\frac{s}{\\sqrt{n}}}\\) instead, follows \\(t\\) distribution \\(n-1\\) degrees freedom.Based distribution, test statistic hypothesis test mean \\[\\begin{equation}\n\\hat{t} =  \\frac{\\bar{x}_n - \\mu_0}{\\frac{s}{\\sqrt{n}}} = \\frac{\\bar{x}_n - \\mu_0}{SE(\\bar{x}_n)},\n\\tag{9.1}\n\\end{equation}\\]\\(SE(\\bar{x}_n)\\) \\(\\frac{s}{\\sqrt{n}}\\). can called \\(t\\) statistic reflect test statistic based \\(t\\) distribution. \\(t\\) statistic follows \\(t_{n-1}\\) distribution (subscript refers degrees freedom) either 2 conditions met sampling distribution \\(\\bar{X}_n\\) valid, assume null hypothesis true, true mean data \\(\\mu_0\\).Note: number hypothesis tests involving estimators parameters also based \\(t_{df}\\) distribution, \\(t\\) statistics. However, \\(t\\) statistics take general form\\[\\begin{equation}\n\\hat{t} =  \\frac{\\text{estimated value }- \\text{ value } H_0}{\\text{standard error estimator}}.\n\\tag{9.2}\n\\end{equation}\\]Notice equation (9.2) generalization equation (9.1).go back Freshman 15 example. Suppose researchers collect random sample 50 first-year college students. average weight gain 14 pounds, standard deviation 3 pounds. Derive value \\(t\\) statistic.Using equation (9.1), \\(t\\) statistic :\\[\n\\begin{split}\n\\hat{t} &= \\frac{14-15}{\\frac{3}{\\sqrt{50}}} \\\\\n        &= -2.357023.\n\\end{split}\n\\]\n\\(t\\) statistic valid since sample size 50, usually large enough approximation \\(t\\) distribution valid.Remember test statistic measure far sample data expected value null hypothesis true. Larger values test statistic (magnitude) imply data deviate null hypothesis, .e. data provide evidence null hypothesis.\\(t\\) statistic also nice definition: distance sample estimator value parameter null hypothesis true, terms number standard errors. Freshman 15 example, sample mean 2.357 standard errors smaller hypothesized value 15.","code":""},{"path":"hypothesis-testing.html","id":"factors","chapter":"9 Hypothesis Testing","heading":"9.2.2.1 Factors Affecting \\(t\\) Statistic for Mean","text":"established larger values test statistic (magnitude) imply evidence null hypothesis. \\(t\\) statistic mean, factors affect magnitude :difference sample mean \\(\\bar{x}\\) data value population mean null true \\(\\mu_0\\). difference called effect size. Larger effect sizes lead larger test statistics, magnitude. make sense since larger effect size implies sample mean deviates expected mean null hypothesis true.difference sample mean \\(\\bar{x}\\) data value population mean null true \\(\\mu_0\\). difference called effect size. Larger effect sizes lead larger test statistics, magnitude. make sense since larger effect size implies sample mean deviates expected mean null hypothesis true.sample size \\(n\\). Larger sample sizes lead larger magnitudes \\(t\\) statistic, since \\(\\frac{\\bar{x}_n - \\mu_0}{\\frac{s}{\\sqrt{n}}}\\) can written \\(\\sqrt{n}\\frac{\\bar{x}_n - \\mu_0}{s}\\). implies even effect size stays , larger sample sizes provide evidence null.sample size \\(n\\). Larger sample sizes lead larger magnitudes \\(t\\) statistic, since \\(\\frac{\\bar{x}_n - \\mu_0}{\\frac{s}{\\sqrt{n}}}\\) can written \\(\\sqrt{n}\\frac{\\bar{x}_n - \\mu_0}{s}\\). implies even effect size stays , larger sample sizes provide evidence null.","code":""},{"path":"hypothesis-testing.html","id":"decision","chapter":"9 Hypothesis Testing","heading":"9.2.3 Making a Decision","text":"calculating value test statistic, need assess data provide enough evidence null hypothesis . two approaches making assessment: one involves p-value, involves critical value.Using p-value, reject null hypothesis p-value less significance level (.e. data enough evidence null hypothesis). Otherwise, fail reject null hypothesis (.e. data enough evidence null hypothesis)significance level test, denoted \\(\\alpha\\), probability wrongly rejecting null hypothesis null hypothesis actually true. error called type error, define formally later subsection. conduct hypothesis test significance level \\(\\alpha = 0.05\\), saying willing 5% probability wrongly concluding enough evidence null hypothesis, actually true.Using critical value, reject null hypothesis test statistic extreme critical value direction alternative hypothesis (.e. data enough evidence null hypothesis). Otherwise, fail reject null hypothesis (.e. data enough evidence null hypothesis).approaches based sampling distribution estimator, value significance level. approaches always lead conclusion, need use one .","code":""},{"path":"hypothesis-testing.html","id":"p-value","chapter":"9 Hypothesis Testing","heading":"9.2.3.1 P-Value","text":"p-value probability observing value test statistic, value extreme direction alternative hypothesis, null hypothesis true.Informally, also probability observing value sample mean, value extreme direction alternative hypothesis, null hypothesis true.Recall hypothesis test mean, \\(t\\) statistic follows \\(t_{n-1}\\) distribution, calculations assume null hypothesis true. Visually, can use areas PDF \\(t_{n-1}\\) illustrate p-value found. specific area PDF depends three alternative hypotheses using. Figures 9.1, 9.2, 9.3 , examples relevant areas PDF value \\(t\\) statistic \\(\\hat{t} = 1\\).\\(H_a: \\mu \\neq \\mu_0\\), .e. two-sided alternative, corresponding areas PDF \\(t_{n-1}\\) distribution p-value shown Figure 9.1:\nFigure 9.1: Finding P-value, Two-Sided Alternative\narea right \\(|\\hat{t}|\\) area left -\\(|\\hat{t}|\\). Using probability notation, \\(P(|t_{n-1}| \\geq 1)\\).\\(H_a: \\mu > \\mu_0\\), .e. population mean greater specified value, corresponding area PDF \\(t_{n-1}\\) distribution p-value shown Figure 9.2:\nFigure 9.2: Finding P-value, Greater Alternative\narea right \\(\\hat{t}\\). Using probability notation, \\(P(t_{n-1} \\geq 1)\\).\\(H_a: \\mu < \\mu_0\\), .e. population mean less specified value, corresponding area PDF \\(t_{n-1}\\) distribution p-value shown Figure 9.3:\nFigure 9.3: Finding P-value, Less Alternative\narea left \\(\\hat{t}\\). Using probability notation, \\(P( t_{n-1} \\leq 1)\\).View video explains finding p-value little bit :Going back Freshman 15 example, found \\(t\\) statistic −2.357023. earlier written alternative hypothesis \\(H_a: \\mu < 15\\), find PDF \\(t_{49}\\) distribution using area left \\(-2.357023\\), manner similar Figure 9.3.\nFigure 9.4: Finding P-value, Freshman 15 Example\nUsing R, p-value 0.01123. test conducted 5% significance level, reject null hypothesis, since p-value less 0.05.Thought question: Suppose research question Freshman 15 example tweaked, alternative hypothesis \\(H_a:\\mu > 15\\). Assuming everything else stays , show p-value now 0.98877. alternative hypothesis \\(H_a: \\mu \\neq 15\\), show p-value 0.02246.","code":"\npt(-2.357023, 49) ##enter t stat, then df## [1] 0.01123111"},{"path":"hypothesis-testing.html","id":"critical-value-1","chapter":"9 Hypothesis Testing","heading":"9.2.3.2 Critical Value","text":"Informally, critical value value test statistic considered “enough” say enough evidence null hypothesis reject , based value significance level.use PDF \\(t\\) distribution visualize critical . want find “cut ” value, denoted \\(t_{n-1}^*\\), distribution area PDF direction alternative hypothesis equal significance level \\(\\alpha\\). Since three options alternative hypothesis, 3 different areas PDF consider. Figures 9.5, 9.6, 9.7 , examples relevant areas PDF \\(\\alpha=0.05\\) \\(n=50\\):\\(H_a: \\mu \\neq \\mu_0\\), .e. two-sided alternative, want areas PDF \\(t_{49}\\) distribution right \\(t_{49}^*\\) left \\(-t_{49}^*\\) equal 0.05. shown Figure 9.5:\nFigure 9.5: Critical Value 2-Sided Alternative, 0.05 Sig Level\nNote since \\(t\\) distribution symmetric areas blue add 0.05, shaded area must 0.025. \\(-t_{49}^*\\) \\(t_{49}^*\\) correspond 2.5th 97.5th percentiles \\(t_{49}\\) distribution respectively. can show \\(-t_{49}^*\\) -2.009575 typingand symmetry \\(t\\) distribution, \\(t_{49}^*\\) 2.009575. critical value therefore \\(t_{49}^* = 2.009575\\) 2-sided test. reject null hypothesis \\(t\\) statistic right 2.009575 left -2.009575.general, significance level \\(\\alpha\\), negative critical value \\(-t_{n-1}^*\\) corresponds \\((\\alpha/2) \\times 100\\)th percentile, positive critical value \\(t_{n-1}^*\\) corresponds \\((1 - \\alpha/2) \\times 100\\)th percentile appropriate \\(t_{n-1}\\) distribution, \\(H_a: \\mu \\neq \\mu_0\\).\\(H_a: \\mu > \\mu_0\\), .e. population mean greater specified value, want area PDF \\(t_{49}\\) distribution right \\(t_{49}^*\\) equal 0.05. shown Figure 9.6:\nFigure 9.6: Critical Value Greater Alternative, 0.05 Sig Level\nNote \\(t_{49}^*\\) corresponds 95th percentile \\(t_{49}\\) distribution. can show \\(t_{49}^*\\) 1.677 typingThe critical value therefore \\(t_{49}^* = 1.676551\\) \\(H_a: \\mu > \\mu_0\\). reject null hypothesis \\(t\\) statistic right 1.676551.general, significance level \\(\\alpha\\), critical value \\(t_{n-1}^*\\) corresponds \\((1 - \\alpha) \\times 100\\)th percentile appropriate \\(t_{n-1}\\) distribution, \\(H_a: \\mu > \\mu_0\\).\\(H_a: \\mu < \\mu_0\\), .e. population mean less specified value, want area PDF \\(t_{49}\\) distribution left \\(t_{49}^*\\) equal 0.05. shown Figure 9.7:\nFigure 9.7: Critical Value Less Alternative, 0.05 Sig Level\nNote \\(t_{49}^*\\) corresponds 5th percentile \\(t_{49}\\) distribution. can show \\(t_{49}^*\\) -1.677 typingThe critical value therefore \\(t_{49}^* = -1.676551\\) \\(H_a: \\mu < \\mu_0\\). reject null hypothesis \\(t\\) statistic left -1.676551.general, significance level \\(\\alpha\\), critical value \\(t_{n-1}^*\\) corresponds \\((\\alpha) \\times 100\\)th percentile appropriate \\(t_{n-1}\\) distribution, \\(H_a: \\mu < \\mu_0\\).View video explains finding critical value little bit :Going back Freshman 15 example, found \\(t\\) statistic −2.357023. earlier written alternative hypothesis \\(H_a: \\mu < 15\\), critical value 5th percentile \\(t_{49}\\) distribution, found -1.676551. Since \\(t\\) statistic left critical value, reject null hypothesis.Thought question: Consider hypothesis test mean \\(n=10\\). Assume conditions sampling distribution sample mean met. Suppose \\(\\alpha=0.10\\). alternative hypothesis 2-sided, show critical value 1.833113. alternative hypothesis 1-sided, show critical value 1.383029.","code":"\nalpha<-0.05\nn<-50\nqt(alpha/2, n-1)## [1] -2.009575\nalpha<-0.05\nn<-50\nqt(1-alpha, n-1)## [1] 1.676551\nalpha<-0.05\nn<-50\nqt(alpha, n-1)## [1] -1.676551"},{"path":"hypothesis-testing.html","id":"writing-conclusions","chapter":"9 Hypothesis Testing","heading":"9.2.4 Writing Conclusions","text":"previous subsection, decide whether reject null hypothesis based two approaches:p-value test less significance level.test statistic test extreme critical value, direction alternative hypothesis.decision based approaches always never contradict . need use one approaches, redundant. choose one want use, must clear one using.decide :Rejecting null hypothesis. implies enough evidence null hypothesis reject support alternative hypothesis. data support claim alternative hypothesis. Using court trial analogy, equivalent saying enough evidence prove defendants guilt reject assumption defendant innocent, give guilty verdict.Rejecting null hypothesis. implies enough evidence null hypothesis reject support alternative hypothesis. data support claim alternative hypothesis. Using court trial analogy, equivalent saying enough evidence prove defendants guilt reject assumption defendant innocent, give guilty verdict.Failing reject null hypothesis. implies enough evidence null hypothesis reject . data support claim alternative hypothesis. Using court trial analogy, equivalent saying enough evidence prove defendants give guilty verdict.Failing reject null hypothesis. implies enough evidence null hypothesis reject . data support claim alternative hypothesis. Using court trial analogy, equivalent saying enough evidence prove defendants give guilty verdict.common mistake failing reject null hypothesis say data support proves null hypothesis true. Using court trial analogy, guilty verdict saying defendant innocent.making decision, write conclusion context research question. Using Freshman 15 example, rejected null hypothesis. conclusion data support claim average weight gain among first-year college students less 15 pounds.look one example show hypothesis test done start finish.","code":""},{"path":"hypothesis-testing.html","id":"wafer","chapter":"9 Hypothesis Testing","heading":"9.2.5 Worked Example","text":"target thickness silicon wafers used type integrated circuit 245 micrometers. part assessment silicon wafers meeting target thickness, sample 50 wafers obtained thickness one determined, resulting sample mean thickness 245.88 micrometers sample standard deviation 3.60 micrometers. data suggest true average wafer thickness something target value? Test 0.05 significance level.variable investigating thickness silicon wafers, quantitative, performing hypothesis test mean.null alternative hypotheses \\(H_0: \\mu = 245, H_a: \\mu \\neq 245\\).know distribution thickness silicon wafers. sample 50, usually large enough CLT applies, relevant distributions test statistic apply.\\(t\\) statistic \\[\n\\begin{split}\n\\hat{t} &= \\frac{245.88 - 245}{\\frac{3.6}{\\sqrt{50}}} \\\\\n        &= 1.728483.\n\\end{split}\n\\]\nSince two-sided test, p-value can derived finding following area PDF \\(t_{49}\\) distribution, shown Figure 9.8 .\nFigure 9.8: Finding P-value, Silicon Wafer Example\nUsing R, can find p-value typing:Alternatively, can work critical value typing:Since p-value 0.09, greater significance level 0.05, (since test statistic 1.728483 right critical value 2.009575 ), fail reject null hypothesis. data provide evidence null hypothesis.conclusion data support claim average thickness silicon wafers different target value 245 micrometers.","code":"\npt(-1.728483, 49)*2 ## [1] 0.09019905\n##supply test stat, then DF.\n##multiply by 2 since pt only gives area to left of -1.728483, \n##but we also want area to right of 1.728483. \nalpha<-0.05\nqt(1-alpha/2, 49)## [1] 2.009575"},{"path":"hypothesis-testing.html","id":"testprop","chapter":"9 Hypothesis Testing","heading":"9.3 Hypothesis Test for the Proportion","text":"Next, go hypothesis test proportion. another common hypothesis test. sample proportion estimator population proportion. Proportions used summarize categorical variables, whereas means used summarize quantitative variables.general steps hypothesis test mean, changes details. differences :now using sample proportion, \\(\\hat{p}\\), estimate population proportion, \\(p\\), instead using sample mean, \\(\\bar{x}\\), estimate population mean, \\(\\mu\\).now using sample proportion, \\(\\hat{p}\\), estimate population proportion, \\(p\\), instead using sample mean, \\(\\bar{x}\\), estimate population mean, \\(\\mu\\).test statistic based standard normal distribution, \\(t_{df}\\) distribution.test statistic based standard normal distribution, \\(t_{df}\\) distribution.exact components involved calculating test statistic bit different, although still takes general form suggested equation (9.2).exact components involved calculating test statistic bit different, although still takes general form suggested equation (9.2).Thought question: Can make educated guess formula test statistic testing proportion, reading ?","code":""},{"path":"hypothesis-testing.html","id":"null-and-alternative-hypotheses-1","chapter":"9 Hypothesis Testing","heading":"9.3.1 Null and Alternative Hypotheses","text":"null hypothesis still statement says parameter equal specific value. Let \\(p_0\\) denote specific value. write \\(H_0: p = p_0\\).null hypothesis still statement says parameter equal specific value. Let \\(p_0\\) denote specific value. write \\(H_0: p = p_0\\).alternative hypothesis statement null hypothesis. alternative hypothesis say parameter :\nDifferent specific value, .e. \\(H_a: p \\neq p_0\\).\nGreater specific value, .e. \\(H_a: p > p_0\\).\nLess specific value, .e. \\(H_a: p < p_0\\).\nalternative hypothesis statement null hypothesis. alternative hypothesis say parameter :Different specific value, .e. \\(H_a: p \\neq p_0\\).Different specific value, .e. \\(H_a: p \\neq p_0\\).Greater specific value, .e. \\(H_a: p > p_0\\).Greater specific value, .e. \\(H_a: p > p_0\\).Less specific value, .e. \\(H_a: p < p_0\\).Less specific value, .e. \\(H_a: p < p_0\\).","code":""},{"path":"hypothesis-testing.html","id":"test-statistic-1","chapter":"9 Hypothesis Testing","heading":"9.3.2 Test Statistic","text":"Section 8.3.1, established sampling distribution sample proportions. conditions met, \\(\\hat{p}\\) approximately \\(N\\left(p, \\frac{p(1-p)}{n}\\right)\\), \\(p\\) denotes true population proportion. Therefore, distribution \\(\\hat{p}\\) standardization approximately standard normal, .e. \\(\\frac{\\hat{p}-p}{\\sqrt{\\frac{p(1-p)}{n}}}\\) approximately standard normal.Based distribution, test statistic hypothesis test proportion \\[\\begin{equation}\n\\hat{z} =  \\frac{\\hat{p} - p_0}{\\sqrt{\\frac{p_0(1-p_0)}{n}}} .\n\\tag{9.3}\n\\end{equation}\\]test statistic \\(z\\) statistic, since follows standard normal distribution.Notice equation (9.3), use \\(p_0\\), value population proportion assume null hypothesis true, place population proportion, unknown. test statistic still calculated assuming null hypothesis true.Also notice equation (9.3) follows general form suggested equation (9.2). turns \\(z\\) statistics also take general form.","code":""},{"path":"hypothesis-testing.html","id":"conditions-for-hypothesis-test-for-the-proportion","chapter":"9 Hypothesis Testing","heading":"9.3.2.1 Conditions for Hypothesis Test for the Proportion","text":"CLT informs us \\(n\\) large enough, sample proportion \\(\\hat{p}\\) can approximated normal distribution. large large enough? , various rules thumb recommended, usually follow along lines needing least certain number expected successes, \\(n p_0\\), expected failures, \\(n(1-p_0)\\) sample. Values least 5 10 usually recommended. Just bear mind approximation works better expected number successes failures, \\(np_0\\) \\(n(1-p_0)\\), increases.","code":""},{"path":"hypothesis-testing.html","id":"making-a-decision-and-writing-conclusions","chapter":"9 Hypothesis Testing","heading":"9.3.3 Making a Decision and Writing Conclusions","text":"make decision whether reject null hypothesis manner hypothesis test mean. difference test statistic follows standard normal instead \\(t\\) distribution. Visually lot ideas similar, since bell-shaped distributions.Conclusions also written manner.","code":""},{"path":"hypothesis-testing.html","id":"worked-example-6","chapter":"9 Hypothesis Testing","heading":"9.3.4 Worked Example","text":"university library ordinarily complete shelf inventory done every year. new shelving rules instituted previous year, head librarian believes may possible save money postponing inventory. librarian decides select random 1000 books library’s collection searched preliminary manner. evidence indicates strongly true proportion misshelved unlocatable books less 2%, inventory postponed. Among 1000 books searched, 15 misshelved unlocatable. Test relevant hypotheses advise librarian , using significance level 0.05.variable investigating whether book misshelved unlocatable, categorical, performing hypothesis test proportion.null alternative hypotheses \\(H_0: p = 0.02, H_a: p < 0.02\\).assess can conduct hypothesis test proportion, need check \\(n p_0 \\geq 10\\) \\(n(1-p_0) \\geq 10\\). data, \\(n p_0 = 1000 \\times 0.02 = 20 \\geq 10\\),\\(n (1-p_0) = 1000 \\times 0.98 = 980 \\geq 10\\),can proceed.\\(z\\) statistic \\[\n\\begin{split}\n\\hat{z} &= \\frac{\\frac{15}{1000} - 0.02}{\\sqrt{\\frac{0.02 \\times 0.98}{1000}}} \\\\\n        &= -1.129385.\n\\end{split}\n\\]Since one-sided test less alternative hypothesis, p-value can derived finding following area PDF \\(Z\\) distribution, shown Figure 9.9 .\nFigure 9.9: Finding P-value, Library Example\nUsing R, can find p-value typing:Alternatively, can work critical value typing:Since p-value 0.129, greater significance level 0.05, (since test statistic -1.129385 left critical value -1.644854), fail reject null hypothesis. data provide evidence null hypothesis.conclusion data support claim true proportion misshelved unlocatable books less 2%. enough evidence postpone inventory.","code":"\npnorm(-1.129385)## [1] 0.1293677\nqnorm(0.05)## [1] -1.644854"},{"path":"hypothesis-testing.html","id":"infcomments","chapter":"9 Hypothesis Testing","heading":"9.4 Additional Comments on Hypothesis Test of Parameters","text":"","code":""},{"path":"hypothesis-testing.html","id":"inference","chapter":"9 Hypothesis Testing","heading":"9.4.1 Two-Sided Tests and Confidence Intervals","text":"Notice hypothesis tests confidence intervals based sampling distribution relevant estimator? look critical value two-sided test critical value confidence interval derived, notice found exact manner. Refer Figure 9.5 Figure 8.2, notice pictures look exactly , middle portion PDF area \\(1-\\alpha\\) two tail ends area \\(\\alpha\\).implies conclusions \\((1-\\alpha) \\times 100\\%\\) confidence interval consistent 2-sided hypothesis test conducted significance \\(\\alpha\\). Also notice use notation \\(\\alpha\\) confidence level significance level?null hypothesis rejected 2-sided test, value parameter null hypothesis lie outside corresponding confidence interval. make sense since confidence interval provides range plausible values parameter values outside interval considered “ruled ”.null hypothesis rejected 2-sided test, value parameter null hypothesis lie outside corresponding confidence interval. make sense since confidence interval provides range plausible values parameter values outside interval considered “ruled ”.null hypothesis rejected 2-sided test, value parameter null hypothesis lie inside corresponding confidence interval. make sense since confidence interval provides range plausible values parameter values outside interval considered “ruled ”.null hypothesis rejected 2-sided test, value parameter null hypothesis lie inside corresponding confidence interval. make sense since confidence interval provides range plausible values parameter values outside interval considered “ruled ”.go back thickness silicon wafers worked example Section 9.2.5. null alternative hypotheses \\(H_0: \\mu = 245, H_a: \\mu \\neq 245\\) ended failing reject null hypothesis 0.05 significance level. informs us construct 95% confidence interval mean, interval include value 245, value parameter null hypothesis.Thought question: Show 95% confidence interval mean thickness silicon wafers worked example (244.8569, 246.9031) contains value 245.","code":""},{"path":"hypothesis-testing.html","id":"type-i-and-type-ii-errors","chapter":"9 Hypothesis Testing","heading":"9.4.2 Type I and Type II Errors","text":"decision make hypothesis testing going perfect, inherent uncertainty estimators due random sampling. may sometimes reject null hypothesis true, may fail reject null hypothesis alternative hypothesis true. errors called type type II errors respectively.go back thickness silicon wafers worked example Section 9.2.5. null alternative hypotheses \\(H_0: \\mu = 245, H_a: \\mu \\neq 245\\).type error study concluding thickness silicon wafers meeting target thickness actually .type error study concluding thickness silicon wafers meeting target thickness actually .type II error study concluding enough evidence thickness silicon wafers meeting target thickness actually .type II error study concluding enough evidence thickness silicon wafers meeting target thickness actually .Next, discuss can measure control probability making errors.","code":""},{"path":"hypothesis-testing.html","id":"significance-level-and-type-i-error","chapter":"9 Hypothesis Testing","heading":"9.4.2.1 Significance Level and Type I Error","text":"probability making type error controlled significance level, \\(\\alpha\\), hypothesis test. significance level probability wrongly rejecting null hypothesis null hypothesis actually true. conduct hypothesis test significance level \\(\\alpha = 0.05\\), saying willing 5% probability making type error null hypothesis true.value significance level hypothesis made consultation subject matter expert understanding practical cost making type error. ethical standpoint, value decided upon collecting evaluating data, significance level adjusted order able conclude null hypothesis rejected.run Monte Carlo simulation demonstrate significance level determines probability making type error. simulation similar one done Section 8.2.3.3 explain coverage probabilities. realize probability making type error directly related coverage probability continues idea written Section 9.4.1 confidence intervals 2-sided hypothesis tests consistent. code following:Simulate \\(X_1, \\cdots, X_{10}\\) ..d. standard normal.Calculate \\(t\\) statistic hypothesis test using equation (9.1).\nhypotheses \\(H_0: \\mu = 0, H_a: \\mu \\neq 0\\).\nNote data simulated standard normal, null hypothesis true simulation.\nhypotheses \\(H_0: \\mu = 0, H_a: \\mu \\neq 0\\).Note data simulated standard normal, null hypothesis true simulation.Assess calculated \\(t\\) statistic extreme critical value.\nset significance level \\(\\alpha = 0.05\\).\ncritical value \\(t_9^* = 2.262157\\) since conducting 2-sided test \\(n=10\\).\nset significance level \\(\\alpha = 0.05\\).critical value \\(t_9^* = 2.262157\\) since conducting 2-sided test \\(n=10\\).Repeat steps total 10 thousand replicates.Count number times reject null hypothesis, divide number replicates. value estimates probability making type error.\nthings correctly estimated probability making type error close value significance level 0.05.\nthings correctly estimated probability making type error close value significance level 0.05.estimated probability making type error 0.0516, matches significance level set 0.05.Thought question: Feel free play around code adjust value significance level, change alternative hypothesis 1-sided test, factor. estimated probability making type error always match significance level.","code":"\n##Prob of Type I Error \n\nn<-10 ## sample size of each random sample\nalpha<-0.05 ##sig level\nreps<-10000 ##no of replicates\n\nrejected<-array(0, reps) ##store assessment if the H0 is rejected or not for sample\n\nset.seed(100)\nfor (i in 1:reps)\n  \n{\n  \n  X<-rnorm(n) ##draw n data points from standard normal\n  ##calculate elements needed for CI of mean\n  xbar<-mean(X) ##sample mean\n  SE<-sd(X)/sqrt(n) ##SE of sample mean\n  t.stat<-xbar/SE\n  crit<-qt(1-alpha/2, n-1) ##critical value\n  rejected[i]<-abs(t.stat) > crit ##assess if absolute value of t.stat is greater than critical value\n  \n}\n\n##find proportion of random samples that wrongly rejected H0\n##this is estimated prob of making type I error\nsum(rejected)/reps ## [1] 0.0516"},{"path":"hypothesis-testing.html","id":"power","chapter":"9 Hypothesis Testing","heading":"9.4.2.2 Power and Type II Error","text":"power test inversely related probability making type II error. power test probability able correctly reject null alternative hypothesis, alternative hypothesis true. Based definition, power complement probability making type II error, .e. power test 1 minus probability making type II error.Ideally, like power hypothesis test large. factors power hypothesis test? Recall power measures probability able correctly reject null hypothesis, larger values (magnitude) test statistic provide evidence null hypothesis. factors make test statistic larger (magnitude) also lead increase power hypothesis test. mentioned Section 9.2.2.1, :effect size, difference sample mean \\(\\bar{x}\\) data value population mean null true \\(\\mu_0\\). Larger effect sizes lead larger test statistics, magnitude. make sense since larger effect size implies sample mean deviates expected mean null hypothesis true.effect size, difference sample mean \\(\\bar{x}\\) data value population mean null true \\(\\mu_0\\). Larger effect sizes lead larger test statistics, magnitude. make sense since larger effect size implies sample mean deviates expected mean null hypothesis true.sample size \\(n\\). Larger sample sizes lead larger magnitudes test statistic, see equations (9.1) (9.3).sample size \\(n\\). Larger sample sizes lead larger magnitudes test statistic, see equations (9.1) (9.3).standard error estimator. Smaller standard errors estimator lead larger magnitudes test statistic, see equations (9.1) (9.3).standard error estimator. Smaller standard errors estimator lead larger magnitudes test statistic, see equations (9.1) (9.3).Another factor increases power hypothesis test raise significance level \\(\\alpha\\). Think following extreme example: Suppose set significance level low possible, \\(\\alpha=0\\). hypothesis test always reject null hypothesis, never make type error. always rejecting null hypothesis means alternative hypothesis true, always make type II error, power 0. illustrates probabilities making type type II error inverse relationship. Reducing one done expense .Another factor increases power hypothesis test raise significance level \\(\\alpha\\). Think following extreme example: Suppose set significance level low possible, \\(\\alpha=0\\). hypothesis test always reject null hypothesis, never make type error. always rejecting null hypothesis means alternative hypothesis true, always make type II error, power 0. illustrates probabilities making type type II error inverse relationship. Reducing one done expense .Please view video provides visual explanation probabilities making type type II error inverse relationship:Calculating power hypothesis test can done hand usually simple exercise. instead use Monte Carlo simulations estimate power hypothesis test.Suppose want estimate power hypothesis test mean data ..d \\(N(1,1)\\) (population mean 1), sample size 10, significance level set 0.05, hypotheses \\(H_0: \\mu = 0, H_a: \\mu \\neq 0\\). code following:Simulate \\(X_1, \\cdots, X_{10}\\) ..d. \\(N(1,1)\\).Calculate \\(t\\) statistic hypothesis test using equation (9.1).\nhypotheses \\(H_0: \\mu = 0, H_a: \\mu \\neq 0\\).\nNote data simulated \\(N(1,1)\\), alternative hypothesis true simulation.\nhypotheses \\(H_0: \\mu = 0, H_a: \\mu \\neq 0\\).Note data simulated \\(N(1,1)\\), alternative hypothesis true simulation.Assess calculated \\(t\\) statistic extreme critical value.\nset significance level \\(\\alpha = 0.05\\).\ncritical value \\(t_9^* = 2.262157\\) since conducting 2-sided test \\(n=10\\).\nset significance level \\(\\alpha = 0.05\\).critical value \\(t_9^* = 2.262157\\) since conducting 2-sided test \\(n=10\\).Repeat steps total 10 thousand replicates.Count number times reject null hypothesis, divide number replicates. value estimates probability correctly rejecting null hypothesis, .e. power test.estimated power test 0.8044. data come \\(N(1,1)\\), power testing \\(H_0: \\mu = 0, H_a: \\mu \\neq 0\\) using sample size 10 significance level 0.05, 0.8044.Note: must specify specific value parameter makes alternative hypothesis true. simulation, simulated data distribution population mean 1. value power change used another value mean distribution. value directly affects effect size. power specific value alternative hypothesis, well specific values \\(n\\) \\(\\alpha\\).Power analysis power calculations often done well-designed experiments determine minimum sample size needed hypothesis test guarantee hypothesis test achieves certain power pre-determined significance level specific alternative hypothesis (impacts effect size).Thought question: tweak code power analysis find minimum sample size needed estimated power least 0.90 (assuming every factor remains )?","code":"\nn<-10 ## sample size of each random sample\nalpha<-0.05 ##sig level\nreps<-10000 ## no of replicates\n\nrejected<-array(0, reps) ##store assessment if the H0 is rejected or not\n\nset.seed(100)\nfor (i in 1:reps)\n  \n{\n  \n  X<-rnorm(n, mean=1, sd=1) ##draw n data points from N(1,1)\n  ##calculate elements needed for CI of mean\n  xbar<-mean(X) ##sample mean\n  SE<-sd(X)/sqrt(n) ##SE of sample mean\n  t.stat<-xbar/SE\n  crit<-qt(1-alpha/2, n-1) ##critical value\n  rejected[i]<-abs(t.stat) > crit ##assess if absolute value of t.stat is greater than critical value\n  \n}\n\n##find proportion of random samples that correctly rejected H0\n##this estimates power since mu=1 so Ha is true\nsum(rejected)/reps ## [1] 0.8044"},{"path":"hypothesis-testing.html","id":"reporting-conclusions","chapter":"9 Hypothesis Testing","heading":"9.4.3 Reporting Conclusions","text":"may noticed make binary decision end hypothesis test: either reject fail reject null hypothesis, comparing test statistic critical value comparing p-value (depends value test statistic) significance level.decision binary, note numerical value test statistic binary. mentioned larger values (magnitude) imply evidence null hypothesis. p-values 0.049 0.051 really much different? p-value also can interpreted manner: smaller values imply evidence null hypothesis.Therefore, writing conclusions, recommended also provide p-value value test statistic, instead just reporting “null hypothesis rejected” “null hypothesis rejected”. Readers make assessment strong evidence null hypothesis, based judgement value significance level .","code":""},{"path":"hypothesis-testing.html","id":"statistical-vs-practical-significance","chapter":"9 Hypothesis Testing","heading":"9.4.4 Statistical Vs Practical Significance","text":"However, even p-value reported, need careful conflating “statistically significant” result (.e. null hypothesis rejected) “practically significant” result.earlier mentioned Section 9.4.2.2 power hypothesis test can increased sample size gets larger. implies may able reject null hypothesis small effect size huge sample size.Consider hypothetical example. \\(H_0: \\mu = 100, H_a: \\mu \\neq 100\\), \\(\\bar{x} = 101, \\sigma=10\\), various values sample size \\(n\\). Suppose values represent IQ scores. work p-values well corresponding 95% confidence intervals mean, present results Table 9.1 .Table 9.1:  Sample Size P-Values CIsMost probably agree effect size example small, yet sample size 400 , reject null hypothesis. sample size huge, p-value extremely small, may interpret strong evidence null hypothesis, “statistical significance”. hand, small effect size tells us practical significance may little consequence.couple recommendations reporting results hypothesis test:possible, report corresponding confidence interval, reader can see range plausible values parameter make decision. However, every hypothesis test associated confidence interval; see one class hypothesis tests next subsection.possible, report corresponding confidence interval, reader can see range plausible values parameter make decision. However, every hypothesis test associated confidence interval; see one class hypothesis tests next subsection.Researchers need establish minimum effect size result considered practically significant. ethical standpoint, value decided upon collecting evaluating data, value adjusted order able conclude null hypothesis rejected.Researchers need establish minimum effect size result considered practically significant. ethical standpoint, value decided upon collecting evaluating data, value adjusted order able conclude null hypothesis rejected.remind , use hypothesis tests test “statistical significance”. testing whether deviation sample estimate value null hypothesis true can attributed random sampling alone . reject null hypothesis, deviation unlikely attributed random sampling alone.summarize, hypothesis testing used assess “statistical significance”, use subject matter knowledge assess “practical significance”.","code":""},{"path":"hypothesis-testing.html","id":"GOF","chapter":"9 Hypothesis Testing","heading":"9.5 Goodness of Fit Tests","text":"Goodness Fit (GOF) tests class hypothesis tests assess deviation observed data hypothesized distribution model, instead deviation estimators parameters. framework still :State hypothesis, based research question.Assume hypothesis true, compute metric measures much observed data deviate assumed hypothesis.Compare metric sort threshold assess observed data deviates “far enough” assumed hypothesis, make decision.GOF tests, hypotheses :\\(H_0:\\) data follow assumed distribution.\nRemember null hypothesis statement involving equality. One way remember data follow assumed distribution, density plot consistent PDF assumed distribution.\nRemember null hypothesis statement involving equality. One way remember data follow assumed distribution, density plot consistent PDF assumed distribution.\\(H_a:\\) data follow assumed distribution.\none alternative hypothesis GOF tests.\none alternative hypothesis GOF tests.lot different test statistics, just need remember larger values (magnitude) indicate data deviate null hypothesis.make conclusions manner, comparing test statistic critical value, p-value significance level.One interesting thing note GOF tests usually used assess data consistent distribution model. may done see can proceed another procedure depends whether data follow distribution. instances, like fail reject null hypothesis. Even though failing reject null hypothesis imply proven data follow assumed distribution, don’t evidence suggest follow assumed distribution, typically proceed.","code":""},{"path":"hypothesis-testing.html","id":"shapiro-wilk-test-for-normality","chapter":"9 Hypothesis Testing","heading":"9.5.1 Shapiro-Wilk Test for Normality","text":"actually seen one test Sections 5.6.2.1 7.2: Shapiro-Wilk test normality. test used assess data follow normal distribution. hypotheses : \\(H_0:\\) data follow normal distribution, \\(H_a:\\) data follow normal distribution.go details test statistic sampling distribution test statistic.Next, use couple examples simulated data:first example, simulate 25 data points standard normal, apply Shapiro-Wilk test. expected, test fails reject null hypothesis, p-value 0.1489.second example, simulate 25 data points non-normal distribution. used \\(\\chi_1^2\\) distribution (called chi-squared distribution 1 degree freedom), apply Shapiro-Wilk test. expected, test rejects null hypothesis based extremely small p-value.","code":"\nn<-25\nset.seed(8)\nX<-rnorm(n)\nshapiro.test(X)## \n##  Shapiro-Wilk normality test\n## \n## data:  X\n## W = 0.94011, p-value = 0.1489\nn<-25\nset.seed(8)\nX<-rchisq(n, df=1)\nshapiro.test(X)## \n##  Shapiro-Wilk normality test\n## \n## data:  X\n## W = 0.77791, p-value = 9.951e-05"},{"path":"linear-regression.html","id":"linear-regression","chapter":"10 Linear Regression","heading":"10 Linear Regression","text":"","code":""},{"path":"linear-regression.html","id":"introduction-6","chapter":"10 Linear Regression","heading":"10.1 Introduction","text":"broad range statistical methods available us learn data. Broadly speaking, methods can classified supervised unsupervised. Supervised methods involve relating response variable predictors, whereas unsupervised methods make distinction response variables predictors used find structure patterns data.Supervised methods generally two primary uses:Association: Quantify relationship variables. change predictor variable change value response variable?Prediction: Predict future value response variable, using information predictor variables.always distinguish response variable, denoted \\(y\\), predictor variable, denoted \\(x\\). supervised methods, say response variable can approximated mathematical function, denoted \\(f\\), predictor variable, .e.\\[\ny \\approx f(x).\n\\]Oftentimes, write relationship \\[\ny = f(x) + \\epsilon,\n\\]\\(\\epsilon\\) denotes random error term, mean 0. error term predicted based data .various methods estimate \\(f\\). estimate \\(f\\), can use method association / prediction.module, introduce one traditional supervised methods: linear regression. used single response variable quantitative. predictors quantitative categorical.","code":""},{"path":"linear-regression.html","id":"motivation","chapter":"10 Linear Regression","heading":"10.1.1 Motivation","text":"learn linear regression?Linear regression widely used many fields, , certain conditions, well two primary purposes supervised methods: association prediction. methods may better one purposes, usually expense purpose. important thing know questions order select right method best question.Linear regression widely used many fields, , certain conditions, well two primary purposes supervised methods: association prediction. methods may better one purposes, usually expense purpose. important thing know questions order select right method best question.Linear regression fairly easy interpret explain others may want know method works. methods generally complicated can feel like black-box explaining others, leading less confidence method.Linear regression fairly easy interpret explain others may want know method works. methods generally complicated can feel like black-box explaining others, leading less confidence method.lot ideas used methods can viewed extensions variations linear regression. understand linear regression works, becomes easier understand methods work.lot ideas used methods can viewed extensions variations linear regression. understand linear regression works, becomes easier understand methods work.","code":""},{"path":"linear-regression.html","id":"toy-example","chapter":"10 Linear Regression","heading":"10.1.2 Toy Example","text":"common way visualizing relationship one quantitative predictor variable one quantitative response variable scatter plot. simulated example , data 6000 UVa undergraduate students amount time spend studying week (minutes), many courses taking semester (3 4 credit courses). Figure 10.1 displays scatter plot.\nFigure 10.1: Scatterplot Study Time Number Courses Taken\nFigure 10.1 help us following questions:study time number courses taken related one another?strong relationship?use data make prediction study time student scatter plot?questions can answered using linear regression.","code":"\n##create dataframe\ndf<-data.frame(study,courses)\n\n##fit regression\nresult<-lm(study~courses, data=df)\n##create scatterplot with regression line overlaid\nplot(df$courses, df$study, xlab=\"# of Courses\", ylab=\"Study Time (Mins)\")\nabline(result)"},{"path":"linear-regression.html","id":"module-roadmap-8","chapter":"10 Linear Regression","heading":"10.1.3 Module Roadmap","text":"Section 10.2 sets linear regression model covers assumptions made.Section 10.3 goes linear regression model estimated.Section 10.4 goes sampling distribution estimators linear regression, well associated confidence intervals hypothesis tests. main tools used focused primary goal association.Section 10.5 goes measure model accuracy linear regression. measures used focused prediction.","code":""},{"path":"linear-regression.html","id":"SLR","chapter":"10 Linear Regression","heading":"10.2 Simple Linear Regression","text":"module, keep things simple considering single quantitative predictor. regression called simple linear regression (SLR) emphasize one predictor considered. briefly touch multiple linear regression (MLR) multiple predictors involved later module, learn linear regression next semester.","code":""},{"path":"linear-regression.html","id":"model-setup","chapter":"10 Linear Regression","heading":"10.2.1 Model Setup","text":"SLR, function \\(f\\) relates predictor variable response variable typically \\(\\beta_0 + \\beta_1 x\\). Mathematically, express \\[\ny \\approx f(x) = \\beta_0 + \\beta_1 x,\n\\]words, response variable approximately linear relationship predictor variable. SLR model written \\[\\begin{equation}\ny_i=\\beta_0+\\beta_{1}x_i + \\epsilon_i,\n\\tag{10.1}\n\\end{equation}\\]\\(y_i\\) denotes value response variable observation \\(\\),\\(x_i\\) denotes value predictor observation \\(\\),\\(\\epsilon_i\\) denotes value error observation \\(\\),\\(\\beta_0\\) \\(\\beta_1\\) parameters SLR model, want estimate . parameters sometimes called regression coefficients.\\(\\beta_1\\) also called slope.\\(\\beta_0\\) also called intercept.linear regression, make assumptions error term \\(\\epsilon\\):\\[\\begin{equation}\n\\epsilon_1,\\ldots,\\epsilon_n \\ ..d. \\sim N(0,\\sigma^2).\n\\tag{10.2}\n\\end{equation}\\]assumptions mean value predictor variable \\(x\\), response variable:follows normal distribution,expected value equal \\(\\beta_0+\\beta_{1} x\\), .e.\\[\\begin{equation}\nE(Y|X=x) = \\beta_0+\\beta_{1} x\n\\tag{10.3}\n\\end{equation}\\]variance equal \\(\\sigma^2\\).View video explains derived:words, conditional distribution \\(Y|X=x\\) \\(N(\\beta_0+\\beta_{1} x, \\sigma^2)\\). Applying study time example, implies :students take 3 courses, study time follows \\(N(\\beta_0 + 3\\beta_1, \\sigma^2)\\) distribution,students take 4 courses, study time follows \\(N(\\beta_0 + 4\\beta_1, \\sigma^2)\\) distribution,students take 5 courses, study time follows \\(N(\\beta_0 + 5\\beta_1, \\sigma^2)\\) distribution.subset dataframe three subsets, one students take 3 courses, another subset students take 4 courses, another subset students take 5 courses, create density plot study times subset, density plot follow normal distribution, different means, spread.Let us take look density plots Figures 10.2, 10.3, 10.4 :\nFigure 10.2: Distribution Study Time 3, 4, 5 Classes Taken\n\nFigure 10.3: Distribution Study Time 3, 4, 5 Classes Taken\n\nFigure 10.4: Distribution Study Time 3, 4, 5 Classes Taken\nNotice plots normal, different means (centers), similar spreads.notation left hand side equation (10.3) denotes expected value response variable, fixed value predictor variable. Therefore, regression coefficients can interpreted following manner:\\(\\beta_1\\) denotes change response variable, average, predictor increases one unit.\\(\\beta_0\\) denotes mean response variable predictor 0.","code":"\nlibrary(tidyverse)\n##subset dataframe\nx.3<-df[which(df$courses==3),]\n##density plot of study time for students taking 3 courses\nggplot(x.3,aes(x=study))+\n  geom_density()+\n  labs(x=\"Study Time (Mins)\", title=\"Dist of Study Times with 3 Courses\")\n##subset dataframe\nx.4<-df[which(df$courses==4),]\n##density plot of study time for students taking 4 courses\nggplot(x.4,aes(x=study))+\n  geom_density()+\n  labs(x=\"Study Time (Mins)\", title=\"Dist of Study Times with 4 Courses\")\n##subset dataframe\nx.5<-df[which(df$courses==5),]\n##density plot of study time for students taking 5 courses\nggplot(x.5,aes(x=study))+\n  geom_density()+\n  labs(x=\"Study Time (Mins)\", title=\"Dist of Study Times with 5 Courses\")"},{"path":"linear-regression.html","id":"assessing-assumptions","chapter":"10 Linear Regression","heading":"10.2.2 Assessing Assumptions","text":"assumptions error terms, \\(\\epsilon\\), expressed equation (10.2), can re-stated words following 4 assumptions:value predictor, errors mean 0.implies \\(f(x) = \\beta_0 + \\beta_1 x\\) approximates relationship variables well.scatter plot variables show linear relationship.important assumption 4. met, predictions biased, words, predictions systematically - - predict value response variable.plots Figure 10.5 based simulated data.scatter plot shown Figure 10.5() example assumption met. move left right plot, data points generally evenly scattered sides regression line overlaid.scatter plot shown Figure 10.5() example assumption met. move left right plot, data points generally evenly scattered sides regression line overlaid.scatter plot shown Figure 10.5(b) example assumption met. move left right plot Figure 10.5(b), data points generally evenly scattered sides regression line overlaid. shape plots look like curve rather straight line.scatter plot shown Figure 10.5(b) example assumption met. move left right plot Figure 10.5(b), data points generally evenly scattered sides regression line overlaid. shape plots look like curve rather straight line.\nFigure 10.5: Assumption 1 Assessment\nvalue predictor, errors variance denoted \\(\\sigma^2\\).implies scatter plot, vertical variation data points around regression equation magnitude everywhere.assumption met, hypothesis tests confidence intervals linear regression unreliable.plots Figure 10.6 based simulated data.scatter plot shown Figure 10.6() example assumption met (figure actually Figure 10.5(), data produced plots satisfy assumptions). move left right plot, vertical variation data points regression line approximately constant.scatter plot shown Figure 10.6() example assumption met (figure actually Figure 10.5(), data produced plots satisfy assumptions). move left right plot, vertical variation data points regression line approximately constant.scatter plot shown Figure 10.6(b) example assumption met. move left right plot Figure 10.6(b), vertical variation data points regression line becomes larger value response variable gets larger, variance constant.scatter plot shown Figure 10.6(b) example assumption met. move left right plot Figure 10.6(b), vertical variation data points regression line becomes larger value response variable gets larger, variance constant.\nFigure 10.6: Assumption 2 Assessment\nerrors independent.implies observations independent. usually -product observations sampled. knowing data collection method help assess whether assumption met.assumption met, hypothesis tests confidence intervals linear regression unreliable.errors normally distributed.considered least important 4 assumptions, especially large sample sizes. due Central Limit Theorem. linear regression, write conditional expectation (mean) response variable equal \\(f(x) = \\beta_0 + \\beta_1 x\\), hypothesis tests confidence intervals linear regression likely reliable.Thought question: Look scatter plot toy example Figure 10.1. Can explain scatter plot shows first two assumptions met linear regression?View video explanation assess assumptions 1 2 scatter plot data:","code":""},{"path":"linear-regression.html","id":"estSLR","chapter":"10 Linear Regression","heading":"10.3 Estimating Regression Coefficients","text":"two methods estimating regression coefficients: method least squares method maximum likelihood. large sample sizes, methods give similar results.go method least squares first, since method usually used explain new learners conceptually easier understand.","code":""},{"path":"linear-regression.html","id":"method-of-least-squares","chapter":"10 Linear Regression","heading":"10.3.1 Method of Least Squares","text":"equations (10.3) (10.1), notice estimate regression coefficients \\(\\beta_0, \\beta_1\\). unable obtain numerical values parameters data entire population. use data sample estimate parameters. estimate \\(\\beta_0,\\beta_1\\) using \\(\\hat{\\beta}_0,\\hat{\\beta}_1\\) based sample observations \\((x_i,y_i)\\) size \\(n\\).Following equations (10.3) (10.1), sample versions \\[\\begin{equation}\n\\hat{y}=\\hat{\\beta}_0+\\hat{\\beta}_1 x\n\\tag{10.4}\n\\end{equation}\\]\\[\\begin{equation}\ny=\\hat{\\beta}_0+\\hat{\\beta}_1 x + e\n\\tag{10.5}\n\\end{equation}\\]respectively. Eqaution (10.4) called fitted line. line overlaid scatter plot toy example Figure 10.1 represents fitted line. Equation (10.5) called estimated SLR model.\\(\\hat{\\beta}_1,\\hat{\\beta}_0\\) estimators \\(\\beta_1,\\beta_0\\) respectively. estimators can interpreted following manner:\\(\\hat{\\beta}_1\\) denotes change predicted \\(y\\) \\(x\\) increases 1 unit. Alternatively, estimates change \\(y\\), average, \\(x\\) increases 1 unit.\\(\\hat{\\beta}_0\\) denotes predicted \\(y\\) \\(x=0\\). Alternatively, estimates average \\(y\\) \\(x=0\\).equation (10.5), notice use \\(e\\) denote residual, words, “error” sample.equations (10.4) (10.5), following quantities can compute:Fitted values predicted values:\\[\\begin{equation}\n\\hat{y} = \\hat{\\beta}_0+\\hat{\\beta}_1 x.\n\\tag{10.6}\n\\end{equation}\\]fitted values predicted values response variable predictor equal specific value. Visually, fitted line represents fitted value vary value predictor.Residuals:\\[\\begin{equation}\ne_i = y_i-\\hat{y}_i.\n\\tag{10.7}\n\\end{equation}\\]residuals differences actual values response variable corresponding predicted values based fitted line. Visually, residual vertical distance data point scatter plot fitted line, shown Figure 10.7 :\nFigure 10.7: Example Residuals. Picture https://www.statology.org/residuals/\nSum Squared Residuals:\\[\\begin{equation}\nSS_{res} =  \\sum\\limits_{=1}^n(y_i-\\hat{y}_i)^2.\n\\tag{10.8}\n\\end{equation}\\]compute estimated coefficients \\(\\hat{\\beta}_1,\\hat{\\beta}_0\\) using method least squares, .e. choose numerical values \\(\\hat{\\beta}_1,\\hat{\\beta}_0\\) minimize \\(SS_{res}\\) given equation (10.8). find line minimizes sum squared residuals scatter plot, hence name method least squares.minimizing \\(SS_{res}\\) respect \\(\\hat{\\beta}_0\\) \\(\\hat{\\beta}_1\\), estimated coefficients simple linear regression equation \\[\\begin{equation}\n\\hat{\\beta}_1 = \\frac{\\sum\\limits_{=1}^n(x_i-\\bar{x})(y_i-\\bar{y})}{\\sum\\limits_{=1}^n(x_i-\\bar{x})^2}\n\\tag{10.9}\n\\end{equation}\\]\\[\\begin{equation}\n\\hat{\\beta}_0 = \\bar{y}- \\hat{\\beta}_1 \\bar{x}\n\\tag{10.10}\n\\end{equation}\\]\\(\\hat{\\beta}_1, \\hat{\\beta}_0\\) called least squares estimators, emphasize values found minimizing \\(SS_{res}\\).minimization \\(SS_{res}\\) respect \\(\\hat{\\beta}_0\\) \\(\\hat{\\beta}_1\\) done taking partial derivatives (10.8) respect \\(\\hat{\\beta}_1\\) \\(\\hat{\\beta}_0\\), setting two partial derivatives equal 0, solving two equations \\(\\hat{\\beta}_1\\) \\(\\hat{\\beta}_0\\).View video explanation method least squares:Let’s take look estimated coefficients study time example:sample 6000 students, \\(\\hat{\\beta}_1\\) = 120.3930985. predicted study time increases 120.3930985 minutes additional course taken.\\(\\hat{\\beta}_0\\) = 58.4482853. predicted study time 58.4482853 courses taken. Notice value make sense, student taking 0 courses. look data, number courses taken 3, 4, 5. use regression \\(3 \\leq x \\leq 5\\). use values \\(x\\) outside range data. Making predictions response variable predictors outside range data called extrapolation done.Thought question: response variable toy example study time, minutes. Suppose convert values hours dividing 60. numerical value estimated coefficients change? interpretation estimated coefficients change?Note: often get question minimize \\(SS_{res}\\) quantity minimizing sum absolute value residuals \\(\\sum_{=1}^n |y_i - \\hat{y}_i|\\)? state famous theorem without proving . Gauss Markov Theorem: assumptions regression model (stated equation (10.2)), least squares estimators \\(\\hat{\\beta}_1\\) \\(\\hat{\\beta}_0\\) unbiased minimum variance among unbiased linear estimators. means way deriving unbiased estimators result estimators larger variance least squares estimators.","code":"\n##fit regression\nresult<-lm(study~courses, data=df) ##supply y, then x, and specify dataframe via data\n##print out the estimated coefficients\nresult## \n## Call:\n## lm(formula = study ~ courses, data = df)\n## \n## Coefficients:\n## (Intercept)      courses  \n##       58.45       120.39"},{"path":"linear-regression.html","id":"method-of-maximum-likelihood","chapter":"10 Linear Regression","heading":"10.3.2 Method of Maximum Likelihood","text":"give brief overview maximum likelihood estimation carried simple linear regression. earlier written conditional distribution \\(Y|X=x\\) \\(N(\\beta_0+\\beta_{1} x, \\sigma^2)\\). know PDF normal distribution takes form equation (4.11), implies can write PDF distribution \\[\\begin{equation}\nf_{Y|X}(y|x) = \\frac{1}{\\sigma \\sqrt{2 \\pi}} \\exp \\left(-\\frac{(y - (\\beta_0+\\beta_{1} x))^2}{2 \\sigma^2} \\right).\n\\tag{10.11}\n\\end{equation}\\]can write corresponding log-likelihood function, using equation (7.2), \\[\\begin{equation}\n\\ell(\\beta_0, \\beta_1, \\sigma^2 | \\boldsymbol{y}, \\boldsymbol{x}) = -\\frac{n}{2} \\log(2\\pi) - n \\log(\\sigma) - \\frac{1}{2 \\sigma^2} \\sum_{=1}^n \\left(y_i - (\\beta_0 + \\beta_1 x_i) \\right)^2.\n\\tag{10.12}\n\\end{equation}\\]take partial derivatives equation (10.12) respect \\(\\hat{\\beta}_1\\) \\(\\hat{\\beta}_0\\), set two partial derivatives equal 0, solve two equations \\(\\hat{\\beta}_1\\) \\(\\hat{\\beta}_0\\). end solutions method least squares.code finds estiamted coeffficients study time example. glm() function fits via method maximum likelihood, whereas lm() function earlier fits via method least squares.","code":"\n##fit regression\nresult.mle<-glm(study~courses, data=df) ##we use glm() function for ML instead\n##print out the estimated coefficients. Notice they are the same. \nresult.mle## \n## Call:  glm(formula = study ~ courses, data = df)\n## \n## Coefficients:\n## (Intercept)      courses  \n##       58.45       120.39  \n## \n## Degrees of Freedom: 5999 Total (i.e. Null);  5998 Residual\n## Null Deviance:       63300000 \n## Residual Deviance: 5317000   AIC: 57750"},{"path":"linear-regression.html","id":"SLRinf","chapter":"10 Linear Regression","heading":"10.4 Inference with Simple Linear Regression","text":"process using data sample draw conclusion population called (statistical) inference. Two methods associated inference confidence intervals hypothesis testing.common inference deals slope, \\(\\beta_1\\). usually assessing whether slope 0 , slope 0 implies linear relationship variables (slope 0, value response affected value predictor).cover confidence interval hypothesis test slope.","code":""},{"path":"linear-regression.html","id":"properties-of-least-squares-estimators","chapter":"10 Linear Regression","heading":"10.4.1 Properties of Least Squares Estimators","text":"proceeding, state properties associated least squares estimators \\(\\hat{\\beta}_1, \\hat{\\beta}_0\\). key realize sampling distribution least squares estimators known. torn formulas lot computed using R. proof beyond scope class, countless books provide proofs interested.\\(\\mbox{E}(\\hat{\\beta}_1) = \\beta_1\\), \\(\\mbox{E}(\\hat{\\beta}_0) = \\beta_0\\). words, least squares estimators unbiased.\\(\\mbox{E}(\\hat{\\beta}_1) = \\beta_1\\), \\(\\mbox{E}(\\hat{\\beta}_0) = \\beta_0\\). words, least squares estimators unbiased.variance \\(\\hat{\\beta}_1\\) isThe variance \\(\\hat{\\beta}_1\\) \\[\\begin{equation}\n\\mbox{Var}(\\hat{\\beta}_1) = \\frac{\\sigma^{2}}{\\sum_{=1}^n{(x_{}-\\bar{x})^{2}}}\n\\tag{10.13}\n\\end{equation}\\]variance \\(\\hat{\\beta}_0\\) \\[\\begin{equation}\n\\mbox{Var}(\\hat{\\beta}_0) = \\sigma^2 \\left[\\frac{1}{n} + \\frac{\\bar{x}^2}{\\sum_{=1}^n (x_i -\\bar{x})^2}\\right]\n\\tag{10.14}\n\\end{equation}\\]\\(\\hat{\\beta}_1\\) \\(\\hat{\\beta}_0\\) follow normal distribution.Note equations (10.13) (10.14), use \\(s^2 = \\frac{SS_{res}}{n-2} = MS_{res}\\) estimate \\(\\sigma^2\\), variance errors, since \\(\\sigma^2\\) unknown value.imply standardize \\(\\hat{\\beta}_1\\) \\(\\hat{\\beta}_0\\), standardized quantities follow \\(t_{n-2}\\) distribution, .e.\\[\\begin{equation}\n\\frac{\\hat{\\beta}_1 - \\beta_1}{se(\\hat{\\beta}_1)}\\sim t_{n-2}\n\\tag{10.15}\n\\end{equation}\\]\\[\\begin{equation}\n\\frac{\\hat{\\beta}_0 - \\beta_0}{se(\\hat{\\beta}_0)}\\sim t_{n-2},\n\\tag{10.16}\n\\end{equation}\\]\\[\\begin{equation}\nse(\\hat{\\beta}_1) = \\sqrt{\\frac{MS_{res}}{\\sum_{=1}^n{(x_{}-\\bar{x})^{2}}}} = \\frac{s}{\\sqrt{\\sum_{=1}^n{(x_{}-\\bar{x})^{2}}}}\n\\tag{10.17}\n\\end{equation}\\]\\[\\begin{equation}\nse(\\hat{\\beta}_0) = \\sqrt{MS_{res}\\left[\\frac{1}{n} + \\frac{\\bar{x}^2}{\\sum_{=1}^n (x_i -\\bar{x})^2}\\right]} = s \\sqrt{\\frac{1}{n} + \\frac{\\bar{x}^2}{\\sum_{=1}^n (x_i -\\bar{x})^2}}\n\\tag{10.18}\n\\end{equation}\\]Note: standardized quantities equations (10.15) (10.16) follow \\(t_{n-2}\\) distribution. mentioned Section 8.2.4.3, lose 1 degree freedom every equation must satisfied. SLR, 2 equations, equations (10.9) (10.10), must satisfied calculating least squares estimators, therefore \\(n-2\\) degrees freedom.","code":""},{"path":"linear-regression.html","id":"confidence-interval","chapter":"10 Linear Regression","heading":"10.4.2 Confidence Interval","text":"equation (10.15), note standardized \\(\\hat{\\beta}_1\\) follows \\(t_{n-2}\\) distribution, symmetric. implies confidence interval slope takes form expressed equation (8.2), \\(\\text{point estimate } \\pm \\text{ margin error}\\). also know margin error critical value multiplied standard error estimator.Thought question: scrolling , can write formula confidence interval slope?\\(100(1-\\alpha)\\%\\) CI \\(\\beta_1\\) \\[\\begin{equation}\n\\hat{\\beta}_1 \\pm t_{n-2}^*  se(\\hat{\\beta}_1) = \\hat{\\beta}_1 \\pm t_{n-2}^* {\\sqrt\\frac{MS_{res}}{\\sum_{=1}^n(x_i - \\bar{x})^{2}}}.\n\\tag{10.19}\n\\end{equation}\\]critical value \\(t_{n-2}^*\\) found usual manner; corresponds \\((1 - \\alpha/2) \\times 100\\)th percentile \\(t_{n-2}\\) distribution.Going back study time example, 95% CI slope (119.470237, 121.3159601).interpretation CI 95% probability random interval (119.470237, 121.3159601) contains true value slop.interval excludes 0, linear relationship number courses study time.","code":"\n##CI for coefficients\nconfint(result,level = 0.95)[2,]##    2.5 %   97.5 % \n## 119.4702 121.3160"},{"path":"linear-regression.html","id":"hypothesis-testing-1","chapter":"10 Linear Regression","heading":"10.4.3 Hypothesis Testing","text":"context SLR, usually want test slope \\(\\beta_1\\) 0 . slope 0, linear relationship variables. hypotheses typically \\(H_0: \\beta_1 = 0, H_a: \\beta_1 \\neq 0.\\)equation (10.15), noted standardized \\(\\hat{\\beta}_1\\) follows \\(t_{n-2}\\) distribution, test statistic t-statistic takes general form equation (9.2).Thought question: scrolling , can make educated guess formula t-statistic testing slope?test statistic \\[\\begin{equation}\nt = \\frac{\\hat{\\beta}_1 - \\text{ value } H_0}{se(\\hat{\\beta}_1)}.\n\\tag{10.20}\n\\end{equation}\\]p-value critical value found usual manner, based \\(t_{n-2}\\) distribution, used usual manner.go back study time example:R, reported t-statistic p-value hypothesis test slope 255.74125 approximately 0 respectively. reject null hypothesis: data support claim linear relationship study time number courses taken. Note R also reports quantities hypothesis test intercept, although test rarely used less practical implications.Note: reported values based two-sided alternative hypothesis null hypothesis \\(H_0: \\beta_1 = 0\\). different hypotheses perform calculations .","code":"\nsummary(result)$coefficients ##print out est coeffs, SEs, t statistics, and p-vals##              Estimate Std. Error   t value      Pr(>|t|)\n## (Intercept)  58.44829  1.9218752  30.41211 4.652442e-189\n## courses     120.39310  0.4707614 255.74125  0.000000e+00"},{"path":"linear-regression.html","id":"correlation","chapter":"10 Linear Regression","heading":"10.4.4 Correlation","text":"Another commonly used measure linear regression correlation, explored earlier Section 5.4.2. Visually, variables higher correlation (magnitude) means observations fall closer fitted line. correlation 1 -1 mean observations fall perfectly fitted line.","code":""},{"path":"linear-regression.html","id":"coefficient-of-determination","chapter":"10 Linear Regression","heading":"10.4.5 Coefficient of Determination","text":"Another measure commonly used linear regression coefficient determination, usually denoted \\(R^2\\). value represents proportion variation response variable can explained linear regression. SLR, numerically equal squared sample correlation.notes \\(R^2\\):\\(0 \\leq R^2 \\leq 1\\).Values closer 1 indicate better fit; values closer 0 indicate poorer fit.Often reported percentage.","code":""},{"path":"linear-regression.html","id":"linear-regression-in-r","chapter":"10 Linear Regression","heading":"10.4.6 Linear Regression in R","text":"Next, perform simple linear regression real data set using R. work data set elmhurst openintro package R.Type ?openintro::elmhurst read documentation data sets R. Always seek understand background data! key pieces information :random sample 50 students (freshman 2011 class Elmhurst College).Family income student (units missing).Gift aid, $1000s.want explore family income may related gift aid, simple linear regression framework.","code":"\nlibrary(tidyverse)\nlibrary(openintro)\nData<-openintro::elmhurst"},{"path":"linear-regression.html","id":"visualization","chapter":"10 Linear Regression","heading":"Visualization","text":"always verify scatter plot relationship (approximately) linear proceeding correlation simple linear regression!\nFigure 10.8: Scatter Plot Worked Example\nnote observations fairly evenly scattered sides regression line, linear association exists. see negative linear association. family income increases, gift aid, average, decreases.also see observation weird values may warrant investigation.","code":"\n##scatterplot of gift aid against family income\nggplot2::ggplot(Data, aes(x=family_income,y=gift_aid))+\n  geom_point()+\n  geom_smooth(method = \"lm\", se=FALSE)+\n  labs(x=\"Family Income\", y=\"Gift Aid\", title=\"Scatterplot of Gift Aid against Family\")## `geom_smooth()` using formula = 'y ~ x'"},{"path":"linear-regression.html","id":"regression","chapter":"10 Linear Regression","heading":"Regression","text":"use lm() function fit regression model. supply response variable, predictor, ~ , specify dataframe via data argument.Use summary() function display relevant information regression:see following values:\\(\\hat{\\beta}_1 =\\) -0.0430717. estimated slope informs us predicted gift aid decreases 0.0430717 thousands dollars ($43.07) per unit increase family income.\\(\\hat{\\beta}_0 =\\) 24.319329. students family income, predicted gift aid $24 319.33. Note: scatter plot, observation 0 family income. must careful extrapolating making predictions regression. make predictions family incomes minimum maximum values family incomes data.\\(s\\) = 4.7825989, estimate standard deviation error terms, \\(\\sigma\\). reported residual standard error R. Squaring gives estimated variance error terms.\\(R^2 =\\) 0.2485582. coefficient determination informs us 24.86% variation gift aid can explained family income. reported multiple R-squared R.","code":"\n##regress gift aid against family income\nresult<-lm(gift_aid~family_income, data=Data)\n##look at information regarding regression\nsummary(result)## \n## Call:\n## lm(formula = gift_aid ~ family_income, data = Data)\n## \n## Residuals:\n##      Min       1Q   Median       3Q      Max \n## -10.1128  -3.6234  -0.2161   3.1587  11.5707 \n## \n## Coefficients:\n##               Estimate Std. Error t value Pr(>|t|)    \n## (Intercept)   24.31933    1.29145  18.831  < 2e-16 ***\n## family_income -0.04307    0.01081  -3.985 0.000229 ***\n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n## \n## Residual standard error: 4.783 on 48 degrees of freedom\n## Multiple R-squared:  0.2486, Adjusted R-squared:  0.2329 \n## F-statistic: 15.88 on 1 and 48 DF,  p-value: 0.0002289"},{"path":"linear-regression.html","id":"hypothesis-testing-2","chapter":"10 Linear Regression","heading":"Hypothesis Testing","text":"coefficients, can see results hypothesis tests \\(\\beta_1\\) \\(\\beta_0\\). Specifically, \\(\\beta_1\\):\\(\\hat{\\beta}_1\\) = -0.0430717\\(se(\\hat{\\beta}_1)\\) = 0.0108095the test statistic \\(t\\) = -3.984621the corresponding p-value 2.2887345^{-4}can work p-value using R (slight difference due rounding):find critical value using R:Either way, end rejecting null hypothesis. data support claim linear association gift aid family income.Remember \\(t\\) tests regression coefficients based \\(H_0: \\beta_j = 0, H_a: \\beta_j \\neq 0\\). reported p-value based set null alternative hypotheses. null alternative hypotheses different, need compute test statistic p-value.","code":"\n##pvalue\n2*pt(-abs(-3.985), df = 50-2)## [1] 0.0002285996\n##critical value\nqt(1-0.05/2, df = 50-2)## [1] 2.010635"},{"path":"linear-regression.html","id":"confidence-intervals-1","chapter":"10 Linear Regression","heading":"Confidence Intervals","text":"find 95% confidence intervals coefficients, use confint() function:95% CI \\(\\beta_1\\) (-0.0648056, -0.0213378). 95% confidence additional thousand dollars family income, predicted gift aid decreases $21.3378 $64.8056.","code":"\n##to produce 95% CIs for all regression coefficients\nconfint(result,level = 0.95)##                     2.5 %      97.5 %\n## (Intercept)   21.72269421 26.91596380\n## family_income -0.06480555 -0.02133775"},{"path":"linear-regression.html","id":"extract-values-from-r-objects","chapter":"10 Linear Regression","heading":"Extract Values from R Objects","text":"can actually extract values reported summary(result). see can extracted R object, use names() function:extract estimated coefficients:Notice information presented table. extract specific value, can specify row column indices:, extract values residual standard error \\(R^2\\).","code":"\n##see what can be extracted from summary(result)\nnames(summary(result))##  [1] \"call\"          \"terms\"         \"residuals\"     \"coefficients\" \n##  [5] \"aliased\"       \"sigma\"         \"df\"            \"r.squared\"    \n##  [9] \"adj.r.squared\" \"fstatistic\"    \"cov.unscaled\"\n##extract coefficients\nsummary(result)$coefficients##                  Estimate Std. Error   t value     Pr(>|t|)\n## (Intercept)   24.31932901 1.29145027 18.831022 8.281020e-24\n## family_income -0.04307165 0.01080947 -3.984621 2.288734e-04\n##extract slope\nsummary(result)$coefficients[2,1]## [1] -0.04307165\n##extract intercept\nsummary(result)$coefficients[1,1]## [1] 24.31933"},{"path":"linear-regression.html","id":"prediction","chapter":"10 Linear Regression","heading":"Prediction","text":"use regression models prediction. Suppose want predict gift aid student family income 50 thousand dollars (assuming unit thousands dollars). use predict() function:student’s predicted gift aid $22 165.75. Alternatively, calculated plugging \\(x=50\\) estimated SLR equation:","code":"\n##create data point for prediction\nnewdata<-data.frame(family_income=50)\n##predicted gift aid when x=50\npredict(result,newdata)##        1 \n## 22.16575\nsummary(result)$coefficients[1,1] + summary(result)$coefficients[2,1]*50## [1] 22.16575"},{"path":"linear-regression.html","id":"correlation-1","chapter":"10 Linear Regression","heading":"Correlation","text":"use cor() function find correlation two quantitative variables:correlation -0.4985561. moderate, negative linear association family income gift aid.View video need little explanation using R linear regression:","code":"\n##correlation\ncor(Data$family_income,Data$gift_aid)## [1] -0.4985561"},{"path":"linear-regression.html","id":"SLRacc","chapter":"10 Linear Regression","heading":"10.5 Assessing Model Accuracy","text":"introduction module, wrote supervised methods linear regression two primary uses:Association: Quantify relationship variables. change predictor variable change value response variable?Prediction: Predict future value response variable, using information predictor variables.Section 10.4, focused sampling distribution least squares estimators used construct confidence intervals perform hypothesis tests. tools useful trying explain relationship response variable predictor, first primary use supervised methods. focus lot traditional research wide variety applications, example:Medicine: increasing dose medication reduce severity symptoms?Advertising: advertising various media affect sales?Environmental science: increased greenhouse emissions affect air quality?Education: use AI tools aid student learning outcomes?section, focus second primary use supervised methods: prediction. focus prediction, mainly care close predicted (fitted) values true values. lot mentioned Section 10.4 can actually ignored (assumption 1 regression assumptions, since predictions definitely suffer met) focus prediction. Prediction domain predictive analytics. predictive analytics can used wide variety applications, kind questions answer differ earlier questions, example:Hospitality: Determining staffing needs casino. want able predict right number staff hire: -staffing costs money -staffing results unhappy customers.Online ads: Based user’s history Amazon, Amazon like able recommend products user highly likely purchase.Personalized medicine: Based patient’s history, effective treatment plan?Dynamic pricing: airlines use dynamic pricing adjusts price maximize profits, based real-time market conditions.questions, want know model can predict value response variable well; necessarily want know predictors may explain response variable.","code":""},{"path":"linear-regression.html","id":"metrics-for-model-accuracy","chapter":"10 Linear Regression","heading":"10.5.1 Metrics for Model Accuracy","text":"introduce common metrics measure model accuracy. talking framework linear regression response variable quantitative, metrics apply supervised methods response variable quantitative. separate metrics response variable categorical learn next semester.","code":""},{"path":"linear-regression.html","id":"mean-squared-error","chapter":"10 Linear Regression","heading":"10.5.1.1 Mean-Squared Error","text":"mean-squared error (MSE) commonly used metric assess model accuracy. MSE predictions \\[\\begin{equation}\nMSE(\\hat{y}) = \\frac{\\sum_{=1}^n (y_i - \\hat{y}_i)^2}{n}.\n\\tag{10.21}\n\\end{equation}\\]MSE can interpreted average squared prediction error. compare equation (10.21) equation (10.8), may notice numerator MSE \\(SS_{res}\\).may also recall MSE introduced Section 7.4.5. section, measuring average squared difference estimator parameter, rather average squared difference predicted value true value. terminology standpoint, MSEs: one MSE estimator, MSE predictions, may need clarify MSE discussed someone just uses term MSE.come surprise MSE equation (10.21) can decomposed like MSE estimator equation (7.5):\\[\\begin{equation}\nMSE(\\hat{y}) = Var(\\hat{y}) + Bias(\\hat{y})^2.\n\\tag{10.22}\n\\end{equation}\\]MSE can decomposed variance predicted values squared bias predicted values. linear regression setting, bias theoretically 0 find correct form \\(f(x)\\) relate response variable predictors.","code":""},{"path":"linear-regression.html","id":"root-mean-squared-error","chapter":"10 Linear Regression","heading":"10.5.1.2 Root Mean-Squared Error","text":"MSE popular metric model accuracy decomposition per equation (10.22). However, interpreting numerical value can challenging, unit associated MSE squared unit response variable, poses difficulty making judgement whether reported value MSE actually large small.root mean-squared error (RMSE) suggested, since unit response variable, making easy make judgement whether reported value large small:\\[\\begin{equation}\nRMSE(\\hat{y}) = \\sqrt{\\frac{\\sum_{=1}^n (y_i - \\hat{y}_i)^2}{n}} = \\sqrt{MSE(\\hat{y})}.\n\\tag{10.23}\n\\end{equation}\\]RMSE simply square root MSE. Interpreting RMSE gets tricky; tempting interpret average prediction error, quite correct , since \\(\\sqrt{\\frac{\\sum_{=1}^n (y_i - \\hat{y}_i)^2}{n}}\\) \\(\\frac{\\sum_{=1}^n \\sqrt{(y_i - \\hat{y}_i)^2}}{n}\\).","code":""},{"path":"linear-regression.html","id":"mean-absolute-deviation","chapter":"10 Linear Regression","heading":"10.5.1.3 Mean Absolute Deviation","text":"response difficulty interpreting RMSE, another metric, called mean absolute deviation (MAD) proposed:\\[\\begin{equation}\nMAD(\\hat{y}) = \\frac{\\sum_{=1}^n |y_i - \\hat{y}_i|}{n}.\n\\tag{10.24}\n\\end{equation}\\]unit MAD response variable, can interpreted average prediction error.","code":""},{"path":"linear-regression.html","id":"additional-comments-about-mse-rmse-mad","chapter":"10 Linear Regression","heading":"10.5.1.4 Additional Comments about MSE, RMSE, MAD","text":"rest module, focusing MSE RMSE, certain properties proven mathematically. discuss properties next.","code":""},{"path":"linear-regression.html","id":"model-complexity","chapter":"10 Linear Regression","heading":"10.5.2 Model Complexity","text":"Thus far, considered simple linear regression, .e. one predictor model. approach useful want focus single predictor related response variable. However, surprising learn many settings, can improve predictions response variable eitherincorporating additional predictors (useful), orconsidering polynomials predictor (relationship response variable predictor linear), oradding interactions predictors, orusing combination steps.additional predictors, linear regression model written \\[\\begin{equation}\ny=\\beta_0+\\beta_{1}x_1 + \\beta_2 x_2 + \\cdots + \\beta_k x_k + \\epsilon,\n\\tag{10.25}\n\\end{equation}\\]incorporate \\(k\\) quantitative predictors \\(x_1, \\cdots, x_k\\).relationship response variable predictor linear, linear regression model can written \\[\\begin{equation}\ny=\\beta_0+\\beta_{1}x + \\beta_2 x^2 + \\cdots + \\beta_k x^k + \\epsilon,\n\\tag{10.26}\n\\end{equation}\\]consider \\(k\\)-degree polynomial \\(f(x)\\) approximate relationship response variable predictor.Interactions predictors occur effect one predictor response variable depends value another predictor. Consider predicting recovery rates patients, based amount medication administered age patient. Perhaps higher doses medication speeds recovery, young patients. However, medication may work old patients. effect medication recovery depends age patient, interaction amount medicine administered age patient. scenario, linear regression model can written \\[\\begin{equation}\ny=\\beta_0+\\beta_{1}x_1 + \\beta_2 x_2 + \\beta_3 x_1 x_2 + \\epsilon,\n\\tag{10.27}\n\\end{equation}\\]term \\(x_1 x_2\\) represents considering interaction predictors.also possible linear regression model multiple predictors, higher order polynomials, interactions. example, \\(y=\\beta_0+\\beta_{1}x_1 + \\beta_2 x_2 + \\beta_3 x_3 + \\beta_4 x_3^2 + \\beta_5 z_1 x_2 + \\epsilon\\).class, consider comparing complexity models nested. Two models nested one model can derived setting parameters complex model equal 0. example, SLR model equation (10.1) nested within model equation (10.25), can get SLR model setting \\(\\beta_2 = \\beta_3 = \\cdots = \\beta_k = 0\\) equation (10.25).Thought question: Can explain SLR model equation (10.1) nested within model equation (10.26)? Can explain SLR model equation (10.1) nested within model equation (10.27)?complexity linear regression models can measured number parameters model, denote \\(p\\). SLR model equation (10.1), \\(p=2\\), since \\(\\beta_0\\) \\(\\beta_1\\).Note: consider models complex flexible, fairly often call model complexity sometimes called model flexibility.many potential models consider. Metrics involving model accuracy can help us decide model best terms prediction accuracy, selecting model smallest MSE (RMSE).","code":""},{"path":"linear-regression.html","id":"training-test-split","chapter":"10 Linear Regression","heading":"10.5.3 Training-Test Split","text":"reading previous subsections, one may consider following strategy assessing model accuracy choosing several nested models: fit models consideration using available data, choose model smallest MSE (RMSE).seemingly intuitive, stragedy work. property MSE RMSE nested models values decrease model gets complex! go whole proof , intuitively, one can see complex model never worse less complex model: happens additional parameters 0. property suggests using strategy always end selecting complex model, complex model always smallest MSE RMSE.Take step back remind actually want prediction: fitting model, want able accurately predict values response variable future. fit model well based current data, model may poorly new data. called overfitting.split given data set two portions. One portion, called training data, used fit (train) model. portion, called test data, used assess prediction accuracy model. test data never used fit model, test data play role new data.MSE (RMSE) calculated portions, called training MSE test MSE. relationship training test MSEs complexity nested models shown Figure 10.9:\nFigure 10.9: Training & Test MSE vs Complexity\nmake following comments Figure 10.9:blue curve represents training MSE behaves models get complex. models get complex, training MSE never increases.black curve represents test MSE behaves models get complex. test MSE minimum find appropriate level complexity, denoted red line. call model optimal model. test MSE increases models get complex, less complex, optimal model.Overfitted models models complex optimal model.\nOverfitted models generally low training MSE, high test MSE. large difference two also indicative overfitting.\nOverfitted models generally low training MSE, high test MSE. large difference two also indicative overfitting.Underfitted models models less complex optimal model.\nUnderfitted models generally high training MSE high test MSE.\nUnderfitted models generally high training MSE high test MSE.average, training MSE lower test MSE model. model usually estimated minimizing quantity related training MSE. example, minimize \\(SS_{res}\\), numerator training MSE, linear regression.","code":""},{"path":"linear-regression.html","id":"bias-variance-trade-off-model-complexity","chapter":"10 Linear Regression","heading":"10.5.4 Bias-Variance Trade-Off & Model Complexity","text":"equation (10.22), see MSE can decomposed variance predicted values squared bias predicted values. Figure 10.9, see MSE related complexity model. surprise see complexity models related bias variance predictions models. relationship called bias-variance tradeoff. tradeoff tells us model produces predictions biased, predictions smaller variance, vice versa. summarize relationship model complexity bias variance predictions:Bias increase model gets less complex. Models simple (complex enough) include important predictors, misspecify relationship response predictors, fail include relevant interactions.Bias increase model gets less complex. Models simple (complex enough) include important predictors, misspecify relationship response predictors, fail include relevant interactions.Variance increase model gets complex. Models complex include unnecessary predictors, include higher order polynomials needed, include interactions needed.Variance increase model gets complex. Models complex include unnecessary predictors, include higher order polynomials needed, include interactions needed.can see minimize test MSE, need find right balance bias variance model, finding right level complexity.","code":""},{"path":"linear-regression.html","id":"monte-carlo-simulations","chapter":"10 Linear Regression","heading":"10.5.5 Monte Carlo Simulations","text":"Next, use Monte Carlo simulations illustrate MSE, bias, variance, model complexity related. simulation, going simulate data model \\(y = x^2 + \\epsilon\\), \\(\\epsilon\\) ..d. \\(N(0,1)\\). true relationship response variable predictor \\(f(x) = x^2\\). write function simulates data model:Note linear regression setting, assumptions made predictor, free simulate predictor distribution. need sure specify \\(y = f(x) + \\epsilon\\) errors ..d. Normal mean 0 fixed value variance, set 1.consider fitting different fitted lines simulated data:Fitted line 1: \\(\\hat{y} = \\hat{\\beta}_0,\\) \\(\\hat{f}(x) = \\hat{\\beta}_0\\).Fitted line 2: \\(\\hat{y} = \\hat{\\beta}_0 + \\hat{\\beta}_1 x,\\) \\(\\hat{f}(x) = \\hat{\\beta}_0 + \\hat{\\beta}_1 x\\).Fitted line 3: \\(\\hat{y} = \\hat{\\beta}_0 + \\hat{\\beta}_1 x + \\hat{\\beta}_2 x^2,\\) \\(\\hat{f}(x) = \\hat{\\beta}_0 + \\hat{\\beta}_1 x + \\hat{\\beta}_2 x^2\\).Fitted line 4: \\(\\hat{y} = \\hat{\\beta}_0 + \\hat{\\beta}_1 x + \\cdots + \\hat{\\beta}_9 x^9,\\) \\(\\hat{f}(x) = \\hat{\\beta}_0 + \\hat{\\beta}_1 x + \\cdots + \\hat{\\beta}_9 x^9\\).true relationship \\(y = f(x) = x^2\\), fitted lines 1 2 complex enough, fitted line 3 optimal, fitted line 4 complex.simulate 1 replicate 1000 observations, fit four models.Figure 10.10, create scatter plot simulated data, overlay lines represent four fitted lines (blue, red, green, orange). also overlay curve represent true relationship variables, black.\nFigure 10.10: Four Polynomial Models Fitted Simulated Dataset\nFigure 10.10, make following comments:Fitted line 1, blue, generally deviates far truth (black).Fitted line 2, red, closer truth, portions match truth.Fitted line 3, green, appears best matches closest truth, generally.Fitted line 4, orange, quite match truth, especially extreme ends x-axis.course, comments based 1 simulated data set, create many replicates data set create similar scatter plot, expect see similar characteristics.illustrate idea behind bias variance predictions, simulate 3 replicates data sets, size 1000, show fitted lines 1 4 replicate Figure 10.11. Let us remind written earlier bias variance tradeoff complexity models:Fitted line 1 simple, expect high degree bias low degree variance.Fitted line 4 complex, expect bias theoretically 0, highest degree variance.\nFigure 10.11: Fitted Lines 1 & 4 Three Different Simulated Datasets\nfitted line 1:see blue line always poor job capturing truth data, therefore, high bias systematically - -predict.However, blue lines pretty similar across three simulated data sets. Therefore, low variance line pretty much .fitted line 4:see orange lines different across three simulated data sets. Therefore, high variance.Fitted line 4 , average, theoretically unbiased. However, unbiased highly variable fitted line can problematic, since observe one data set real life, average predictions many data sets. also want fitted lines low variance.now carry Monte Carlo simulations, 1000 replicates. Ideally, number replicates , simulation takes time run (fitting high degree polynomials computationally expensive). use 1000 replicates compute MSE, bias, variance four fitted lines, evaluated point \\(x=0.1\\). simulation following:Simulate 1 data set 1000 observations.Fitted fitted lines 1 4 data set.Compute predicted value \\(\\hat{y}\\) \\(x=0.9\\) based four fitted lines, record values.Repeat steps 1000 replicates.Let’s take look distributions \\(\\hat{y}\\)s fitted line, shown Figure 10.12.\nFigure 10.12: Box Plots Predictions Four Fitted Lines\nblue horizontal line represents true value \\(y\\) \\(x=0.1\\). Let \\(y_0\\) denote value, .e. \\(y_0 = f(0.1) = 0.1^2\\). Figure 10.12 pretty much matches expected:variance, spread, \\(\\hat{y}\\)s higher model gets complex.Fitted lines 1 2 biased, since middle boxplots far away blue line, fitted lines 3 4 unbiased, since middle boxplots matches blue line.evaluate test MSE, bias, variance \\(\\hat{y}\\) fitted line:MSE: average squared difference \\(\\hat{y}\\) \\(y_0\\).Bias: average difference \\(\\hat{y}\\) \\(y_0\\).Variance: sample variance \\(\\hat{y}\\).Table 10.1: Metrics Four Fitted LinesThe results Table 10.1 pretty much match expectations:variance predictions increase model gets complicated (higher degree).(squared) bias predictions pretty much works opposite direction: decreases model gets complicated. used squared bias since bias positive negative get rid sign way. Also, know MSE equal variance plus squared bias. bias fitted lines 3 4 theoretically 0.test MSE smallest fitted line 3, form \\(\\hat{f}(x)\\) matches true \\(f(x)\\). see optimal model balances bias-variance tradeoff.","code":"\n##function to simulate y = x^2 + eps\nget.sim.data <- function(samp.size, sigma)\n  \n{\n  \n  x<-runif(n=samp.size) ##note that there are no assumptions made about x so can simulate from any distribution\n  eps<-rnorm(n=samp.size, sd=sigma)\n  y<-x^2 + eps ##since f(x) = 2x^2\n  \n  data.frame(x,y) ##return a dataframe with x and y\n}\nset.seed(11)\n\nn <- 1000\n\nData.sim<-get.sim.data(n,1)\n\nfit1 <- lm(y ~ 1,                   data = Data.sim)\nfit2 <- lm(y ~ x,                   data = Data.sim)\nfit3 <- lm(y ~ poly(x, degree = 2), data = Data.sim)\nfit4 <- lm(y ~ poly(x, degree = 9), data = Data.sim)"},{"path":"bayesian-modeling-fundamentals.html","id":"bayesian-modeling-fundamentals","chapter":"11 Bayesian Modeling Fundamentals","heading":"11 Bayesian Modeling Fundamentals","text":"module based Bayes Rules! (Johnson, Ott, Dogucu), Chapters 1 2. can access book free https://www.bayesrulesbook.com/.book extremely readable, written notes module.","code":""},{"path":"bayesian-modeling-fundamentals.html","id":"introduction-7","chapter":"11 Bayesian Modeling Fundamentals","heading":"11.1 Introduction","text":"Chapter 1 provides brief history Bayesian philosophy probability, well big picture overview philosophy used real life.Chapter 2 provides framework associated Bayes modeling. lot ideas covered Module 2, although terminology associated Bayes modeling used instead.","code":""},{"path":"the-beta-binomial-model-and-factors-in-bayesian-modeling.html","id":"the-beta-binomial-model-and-factors-in-bayesian-modeling","chapter":"12 The Beta-Binomial Model, and Factors in Bayesian Modeling","heading":"12 The Beta-Binomial Model, and Factors in Bayesian Modeling","text":"module based Bayes Rules! (Johnson, Ott, Dogucu), Chapters 3 4. can access book free https://www.bayesrulesbook.com/.book extremely readable, written notes module.","code":""},{"path":"the-beta-binomial-model-and-factors-in-bayesian-modeling.html","id":"introduction-8","chapter":"12 The Beta-Binomial Model, and Factors in Bayesian Modeling","heading":"12.1 Introduction","text":"Chapter 3 introduces Beta-Binomial model, used parameter interest support 0 1 (typically probability), data follow binomial distribution. lot ideas continuation ideas Chapter 2, now applied realistic setting.Chapter 4 goes prior data affect posterior model.","code":""}]
