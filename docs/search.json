[{"path":"index.html","id":"preface","chapter":"Preface","heading":"Preface","text":"examples preface based OpenIntro Statistics (Diez, Ceytinka-Rundel, Barr), Chapter 9.4 9.5, provide background information. can access book free https://www.openintro.org/book/os/main goal using data science understand data. Broadly speaking, involve building statistical model predicting, estimating response variable based one predictors. models used wide variety fields finance, medicine, public policy, sports, . look couple examples.","code":""},{"path":"index.html","id":"examples","chapter":"Preface","heading":"0.1 Examples","text":"","code":""},{"path":"index.html","id":"example-1-mario-kart-auction-prices","chapter":"Preface","heading":"0.1.1 Example 1: Mario Kart Auction Prices","text":"first example, look Ebay auctions video game called Mario Kart played Nintendo Wii. want predict price auction based whether game new , whether auction’s main photo stock photo, duration auction days, number Wii wheels included auction.model can use example linear regression model:Generally speaking, linear regression equation takes following form:\\[\n\\hat{y} = \\hat{\\beta}_0 + \\hat{\\beta}_1 x_1 + \\hat{\\beta}_2 x_2 + \\cdots + \\hat{\\beta}_k x_k\n\\]\\(\\hat{y}\\) denotes predicted value response variable, price action example, \\(x_1, x_2, \\cdots, x_k\\) denote values predictors. example, : \\(x_1\\) whether game new , \\(x_2\\) whether auction’s main photo stock photo, \\(x_3\\) duration auction days, \\(x_4\\) number Wii wheels included auction. \\(\\hat{\\beta}_0, \\hat{\\beta}_1, \\cdots, \\hat{\\beta}_k\\) represent estimated regression parameters. know values , can easily plug values predictors obtain predicted price auction.Fitting model R, obtain estimated regression parameters::\\[\n\\hat{y} = 43.5201 - 2.5816 x_1 - 6.7542 x_2 + 0.3788 x_3 + 9.9476 x_4\n\\]auction Mario Kart game used, uses stock photo, listed 2 days, comes 0 wheels, predicted price \\(\\hat{y} = 43.5201 - 2.5816 - 6.7542 + 0.3788 \\times 2 = 34.94\\) 35 dollars.","code":"\nlibrary(openintro)\n\nData<-mariokart\n##fit model\nresult<-lm(total_pr~cond+stock_photo+duration+wheels, data=Data)\n##get estimated regression parameters\nresult## \n## Call:\n## lm(formula = total_pr ~ cond + stock_photo + duration + wheels, \n##     data = Data)\n## \n## Coefficients:\n##    (Intercept)        condused  stock_photoyes        duration          wheels  \n##        43.5201         -2.5816         -6.7542          0.3788          9.9476"},{"path":"index.html","id":"example-2-job-application-callback-rates","chapter":"Preface","heading":"0.1.2 Example 2: Job Application Callback Rates","text":"example, look data experiment sought evaluate effect race gender job application callback rates. experiment, researchers created fake resumes job postings Boston Chicago see resumes resulted callback. fake resumes included relevant information applicant’s educational attainment, many year’s experience applicant well first last name. names fake resume meant imply applicant’s race gender. two races considered (Black White) two genders considered (Make Female) experiment.Prior experiment, researchers conducted surveys check racial gender associations names fake resumes; names passed certain threshold surveys included experiment.model can used example logistic regression modelGenerally speaking, logistic regression equation takes following form\\[\n\\log (\\frac{\\hat{\\pi}}{1-\\hat{\\pi}}) = \\hat{\\beta}_0 + \\hat{\\beta}_1 x_1 + \\hat{\\beta}_2 x_2 + \\cdots + \\hat{\\beta}_k x_k\n\\]\\(\\hat{\\pi}\\) denotes predicted probability applicant receives call back. \\(x_1, x_2, \\cdots, x_k\\) denote values predictors. example, : \\(x_1\\) city job posting located , \\(x_2\\) whether applicant college degree , \\(x_3\\) experience applicant, \\(x_4\\) associated race applicant, \\(x_5\\) associated gender applicant. Similar linear regression, \\(\\hat{\\beta}_0, \\hat{\\beta}_1, \\cdots, \\hat{\\beta}_k\\) represent estimated regression parameters. know values , can easily plug values predictors obtain predicted probability applicant characteristics receive callback.Fitting model R, obtain estimated regression parametersso \\[\n\\log (\\frac{\\hat{\\pi}}{1-\\hat{\\pi}}) = -2.63974 - 0.39206 x_1 - 0.0655 x_2 + 0.03152 x_3 + 0.44299 x_4 - 0.22814 x_5\n\\]applicant Boston, college degree, 10 years experience name associated Black male, logistic regression equation becomes \\(\\log (\\frac{\\hat{\\pi}}{1-\\hat{\\pi}}) = -2.63974 - 0.0655 + 0.03152 \\times 10 - 0.22814 = -2.61818\\). little bit algebra solve, get \\(\\hat{\\pi} = 0.06797751\\). applicant 6.8 percent chance receiving callback.","code":"\nData2<-resume\n##fit model\nresult2<-glm(received_callback~job_city + college_degree+years_experience+race+gender, family=\"binomial\", data=Data2)\n##get estimated regression parameters\nresult2## \n## Call:  glm(formula = received_callback ~ job_city + college_degree + \n##     years_experience + race + gender, family = \"binomial\", data = Data2)\n## \n## Coefficients:\n##      (Intercept)   job_cityChicago    college_degree  years_experience  \n##         -2.63974          -0.39206          -0.06550           0.03152  \n##        racewhite           genderm  \n##          0.44299          -0.22814  \n## \n## Degrees of Freedom: 4869 Total (i.e. Null);  4864 Residual\n## Null Deviance:       2727 \n## Residual Deviance: 2680  AIC: 2692"},{"path":"index.html","id":"how-were-estimated-parameters-calculated","chapter":"Preface","heading":"0.2 How were Estimated Parameters Calculated?","text":"two examples, notice used R functions, supplied names variables, R functions generated values estimated parameters? One thing learn functions actually calculate numbers. turns calculations based foundational concepts associated measures uncertainty, probability, expected values. learning concepts class.want know calculations performed? understand intuition logic behind models built. becomes lot easier work models understand logic (example, know models can used used, know steps take notice data certain characteristics, etc), instead memorizing bunch steps.presenting models data people, people may occasionally questions methods models. trust model? trust numbers seem come black box?Notice used two different models, linear regression logistic regression, examples 1 2. use models? swapped type model used examples? answer actually . One main considerations deciding model use identify response variable quantitative categorical. learn linear regression model works response variable quantitative, logistic regression model works response variable categorical.","code":""},{"path":"index.html","id":"the-course-understanding-uncertainty","chapter":"Preface","heading":"0.3 The Course: Understanding Uncertainty","text":"mentioned previous section, learning foundational concepts associated measures uncertainty, probability, expected values. concepts help explain intuition statistical models built.end course, apply concepts revisit linear regression logistic regression models. two widely used models used data science, relatively easier understand explain. modern methods (learn future classes) decision trees neural networks can viewed extensions linear logistic regression models.","code":""},{"path":"descriptive-statistics.html","id":"descriptive-statistics","chapter":"1 Descriptive Statistics","heading":"1 Descriptive Statistics","text":"module based OpenIntro Statistics (Diez, Ceytinka-Rundel, Barr), Chapter 2.1. can access book free https://www.openintro.org/book/os/ Please note cover additional topics, skip certain topics book.","code":""},{"path":"descriptive-statistics.html","id":"uncertainty-with-data","chapter":"1 Descriptive Statistics","heading":"1.1 Uncertainty with Data","text":"analyzing data, always going degree uncertainty, randomness lot phenomena observe world. event random individual outcomes event unpredictable. example, weight next baby born local hospital. Without knowing information biological parents, high degree uncertainty try predict baby’s weight. Even know detailed information biological parents (example tall), may feel confident predicting baby likely heavier average, certain prediction.end hand, event deterministic can predict individual outcomes event certainty. example, know length cube 2 inches, know sure volume \\(2^3 = 8\\) cubic inches, based rules mathematics. volume cube length 2 inches always going 8 cubic inches, volume deterministic.Thought question: think data see real life. Write . data random deterministic?explore tools help us quantify uncertainty data. module, explore fairly standard tools used describe data give us idea degree uncertainty data. describing data quantitative, usually describe following: shape distribution, average typical value, spread uncertainty.","code":""},{"path":"descriptive-statistics.html","id":"visualizing-data","chapter":"1 Descriptive Statistics","heading":"1.2 Visualizing Data","text":"Data visualization representation information form pictures. Imagine access weights newborn babies local hospital. Examining numerical value time consuming. instead, can use visualizations give us idea values weights. example, weights newborns common? proportion babies dangerously low weights (may indicate health risks)? Good data visualizations can give us information fairly quickly. Next, explore common visualizations used quantitative (numerical) variables.","code":""},{"path":"descriptive-statistics.html","id":"dot-plots","chapter":"1 Descriptive Statistics","heading":"1.2.1 Dot Plots","text":"start dot plot, basic visualization quantitative variable. use loan50 dataset openintro package. data originally consist thousands loans made Lending Club platform, randomly select 50 loans. Let us study interest rate loans 50 applicants received.simplicity, round numerical values interest rates nearest whole number:can create corresponding dot plot, per Figure 1.1:\nFigure 1.1: Dot Plot 50 Interest Rates (rounded)\nNotice 1 black dot corresponds interest rate 20 (presumably percent), one applicant rounded interest rate 20 percent. 8 black dots correspond interest rate 10 percent, 8 applicants rounded interest rate 10 percent. interest rates 10 percent much commonly occurring interest rate 20 percent. can use height, number dots, help us glean often value certain interest rate occurs. Based dotplot, interest rates 5 11 percent common, higher values less common.Note: get torn details code produce dot plot. chosen present dot plot way highlight use , without getting bogged details can produced. using dot plots class.","code":"\nlibrary(tidyverse)\nlibrary(openintro)\n\n##create object for data\nData<-loan50\n##round interest rate to whole number\nData<- Data%>%\n  mutate(r_int_rate = round(interest_rate))\n##dotplot\nggplot(Data,aes(x=r_int_rate))+\n  geom_dotplot(binwidth=1)+\n  theme(\n    axis.text.y = element_blank(),  # Remove y-axis labels\n    axis.title.y = element_blank(), # Remove y-axis title\n    axis.ticks.y = element_blank()  # Remove y-axis ticks\n  )+ \n  labs(x=\"Interest Rates (Rounded)\")"},{"path":"descriptive-statistics.html","id":"histograms","chapter":"1 Descriptive Statistics","heading":"1.2.2 Histograms","text":"turns dot plots often useful large data sets, provide general idea visualizations larger data sets work. height dots inform us frequency values occurring.visualization commonly used larger data sets histogram. Instead displaying common value variable exists, think values belonging bin values. example, can create bin contains interest rates 5 7.5 percent, another bin containing interest rates 7.5 10 percent, . things note histograms:convention, values lie exactly boundary bin belong lower bin. example, interest rate exactly 12.5 percent belong bin 10 12.5 percent, bin 12.5 15 percent.convention, values lie exactly boundary bin belong lower bin. example, interest rate exactly 12.5 percent belong bin 10 12.5 percent, bin 12.5 15 percent.bin width. example, width 2.5.bin width. example, width 2.5.create histogram (using original interest rates) , per Figure 1.2:\nFigure 1.2: Historgram 50 Interest Rates\nSimilar dot plot Figure 1.1, height histogram inform us values commonly occurring. can see histogram interest rates 5 10 percent common, much loans interest rates greater 20 percent. say certainty randomly selected loan applicant interest rate 5 10 percent interest rate greater 20 percent.","code":"\n##set up sequence to specify the bins\ns25<-seq(5,27.5,2.5)\n\nggplot(Data,aes(x=interest_rate))+\n  geom_histogram(breaks=s25,fill=\"blue\",color=\"orange\")+\n  labs(x=\"Interest Rate\", title=\"Histogram of Interest Rates\")"},{"path":"descriptive-statistics.html","id":"shapes-of-distribution","chapter":"1 Descriptive Statistics","heading":"1.2.2.1 Shapes of Distribution","text":"Histograms can also give us idea shape distribution interest rates. histogram Figure 1.2, loans less 15 percent, small number loans greater 20 percent. can say greater certainty loan interest rate less 15 percent. data tail right histogram, shape said right-skewed. variable said right-skewed, large values variable much less common small values variable; smaller values likely occur.histogram reverse characteristic, .e. data tail left instead, shape said left-skewed. implies small values variable much less common large values variable; larger values likely occur.histogram reverse characteristic, .e. data tail left instead, shape said left-skewed. implies small values variable much less common large values variable; larger values likely occur.Histograms trail similarly directions called symmetric. Large small values variable equally likely.Histograms trail similarly directions called symmetric. Large small values variable equally likely.Histograms peak middle, trail sides symmetic, also bell-shaped, normal distribution. Note: turns one assumptions linear regression response variable follow normal distribution. may seem restrictive, however, see later modules assumption particular crucial circumstances.Histograms peak middle, trail sides symmetic, also bell-shaped, normal distribution. Note: turns one assumptions linear regression response variable follow normal distribution. may seem restrictive, however, see later modules assumption particular crucial circumstances.Thought question: Can think real life variables symmetric, right-skewed, left-skewed distributions? Feel free search internet examples.","code":""},{"path":"descriptive-statistics.html","id":"considerations-with-histograms","chapter":"1 Descriptive Statistics","heading":"1.2.2.2 Considerations with Histograms","text":"interest rate example, may noticed made specific choice width bins created histograms. turns width bins can impact shape histogram, potentially, interpret histogram.Consider creating histogram bin width 0.5, instead 2.5, per Figure 1.3:\nFigure 1.3: Historgram 50 Interest Rates, Bin Width 0.5\nComparing Figure 1.3 Figure 1.2, note following:Visually, histogram looks jagged smaller bin width, whereas histogram looks smoother larger bin width.Visually, histogram looks jagged smaller bin width, whereas histogram looks smoother larger bin width.Smaller bin widths may preferred need information smaller ranges interest rates. However, can difficult write general trends.Smaller bin widths may preferred need information smaller ranges interest rates. However, can difficult write general trends.Larger bin widths may useful trying look general trends interest rates.Larger bin widths may useful trying look general trends interest rates.Thought question: happens create histogram bin width large?","code":"\n##set up sequence to specify the bins. width now 0.5\ns05<-seq(5,27.5,0.5)\n\nggplot(Data,aes(x=interest_rate))+\n  geom_histogram(breaks=s05,fill=\"blue\",color=\"orange\")+\n  labs(x=\"Interest Rate\", title=\"Histogram of Interest Rates\")"},{"path":"descriptive-statistics.html","id":"density-plots","chapter":"1 Descriptive Statistics","heading":"1.2.3 Density Plots","text":"Another visualization quantitative variable density plot. density plot can viewed smoothed version histogram. can use heights inform us values common. create density plot interest rates Figure 1.4:\nFigure 1.4: Density Plot 50 Interest Rates\nBased Figure 1.4, see low interest rates (5 12.5 percent) much common high interest rates (higher 20 percent). things note interpreting density plots:area density plot always equals 1.find proportion interest rates two values, example 10 15 percent, integrate density plot range, .e. \\(\\int_{10}^{15} f(x) dx\\), \\(f(x)\\) mathematical equation describes density plot.values vertical axis equal probabilities (common misconception).density plot found using method called kernel density estimation (KDE). details KDE later module need cover quite bit material .","code":"\n##density plot\nplot(density(Data$interest_rate), main=\"Density Plot of Interest Rates\")"},{"path":"descriptive-statistics.html","id":"considerations-with-density-plots","chapter":"1 Descriptive Statistics","heading":"1.2.3.1 Considerations with Density Plots","text":"Similar bins histograms, density plots affected bandwidth. Larger bandwidths lead smoother density plots, smaller bandwidths lead jagged density plots. create density plot uses bandwidth twice default Figure 1.5 :\nFigure 1.5: Density Plot 50 Interest Rates Larger Bandwidth\nNotice Figure 1.5 little peak interest rates 15 20 (existed Figures 1.4 also 1.2) longer exists. Using bandwidths large can smooth peaks.Thought question: create density plot bandwidth small?Thought question: bin widths histograms bandwidths density plots related?","code":"\nplot(density(Data$interest_rate, adjust=2), main=\"Density Plot of Interest Rates, with Bandwidth Twice the Default\")"},{"path":"descriptive-statistics.html","id":"ordered-statistics","chapter":"1 Descriptive Statistics","heading":"1.3 Ordered Statistics","text":"idea behind ordered statistics pretty self-explanatory: take numerical variable, order values smallest largest. Going back example interest rates 50 loan applicants, let \\(X\\) denote interest rate. \\(x_{(1)}\\) denote interest rate smallest, \\(x_{(2)}\\) denotes second smallest interest rate, \\(x_{(50)}\\) denotes largest interest rate sample 50.","code":""},{"path":"descriptive-statistics.html","id":"quantiles","chapter":"1 Descriptive Statistics","heading":"1.3.1 Quantiles","text":"Quantiles partition range numerical data continuous intervals (groups) (nearly) equal proportions. Common quartiles names:Quartiles: 4 groupsPercentiles: 100 groupsThere one less quantiles number groups. go quartiles detail.","code":""},{"path":"descriptive-statistics.html","id":"quart","chapter":"1 Descriptive Statistics","heading":"1.3.1.1 Quartiles","text":"Quartiles divide data 4 groups, group (nearly) equal number observations. three quartiles, denoted \\(Q_1, Q_2, Q_3\\).first group values negative infinity \\(Q_1\\).second group values negative \\(Q_1\\) \\(Q_2\\).third group values negative \\(Q_2\\) \\(Q_3\\).fourth group values negative \\(Q_3\\) infinity.\\(Q_2\\), sometimes called second quartile, easiest value find. also called median data. Going back interest rates 50 loan applicants. Using ordered statistics, median middle observation. Since even number observations, two middle observations, \\(x_{(25)}\\) \\(x_{(26)}\\). situation, median average two middle observations. Using R, find median :half interest rates less 9.93 percent, half interest rates greater 9.93 percent. might also recognize another term median: 50th percentile, 50 percent interest rates less 9.93.find middle observation(s) based sample size \\(n\\):\\(n\\) even, 2 middle observations position \\(\\frac{n}{2}\\) \\(\\frac{n}{2} + 1\\) ordered statistics.\\(n\\) odd, middle observation position \\(\\frac{n}{2} + 0.5\\) ordered statistics.\\(Q_1\\) \\(Q_3\\) (also called first third quartiles) found together, finding \\(Q_2\\). Note \\(Q_2\\) divides data two groups. Using interest rates example, one group contains \\(x_{(1)}, \\cdots, x_{(25)}\\), another group contains \\(x_{(26)}, \\cdots, x_{(50)}\\). \\(Q_1\\) median first group, \\(Q_3\\) median second group. 50 loan applicants:\\(Q_1\\) \\(x_{(13)}\\), \\(Q_3\\) \\(x_{(38)}\\).find values R, type:\\(Q_1\\) 7.96 percent, \\(Q_3\\) 14.08 percent. turns \\(Q_1\\) also 25th percentile, \\(Q_3\\) also 75th percentile, definition.Remember wrote following earlier:first group values negative infinity \\(Q_1\\). quarter observations interest rates less 7.96 percent.second group values negative \\(Q_1\\) \\(Q_2\\). quarter observations interest rates 7.96 9.93 percent.third group values negative \\(Q_2\\) \\(Q_3\\). quarter observations interest rates 9.93 14.08 percent.fourth group values negative \\(Q_3\\) infinity. quarter observations interest rates 14.08 percent.Note: may notice used type = 1 inside quantile() function. Using type = 1 gives values first third quartiles based method just described. actually several ways find quantiles, may result slightly differing values, although generally meet definition \\(Q_1\\) 25th percentile, \\(Q_3\\) 75th percentile.","code":"\nmedian(Data$interest_rate)## [1] 9.93\nquantile(Data$interest_rate, prob=c(0.25,0.75), type = 1)##   25%   75% \n##  7.96 14.08"},{"path":"descriptive-statistics.html","id":"percentiles","chapter":"1 Descriptive Statistics","heading":"1.3.1.2 Percentiles","text":"Another common quantile percentile. general k-th percentile value data point \\(k\\) percent observations found. earlier example, said \\(Q_3\\) interest rates 14.08 percent, also 75th percentile. 75 percent interest rates less 14.08 percent.go details finding percentiles hand.","code":""},{"path":"descriptive-statistics.html","id":"box-plots","chapter":"1 Descriptive Statistics","heading":"1.3.2 Box Plots","text":"Another visualization used summarize quantitative data box plot. box plot summarizes 5-number summary. 5 numbers minimum, \\(Q_1, Q_2, Q_3\\), maximum. Using interest rate data, box plot shown Figure 1.6:\nFigure 1.6: Box Plot Interest Rates\npeople call box plot box whisker plot.boundaries box represent \\(Q_1\\) \\(Q_3\\).thick line box represents median.two whiskers either side box extend minimum maximum, outliers exist. outliers exist, whiskers extend minimum maximum values outliers.Generally, one quantitative variable, outlier observation whose numerical value far away rest data. words, lot smaller larger relative rest data.50 loans, two loan applicants interest rates around 25 percent flagged lot larger rest loans, reasonable since loans lot smaller 20 percent.go details outliers determined box plots. interested, can read Chapter 2.1.5 OpenIntro Statistics (Diez, Ceytinka-Rundel, Barr).Notice much large values (\\(Q_3\\) maximum) median, compared distance small values (\\(Q_1\\) minimum) median. indicates distribution interest rates right-skewed. Compare boxplot interest rates Figure 1.6 corresponding histogram (Figure 1.2) density plot (Figure 1.4).Thought question: can sketch box plot represents variable left-skewed? variable symmetric?","code":"\n##box plot\nggplot(Data,aes(y=interest_rate))+\n  geom_boxplot()+\n  labs(y=\"Interest Rate\", title=\"Box Plot of Interest Rates\")"},{"path":"descriptive-statistics.html","id":"empirical-cumulative-distribution-function","chapter":"1 Descriptive Statistics","heading":"1.3.3 Empirical Cumulative Distribution Function","text":"previous sections, can see use histograms, density plots, box plots inform us proportion observations take certain values, values data correspond certain percentiles. However, limited quartiles percentile using box plots, need find areas density plot (using integration, trivial task), add frequencies histogram (can time consuming).plot can easily give us values variable correspond percentiles empirical cumulative distribution function (ECDF) plot.Let \\(X\\) denote random variable, observed \\(n\\) observations \\(X\\) denoted \\(x_1, \\cdots, x_n\\). Let \\(x_{(1)}, \\cdots x_{(n)}\\) denote ordered statstics \\(n\\) observations. ECDF, denoted \\(\\hat{F}_n(x)\\) proportion sample observations less equal value \\(x\\) random variable. Mathematically, ECDF :\\[\n\\hat{F}_n(x) =\n  \\begin{cases}\n   0, & \\text{} x < x_{(1)} \\\\\n   \\frac{k}{n},       & \\text{} x_{(k)} \\leq x < x_{(k+1)}, k = 1, \\cdots, n-1\\\\\n   1, & \\text{} x \\geq x_{(n)}.\n  \\end{cases}\n\\]\nshall use simple toy example illustrate ECDF constructed. Suppose ask 5 people many times go gym (least 20 minutes) typical work week. answers : 3, 0, 1, 5, 3. random variable \\(X\\) many times person goes gym least 20 minutes, ordered statistics \\(x_{(1)} = 0, x_{(2)} = 1, x_{(3)} = 3, x_{(4)} = 3, x_{(5)} = 5\\). Using mathematical definition ECDF, :\\(\\hat{F}_n(x) = 0\\) \\(x < x_{(1)} = 0\\).\\(\\hat{F}_n(x) = \\frac{1}{5}\\) \\(0 \\leq x < x_{(2)} = 1\\).\\(\\hat{F}_n(x) = \\frac{2}{5}\\) \\(1 \\leq x < x_{(3)} = 3\\).\\(\\hat{F}_n(x) = \\frac{4}{5}\\) \\(3 \\leq x < x_{(5)} = 5\\). value special example since two observations \\(x=3\\).\\(\\hat{F}_n(x) = 1\\) \\(x \\geq 5\\).corresponding ECDF plot shown Figure 1.7:\nFigure 1.7: ECDF Plot Toy Example\ncan easily find percentiles plot, example, 40th percentile equal 1, going gym week. 20 percent observations go gym less 1 time week.Next, create ECDF plot interest rates 50 loan applicants.\nFigure 1.8: ECDF Plot Interest Rates\noverlaid horizontal line 80th percentile, can read horizontal axis corresponds interest rate 17 percent. 80 percent loan applicants interest rate less 17 percent.Thought question: try using histogram density plot interest rates (Figures 1.2 1.4) find interest rate corresponds 80th percentile. easy perform?","code":"\n##toy data\ny<-c(3, 0, 1, 5, 3)\n##ECDF plot\nplot(ecdf(y), main = \"ECDF for Toy Example\")\nplot(ecdf(Data$interest_rate), main = \"ECDF Plot of Interest Rates\")\nabline(h=0.8)"},{"path":"descriptive-statistics.html","id":"measures-of-centrality","chapter":"1 Descriptive Statistics","heading":"1.4 Measures of Centrality","text":"far, used visualizations summarize shape distribution quantitative variable. Next, look common measures centrality. Loosely speaking, measures centrality measures describe average typical value quantitative variable. common measures centrality mean, median, mode.","code":""},{"path":"descriptive-statistics.html","id":"mean","chapter":"1 Descriptive Statistics","heading":"1.4.1 Mean","text":"sample mean simply average value variable sample. sample mean random variable \\(X\\) denoted \\(\\bar{x}\\), found :\\[\\begin{equation}\n\\bar{x} = \\frac{\\sum_{=1}^n x_i}{n}.\n\\tag{1.1}\n\\end{equation}\\], toy example 5 people often go gym week, sample mean \\(\\bar{x} = \\frac{3+0+1+5+3}{5} = 2.4\\).","code":""},{"path":"descriptive-statistics.html","id":"median","chapter":"1 Descriptive Statistics","heading":"1.4.2 Median","text":"went find median section 1.3.1.1. median value middle observation ordered statistics. also called \\(Q_2\\), second quartile, 50th percentile, approximately 50 percent observations values smaller median., toy example 5 people often go gym week, sample median \\(x_{(3)} = 3\\). 50 percent people went gym less 3 times week.","code":""},{"path":"descriptive-statistics.html","id":"mode","chapter":"1 Descriptive Statistics","heading":"1.4.3 Mode","text":"Another measure mode. Mathematically speaking, mode commonly occurring value data. toy example, mode 3, since 3 occurs twice occurs often data.","code":""},{"path":"descriptive-statistics.html","id":"considerations","chapter":"1 Descriptive Statistics","heading":"1.4.4 Considerations","text":"things consider using measures centrality:mean measure people comfortable , however, caution needs used variable skewed, extreme outliers drastically alter value mean. Using toy example gym, suppose person visits gym visits 50 times, instead 5. numerical value sample mean explodes, give good representation central value many visits gym person makes week. mean fine variable symmetric.mean measure people comfortable , however, caution needs used variable skewed, extreme outliers drastically alter value mean. Using toy example gym, suppose person visits gym visits 50 times, instead 5. numerical value sample mean explodes, give good representation central value many visits gym person makes week. mean fine variable symmetric.median measure recommended skewed distributions, since order associated ordered statistics influenced extreme outliers. Using gym example, previous bullet point, median unaffected.median measure recommended skewed distributions, since order associated ordered statistics influenced extreme outliers. Using gym example, previous bullet point, median unaffected.mean larger median indication distribution right-skewed. Using interest rate example, :mean larger median indication distribution right-skewed. Using interest rate example, :consistent right skew saw histogram density plot Figures 1.2 1.4. Conversely, left-skewed distribution usually mean smaller median. symmetric distribution typically similar values mean median.mean considered sensitive measure, since numerical value can drastically affected outliers. median considered robust measure, since numerical value resistant less affected outliers.mean considered sensitive measure, since numerical value can drastically affected outliers. median considered robust measure, since numerical value resistant less affected outliers.mathematical definition mode can difficult use variables continuous, since likely observations value variable continuous. instance, mode typically refers bin histogram tallest. , using histogram Figure 1.2 interest rates, mode 7.5 10 percent.mathematical definition mode can difficult use variables continuous, since likely observations value variable continuous. instance, mode typically refers bin histogram tallest. , using histogram Figure 1.2 interest rates, mode 7.5 10 percent.","code":"\nmean(Data$interest_rate)## [1] 11.5672\nmedian(Data$interest_rate)## [1] 9.93"},{"path":"descriptive-statistics.html","id":"measures-of-uncertainty","chapter":"1 Descriptive Statistics","heading":"1.5 Measures of Uncertainty","text":"previous sections, learned summarizing features quantitative variable, using visualizations summarize shape, using measures centrality describe average typical values variable. One feature can summarize spread, associated values quantitative variable. Measures spread considered way measure uncertainty. Data larger spread uncertainty.","code":""},{"path":"descriptive-statistics.html","id":"variance-and-standard-deviation","chapter":"1 Descriptive Statistics","heading":"1.5.1 Variance and Standard Deviation","text":"One measure spread variance. sample variance random variable \\(X\\) denoted \\(s^2\\), sometimes \\(s_x^2\\), found :\\[\\begin{equation}\ns^2 = \\frac{\\sum_{=1}^n (x_i-\\bar{x})^2}{n-1}.\n\\tag{1.2}\n\\end{equation}\\]variance can interpreted approximate average squared distance observations mean. formula equation (1.2) may look bit complicated, let us use toy example asked 5 people often go gym workweek. answers : 3, 0, 1, 5, 3, earlier found sample mean \\(\\bar{x} = 2.4\\). calculate sample variance:\\[\n\\begin{split}\ns^2 &= \\frac{\\sum_{=1}^n (x_i-\\bar{x})^2}{n-1}\\\\\n&= \\frac{(3-2.4)^2 + (0-2.4)^2 + (1-2.4)^2 + (5-2.4)^2 + (3-2.4)^2}{5-1} \\\\\n&= 3.8\n\\end{split}\n\\]Notice numerator equation (1.2): take difference observed value sample mean, square differences, add squared differences. divide \\(n-1\\), rather \\(n\\), hence sample variance approximate averaged squared distance observations mean. nuance mathematics divide \\(n-1\\) instead \\(n\\), may intuitive . turns dividing \\(n-1\\) makes sample variance unbiased estimator true variance population (denoted \\(\\sigma^2\\)) reliable divided \\(n\\). go detail later module covering additional concepts.Larger values sample variance indicate observations generally away sample mean, indicating larger spread, higher degree uncertainty future values.Thought question: mean sample variance set observations 0? indicate little () uncertainty set observations?Another related measure sample standard deviation, square root sample variance. Similar variance, larger values indicated spread data.","code":""},{"path":"descriptive-statistics.html","id":"interquartile-range","chapter":"1 Descriptive Statistics","heading":"1.5.2 Interquartile Range","text":"Another measure spread interquartile range (IQR), difference third first queartiles,\\[\\begin{equation}\nIQR = Q_3 - Q_1.\n\\tag{1.3}\n\\end{equation}\\]IQR considered robust measure spread, sample variance standard deviations considered sensitive.","code":""},{"path":"probability.html","id":"probability","chapter":"2 Probability","heading":"2 Probability","text":"module based Introduction Probability (Blitzstein, Hwang), Chapters 1 2. can access book free https://stat110.hsites.harvard.edu/ (click Book). Please note cover additional topics, skip certain topics book. may skip Sections 1.4 1.5, examples 2.4.5 2.5.12 book.","code":""},{"path":"probability.html","id":"probability-1","chapter":"2 Probability","heading":"2.1 Probability","text":"way quantifying uncertainty probability. Think statements: “100% certain rain next hour” “50% certain rain next hour”. percentages used reflect degree certainty event happening. first statement reflects certainty; second reflects uncertainty statement implies belief equally likely rain . module, learn basic concepts probability.","code":""},{"path":"probability.html","id":"why-study-probability","chapter":"2 Probability","heading":"2.1.1 Why Study Probability?","text":"book (Section 1.1) lists 10 different applications probability, many applications. go far say anything deals data also deal probability.","code":""},{"path":"probability.html","id":"frequentiest-and-bayesian-view-of-probability","chapter":"2 Probability","heading":"2.1.2 Frequentiest and Bayesian View of Probability","text":"couple main viewpoints interpret probability: frequentist Bayesian. Consider statement “flip fair coin, coin 50% chance landing heads”.frequentist viewpoint views probability relative frequency associated event repeated infinite number times. interpret 50% probability : flip coin many many times, 50% times result coin landing heads.frequentist viewpoint views probability relative frequency associated event repeated infinite number times. interpret 50% probability : flip coin many many times, 50% times result coin landing heads.Bayesian viewpoint views probability measure belief, certainty, event happen. interpret 50% probability : heads tails equally likely occur coin flip.Bayesian viewpoint views probability measure belief, certainty, event happen. interpret 50% probability : heads tails equally likely occur coin flip.coin flip example, interpretations reasonable. However, instances, frequentist interpretation may interpretable repeat event many times. example, earlier statement rain: “50% certain rain next hour”. Whether rain next hour repeatable event, frequentist interpretation makes less sense .","code":""},{"path":"probability.html","id":"key-concepts-in-probability","chapter":"2 Probability","heading":"2.2 Key Concepts in Probability","text":"section, cover basic terminology foundational ideas probability.","code":""},{"path":"probability.html","id":"sample-space","chapter":"2 Probability","heading":"2.2.1 Sample Space","text":"sample space experiment, denoted \\(S\\), set possible outcomes experiment.rest module, use following example: consider standard deck 52 cards, draw one card random. card drawn? sample space experiment can viewed list 52 cards, per Figure 2.1 .\nFigure 2.1: Sample Space Drawing One Card Standard Deck. Picture https://en.wikipedia.org/wiki/Standard_52-card_deck\ndefinition sample space may appear elementary, writing sample space almost always first step performing probability calculations.","code":""},{"path":"probability.html","id":"event","chapter":"2 Probability","heading":"2.2.2 Event","text":"event subset sample space, usually denoted upper case letter. example, let \\(\\) denote event draw card black suit (spades clubs), let \\(B\\) denote event draw picture card (Jack, Queen, King). Events \\(\\) \\(B\\) shown Figures Figure 2.2 Figure 2.3 .\nFigure 2.2: Event \\(\\) (Blue)\n\nFigure 2.3: Event \\(B\\) (gold)\nsample space experiment can finite infinite. card example, sample space finite since can actually write possible outcomes. number possible outcomes infinite (.e. write entire list possible outcomes), sample space infinite.assign probability event. probability event \\(\\) happening \\(P()\\). outcome sample space equally likely finite sample space, probability event number outcomes belonging event divided number outcomes sample space. Using card example, \\(P() = \\frac{26}{52} = \\frac{1}{2}\\) \\(P(B) = \\frac{12}{52} = \\frac{3}{13}\\).","code":""},{"path":"probability.html","id":"complements","chapter":"2 Probability","heading":"2.2.3 Complements","text":"complement event set outcomes belong event. example, complement \\(\\), denoted \\(^c\\), drawing card red suit (hearts diamonds). One way think complements complement event event happening. Loking Figure 2.2, cards outlined blue. example, \\(P(^c) = \\frac{26}{52} = \\frac{1}{2}\\).Thought question: probability drawing non picture card?examples, might realize probability associated complement event can found subtracting probability event 1, .e.\\[\\begin{equation}\nP(^c) = 1 - P().\n\\tag{2.1}\n\\end{equation}\\]Sometimes, calculation probability complement event much less tedious probability event. instance, equation (2.1) useful.","code":""},{"path":"probability.html","id":"unions","chapter":"2 Probability","heading":"2.2.4 Unions","text":"union events least one events happen. example, union events \\(\\) \\(B\\), denoted \\(\\cup B\\), event card drawn either black suit, picture card, black suit picture card. reflected Figure 2.4.\nFigure 2.4: Union , B (blue gold, blue gold)\nfind \\(P(\\cup B)\\), can refer Figure 2.4 just count number outcomes belong either event \\(\\) (black suit) event \\(B\\) (picture card), find \\(\\frac{32}{52}\\).union \\(\\) \\(B\\) can viewed event either event \\(\\) \\(B\\) () happens.","code":""},{"path":"probability.html","id":"intersections","chapter":"2 Probability","heading":"2.2.5 Intersections","text":"intersection events events happen. Using example, intersection events \\(\\) \\(B\\) denoted \\(\\cap B\\), event card drawn black suit picture card. Using Figure 2.4, outcomes belonging \\(\\cap B\\) cards outlined blue gold. probability \\(P(\\cap B) = \\frac{6}{32}\\).","code":""},{"path":"probability.html","id":"addition-rule","chapter":"2 Probability","heading":"2.2.6 Addition rule","text":"common mistake can made calculating \\(P(\\cup B)\\) just add probabilities individual event, mistake say probability \\(\\frac{26}{52} + \\frac{12}{52} = \\frac{38}{52}\\). problem approach outcomes belong events (black picture cards) get counted twice, want count . leads following formula calculating probabilities involving unions two events, sometimes called addition rule probability:\\[\\begin{equation}\nP(\\cup B) = P() + P(B) - P(\\cap B).\n\\tag{2.2}\n\\end{equation}\\]Using equation (2.2), \\(P(\\cup B) = \\frac{26}{52} + \\frac{12}{52} - \\frac{6}{32} = \\frac{32}{52}\\).","code":""},{"path":"probability.html","id":"disjoint-or-mutually-exclusive-events","chapter":"2 Probability","heading":"2.2.7 Disjoint or Mutually Exclusive Events","text":"previous discussion leads idea disjoint, mutually exclusive events. Events disjoint happen simultaneously. card example, events \\(\\) \\(B\\) disjoint, since \\(\\) \\(B\\) can happen simultaneously, since card drawn can black picture card, e.g. draw king spades.Using Figure 2.4 visual example, can see events \\(\\) \\(B\\) disjoint since outcomes blue overlap outcomes gold.Suppose define another event, \\(C\\), denote card drawn Ace. events \\(B\\) \\(C\\) disjoint since card drawn picture card ace. definition disjoint events leads following: events disjoint, probability intersection 0.Using Figure 2.5 visual example, can see events \\(B\\) \\(C\\) disjoint since outcomes gold pink overlap.\nFigure 2.5: Events B, C (gold pink respectively)\nApplying idea equation (2.2), following disjoint events: disjoint events, probability least one event happening sum probabilities event.","code":""},{"path":"probability.html","id":"axioms-of-probability","chapter":"2 Probability","heading":"2.2.8 Axioms of Probability","text":"following called axioms probability, considered foundation properties associated probability:probability event, \\(E\\), non negative, .e. \\(P(E) \\geq 0\\).probability least one outcome sample space occurs 1, .e.\\(P(S) = 1\\).\\(A_1, A_2, \\cdots\\) disjoint events, \\[\nP(\\bigcup\\limits_{=1}^{\\infty} A_{}) = \\sum_{=1}^{\\infty} P(A_i).\n\\]\nwords, disjoint events, probability least one event happens sum individual probabilities.Note: writers list three axioms. book combines first two axioms 1, write two axioms.can easily see equations (2.1) (2.2) can derived axioms. Note equations axioms apply circumstances, regardless whether sample space finite .","code":""},{"path":"probability.html","id":"conditional-probability","chapter":"2 Probability","heading":"2.3 Conditional Probability","text":"concept conditional probability appears almost statistical data science models. statistical models logistic regression, trying use observable data (called predictors, input variables, etc) model probabilities associated different values outcome random (called response variable, output variable, etc). observable data predictive outcome, probabilities associated outcome indicate greater certainty, observable data. Conditional probabilities allows us incorporate observable data, evidence, evaluating uncertainty random outcomes.Consider headed lunch, need decide want bring umbrella (assuming bring umbrella think going rain). working windowless basement internet, high degree uncertainty evaluating rain . However, look outside observe current weather conditions heading , likely higher degree certainty evaluating rain . Conditional probabilities allow us incorporate see prediction random event.use language probability denote example, let \\(R\\) denote event rain go lunch. working windowless basement internet, calculating \\(P(R)\\), probability rain go lunch. able incorporate current weather conditions, probability denoted \\(P(R|data)\\), data denotes current observe weather conditions. \\(P(R|data)\\) can read probability rain go lunch, given observed weather. example, can see \\(P(R)\\) \\(P(R|data)\\) different, since update probability given useful information. Notice \\(|\\) symbol inside probability. symbol implies working conditional probability, given observed information listed \\(|\\).","code":""},{"path":"probability.html","id":"def","chapter":"2 Probability","heading":"2.3.1 Definition","text":"\\(X\\) \\(Y\\) events, \\(P(X)>0\\), conditional probability \\(Y\\) given \\(X\\), denoted \\(P(Y|X)\\), \\[\\begin{equation}\nP(Y|X) = \\frac{P(Y \\cap X)}{P(X)}.\n\\tag{2.3}\n\\end{equation}\\]definition, want update probability \\(Y\\) happening, given observed \\(X\\). \\(X\\) can viewed observable data evidence want incorporate.Bayesian viewpoint probability, \\(P(Y)\\) called prior probability \\(Y\\) since reflects belief \\(Y\\) observing data. \\(P(Y|X)\\) called posterior probability \\(Y\\), reflects update belief $Y incorporating observed data.Let us go back standard deck cards example. Let us find \\(P(B|)\\), probability card picture card, given know card black suit. Visually, can use definition conditional probability using Figure 2.6 .\nFigure 2.6: Events , given B\ntold card black suit, 26 possible outcomes consider, red cards eliminated crossed Figure 2.6. 26 outcomes, many picture cards? probability \\(P(B|)\\) \\(\\frac{6}{26}\\).Figure 2.6 represents frequentist viewpoint conditional probability: \\(P(B|)\\) represents long run proportion picture cards among cards black suits.can also apply equation (2.3): \\(P(B|) = \\frac{\\frac{6}{52}}{\\frac{1}{2}} = \\frac{6}{26}\\) gives answer.Thought question: work probability card drawn black suit, given know card picture card.can see example general \\(P(Y|X) \\neq P(X|Y)\\). informs us need extremely careful writing conditional probabilities interpreting , knowing one matters analysis. example, probability feel unwell given flu close 1, probability flu given feel unwell close 1 (since many things can feel unwell). confusion regarding conditional probabilities sometimes called confusion inverse prosecutor’s fallacy. fallacy wrongly assumes probability fingerprint match given person innocent small, means probability person innocent given fingerprint match must also small. going fallacy detail, need cover concepts.","code":""},{"path":"probability.html","id":"multiplication-rule","chapter":"2 Probability","heading":"2.3.2 Multiplication Rule","text":"equation (2.3), multiplication rule probability\\[\\begin{equation}\nP(Y \\cap X) = P(Y|X) \\times P(X) = P(X|Y) \\times P(Y).\n\\tag{2.4}\n\\end{equation}\\]multiplication rule useful finding probability multiple events happening, aespecially events happen sequentially. example, consider drawing two cards, without replacement, standard deck cards. Without replacement means drawing first card, returned deck, 51 cards remaining first draw. Let \\(D_1\\) \\(D_2\\) denote events first draw diamond suit second draw diamond suit respectively. want find probability cards drawn diamond suits. probability can written \\(P(D_1 \\cap D_2) = P(D_1) \\times P(D_2|D_1)  = \\frac{13}{52} \\times \\frac{12}{51}  = \\frac{156}{2652}\\).","code":""},{"path":"probability.html","id":"independent-events","chapter":"2 Probability","heading":"2.3.3 Independent Events","text":"Events independent knowledge whether one event happens change probability event happening. implies \\(X\\) \\(Y\\) independent events, definition conditional probability simplifies \\(P(Y|X) = P(Y)\\). Likewise \\(P(X|Y) = P(X)\\). Applying multiplication rule, following multiplication rule independent events\\[\\begin{equation}\nP(Y \\cap X) = P(Y) \\times P(X).\n\\tag{2.5}\n\\end{equation}\\]probability events happening just product probabilities individual event, events independent.Going back example standard deck cards, \\(\\) denotes event draw card black suit (spades clubs), \\(B\\) denotes event draw picture card (Jack, Queen, King). earlier found \\(P(B) = \\frac{12}{52}\\) \\(P(B|) = \\frac{6}{26}\\). Notice two probabilities numerically equal, informs us events independent. Knowing whether card black suit change probability card picture card. makes sense intuitively since proportion cars picture black red suits.","code":""},{"path":"probability.html","id":"bayes-rule","chapter":"2 Probability","heading":"2.3.4 Bayes’ Rule","text":"definition conditional probability equation (2.3) multiplication rule equation (2.4) give us Bayes’ rule\\[\\begin{equation}\nP(Y|X) = \\frac{P(X|Y)P(Y)}{P(X)}.\n\\tag{2.6}\n\\end{equation}\\]Bayes’ rule useful want find \\(P(Y|X)\\) information regarding \\(P(X|Y)\\) available. fairly popular model called linear discriminant analysis, models conditional probability using Bayes’ rule.","code":""},{"path":"probability.html","id":"odds","chapter":"2 Probability","heading":"2.3.5 Odds","text":"odds event \\(Y\\) \\[\\begin{equation}\nodds(Y) = \\frac{P(Y)}{P(Y^c)}.\n\\tag{2.7}\n\\end{equation}\\]may realize left hand side equation @ref{eq:odds} equal left hand side logistic regression equation.Using equation (2.7), can switch odds probability easily\\[\\begin{equation}\nP(Y) = \\frac{odds(Y)}{1 + odds(Y)}.\n\\tag{2.8}\n\\end{equation}\\]","code":""},{"path":"probability.html","id":"odds-form-of-bayes-rule","chapter":"2 Probability","heading":"2.3.6 Odds Form of Bayes’ Rule","text":"Using Bayes’ rule equation (2.6) definition odds equation (2.7), odds form Bayes’ rule\\[\\begin{equation}\n\\frac{P(Y|X)}{P(Y^c|X)} = \\frac{P(X|Y)}{P(X|Y^c)} \\frac{P(Y)}{P(Y^c)}.\n\\tag{2.9}\n\\end{equation}\\]","code":""},{"path":"probability.html","id":"law-of-total-probability","chapter":"2 Probability","heading":"2.3.7 Law of Total Probability","text":"Let \\(Y_1, Y_2, \\cdots, Y_n\\) partition sample space (\\(Y_1, Y_2, \\cdots, Y_n\\) disjoint union sample space), \\(P(Y_i) > 0)\\) \\(\\). \\[\\begin{equation}\n\\begin{split}\nP(X) &= \\sum_{=1}^n P(X|Y_i) \\times P(Y_i)\\\\\n    &= P(X|Y_1) \\times P(Y_1) + P(X|Y_2) \\times P(Y_2) + \\cdots + P(X|Y_n) \\times P(Y_n).\n\\end{split}\n\\tag{2.10}\n\\end{equation}\\]law total probability informs us way find probability \\(X\\). can divide sample space disjoint sets \\(Y_i\\), find conditional probability \\(X\\) within set, take weighted sum conditional probabilities, weighted \\(P(Y_i)\\). useful conditional probability set easy obtain.law total probability equation (2.10) can applied denominator Bayes’ rule equation (2.6) following variation Bayes’ rule:\\[\\begin{equation}\nP(Y|X) = \\frac{P(X|Y)P(Y)}{\\sum_{=1}^n P(X|Y_i) \\times P(Y_i)}.\n\\tag{2.11}\n\\end{equation}\\]","code":""},{"path":"probability.html","id":"worked-example","chapter":"2 Probability","heading":"2.3.8 Worked Example","text":"","code":""},{"path":"probability.html","id":"approach-1-using-bayes-rule","chapter":"2 Probability","heading":"2.3.8.1 Approach 1: Using Bayes’ Rule","text":"consider worked example apply Bayes’ rule law total probability. Suppose email can divided three categories: \\(E_1\\) denotes spam email, \\(E_2\\) denotes important email, \\(E_3\\) denotes important email. email must belong one categories. Let \\(F\\) denote event email contains word “free”. past data, following information:\\(P(E_1) = 0.2, P(E_2) = 0.5, P(E_3) = 0.3\\).word “free” appears 99% spam email, \\(P(F|E_1) = 0.99\\).word “free” appears 10% important email, \\(P(F|E_2) = 0.1\\).word “free” appears 5% important email, \\(P(F|E_3) = 0.05\\).receive email word free. probability spam? want find \\(P(E_1|F)\\). Using equation (2.11), \\[\n\\begin{split}\nP(E_1|F) &= \\frac{P(E_1 \\cap F)}{P(F)}\\\\\n&= \\frac{P(F|E_1) \\times P(E_1)}{P(F|E_1) \\times P(E_1) + P(F|E_2) \\times P(E_2) + P(F|E_3) \\times P(E_3)} \\\\\n&= \\frac{0.99 \\times 0.2}{0.99 \\times 0.2 + 0.1 \\times 0.5 + 0.05 \\times 0.3}\\\\\n&= 0.7528517\n\\end{split}\n\\]","code":""},{"path":"probability.html","id":"approach-2-using-tree-diagrams","chapter":"2 Probability","heading":"2.3.8.2 Approach 2: Using Tree Diagrams","text":"tree diagram useful finding conditional probabilities probabilities involving intersections. visual way displaying information hand, conditional probabilities disjoint sets probabilities disjoint set. toy example, disjoint sets type email receive, \\(E_1, E_2, E_3\\), conditional probabilities disjoint sets, .e. \\(P(F|E_1), P(F|E_2)\\) \\(P(F|E_3)\\). can put information visual first splitting sample space disjoint sets \\(E_1, E_2, E_3\\), splitting disjoint set whether email word “free” (\\(F\\)) (\\(F^c\\)). information displayed tree diagram Figure 2.7.\nFigure 2.7: Tree Diagram Email Example\nsplit represented branch, write corresponding probability branch. want find probability received email spam given contains word “free”, \\(P(E_1|F)\\), using definition conditional probability equation (2.3)\\[\nP(E_1|F) = \\frac{P(E_1 \\cap F)}{P(F)}.\n\\]Looking tree diagram Figure 2.7, can label branches lead numerator \\(P(E_1 \\cap F)\\), probability email spam contains word free. shown tree diagram Figure 2.8 highlighting corresponding branches blue.\nFigure 2.8: Tree Diagram Email Example, Branch Numerator Blue\n\\(P(E_1 \\cap F) = 0.2 \\times 0.99 = 0.198\\). need find denominator \\(P(F)\\). Looking Figure 2.7, can see three branches lead email containing word free: \\(P(E_1 \\cap F)\\) \\(P(E_2 \\cap F)\\) \\(P(E_3 \\cap F)\\). shown tree diagram Figure 2.9 highlighting corresponding branches gold.\nFigure 2.9: Tree Diagram Email Example, Branches Denominator Gold\nknow probability branch, add obtain denominator \\(P(F) = 0.2 \\times 0.99 + 0.5 \\times 0.1 + 0.3 \\times 0.05 = 0.263.\\) Putting pieces together, \\[\nP(E_1|F) = \\frac{P(E_1 \\cap F)}{P(F)} = \\frac{0.198}{0.263} = 0.7528517.\n\\]Note: compare intermediate calculations approach, end using calculations approach 1, without referring associated equations.","code":""},{"path":"probability.html","id":"confusion-of-the-inverse","chapter":"2 Probability","heading":"2.4 Confusion of the Inverse","text":"now ready talk prosecutor’s fallacy, confusion inverse, earlier mention section 2.3.1. essence, confusion happens falsely equate \\(P(X|Y)\\) equal \\(P(Y|X)\\). fact, large value \\(P(X|Y)\\) necessarily imply \\(P(Y|X)\\) also large. term prosecutor’s fallacy confusion applied criminal trial, e.g. probability abusive relationship ends murder small, probability abuse relationship ended murder lot higher.go examples based real life.","code":""},{"path":"probability.html","id":"disease-diagnostics","chapter":"2 Probability","heading":"2.4.1 Disease Diagnostics","text":"Suppose testing patient rare disease, estimated prevalent 0.5% people. Suppose medical test disease accurate. can number definitions accuracy. disease diagnostics, couple measures sensitivity, proportion people disease test positive, specificity, proportion people without disease test negative. positive test indicates person disease. Suppose sensitivity specificity high: 0.95 0.9 respectively. Suppose patient tests positive, probability patient actually disease? Assume test always indicates positive negative.example, let \\(D\\) denote event patient disease, let + denote event patient tests positive test, - denote event patient tests negative test. Given information, \\(P(D) = 0.005\\).\\(P(+|D) = 0.95\\).\\(P(-|D^c) = 0.9\\).wish find \\(P(D|+)\\). Using Bayes rule Law Total probability, \\[\n\\begin{split}\nP(D|+) &= \\frac{P(D \\cap +)}{P(+)}\\\\\n&= \\frac{P(+|D) \\times P(D)}{P(+|D) \\times P(D) + P(+|D^c) \\times P(D^c)} \\\\\n&= \\frac{0.95 \\times 0.005}{0.95 \\times 0.005 + 0.1 \\times 0.995 }\\\\\n&= 0.04556355\n\\end{split}\n\\]small probability, patient highly unlikely actually rare disease. test high sensitivity \\(P(+|D) = 0.95\\), imply patient tests positive actually disease, since \\(P(D|+)\\) low. implication rare disease, positive test imply high probability disease, even test accurate.result make sense? Essentially, large proportion small population still numerically much smaller small proportion large population. disease rare, small population people disease, almost detected test. also extremely large population people without disease, even small proportion erroneously test positive still fairly large number. among positive tests, people disease. consider following table based population 20 thousand people.Look first column, shows number people test positive. see large proportion diseased people detected, since relatively people disease, number small, 95. small proportion people disease test positive disease, small proportion large population results relatively larger number, 1990. people test positive, \\(95 + 1990 = 2085\\) actually disease. Therefore \\(P(D|+) = \\frac{95}{2085} = 0.04556355\\).can also explain result Bayes’ viewpoint probability. Without knowing information results test, prior probability \\(P(D) = 0.005\\). However, upon seeing person positive, updated posterior probability \\(P(D|+) = 0.04556355\\), increase 0.005 knew knowing. updated posterior probability 9 times prior. believe person likely disease upon viewing positive test, knew nothing test result. posterior probability still small since value depends two pieces information: prior \\(P(D)\\) sensitivity \\(P(+|D)\\). product values belong numerator calculating \\(P(D|+)\\). denominator \\(P(+|D) \\times P(D) + P(+|D^c) \\times P(D^c)\\). prior \\(P(D)\\) extremely low, \\(P(D^c)\\) extremely close 1, since person either disease disease. \\(P(D)\\) belong extremely low, numerator close 0, value denominator close \\(P(+|D^c) \\times P(D^c)\\), therefore \\(P(D|+)\\) small.Notice talking rare diseases? confusion inverse, thinking high sensitivity implies person likely disease test positive, applies rare diseases. disease prevalent, high sensitivity likely imply person disease test positive.take tests rare diseases? ? go test . turns test positive twice rare disease, probability disease increases lot tested tested positive.perform calculation, use odds form Bayes’ rule, per equation (2.9)\\[\n\\begin{split}\n\\frac{P(D|T_1 \\cap T_2)}{P(D^c|T_1 \\cap T_2)} &= \\frac{P(T_1 \\cap T_2 | D)}{P(T_1 \\cap T_2 | D^c)} \\frac{P(D)}{P(D^c)}\\\\\n&= \\frac{0.95^2}{0.1^2} \\frac{0.005}{0.995} \\\\\n&= 0.4535176\n\\end{split}\n\\]\\(T_1\\) \\(T_2\\) denote events person test positive first test second test respectively. also assume results test independent previous tests.odds disease given person positive twice 0.4535176. Therefore, using equation (2.8), corresponding probability disease given person tested positive twice \\(P(D|T_1 \\cap T_2) = \\frac{0.4535176}{1+0.4535176} = 0.3120138\\). See posterior probability increased two positive tests, 1 positive test.Thought question: perform calculations show posterior probability person disease person tests positive 3 tests 0.8116199.Thought question: notice certain pattern emerging performing calculations person undergoes tests? write either mathematical equation, even function R, allows us quickly compute probability person disease given person tested positive \\(k\\) times, \\(k\\) can denote non negative integer?","code":""},{"path":"probability.html","id":"prosecutors-fallacy","chapter":"2 Probability","heading":"2.4.2 Prosecutor’s Fallacy","text":"confusion inverse also called prosecutor’s fallacy (sometimes also called defense attorney’s fallacy depending side making mistake) occurs legal setting. Generally, confusion comes equating P(evidence|innocent) P(innocent|evidence).book provides discussion Section 2.8, examples 2.8.1 2.8.2.","code":""}]
