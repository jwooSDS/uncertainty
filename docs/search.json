[{"path":"index.html","id":"preface","chapter":"Preface","heading":"Preface","text":"examples preface based OpenIntro Statistics (Diez, Ceytinka-Rundel, Barr), Chapter 9.4 9.5, provide background information. can access book free https://www.openintro.org/book/os/main goal using data science understand data. Broadly speaking, involve building statistical model predicting, estimating response variable based one predictors. models used wide variety fields finance, medicine, public policy, sports, . look couple examples.","code":""},{"path":"index.html","id":"examples","chapter":"Preface","heading":"0.1 Examples","text":"","code":""},{"path":"index.html","id":"example-1-mario-kart-auction-prices","chapter":"Preface","heading":"0.1.1 Example 1: Mario Kart Auction Prices","text":"first example, look Ebay auctions video game called Mario Kart played Nintendo Wii. want predict price auction based whether game new , whether auction’s main photo stock photo, duration auction days, number Wii wheels included auction.model can use example linear regression model:Generally speaking, linear regression equation takes following form:\\[\n\\hat{y} = \\hat{\\beta}_0 + \\hat{\\beta}_1 x_1 + \\hat{\\beta}_2 x_2 + \\cdots + \\hat{\\beta}_k x_k\n\\]\\(\\hat{y}\\) denotes predicted value response variable, price action example, \\(x_1, x_2, \\cdots, x_k\\) denote values predictors. example, : \\(x_1\\) whether game new , \\(x_2\\) whether auction’s main photo stock photo, \\(x_3\\) duration auction days, \\(x_4\\) number Wii wheels included auction. \\(\\hat{\\beta}_0, \\hat{\\beta}_1, \\cdots, \\hat{\\beta}_k\\) represent estimated regression parameters. know values , can easily plug values predictors obtain predicted price auction.Fitting model R, obtain estimated regression parameters::\\[\n\\hat{y} = 43.5201 - 2.5816 x_1 - 6.7542 x_2 + 0.3788 x_3 + 9.9476 x_4\n\\]auction Mario Kart game used, uses stock photo, listed 2 days, comes 0 wheels, predicted price \\(\\hat{y} = 43.5201 - 2.5816 - 6.7542 + 0.3788 \\times 2 = 34.94\\) 35 dollars.","code":"\nlibrary(openintro)\n\nData<-mariokart\n##fit model\nresult<-lm(total_pr~cond+stock_photo+duration+wheels, data=Data)\n##get estimated regression parameters\nresult## \n## Call:\n## lm(formula = total_pr ~ cond + stock_photo + duration + wheels, \n##     data = Data)\n## \n## Coefficients:\n##    (Intercept)        condused  stock_photoyes        duration          wheels  \n##        43.5201         -2.5816         -6.7542          0.3788          9.9476"},{"path":"index.html","id":"eg2","chapter":"Preface","heading":"0.1.2 Example 2: Job Application Callback Rates","text":"example, look data experiment sought evaluate effect race gender job application callback rates. experiment, researchers created fake resumes job postings Boston Chicago see resumes resulted callback. fake resumes included relevant information applicant’s educational attainment, many year’s experience applicant well first last name. names fake resume meant imply applicant’s race gender. two races considered (Black White) two genders considered (Make Female) experiment.Prior experiment, researchers conducted surveys check racial gender associations names fake resumes; names passed certain threshold surveys included experiment.model can used example logistic regression modelGenerally speaking, logistic regression equation takes following form\\[\n\\log (\\frac{\\hat{\\pi}}{1-\\hat{\\pi}}) = \\hat{\\beta}_0 + \\hat{\\beta}_1 x_1 + \\hat{\\beta}_2 x_2 + \\cdots + \\hat{\\beta}_k x_k\n\\]\\(\\hat{\\pi}\\) denotes predicted probability applicant receives call back. \\(x_1, x_2, \\cdots, x_k\\) denote values predictors. example, : \\(x_1\\) city job posting located , \\(x_2\\) whether applicant college degree , \\(x_3\\) experience applicant, \\(x_4\\) associated race applicant, \\(x_5\\) associated gender applicant. Similar linear regression, \\(\\hat{\\beta}_0, \\hat{\\beta}_1, \\cdots, \\hat{\\beta}_k\\) represent estimated regression parameters. know values , can easily plug values predictors obtain predicted probability applicant characteristics receive callback.Fitting model R, obtain estimated regression parametersso \\[\n\\log (\\frac{\\hat{\\pi}}{1-\\hat{\\pi}}) = -2.63974 - 0.39206 x_1 - 0.0655 x_2 + 0.03152 x_3 + 0.44299 x_4 - 0.22814 x_5\n\\]applicant Boston, college degree, 10 years experience name associated Black male, logistic regression equation becomes \\(\\log (\\frac{\\hat{\\pi}}{1-\\hat{\\pi}}) = -2.63974 - 0.0655 + 0.03152 \\times 10 - 0.22814 = -2.61818\\). little bit algebra solve, get \\(\\hat{\\pi} = 0.06797751\\). applicant 6.8 percent chance receiving callback.","code":"\nData2<-resume\n##fit model\nresult2<-glm(received_callback~job_city + college_degree+years_experience+race+gender, family=\"binomial\", data=Data2)\n##get estimated regression parameters\nresult2## \n## Call:  glm(formula = received_callback ~ job_city + college_degree + \n##     years_experience + race + gender, family = \"binomial\", data = Data2)\n## \n## Coefficients:\n##      (Intercept)   job_cityChicago    college_degree  years_experience  \n##         -2.63974          -0.39206          -0.06550           0.03152  \n##        racewhite           genderm  \n##          0.44299          -0.22814  \n## \n## Degrees of Freedom: 4869 Total (i.e. Null);  4864 Residual\n## Null Deviance:       2727 \n## Residual Deviance: 2680  AIC: 2692"},{"path":"index.html","id":"how-were-estimated-parameters-calculated","chapter":"Preface","heading":"0.2 How were Estimated Parameters Calculated?","text":"two examples, notice used R functions, supplied names variables, R functions generated values estimated parameters? One thing learn functions actually calculate numbers. turns calculations based foundational concepts associated measures uncertainty, probability, expected values. learning concepts class.want know calculations performed? understand intuition logic behind models built. becomes lot easier work models understand logic (example, know models can used used, know steps take notice data certain characteristics, etc), instead memorizing bunch steps.presenting models data people, people may occasionally questions methods models. trust model? trust numbers seem come black box?Notice used two different models, linear regression logistic regression, examples 1 2. use models? swapped type model used examples? answer actually . One main considerations deciding model use identify response variable quantitative categorical. learn linear regression model works response variable quantitative, logistic regression model works response variable categorical.","code":""},{"path":"index.html","id":"the-course-understanding-uncertainty","chapter":"Preface","heading":"0.3 The Course: Understanding Uncertainty","text":"mentioned previous section, learning foundational concepts associated measures uncertainty, probability, expected values. concepts help explain intuition statistical models built.end course, apply concepts revisit linear regression logistic regression models. two widely used models used data science, relatively easier understand explain. modern methods (learn future classes) decision trees neural networks can viewed extensions linear logistic regression models.","code":""},{"path":"descriptive.html","id":"descriptive","chapter":"1 Descriptive Statistics","heading":"1 Descriptive Statistics","text":"module based OpenIntro Statistics (Diez, Ceytinka-Rundel, Barr), Chapter 2.1. can access book free https://www.openintro.org/book/os/ Please note cover additional topics, skip certain topics book.","code":""},{"path":"descriptive.html","id":"uncertainty-with-data","chapter":"1 Descriptive Statistics","heading":"1.1 Uncertainty with Data","text":"analyzing data, always going degree uncertainty, randomness lot phenomena observe world. event random individual outcomes event unpredictable. example, weight next baby born local hospital. Without knowing information biological parents, high degree uncertainty try predict baby’s weight. Even know detailed information biological parents (example tall), may feel confident predicting baby likely heavier average, certain prediction.end hand, event deterministic can predict individual outcomes event certainty. example, know length cube 2 inches, know sure volume \\(2^3 = 8\\) cubic inches, based rules mathematics. volume cube length 2 inches always going 8 cubic inches, volume deterministic.Thought question: think data see real life. Write . data random deterministic?explore tools help us quantify uncertainty data. module, explore fairly standard tools used describe data give us idea degree uncertainty data. describing data quantitative, usually describe following: shape distribution, average typical value, spread uncertainty.","code":""},{"path":"descriptive.html","id":"visualizing-data","chapter":"1 Descriptive Statistics","heading":"1.2 Visualizing Data","text":"Data visualization representation information form pictures. Imagine access weights newborn babies local hospital. Examining numerical value time consuming. instead, can use visualizations give us idea values weights. example, weights newborns common? proportion babies dangerously low weights (may indicate health risks)? Good data visualizations can give us information fairly quickly. Next, explore common visualizations used quantitative (numerical) variables.","code":""},{"path":"descriptive.html","id":"dot-plots","chapter":"1 Descriptive Statistics","heading":"1.2.1 Dot Plots","text":"start dot plot, basic visualization quantitative variable. use loan50 dataset openintro package. data originally consist thousands loans made Lending Club platform, randomly select 50 loans. Let us study interest rate loans 50 applicants received.simplicity, round numerical values interest rates nearest whole number:can create corresponding dot plot, per Figure 1.1:\nFigure 1.1: Dot Plot 50 Interest Rates (rounded)\nNotice 1 black dot corresponds interest rate 20 (presumably percent), one applicant rounded interest rate 20 percent. 8 black dots correspond interest rate 10 percent, 8 applicants rounded interest rate 10 percent. interest rates 10 percent much commonly occurring interest rate 20 percent. can use height, number dots, help us glean often value certain interest rate occurs. Based dotplot, interest rates 5 11 percent common, higher values less common.Note: get torn details code produce dot plot. chosen present dot plot way highlight use , without getting bogged details can produced. using dot plots class.","code":"\nlibrary(tidyverse)\nlibrary(openintro)\n\n##create object for data\nData<-loan50\n##round interest rate to whole number\nData<- Data%>%\n  mutate(r_int_rate = round(interest_rate))\n##dotplot\nggplot(Data,aes(x=r_int_rate))+\n  geom_dotplot(binwidth=1)+\n  theme(\n    axis.text.y = element_blank(),  # Remove y-axis labels\n    axis.title.y = element_blank(), # Remove y-axis title\n    axis.ticks.y = element_blank()  # Remove y-axis ticks\n  )+ \n  labs(x=\"Interest Rates (Rounded)\")"},{"path":"descriptive.html","id":"histograms","chapter":"1 Descriptive Statistics","heading":"1.2.2 Histograms","text":"turns dot plots often useful large data sets, provide general idea visualizations larger data sets work. height dots inform us frequency values occurring.visualization commonly used larger data sets histogram. Instead displaying common value variable exists, think values belonging bin values. example, can create bin contains interest rates 5 7.5 percent, another bin containing interest rates 7.5 10 percent, . things note histograms:convention, values lie exactly boundary bin belong lower bin. example, interest rate exactly 12.5 percent belong bin 10 12.5 percent, bin 12.5 15 percent.convention, values lie exactly boundary bin belong lower bin. example, interest rate exactly 12.5 percent belong bin 10 12.5 percent, bin 12.5 15 percent.bin width. example, width 2.5.bin width. example, width 2.5.create histogram (using original interest rates) , per Figure 1.2:\nFigure 1.2: Historgram 50 Interest Rates\nSimilar dot plot Figure 1.1, height histogram inform us values commonly occurring. can see histogram interest rates 5 10 percent common, much loans interest rates greater 20 percent. say certainty randomly selected loan applicant interest rate 5 10 percent interest rate greater 20 percent.","code":"\n##set up sequence to specify the bins\ns25<-seq(5,27.5,2.5)\n\nggplot(Data,aes(x=interest_rate))+\n  geom_histogram(breaks=s25,fill=\"blue\",color=\"orange\")+\n  labs(x=\"Interest Rate\", title=\"Histogram of Interest Rates\")"},{"path":"descriptive.html","id":"shapes-of-distribution","chapter":"1 Descriptive Statistics","heading":"1.2.2.1 Shapes of Distribution","text":"Histograms can also give us idea shape distribution interest rates. histogram Figure 1.2, loans less 15 percent, small number loans greater 20 percent. can say greater certainty loan interest rate less 15 percent. data tail right histogram, shape said right-skewed. variable said right-skewed, large values variable much less common small values variable; smaller values likely occur.histogram reverse characteristic, .e. data tail left instead, shape said left-skewed. implies small values variable much less common large values variable; larger values likely occur.histogram reverse characteristic, .e. data tail left instead, shape said left-skewed. implies small values variable much less common large values variable; larger values likely occur.Histograms trail similarly directions called symmetric. Large small values variable equally likely.Histograms trail similarly directions called symmetric. Large small values variable equally likely.Histograms peak middle, trail sides symmetic, also bell-shaped, normal distribution. Note: turns one assumptions linear regression response variable follow normal distribution. may seem restrictive, however, see later modules assumption particular crucial circumstances.Histograms peak middle, trail sides symmetic, also bell-shaped, normal distribution. Note: turns one assumptions linear regression response variable follow normal distribution. may seem restrictive, however, see later modules assumption particular crucial circumstances.Thought question: Can think real life variables symmetric, right-skewed, left-skewed distributions? Feel free search internet examples.","code":""},{"path":"descriptive.html","id":"considerations-with-histograms","chapter":"1 Descriptive Statistics","heading":"1.2.2.2 Considerations with Histograms","text":"interest rate example, may noticed made specific choice width bins created histograms. turns width bins can impact shape histogram, potentially, interpret histogram.Consider creating histogram bin width 0.5, instead 2.5, per Figure 1.3:\nFigure 1.3: Historgram 50 Interest Rates, Bin Width 0.5\nComparing Figure 1.3 Figure 1.2, note following:Visually, histogram looks jagged smaller bin width, whereas histogram looks smoother larger bin width.Visually, histogram looks jagged smaller bin width, whereas histogram looks smoother larger bin width.Smaller bin widths may preferred need information smaller ranges interest rates. However, can difficult write general trends.Smaller bin widths may preferred need information smaller ranges interest rates. However, can difficult write general trends.Larger bin widths may useful trying look general trends interest rates.Larger bin widths may useful trying look general trends interest rates.Thought question: happens create histogram bin width large?","code":"\n##set up sequence to specify the bins. width now 0.5\ns05<-seq(5,27.5,0.5)\n\nggplot(Data,aes(x=interest_rate))+\n  geom_histogram(breaks=s05,fill=\"blue\",color=\"orange\")+\n  labs(x=\"Interest Rate\", title=\"Histogram of Interest Rates\")"},{"path":"descriptive.html","id":"densplots","chapter":"1 Descriptive Statistics","heading":"1.2.3 Density Plots","text":"Another visualization quantitative variable density plot. density plot can viewed smoothed version histogram. can use heights inform us values common. create density plot interest rates Figure 1.4:\nFigure 1.4: Density Plot 50 Interest Rates\nBased Figure 1.4, see low interest rates (5 12.5 percent) much common high interest rates (higher 20 percent). things note interpreting density plots:area density plot always equals 1.find proportion interest rates two values, example 10 15 percent, integrate density plot range, .e. \\(\\int_{10}^{15} f(x) dx\\), \\(f(x)\\) mathematical equation describes density plot.values vertical axis equal probabilities (common misconception).density plot found using method called kernel density estimation (KDE). details KDE later module need cover quite bit material .","code":"\n##density plot\nplot(density(Data$interest_rate), main=\"Density Plot of Interest Rates\")"},{"path":"descriptive.html","id":"considerations-with-density-plots","chapter":"1 Descriptive Statistics","heading":"1.2.3.1 Considerations with Density Plots","text":"Similar bins histograms, density plots affected bandwidth. Larger bandwidths lead smoother density plots, smaller bandwidths lead jagged density plots. create density plot uses bandwidth twice default Figure 1.5 :\nFigure 1.5: Density Plot 50 Interest Rates Larger Bandwidth\nNotice Figure 1.5 little peak interest rates 15 20 (existed Figures 1.4 also 1.2) longer exists. Using bandwidths large can smooth peaks.Thought question: create density plot bandwidth small?Thought question: bin widths histograms bandwidths density plots related?","code":"\nplot(density(Data$interest_rate, adjust=2), main=\"Density Plot of Interest Rates, with Bandwidth Twice the Default\")"},{"path":"descriptive.html","id":"ordered-statistics","chapter":"1 Descriptive Statistics","heading":"1.3 Ordered Statistics","text":"idea behind ordered statistics pretty self-explanatory: take numerical variable, order values smallest largest. Going back example interest rates 50 loan applicants, let \\(X\\) denote interest rate. \\(x_{(1)}\\) denote interest rate smallest, \\(x_{(2)}\\) denotes second smallest interest rate, \\(x_{(50)}\\) denotes largest interest rate sample 50.","code":""},{"path":"descriptive.html","id":"quantiles","chapter":"1 Descriptive Statistics","heading":"1.3.1 Quantiles","text":"Quantiles partition range numerical data continuous intervals (groups) (nearly) equal proportions. Common quartiles names:Quartiles: 4 groupsPercentiles: 100 groupsThere one less quantiles number groups. go quartiles detail.","code":""},{"path":"descriptive.html","id":"quart","chapter":"1 Descriptive Statistics","heading":"1.3.1.1 Quartiles","text":"Quartiles divide data 4 groups, group (nearly) equal number observations. three quartiles, denoted \\(Q_1, Q_2, Q_3\\).first group values negative infinity \\(Q_1\\).second group values negative \\(Q_1\\) \\(Q_2\\).third group values negative \\(Q_2\\) \\(Q_3\\).fourth group values negative \\(Q_3\\) infinity.\\(Q_2\\), sometimes called second quartile, easiest value find. also called median data. Going back interest rates 50 loan applicants. Using ordered statistics, median middle observation. Since even number observations, two middle observations, \\(x_{(25)}\\) \\(x_{(26)}\\). situation, median average two middle observations. Using R, find median :half interest rates less 9.93 percent, half interest rates greater 9.93 percent. might also recognize another term median: 50th percentile, 50 percent interest rates less 9.93.find middle observation(s) based sample size \\(n\\):\\(n\\) even, 2 middle observations position \\(\\frac{n}{2}\\) \\(\\frac{n}{2} + 1\\) ordered statistics.\\(n\\) odd, middle observation position \\(\\frac{n}{2} + 0.5\\) ordered statistics.\\(Q_1\\) \\(Q_3\\) (also called first third quartiles) found together, finding \\(Q_2\\). Note \\(Q_2\\) divides data two groups. Using interest rates example, one group contains \\(x_{(1)}, \\cdots, x_{(25)}\\), another group contains \\(x_{(26)}, \\cdots, x_{(50)}\\). \\(Q_1\\) median first group, \\(Q_3\\) median second group. 50 loan applicants:\\(Q_1\\) \\(x_{(13)}\\), \\(Q_3\\) \\(x_{(38)}\\).find values R, type:\\(Q_1\\) 7.96 percent, \\(Q_3\\) 14.08 percent. turns \\(Q_1\\) also 25th percentile, \\(Q_3\\) also 75th percentile, definition.Remember wrote following earlier:first group values negative infinity \\(Q_1\\). quarter observations interest rates less 7.96 percent.second group values negative \\(Q_1\\) \\(Q_2\\). quarter observations interest rates 7.96 9.93 percent.third group values negative \\(Q_2\\) \\(Q_3\\). quarter observations interest rates 9.93 14.08 percent.fourth group values negative \\(Q_3\\) infinity. quarter observations interest rates 14.08 percent.Note: may notice used type = 1 inside quantile() function. Using type = 1 gives values first third quartiles based method just described. actually several ways find quantiles, may result slightly differing values, although generally meet definition \\(Q_1\\) 25th percentile, \\(Q_3\\) 75th percentile.","code":"\nmedian(Data$interest_rate)## [1] 9.93\nquantile(Data$interest_rate, prob=c(0.25,0.75), type = 1)##   25%   75% \n##  7.96 14.08"},{"path":"descriptive.html","id":"percentiles","chapter":"1 Descriptive Statistics","heading":"1.3.1.2 Percentiles","text":"Another common quantile percentile. general k-th percentile value data point \\(k\\) percent observations found. earlier example, said \\(Q_3\\) interest rates 14.08 percent, also 75th percentile. 75 percent interest rates less 14.08 percent.go details finding percentiles hand.","code":""},{"path":"descriptive.html","id":"box-plots","chapter":"1 Descriptive Statistics","heading":"1.3.2 Box Plots","text":"Another visualization used summarize quantitative data box plot. box plot summarizes 5-number summary. 5 numbers minimum, \\(Q_1, Q_2, Q_3\\), maximum. Using interest rate data, box plot shown Figure 1.6:\nFigure 1.6: Box Plot Interest Rates\npeople call box plot box whisker plot.boundaries box represent \\(Q_1\\) \\(Q_3\\).thick line box represents median.two whiskers either side box extend minimum maximum, outliers exist. outliers exist, whiskers extend minimum maximum values outliers.Generally, one quantitative variable, outlier observation whose numerical value far away rest data. words, lot smaller larger relative rest data.50 loans, two loan applicants interest rates around 25 percent flagged lot larger rest loans, reasonable since loans lot smaller 20 percent.go details outliers determined box plots. interested, can read Chapter 2.1.5 OpenIntro Statistics (Diez, Ceytinka-Rundel, Barr).Notice much large values (\\(Q_3\\) maximum) median, compared distance small values (\\(Q_1\\) minimum) median. indicates distribution interest rates right-skewed. Compare boxplot interest rates Figure 1.6 corresponding histogram (Figure 1.2) density plot (Figure 1.4).Thought question: can sketch box plot represents variable left-skewed? variable symmetric?","code":"\n##box plot\nggplot(Data,aes(y=interest_rate))+\n  geom_boxplot()+\n  labs(y=\"Interest Rate\", title=\"Box Plot of Interest Rates\")"},{"path":"descriptive.html","id":"ecdf","chapter":"1 Descriptive Statistics","heading":"1.3.3 Empirical Cumulative Distribution Function","text":"previous sections, can see use histograms, density plots, box plots inform us proportion observations take certain values, values data correspond certain percentiles. However, limited quartiles percentile using box plots, need find areas density plot (using integration, trivial task), add frequencies histogram (can time consuming).plot can easily give us values variable correspond percentiles empirical cumulative distribution function (ECDF) plot.Let \\(X\\) denote random variable, observed \\(n\\) observations \\(X\\) denoted \\(x_1, \\cdots, x_n\\). Let \\(x_{(1)}, \\cdots x_{(n)}\\) denote ordered statstics \\(n\\) observations. ECDF, denoted \\(\\hat{F}_n(x)\\) proportion sample observations less equal value \\(x\\) random variable. Mathematically, ECDF :\\[\n\\hat{F}_n(x) =\n  \\begin{cases}\n   0, & \\text{} x < x_{(1)} \\\\\n   \\frac{k}{n},       & \\text{} x_{(k)} \\leq x < x_{(k+1)}, k = 1, \\cdots, n-1\\\\\n   1, & \\text{} x \\geq x_{(n)}.\n  \\end{cases}\n\\]\nshall use simple toy example illustrate ECDF constructed. Suppose ask 5 people many times go gym (least 20 minutes) typical work week. answers : 3, 0, 1, 5, 3. random variable \\(X\\) many times person goes gym least 20 minutes, ordered statistics \\(x_{(1)} = 0, x_{(2)} = 1, x_{(3)} = 3, x_{(4)} = 3, x_{(5)} = 5\\). Using mathematical definition ECDF, :\\(\\hat{F}_n(x) = 0\\) \\(x < x_{(1)} = 0\\).\\(\\hat{F}_n(x) = \\frac{1}{5}\\) \\(0 \\leq x < x_{(2)} = 1\\).\\(\\hat{F}_n(x) = \\frac{2}{5}\\) \\(1 \\leq x < x_{(3)} = 3\\).\\(\\hat{F}_n(x) = \\frac{4}{5}\\) \\(3 \\leq x < x_{(5)} = 5\\). value special example since two observations \\(x=3\\).\\(\\hat{F}_n(x) = 1\\) \\(x \\geq 5\\).corresponding ECDF plot shown Figure 1.7:\nFigure 1.7: ECDF Plot Toy Example\ncan easily find percentiles plot, example, 40th percentile equal 1, going gym week. 20 percent observations go gym less 1 time week.Next, create ECDF plot interest rates 50 loan applicants.\nFigure 1.8: ECDF Plot Interest Rates\noverlaid horizontal line 80th percentile, can read horizontal axis corresponds interest rate 17 percent. 80 percent loan applicants interest rate less 17 percent.Thought question: try using histogram density plot interest rates (Figures 1.2 1.4) find interest rate corresponds 80th percentile. easy perform?","code":"\n##toy data\ny<-c(3, 0, 1, 5, 3)\n##ECDF plot\nplot(ecdf(y), main = \"ECDF for Toy Example\")\nplot(ecdf(Data$interest_rate), main = \"ECDF Plot of Interest Rates\")\nabline(h=0.8)"},{"path":"descriptive.html","id":"measures-of-centrality","chapter":"1 Descriptive Statistics","heading":"1.4 Measures of Centrality","text":"far, used visualizations summarize shape distribution quantitative variable. Next, look common measures centrality. Loosely speaking, measures centrality measures describe average typical value quantitative variable. common measures centrality mean, median, mode.","code":""},{"path":"descriptive.html","id":"mean","chapter":"1 Descriptive Statistics","heading":"1.4.1 Mean","text":"sample mean simply average value variable sample. sample mean random variable \\(X\\) denoted \\(\\bar{x}\\), found :\\[\\begin{equation}\n\\bar{x} = \\frac{\\sum_{=1}^n x_i}{n}.\n\\tag{1.1}\n\\end{equation}\\], toy example 5 people often go gym week, sample mean \\(\\bar{x} = \\frac{3+0+1+5+3}{5} = 2.4\\).","code":""},{"path":"descriptive.html","id":"median","chapter":"1 Descriptive Statistics","heading":"1.4.2 Median","text":"went find median section 1.3.1.1. median value middle observation ordered statistics. also called \\(Q_2\\), second quartile, 50th percentile, approximately 50 percent observations values smaller median., toy example 5 people often go gym week, sample median \\(x_{(3)} = 3\\). 50 percent people went gym less 3 times week.","code":""},{"path":"descriptive.html","id":"mode","chapter":"1 Descriptive Statistics","heading":"1.4.3 Mode","text":"Another measure mode. Mathematically speaking, mode commonly occurring value data. toy example, mode 3, since 3 occurs twice occurs often data.","code":""},{"path":"descriptive.html","id":"considerations","chapter":"1 Descriptive Statistics","heading":"1.4.4 Considerations","text":"things consider using measures centrality:mean measure people comfortable , however, caution needs used variable skewed, extreme outliers drastically alter value mean. Using toy example gym, suppose person visits gym visits 50 times, instead 5. numerical value sample mean explodes, give good representation central value many visits gym person makes week. mean fine variable symmetric.mean measure people comfortable , however, caution needs used variable skewed, extreme outliers drastically alter value mean. Using toy example gym, suppose person visits gym visits 50 times, instead 5. numerical value sample mean explodes, give good representation central value many visits gym person makes week. mean fine variable symmetric.median measure recommended skewed distributions, since order associated ordered statistics influenced extreme outliers. Using gym example, previous bullet point, median unaffected.median measure recommended skewed distributions, since order associated ordered statistics influenced extreme outliers. Using gym example, previous bullet point, median unaffected.mean larger median indication distribution right-skewed. Using interest rate example, :mean larger median indication distribution right-skewed. Using interest rate example, :consistent right skew saw histogram density plot Figures 1.2 1.4. Conversely, left-skewed distribution usually mean smaller median. symmetric distribution typically similar values mean median.mean considered sensitive measure, since numerical value can drastically affected outliers. median considered robust measure, since numerical value resistant less affected outliers.mean considered sensitive measure, since numerical value can drastically affected outliers. median considered robust measure, since numerical value resistant less affected outliers.mathematical definition mode can difficult use variables continuous, since likely observations value variable continuous. instance, mode typically refers bin histogram tallest. , using histogram Figure 1.2 interest rates, mode 7.5 10 percent.mathematical definition mode can difficult use variables continuous, since likely observations value variable continuous. instance, mode typically refers bin histogram tallest. , using histogram Figure 1.2 interest rates, mode 7.5 10 percent.","code":"\nmean(Data$interest_rate)## [1] 11.5672\nmedian(Data$interest_rate)## [1] 9.93"},{"path":"descriptive.html","id":"measures-of-spread","chapter":"1 Descriptive Statistics","heading":"1.5 Measures of Spread","text":"previous sections, learned summarizing features quantitative variable, using visualizations summarize shape, using measures centrality describe average typical values variable. One feature can summarize spread, associated values quantitative variable. Measures spread considered way measure uncertainty. Data larger spread uncertainty.","code":""},{"path":"descriptive.html","id":"variance-and-standard-deviation","chapter":"1 Descriptive Statistics","heading":"1.5.1 Variance and Standard Deviation","text":"One measure spread variance. sample variance random variable \\(X\\) denoted \\(s^2\\), sometimes \\(s_x^2\\), found :\\[\\begin{equation}\ns^2 = \\frac{\\sum_{=1}^n (x_i-\\bar{x})^2}{n-1}.\n\\tag{1.2}\n\\end{equation}\\]variance can interpreted approximate average squared distance observations mean. formula equation (1.2) may look bit complicated, let us use toy example asked 5 people often go gym workweek. answers : 3, 0, 1, 5, 3, earlier found sample mean \\(\\bar{x} = 2.4\\). calculate sample variance:\\[\n\\begin{split}\ns^2 &= \\frac{\\sum_{=1}^n (x_i-\\bar{x})^2}{n-1}\\\\\n&= \\frac{(3-2.4)^2 + (0-2.4)^2 + (1-2.4)^2 + (5-2.4)^2 + (3-2.4)^2}{5-1} \\\\\n&= 3.8\n\\end{split}\n\\]Notice numerator equation (1.2): take difference observed value sample mean, square differences, add squared differences. divide \\(n-1\\), rather \\(n\\), hence sample variance approximate averaged squared distance observations mean. nuance mathematics divide \\(n-1\\) instead \\(n\\), may intuitive . turns dividing \\(n-1\\) makes sample variance unbiased estimator true variance population (denoted \\(\\sigma^2\\)) reliable divided \\(n\\). go detail later module covering additional concepts.Larger values sample variance indicate observations generally away sample mean, indicating larger spread, higher degree uncertainty future values.Thought question: mean sample variance set observations 0? indicate little () uncertainty set observations?Another related measure sample standard deviation, square root sample variance. Similar variance, larger values indicated spread data.","code":""},{"path":"descriptive.html","id":"interquartile-range","chapter":"1 Descriptive Statistics","heading":"1.5.2 Interquartile Range","text":"Another measure spread interquartile range (IQR), difference third first queartiles,\\[\\begin{equation}\nIQR = Q_3 - Q_1.\n\\tag{1.3}\n\\end{equation}\\]IQR considered robust measure spread, sample variance standard deviations considered sensitive.","code":""},{"path":"probability.html","id":"probability","chapter":"2 Probability","heading":"2 Probability","text":"module based Introduction Probability (Blitzstein, Hwang), Chapters 1 2. can access book free https://stat110.hsites.harvard.edu/ (click Book). Please note cover additional topics, skip certain topics book. may skip: Sections 1.4, 1.5, Theorem 1.6.3, Examples 1.6.4, 2.4.5, 2.5.12, 2.7.3 book.","code":""},{"path":"probability.html","id":"introduction-to-probability","chapter":"2 Probability","heading":"2.1 Introduction to Probability","text":"way quantifying uncertainty probability. Think statements: “100% certain rain next hour” “50% certain rain next hour”. percentages used reflect degree certainty event happening. first statement reflects certainty; second reflects uncertainty statement implies belief equally likely rain . module, learn basic concepts probability.","code":""},{"path":"probability.html","id":"why-study-probability","chapter":"2 Probability","heading":"2.1.1 Why Study Probability?","text":"book (Section 1.1) lists 10 different applications probability, many applications. go far say anything deals data also deal probability.","code":""},{"path":"probability.html","id":"frequentiest-and-bayesian-view-of-probability","chapter":"2 Probability","heading":"2.1.2 Frequentiest and Bayesian View of Probability","text":"couple main viewpoints interpret probability: frequentist Bayesian. Consider statement “flip fair coin, coin 50% chance landing heads”.frequentist viewpoint views probability relative frequency associated event repeated infinite number times. interpret 50% probability : flip coin many many times, 50% times result coin landing heads.frequentist viewpoint views probability relative frequency associated event repeated infinite number times. interpret 50% probability : flip coin many many times, 50% times result coin landing heads.Bayesian viewpoint views probability measure belief, certainty, event happen. interpret 50% probability : heads tails equally likely occur coin flip.Bayesian viewpoint views probability measure belief, certainty, event happen. interpret 50% probability : heads tails equally likely occur coin flip.coin flip example, interpretations reasonable. However, instances, frequentist interpretation may interpretable repeat event many times. example, earlier statement rain: “50% certain rain next hour”. Whether rain next hour repeatable event, frequentist interpretation makes less sense .","code":""},{"path":"probability.html","id":"key-concepts-in-probability","chapter":"2 Probability","heading":"2.2 Key Concepts in Probability","text":"section, cover basic terminology foundational ideas probability.","code":""},{"path":"probability.html","id":"sample-space","chapter":"2 Probability","heading":"2.2.1 Sample Space","text":"sample space experiment, denoted \\(S\\), set possible outcomes experiment.rest module, use following example: consider standard deck 52 cards, draw one card random. card drawn? sample space experiment can viewed list 52 cards, per Figure 2.1 .\nFigure 2.1: Sample Space Drawing One Card Standard Deck. Picture https://en.wikipedia.org/wiki/Standard_52-card_deck\ndefinition sample space may appear elementary, writing sample space almost always first step performing probability calculations.","code":""},{"path":"probability.html","id":"event","chapter":"2 Probability","heading":"2.2.2 Event","text":"event subset sample space, usually denoted upper case letter. example, let \\(\\) denote event draw card black suit (spades clubs), let \\(B\\) denote event draw picture card (Jack, Queen, King). Events \\(\\) \\(B\\) shown Figures Figure 2.2 Figure 2.3 .\nFigure 2.2: Event \\(\\) (Blue)\n\nFigure 2.3: Event \\(B\\) (gold)\nsample space experiment can finite infinite. card example, sample space finite since can actually write possible outcomes. number possible outcomes infinite (.e. write entire list possible outcomes), sample space infinite.assign probability event. probability event \\(\\) happening \\(P()\\). outcome sample space equally likely finite sample space, probability event number outcomes belonging event divided number outcomes sample space. Using card example, \\(P() = \\frac{26}{52} = \\frac{1}{2}\\) \\(P(B) = \\frac{12}{52} = \\frac{3}{13}\\).","code":""},{"path":"probability.html","id":"complements","chapter":"2 Probability","heading":"2.2.3 Complements","text":"complement event set outcomes belong event. example, complement \\(\\), denoted \\(^c\\), drawing card red suit (hearts diamonds). One way think complements complement event event happening. Loking Figure 2.2, cards outlined blue. example, \\(P(^c) = \\frac{26}{52} = \\frac{1}{2}\\).Thought question: probability drawing non picture card?examples, might realize probability associated complement event can found subtracting probability event 1, .e.\\[\\begin{equation}\nP(^c) = 1 - P().\n\\tag{2.1}\n\\end{equation}\\]Sometimes, calculation probability complement event much less tedious probability event. instance, equation (2.1) useful.","code":""},{"path":"probability.html","id":"unions","chapter":"2 Probability","heading":"2.2.4 Unions","text":"union events least one events happen. example, union events \\(\\) \\(B\\), denoted \\(\\cup B\\), event card drawn either black suit, picture card, black suit picture card. reflected Figure 2.4.\nFigure 2.4: Union , B (blue gold, blue gold)\nfind \\(P(\\cup B)\\), can refer Figure 2.4 just count number outcomes belong either event \\(\\) (black suit) event \\(B\\) (picture card), find \\(\\frac{32}{52}\\).union \\(\\) \\(B\\) can viewed event either event \\(\\) \\(B\\) () happens.","code":""},{"path":"probability.html","id":"intersections","chapter":"2 Probability","heading":"2.2.5 Intersections","text":"intersection events events happen. Using example, intersection events \\(\\) \\(B\\) denoted \\(\\cap B\\), event card drawn black suit picture card. Using Figure 2.4, outcomes belonging \\(\\cap B\\) cards outlined blue gold. probability \\(P(\\cap B) = \\frac{6}{32}\\).","code":""},{"path":"probability.html","id":"addition-rule","chapter":"2 Probability","heading":"2.2.6 Addition rule","text":"common mistake can made calculating \\(P(\\cup B)\\) just add probabilities individual event, mistake say probability \\(\\frac{26}{52} + \\frac{12}{52} = \\frac{38}{52}\\). problem approach outcomes belong events (black picture cards) get counted twice, want count . leads following formula calculating probabilities involving unions two events, sometimes called addition rule probability:\\[\\begin{equation}\nP(\\cup B) = P() + P(B) - P(\\cap B).\n\\tag{2.2}\n\\end{equation}\\]Using equation (2.2), \\(P(\\cup B) = \\frac{26}{52} + \\frac{12}{52} - \\frac{6}{32} = \\frac{32}{52}\\).","code":""},{"path":"probability.html","id":"disjoint-or-mutually-exclusive-events","chapter":"2 Probability","heading":"2.2.7 Disjoint or Mutually Exclusive Events","text":"previous discussion leads idea disjoint, mutually exclusive events. Events disjoint happen simultaneously. card example, events \\(\\) \\(B\\) disjoint, since \\(\\) \\(B\\) can happen simultaneously, since card drawn can black picture card, e.g. draw king spades.Using Figure 2.4 visual example, can see events \\(\\) \\(B\\) disjoint since outcomes blue overlap outcomes gold.Suppose define another event, \\(C\\), denote card drawn Ace. events \\(B\\) \\(C\\) disjoint since card drawn picture card ace. definition disjoint events leads following: events disjoint, probability intersection 0.Using Figure 2.5 visual example, can see events \\(B\\) \\(C\\) disjoint since outcomes gold pink overlap.\nFigure 2.5: Events B, C (gold pink respectively)\nApplying idea equation (2.2), following disjoint events: disjoint events, probability least one event happening sum probabilities event.","code":""},{"path":"probability.html","id":"axioms-of-probability","chapter":"2 Probability","heading":"2.2.8 Axioms of Probability","text":"following called axioms probability, considered foundation properties associated probability:probability event, \\(E\\), non negative, .e. \\(P(E) \\geq 0\\).probability least one outcome sample space occurs 1, .e.\\(P(S) = 1\\).\\(A_1, A_2, \\cdots\\) disjoint events, \\[\nP(\\bigcup\\limits_{=1}^{\\infty} A_{}) = \\sum_{=1}^{\\infty} P(A_i).\n\\]\nwords, disjoint events, probability least one event happens sum individual probabilities.Note: writers list three axioms. book combines first two axioms 1, write two axioms.can easily see equations (2.1) (2.2) can derived axioms. Note equations axioms apply circumstances, regardless whether sample space finite .","code":""},{"path":"probability.html","id":"conditional-probability","chapter":"2 Probability","heading":"2.3 Conditional Probability","text":"concept conditional probability appears almost statistical data science models. statistical models logistic regression, trying use observable data (called predictors, input variables, etc) model probabilities associated different values outcome random (called response variable, output variable, etc). observable data predictive outcome, probabilities associated outcome indicate greater certainty, observable data. Conditional probabilities allows us incorporate observable data, evidence, evaluating uncertainty random outcomes.Consider headed lunch, need decide want bring umbrella (assuming bring umbrella think going rain). working windowless basement internet, high degree uncertainty evaluating rain . However, look outside observe current weather conditions heading , likely higher degree certainty evaluating rain . Conditional probabilities allow us incorporate see prediction random event.use language probability denote example, let \\(R\\) denote event rain go lunch. working windowless basement internet, calculating \\(P(R)\\), probability rain go lunch. able incorporate current weather conditions, probability denoted \\(P(R|data)\\), data denotes current observe weather conditions. \\(P(R|data)\\) can read probability rain go lunch, given observed weather. example, can see \\(P(R)\\) \\(P(R|data)\\) different, since update probability given useful information. Notice \\(|\\) symbol inside probability. symbol implies working conditional probability, given observed information listed \\(|\\).","code":""},{"path":"probability.html","id":"def","chapter":"2 Probability","heading":"2.3.1 Definition","text":"\\(X\\) \\(Y\\) events, \\(P(X)>0\\), conditional probability \\(Y\\) given \\(X\\), denoted \\(P(Y|X)\\), \\[\\begin{equation}\nP(Y|X) = \\frac{P(Y \\cap X)}{P(X)}.\n\\tag{2.3}\n\\end{equation}\\]definition, want update probability \\(Y\\) happening, given observed \\(X\\). \\(X\\) can viewed observable data evidence want incorporate.Bayesian viewpoint probability, \\(P(Y)\\) called prior probability \\(Y\\) since reflects belief \\(Y\\) observing data. \\(P(Y|X)\\) called posterior probability \\(Y\\), reflects update belief $Y incorporating observed data.Let us go back standard deck cards example. Let us find \\(P(B|)\\), probability card picture card, given know card black suit. Visually, can use definition conditional probability using Figure 2.6 .\nFigure 2.6: Events , given B\ntold card black suit, 26 possible outcomes consider, red cards eliminated crossed Figure 2.6. 26 outcomes, many picture cards? probability \\(P(B|)\\) \\(\\frac{6}{26}\\).Figure 2.6 represents frequentist viewpoint conditional probability: \\(P(B|)\\) represents long run proportion picture cards among cards black suits.can also apply equation (2.3): \\(P(B|) = \\frac{\\frac{6}{52}}{\\frac{1}{2}} = \\frac{6}{26}\\) gives answer.Thought question: work probability card drawn black suit, given know card picture card.can see example general \\(P(Y|X) \\neq P(X|Y)\\). informs us need extremely careful writing conditional probabilities interpreting , knowing one matters analysis. example, probability feel unwell given flu close 1, probability flu given feel unwell close 1 (since many things can feel unwell). confusion regarding conditional probabilities sometimes called confusion inverse prosecutor’s fallacy. fallacy wrongly assumes probability fingerprint match given person innocent small, means probability person innocent given fingerprint match must also small. going fallacy detail, need cover concepts.","code":""},{"path":"probability.html","id":"multiplication-rule","chapter":"2 Probability","heading":"2.3.2 Multiplication Rule","text":"equation (2.3), multiplication rule probability\\[\\begin{equation}\nP(Y \\cap X) = P(Y|X) \\times P(X) = P(X|Y) \\times P(Y).\n\\tag{2.4}\n\\end{equation}\\]multiplication rule useful finding probability multiple events happening, aespecially events happen sequentially. example, consider drawing two cards, without replacement, standard deck cards. Without replacement means drawing first card, returned deck, 51 cards remaining first draw. Let \\(D_1\\) \\(D_2\\) denote events first draw diamond suit second draw diamond suit respectively. want find probability cards drawn diamond suits. probability can written \\(P(D_1 \\cap D_2) = P(D_1) \\times P(D_2|D_1)  = \\frac{13}{52} \\times \\frac{12}{51}  = \\frac{156}{2652}\\).","code":""},{"path":"probability.html","id":"independent-events","chapter":"2 Probability","heading":"2.3.3 Independent Events","text":"Events independent knowledge whether one event happens change probability event happening. implies \\(X\\) \\(Y\\) independent events, definition conditional probability simplifies \\(P(Y|X) = P(Y)\\). Likewise \\(P(X|Y) = P(X)\\). Applying multiplication rule, following multiplication rule independent events\\[\\begin{equation}\nP(Y \\cap X) = P(Y) \\times P(X).\n\\tag{2.5}\n\\end{equation}\\]probability events happening just product probabilities individual event, events independent.Going back example standard deck cards, \\(\\) denotes event draw card black suit (spades clubs), \\(B\\) denotes event draw picture card (Jack, Queen, King). earlier found \\(P(B) = \\frac{12}{52}\\) \\(P(B|) = \\frac{6}{26}\\). Notice two probabilities numerically equal, informs us events independent. Knowing whether card black suit change probability card picture card. makes sense intuitively since proportion cars picture black red suits.","code":""},{"path":"probability.html","id":"bayes-rule","chapter":"2 Probability","heading":"2.3.4 Bayes’ Rule","text":"definition conditional probability equation (2.3) multiplication rule equation (2.4) give us Bayes’ rule\\[\\begin{equation}\nP(Y|X) = \\frac{P(X|Y)P(Y)}{P(X)}.\n\\tag{2.6}\n\\end{equation}\\]Bayes’ rule useful want find \\(P(Y|X)\\) information regarding \\(P(X|Y)\\) available. fairly popular model called linear discriminant analysis, models conditional probability using Bayes’ rule.","code":""},{"path":"probability.html","id":"odds","chapter":"2 Probability","heading":"2.3.5 Odds","text":"odds event \\(Y\\) \\[\\begin{equation}\nodds(Y) = \\frac{P(Y)}{P(Y^c)}.\n\\tag{2.7}\n\\end{equation}\\]may realize left hand side equation @ref{eq:odds} equal left hand side logistic regression equation.Using equation (2.7), can switch odds probability easily\\[\\begin{equation}\nP(Y) = \\frac{odds(Y)}{1 + odds(Y)}.\n\\tag{2.8}\n\\end{equation}\\]","code":""},{"path":"probability.html","id":"odds-form-of-bayes-rule","chapter":"2 Probability","heading":"2.3.6 Odds Form of Bayes’ Rule","text":"Using Bayes’ rule equation (2.6) definition odds equation (2.7), odds form Bayes’ rule\\[\\begin{equation}\n\\frac{P(Y|X)}{P(Y^c|X)} = \\frac{P(X|Y)}{P(X|Y^c)} \\frac{P(Y)}{P(Y^c)}.\n\\tag{2.9}\n\\end{equation}\\]","code":""},{"path":"probability.html","id":"law-of-total-probability","chapter":"2 Probability","heading":"2.3.7 Law of Total Probability","text":"Let \\(Y_1, Y_2, \\cdots, Y_n\\) partition sample space (\\(Y_1, Y_2, \\cdots, Y_n\\) disjoint union sample space), \\(P(Y_i) > 0)\\) \\(\\). \\[\\begin{equation}\n\\begin{split}\nP(X) &= \\sum_{=1}^n P(X|Y_i) \\times P(Y_i)\\\\\n    &= P(X|Y_1) \\times P(Y_1) + P(X|Y_2) \\times P(Y_2) + \\cdots + P(X|Y_n) \\times P(Y_n).\n\\end{split}\n\\tag{2.10}\n\\end{equation}\\]law total probability informs us way find probability \\(X\\). can divide sample space disjoint sets \\(Y_i\\), find conditional probability \\(X\\) within set, take weighted sum conditional probabilities, weighted \\(P(Y_i)\\). useful conditional probability set easy obtain.law total probability equation (2.10) can applied denominator Bayes’ rule equation (2.6) following variation Bayes’ rule:\\[\\begin{equation}\nP(Y|X) = \\frac{P(X|Y)P(Y)}{\\sum_{=1}^n P(X|Y_i) \\times P(Y_i)}.\n\\tag{2.11}\n\\end{equation}\\]","code":""},{"path":"probability.html","id":"worked-example","chapter":"2 Probability","heading":"2.3.8 Worked Example","text":"","code":""},{"path":"probability.html","id":"approach-1-using-bayes-rule","chapter":"2 Probability","heading":"2.3.8.1 Approach 1: Using Bayes’ Rule","text":"consider worked example apply Bayes’ rule law total probability. Suppose email can divided three categories: \\(E_1\\) denotes spam email, \\(E_2\\) denotes important email, \\(E_3\\) denotes important email. email must belong one categories. Let \\(F\\) denote event email contains word “free”. past data, following information:\\(P(E_1) = 0.2, P(E_2) = 0.5, P(E_3) = 0.3\\).word “free” appears 99% spam email, \\(P(F|E_1) = 0.99\\).word “free” appears 10% important email, \\(P(F|E_2) = 0.1\\).word “free” appears 5% important email, \\(P(F|E_3) = 0.05\\).receive email word free. probability spam? want find \\(P(E_1|F)\\). Using equation (2.11), \\[\n\\begin{split}\nP(E_1|F) &= \\frac{P(E_1 \\cap F)}{P(F)}\\\\\n&= \\frac{P(F|E_1) \\times P(E_1)}{P(F|E_1) \\times P(E_1) + P(F|E_2) \\times P(E_2) + P(F|E_3) \\times P(E_3)} \\\\\n&= \\frac{0.99 \\times 0.2}{0.99 \\times 0.2 + 0.1 \\times 0.5 + 0.05 \\times 0.3}\\\\\n&= 0.7528517\n\\end{split}\n\\]","code":""},{"path":"probability.html","id":"approach-2-using-tree-diagrams","chapter":"2 Probability","heading":"2.3.8.2 Approach 2: Using Tree Diagrams","text":"tree diagram useful finding conditional probabilities probabilities involving intersections. visual way displaying information hand, conditional probabilities disjoint sets probabilities disjoint set. toy example, disjoint sets type email receive, \\(E_1, E_2, E_3\\), conditional probabilities disjoint sets, .e. \\(P(F|E_1), P(F|E_2)\\) \\(P(F|E_3)\\). can put information visual first splitting sample space disjoint sets \\(E_1, E_2, E_3\\), splitting disjoint set whether email word “free” (\\(F\\)) (\\(F^c\\)). information displayed tree diagram Figure 2.7.\nFigure 2.7: Tree Diagram Email Example\nsplit represented branch, write corresponding probability branch. want find probability received email spam given contains word “free”, \\(P(E_1|F)\\), using definition conditional probability equation (2.3)\\[\nP(E_1|F) = \\frac{P(E_1 \\cap F)}{P(F)}.\n\\]Looking tree diagram Figure 2.7, can label branches lead numerator \\(P(E_1 \\cap F)\\), probability email spam contains word free. shown tree diagram Figure 2.8 highlighting corresponding branches blue.\nFigure 2.8: Tree Diagram Email Example, Branch Numerator Blue\n\\(P(E_1 \\cap F) = 0.2 \\times 0.99 = 0.198\\). need find denominator \\(P(F)\\). Looking Figure 2.7, can see three branches lead email containing word free: \\(P(E_1 \\cap F)\\) \\(P(E_2 \\cap F)\\) \\(P(E_3 \\cap F)\\). shown tree diagram Figure 2.9 highlighting corresponding branches gold.\nFigure 2.9: Tree Diagram Email Example, Branches Denominator Gold\nknow probability branch, add obtain denominator \\(P(F) = 0.2 \\times 0.99 + 0.5 \\times 0.1 + 0.3 \\times 0.05 = 0.263.\\) Putting pieces together, \\[\nP(E_1|F) = \\frac{P(E_1 \\cap F)}{P(F)} = \\frac{0.198}{0.263} = 0.7528517.\n\\]Note: compare intermediate calculations approach, end using calculations approach 1, without referring associated equations.","code":""},{"path":"probability.html","id":"confusion-of-the-inverse","chapter":"2 Probability","heading":"2.4 Confusion of the Inverse","text":"now ready talk prosecutor’s fallacy, confusion inverse, earlier mention section 2.3.1. essence, confusion happens falsely equate \\(P(X|Y)\\) equal \\(P(Y|X)\\). fact, large value \\(P(X|Y)\\) necessarily imply \\(P(Y|X)\\) also large. term prosecutor’s fallacy confusion applied criminal trial, e.g. probability abusive relationship ends murder small, probability abuse relationship ended murder lot higher.go examples based real life.","code":""},{"path":"probability.html","id":"disease-diagnostics","chapter":"2 Probability","heading":"2.4.1 Disease Diagnostics","text":"Suppose testing patient rare disease, estimated prevalent 0.5% people. Suppose medical test disease accurate. can number definitions accuracy. disease diagnostics, couple measures sensitivity, proportion people disease test positive, specificity, proportion people without disease test negative. positive test indicates person disease. Suppose sensitivity specificity high: 0.95 0.9 respectively. Suppose patient tests positive, probability patient actually disease? Assume test always indicates positive negative.example, let \\(D\\) denote event patient disease, let + denote event patient tests positive test, - denote event patient tests negative test. Given information, \\(P(D) = 0.005\\).\\(P(+|D) = 0.95\\).\\(P(-|D^c) = 0.9\\).wish find \\(P(D|+)\\). Using Bayes rule Law Total probability, \\[\n\\begin{split}\nP(D|+) &= \\frac{P(D \\cap +)}{P(+)}\\\\\n&= \\frac{P(+|D) \\times P(D)}{P(+|D) \\times P(D) + P(+|D^c) \\times P(D^c)} \\\\\n&= \\frac{0.95 \\times 0.005}{0.95 \\times 0.005 + 0.1 \\times 0.995 }\\\\\n&= 0.04556355\n\\end{split}\n\\]small probability, patient highly unlikely actually rare disease. test high sensitivity \\(P(+|D) = 0.95\\), imply patient tests positive actually disease, since \\(P(D|+)\\) low. implication rare disease, positive test imply high probability disease, even test accurate.result make sense? Essentially, large proportion small population still numerically much smaller small proportion large population. disease rare, small population people disease, almost detected test. also extremely large population people without disease, even small proportion erroneously test positive still fairly large number. among positive tests, people disease. consider following table based population 20 thousand people.Look first column, shows number people test positive. see large proportion diseased people detected, since relatively people disease, number small, 95. small proportion people disease test positive disease, small proportion large population results relatively larger number, 1990. people test positive, \\(95 + 1990 = 2085\\) actually disease. Therefore \\(P(D|+) = \\frac{95}{2085} = 0.04556355\\).can also explain result Bayes’ viewpoint probability. Without knowing information results test, prior probability \\(P(D) = 0.005\\). However, upon seeing person positive, updated posterior probability \\(P(D|+) = 0.04556355\\), increase 0.005 knew knowing. updated posterior probability 9 times prior. believe person likely disease upon viewing positive test, knew nothing test result. posterior probability still small since value depends two pieces information: prior \\(P(D)\\) sensitivity \\(P(+|D)\\). product values belong numerator calculating \\(P(D|+)\\). denominator \\(P(+|D) \\times P(D) + P(+|D^c) \\times P(D^c)\\). prior \\(P(D)\\) extremely low, \\(P(D^c)\\) extremely close 1, since person either disease disease. \\(P(D)\\) belong extremely low, numerator close 0, value denominator close \\(P(+|D^c) \\times P(D^c)\\), therefore \\(P(D|+)\\) small.Notice talking rare diseases? confusion inverse, thinking high sensitivity implies person likely disease test positive, applies rare diseases. disease prevalent, high sensitivity likely imply person disease test positive.take tests rare diseases? ? go test . turns test positive twice rare disease, probability disease increases lot tested tested positive.perform calculation, use odds form Bayes’ rule, per equation (2.9)\\[\n\\begin{split}\n\\frac{P(D|T_1 \\cap T_2)}{P(D^c|T_1 \\cap T_2)} &= \\frac{P(T_1 \\cap T_2 | D)}{P(T_1 \\cap T_2 | D^c)} \\frac{P(D)}{P(D^c)}\\\\\n&= \\frac{0.95^2}{0.1^2} \\frac{0.005}{0.995} \\\\\n&= 0.4535176\n\\end{split}\n\\]\\(T_1\\) \\(T_2\\) denote events person test positive first test second test respectively. also assume results test independent previous tests.odds disease given person positive twice 0.4535176. Therefore, using equation (2.8), corresponding probability disease given person tested positive twice \\(P(D|T_1 \\cap T_2) = \\frac{0.4535176}{1+0.4535176} = 0.3120138\\). See posterior probability increased two positive tests, 1 positive test.Thought question: perform calculations show posterior probability person disease person tests positive 3 tests 0.8116199.Thought question: notice certain pattern emerging performing calculations person undergoes tests? write either mathematical equation, even function R, allows us quickly compute probability person disease given person tested positive \\(k\\) times, \\(k\\) can denote non negative integer?","code":""},{"path":"probability.html","id":"prosecutors-fallacy","chapter":"2 Probability","heading":"2.4.2 Prosecutor’s Fallacy","text":"confusion inverse also called prosecutor’s fallacy (sometimes also called defense attorney’s fallacy depending side making mistake) occurs legal setting. Generally, confusion comes equating P(evidence|innocent) P(innocent|evidence).book provides discussion Section 2.8, examples 2.8.1 2.8.2.","code":""},{"path":"discrete-random-variables.html","id":"discrete-random-variables","chapter":"3 Discrete Random Variables","heading":"3 Discrete Random Variables","text":"module based Introduction Probability (Blitzstein, Hwang), Chapters 3 4. can access book free https://stat110.hsites.harvard.edu/ (click Book). Please note cover additional topics, skip certain topics book. may skip Sections 3.4, 3.9, Example 4.2.3, Section 4.3, Example 4.4.6, 4.4.7, Theorem 4.4.8, Example 4.4.9, 4.6.4, 4.7.4, 4.7.7, Section 4.9 book.","code":""},{"path":"discrete-random-variables.html","id":"random-variables","chapter":"3 Discrete Random Variables","heading":"3.1 Random Variables","text":"idea behind random variables simplify notation regarding probability, enable us summarize results experiments, make easier quantify uncertainty.","code":""},{"path":"discrete-random-variables.html","id":"example","chapter":"3 Discrete Random Variables","heading":"3.1.1 Example","text":"Consider flipping coin three times recording lands heads tails time. sample space experiment \\(S = \\{HHH, HHT, HTH, THH, HTT, THT, TTH, TTT\\}\\). Given outcome equally likely, probability associated outcome \\(\\frac{1}{8}\\).Suppose want find probability get exactly 2 heads 3 flips. express :\\(P(\\text{two heads three flips})\\), \\(P(HHT \\cup HTH \\cup THH)\\), \\(P()\\) \\(\\) denotes event getting two heads three flips.Another way define random variable \\(X\\) expresses event bit efficiently. Let \\(X\\) denote number heads three flips, another way write \\(P(X=2\\)). idea behind random variables: assign events number.","code":""},{"path":"discrete-random-variables.html","id":"definition","chapter":"3 Discrete Random Variables","heading":"3.1.2 Definition","text":"random variable (RV) function sample space real numbers.convention, denote random variables capital letters. Using 3 coin flip example, \\(X\\) 0, 1, 2, 3. assign number possible outcome sample space.Random variables provide numerical summaries experiment. can useful especially sample space complicated. Random variables can also used non numeric outcomes.","code":""},{"path":"discrete-random-variables.html","id":"discrete-vs-continuous","chapter":"3 Discrete Random Variables","heading":"3.1.3 Discrete Vs Continuous","text":"One key distinctions make random variables determine discrete continuous. way express probabilities random variables depends whether random variable discrete continuous.discrete random variable can take countable (finite infinite) number values.number heads 3 coin flips, \\(X\\) countable finite, since can actually list values can take \\(\\{0,1,2,3 \\}\\) 4 values. \\(X\\) must take one 4 numerical values; number outside list. discrete.random variable countable infinite can list values can take, list end. example, number people using crosswalk 10 year period take values \\(\\{0, 1, 2, 3, \\cdots \\}\\). number take infinite number values, values whole numbers occur. number people using crosswalk 10 year period discrete random variable.continuous random variable can take uncountable number values interval real numbers.example, height American adult continuous random variable, height can take value interval interval, say 40 100 inches. values 40 100 possible.module, focus discrete random variables.support discrete random variable \\(X\\) set values \\(X\\) can take \\(P(X = x) > 0\\), .e. set values non zero probability happening. Using 3 coin flips example, \\(X\\) number heads 3 coin slips, support \\(\\{0,1,2,3 \\}\\). Usually, support discrete random variables integers.Thought question: Can come examples discrete continuous random variables ? Feel free search internet examples well.","code":""},{"path":"discrete-random-variables.html","id":"probability-mass-functions-pmfs","chapter":"3 Discrete Random Variables","heading":"3.2 Probability Mass Functions (PMFs)","text":"use probability describe behavior random variables. called distribution random variable. specifies probabilities events associated random variable. example, probability obtaining 3 heads 3 coin flips, probability obtaining least one head 3 coin flips?discrete random variables, distribution specified probability mass function (PMF). PMF discrete random variable \\(X\\) function \\(P_X(x) = P(X=x)\\). positive \\(x\\) support \\(X\\), 0 otherwise.Note: notation random variables, capital letters \\(X\\) denote random variables, lower case letters \\(x\\) denote actual numerical values. want find probability 2 heads 3 coin flips, write \\(P(X=2)\\), \\(x\\) 2 example.Going back example record number heads 3 coin flips, can write PMF random variable \\(X\\):\\(P_X(0) = P(X=0) = P(TTT) = \\frac{1}{8}\\),\\(P_X(1) = P(X=1) = P(HTT \\cup THT \\cup TTH) = \\frac{3}{8}\\),\\(P_X(2) = P(X=2) = P(HHT \\cup THH \\cup HTH) = \\frac{3}{8}\\),\\(P_X(3) = P(X=3) = P(HHH) = \\frac{1}{8}\\).Fairly often, PMF discrete random variable presented simple table like Table 3.1 :Table 3.1: PMF XOr use simple plot like one Figure 3.1:\nFigure 3.1: PMF X\nPMF provides list possible values random variable corresponding probabilities. words, PMF describes distribution relative frequencies outcome. experiment, observing 1 2 heads equally likely, occur three times often observing 0 3 heads. Observing 0 3 heads also equally likely.","code":"\n##support\nx<-0:3\n## PMF for each value in the support. \nPMFs<-c(1/8, 3/8, 3/8, 1/8)\n## create plot of PMF vs each value in support\nplot(x, PMFs, type=\"h\", main = \"PMF for X\", xlab=\"# of heads\", ylab=\"Probability\", ylim=c(0,1))"},{"path":"discrete-random-variables.html","id":"valid-pmfs","chapter":"3 Discrete Random Variables","heading":"3.2.1 Valid PMFs","text":"Consider discrete random variable \\(X\\) support \\(x_1, x_2, \\cdots\\). PMF \\(P_X(x)\\) \\(X\\) must satisfy:\\(P_X(x) > 0\\) \\(x = x_j\\), \\(P_X(x) = 0\\) otherwise.\\(\\sum_{j=1}^{\\infty} P_X(x_j) = 1\\).words, probabilities associated support greater 0, sum probabilities across whole support must add 1.Thought question: based Table 3.1, can see PMF \\(X\\) valid?","code":""},{"path":"discrete-random-variables.html","id":"pmfhist","chapter":"3 Discrete Random Variables","heading":"3.2.2 PMFs and Histograms","text":"Recall frequentist viewpoint probability, represents relative frequency associated event repeated infinite number times.Consider experiment flip coin 3 times count number heads. support random variable \\(X\\), number heads, \\(\\{0,1,2,3 \\}\\). Imagine performing experiment large number times. time perform experiment, record number heads. performed experiment one million times, recorded one million values number heads, value must support \\(X\\). create histogram one million values number heads, shape histogram close shape plot PMF Figure 3.1. Figure 3.2 shows resulting histogram performing experiment 1 million times.\nFigure 3.2: Histogram Experiment Performed 1 Million Times\ngeneral, PMF experiment match histogram long run.Note: just done use simulations repeat experiment large number times using code.","code":""},{"path":"discrete-random-variables.html","id":"cumulative-distribution-functions-cdfs","chapter":"3 Discrete Random Variables","heading":"3.3 Cumulative Distribution Functions (CDFs)","text":"Another function used describe distribution discrete random variables cumulative distribution function (CDF). CDF random variable \\(X\\) \\(F_X(x) = P(X \\leq x)\\). Notice unlike PMF, definition CDF applies discrete continuous random variables.Going back example record number heads 3 coin flips, can write CDF random variable \\(X\\):\\(F_X(0) = P(X \\leq 0) = P(X=0) = \\frac{1}{8}\\),\\(F_X(1) = P(X \\leq 1) = P(X=0) + P(X=1) = \\frac{1}{8} +  \\frac{3}{8} = \\frac{1}{2}\\),\\(F_X(2) = P(X \\leq 2) = P(X=0) + P(X=1) + P(X=2) = \\frac{1}{2} + \\frac{3}{8} = \\frac{7}{8}\\),\\(F_X(3) = P(X \\leq 3) = P(X=0) + P(X=1) + P(X=2) + P(X=3) = \\frac{7}{8} + \\frac{1}{8} = 1\\).Notice calculations based PMF. find \\(P(X \\leq x)\\), summed PDF values support less equal \\(x\\). Therefore, another way write CDF discrete random variable \\[\\begin{equation}\nF_X(x) = P(X \\leq x) = \\sum_{x_j \\leq x} P(X=x_j).\n\\tag{3.1}\n\\end{equation}\\]Fairly often, CDF discrete random variable presented simple table like Table 3.2 :Table 3.2: CDF XOr simple plot like Figure 3.3 :\nFigure 3.3: CDF X\nCDF discrete random variables always look like step function, increases discrete jumps value support. height jump corresponds PMF value support.Thought question: see similarities CDF empirical cumulative density function (ECDF) section 1.3.3?","code":""},{"path":"discrete-random-variables.html","id":"valid-cdfs","chapter":"3 Discrete Random Variables","heading":"3.3.1 Valid CDFs","text":"CDF \\(F_X(x)\\) \\(X\\) must:non decreasing. means \\(x\\) gets larger, CDF either stays increases. Visually, graph CDF never decreases \\(x\\) increases.approach 1 \\(x\\) approaches infinity approach 0 \\(x\\) approaches negative infinity. Visually, graph CDF equal close 1 large values x, equal close 0 small values x.Thought question: Look CDF example Figure 3.3, see satisfies criteria listed valid CDF.","code":""},{"path":"discrete-random-variables.html","id":"expectations","chapter":"3 Discrete Random Variables","heading":"3.4 Expectations","text":"previous section, see PMFs CDFs can used describe distribution random variable. PMF can viewed long-run version histogram, gives us idea shape distribution. Similar Section 1, also interested measures centrality spread random variables.measure centrality random variables expectation. expectation random variable can interpreted long-run mean random variable, .e. able repeat experiment infinite number times, expectation random variable average result among experiments.discrete random variable \\(X\\) support \\(x_1, x_2, \\cdots,\\), expected value, denoted \\(E(X)\\), \\[\\begin{equation}\nE(X) = \\sum_{j=1}^{\\infty} x_j P(X=x_j).\n\\tag{3.2}\n\\end{equation}\\]can use Table 3.1 example. find expected number heads 3 coin flips, using equation (3.2),\\[\n\\begin{split}\nE(X) &= 0 \\times \\frac{1}{8} + 1 \\times \\frac{3}{8} + 2 \\times \\frac{3}{8} + 3 \\times \\frac{1}{8}\\\\\n       &= 1.5\n\\end{split}\n\\]take product value support random variable corresponding probability, add products.can see another interpretation expected value random variable calculation: weighted average values random variable, weighted probabilities.Intuitively, expected value 1.5 make sense. flip coin 3 times, coin fair, expect half flips land heads, 1.5 flips land heads.","code":""},{"path":"discrete-random-variables.html","id":"linearity-of-expectations","chapter":"3 Discrete Random Variables","heading":"3.4.1 Linearity of Expectations","text":"seen calculate expected value random variable \\(X\\) using equation (3.2). need PMF \\(X\\). Sometimes random variable can viewed sum (difference) random variables, involve multiplication / adding constant random variable. Consider scenarios:Suppose friend fisherman. Let \\(Y\\) random variable describing number fish catch workday, let \\(W\\) random variable describing number fish friend catches workday. can let \\(T = Y+W\\) random variable describing total number fish catch workday.Suppose friend fisherman. Let \\(Y\\) random variable describing number fish catch workday, let \\(W\\) random variable describing number fish friend catches workday. can let \\(T = Y+W\\) random variable describing total number fish catch workday.Suppose sell fish $10 friend sells fish $15. can let \\(R = 10Y + 15W\\) random variable describes revenue generate workday.Suppose sell fish $10 friend sells fish $15. can let \\(R = 10Y + 15W\\) random variable describes revenue generate workday.Suppose friend rent space market sell fish, costs \\5 day rent space. can let \\(G = 10Y + 15W - 5\\) random variable describes gross income day.Suppose friend rent space market sell fish, costs \\5 day rent space. can let \\(G = 10Y + 15W - 5\\) random variable describes gross income day.examples involve new random variables, \\(T, R, G\\) can based previously defined random variables, \\(Y, W\\). turns find expectations new random variables, need expectations previously defined random variables. need find PMFS \\(T, R\\) \\(S\\) apply equation (3.2).can done linearity expectations: Let \\(X\\) \\(Y\\) denote random variables, \\(,b,c\\) denote constants, \\[\\begin{equation}\nE(aX + +c) = aE(X) + (Y) + c.\n\\tag{3.3}\n\\end{equation}\\]Applying equation (3.3) fishing examples:\\(E(T) = E(Y) + E(W)\\),\\(E(R) = 10E(Y) + 15E(W)\\),\\(E(G) = 10E(Y) + 15E(W) - 5\\).need find expected values total number fish, revenue generated, gross income expected values number fish us caught. need PMFs \\(T,R,G\\).","code":""},{"path":"discrete-random-variables.html","id":"visual-explanation","chapter":"3 Discrete Random Variables","heading":"3.4.1.1 Visual Explanation","text":"visual explanation equation (3.3) makes sense, go back previous example \\(X\\) denotes number heads 3 coin flips. Figure 3.1 displays PMF random variable. Let us create PMF new random variable \\(Y=2X\\) display Figure 3.4 :\nFigure 3.4: PMF X Y=2X\nNote red vertical lines represent expected value random variable, since PMFs symmetric, expected value lies right middle support. Comparing PMFs Figure 3.4, get \\(Y\\) multiplying \\(X\\) 2. support \\(Y\\) now \\(\\{0,2,4,6\\}\\) associated probabilities unchanged, heights probabilities vertical axis unchanged. Therefore, center, expected value, multiplied constant.Consider another random variable \\(W = X+3\\). create PMF \\(W\\) display Figure 3.5 :\nFigure 3.5: PMF X W=X+3\nNotice PMFs \\(X\\) \\(W\\) look almost exactly . difference every value support \\(X\\) shifted 3 units. probabilities stay , heights PMFs unchanged. every value shifted 3 units, expected value, long-run average, also gets shifted 3 units. Adding constant random variable shifts expected value accordingly.","code":"\n##support of X\nx<-0:3\n## PMF for each value in the support. \nPMFs<-c(1/8, 3/8, 3/8, 1/8)\nEX<-1.5\n\n##support of Y\ny<-2*x\n## PMF for each value in the support. \nPMFs<-c(1/8, 3/8, 3/8, 1/8)\nEY<-2*EX\n\npar(mfrow=c(2,1))\n\n## create plot of PMF vs each value in support\nplot(x, PMFs, type=\"h\", main = \"PMF for X\", xlab=\"X\", ylab=\"Probability\", ylim=c(0,1))\n##overlay a line representing EX in red\nabline(v=EX, col=\"red\")\n\n## create plot of PMF vs each value in support\nplot(y, PMFs, type=\"h\", main = \"PMF for Y\", xlab=\"Y\", ylab=\"Probability\", ylim=c(0,1))\n##overlay a line representing EY in red\nabline(v=EY, col=\"red\")\n##support of X\nx<-0:3\n## PMF for each value in the support. \nPMFs<-c(1/8, 3/8, 3/8, 1/8)\nEX<-1.5\n\n##support of W\nw<-x+3\n## PMF for each value in the support. \nPMFs<-c(1/8, 3/8, 3/8, 1/8)\nEW<-EX+3\n\npar(mfrow=c(2,1))\n\n## create plot of PMF vs each value in support\nplot(x, PMFs, type=\"h\", main = \"PMF for X\", xlab=\"X\", ylab=\"Probability\", ylim=c(0,1))\n##overlay a line representing EX in red\nabline(v=EX, col=\"red\")\n\n## create plot of PMF vs each value in support\nplot(w, PMFs, type=\"h\", main = \"PMF for w\", xlab=\"W\", ylab=\"Probability\", ylim=c(0,1))\n##overlay a line representing EW in red\nabline(v=EW, col=\"red\")"},{"path":"discrete-random-variables.html","id":"one-more-example","chapter":"3 Discrete Random Variables","heading":"3.4.1.2 One More Example","text":"look one example illustrate usefulness linearity expectations. Consider drunk man walk one-dimensional number line starts 0 position. step drunk man takes, either moves forward, backward, stays spot. steps forward probability \\(p_f\\), backward probability \\(p_b\\), stays spot probability \\(p_s\\), \\(p_f + p_b+p_s = 1\\). Let \\(Y\\) position number line drunk man 2 steps. expected position drunk man two steps, .e. \\(E(Y)\\)? Assume step independent.Using brute force, can find PMF \\(Y\\), find \\(E(Y)\\) using equation (3.2). First, need find sample space \\(Y\\). two steps, sample space \\(\\{-2,-1,0,1,2\\}\\). Next, need find probabilities associated outcome sample space.\\(Y=-2\\), man must move backward step. probability \\(P(Y=-2) = p_b^2\\).Likewise, \\(Y=2\\), man must move forward step. probability \\(P(Y=2) = p_f^2\\).\\(Y=-1\\), man stay first step, move back second, move back first step, stay second. probability \\(P(Y=-1) = p_s p_b + p_b p_s = 2p_b p_s\\).Likewise, \\(Y=1\\), man stay first step, move forward second, move forward first step, stay second. probability \\(P(Y=1) = p_s p_f + p_f p_s = 2p_f p_s\\).\\(Y=0\\), man move forward, backward, move backward forward, stay steps. \\(P(Y=0) = p_f p_b + p_b p_f + p_s^2 = p_s^2 + 2 p_b p_f\\).Using equation @ref(eq:3_EX),\\[\n\\begin{split}\nE(Y) &= -2 \\times p_b^2 + -1 \\times 2p_b p_s + 0 \\times p_s^2 + 2 p_b p_f + 1 \\times 2p_f p_s + 2 \\times p_f^2 \\\\\n       &= 2 (p_f - p_b)\n\\end{split}\n\\]Note: skipped lot messy algebra get end result. Even skipping step, setting PMF quite bit work.Suppose use linearity expectations equation (3.3). Let \\(Y_1, Y_2\\) denote distance man moves step 1 2 respectively. \\(Y = Y_1 + Y_2\\). sample \\(Y_1\\) \\(Y_2\\) : \\(\\{-1,0,1\\}\\). \\(Y_1\\) \\(Y_2\\) following PMF:\\(P(Y_i = -1) = p_b\\)\\(P(Y_i = 0) = p_s\\)\\(P(Y_i = 1) = p_f\\)using equation (3.2),\\[\n\\begin{split}\nE(Y_i) &= -1 \\times p_b + 0 \\times p_s + 1 \\times p_f \\\\\n       &= p_f - p_b\n\\end{split}\n\\]therefore \\(E(Y) = E(Y_1) + E(Y_2) = 2(p_f - p_b)\\). Notice much simpler solution becomes using linearity expectations? Imagine wanted find expected position 500 steps? Writing sample space 500 steps extremely long.","code":""},{"path":"discrete-random-variables.html","id":"law-of-the-unconscious-statistician","chapter":"3 Discrete Random Variables","heading":"3.4.2 Law of the Unconscious Statistician","text":"Suppose PMF random variable \\(X\\), want find \\(E(g(X))\\), \\(g\\) function \\(X\\) (can think \\(g\\) transformation performed \\(X\\)). One idea find PMF \\(g(X)\\) use definition expectation equation (3.2). seen previous subsection finding sample space transforming random variable can challenging. turns can find \\(E(g(X))\\) based PMF \\(X\\), without find PMF \\(g(X)\\).done Law Unconscious Statistician (LOTUS). Let \\(X\\) discrete random variable support \\(\\{x_1, x_2, \\cdots \\}\\), \\(g\\) function applied \\(X\\), \\[\\begin{equation}\nE(g(X)) = \\sum_{=j}^{\\infty} g(x_j) P(X=x_j).\n\\tag{3.4}\n\\end{equation}\\]application LOTUS finding variance discrete random variable.","code":""},{"path":"discrete-random-variables.html","id":"variance","chapter":"3 Discrete Random Variables","heading":"3.4.3 Variance","text":"talked shape distribution discrete random variable, expected value. One measure interested spread associated distribution. One common measure variance random variable.variance random variable \\(X\\) \\[\\begin{equation}\nVar(X) = E[(X - EX)^2]\n\\tag{3.5}\n\\end{equation}\\]standard deviation random variable \\(X\\) squareroot variance\\[\\begin{equation}\nSD(X) = \\sqrt{Var(X)}.\n\\tag{3.6}\n\\end{equation}\\]Looking equation (3.5) little closely, can see natural interpretation variance random variable: average squared distance random variable mean, long-run. equivalent formula variance random variable \\[\\begin{equation}\nVar(X) = E(X^2) - (EX)^2.\n\\tag{3.7}\n\\end{equation}\\]Equation (3.7) easier work equation (3.5) performing actual calculations.Let us now go back original example, \\(X\\) denotes number heads 3 coin flips. Earlier, found PMF random variable, per Table 3.1, found expectation 1.5. find variance \\(X\\) using equation (3.7), find \\(E(X^2)\\) first using LOTUS equation (3.4)\\[\n\\begin{split}\nE(X^2) &= 0^2 \\times \\frac{1}{8} + 1^2 \\times \\frac{3}{8} + 2^2 \\times \\frac{3}{8} + 3^2 \\times \\frac{1}{8} \\\\\n       &= 3\n\\end{split}\n\\]\\(Var(x) = 3 - 1.5^2 = \\frac{3}{4}\\).Thought question: Try find \\(Var(X)\\) using equation (3.5) LOTUS. arrive answer steps may bit complicated.","code":""},{"path":"discrete-random-variables.html","id":"var-prop","chapter":"3 Discrete Random Variables","heading":"3.4.3.1 Properties of Variance","text":"Variance following properties:\\(Var(X+c) = Var(X)\\), \\(c\\) constant. make sense, since add constant random variable, shift \\(c\\) units. shown earlier Figure 3.5, expected value also gets shifted \\(c\\) units. Variance measures average squared distance variable mean. distance, squared distance, \\(X\\) mean unchanged.\\(Var(cX) = c^2 Var(X)\\). Look Figure 3.4, notice distance value support expected value gets multiplied 2 (since \\(Y=2X\\)). multiply random variable \\(c\\), distance value support expected value multiplied \\(c\\). Since variance measures squared distance, variance gets multiplied \\(c^2\\).\\(X\\) \\(Y\\) independent random variables, \\(Var(X+Y) = Var(X) + Var(Y)\\).","code":""},{"path":"discrete-random-variables.html","id":"common-discrete-random-variables","chapter":"3 Discrete Random Variables","heading":"3.5 Common Discrete Random Variables","text":"Next, introduce commonly used distributions may used discrete random variables. number common statistical models (example, logistic regression, Poisson regression) based distributions.","code":""},{"path":"discrete-random-variables.html","id":"bernoulli","chapter":"3 Discrete Random Variables","heading":"3.5.1 Bernoulli","text":"Bernoulli distribution might simplest discrete random variable. support random variable \\(\\{0,1\\}\\). words, value random variable follows Bernoulli distribution either 0 1. Bernoulli distribution also described parameter \\(p\\), probability random variable takes value 1.formally, random variable \\(X\\) follows Bernoulli distribution parameter \\(p\\) \\(P(X=1) = p\\) \\(P(X=0) = 1-p\\), \\(0<p<1\\). Using mathematical notation, can write \\(X \\sim Bern(p)\\) express random variable \\(X\\) distributed Bernoulli parameter \\(p\\). PMF Bernoulli distribution written \\[\\begin{equation}\nP(X=k) = p^k (1-p)^{1-k}\n\\tag{3.8}\n\\end{equation}\\]\\(k=0, 1\\).enough specify random variable follows Bernoulli distribution. need also clearly specify value parameter \\(p\\). Consider following two examples describe two different experiments:Suppose flip fair coin . Let \\(Y=1\\) coin lands heads, \\(Y=0\\) coin lands tails. \\(Y \\sim Bern(\\frac{1}{2})\\) example since coin fair.Suppose flip fair coin . Let \\(Y=1\\) coin lands heads, \\(Y=0\\) coin lands tails. \\(Y \\sim Bern(\\frac{1}{2})\\) example since coin fair.Suppose asked question given 5 multiple choices, 1 correct answer. idea topic, multiple choices help, guess. Let \\(W=1\\) answer correctly, \\(W=0\\) answer incorrectly. \\(W \\sim Bern(\\frac{1}{5})\\).Suppose asked question given 5 multiple choices, 1 correct answer. idea topic, multiple choices help, guess. Let \\(W=1\\) answer correctly, \\(W=0\\) answer incorrectly. \\(W \\sim Bern(\\frac{1}{5})\\).\\(P(Y=1)\\) \\(P(W=1)\\) examples.Fairly often, Bernoulli random variable, event results random variable coded 1 called success, event results random variable coded 0 called failure. setting, parameter \\(p\\) called success probability Bernoulli distribution. experiment Bernoulli distribution can called Bernoulli trial.go back second example section 0.1.2, modeling whether job applicant receives callback . example, let \\(V\\) random variable applicant receives callback, \\(V=1\\) denoting applicant received callback, \\(V=0\\) applicant receive callback. used logistic regression example. turns logistic regression used variable interest follows Bernoulli distribution.","code":""},{"path":"discrete-random-variables.html","id":"properties-of-bernoulli","chapter":"3 Discrete Random Variables","heading":"3.5.1.1 Properties of Bernoulli","text":"Consider \\(X\\) Bernoulli distribution parameter \\(p\\). expectation Bernoulli distribution \\[\\begin{equation}\nE(X) = p\n\\tag{3.9}\n\\end{equation}\\]variance \\[\\begin{equation}\nVar(X) = p(1-p).\n\\tag{3.10}\n\\end{equation}\\]Thought question: Use definition expectations discrete random variables, equation (3.2), PMF Bernoulli random variable, LOTUS prove equations (3.9) (3.10).expected value equal \\(p\\) Bernoulli distribution make sense. Remember support random variable 0 1, \\(P(X=1) = p\\). Using frequentist viewpoint, flip coin record heads tails, repeat coin flipping many times, record bunch 0s 1s represent result coin flips. average bunch 0s 1s just proportion 1s.equation variance Bernoulli distribution exhibits interesting intuitive behavior. Figure 3.6 shows variance behaves vary value \\(p\\):\nFigure 3.6: Variance Bernoulli\nNotice variance maximum \\(p=0.5\\), variance minimum (fact 0) \\(p=0\\) \\(p=1\\). biased coin always lands heads, every coin flip land heads exception. variability result, utmost certainty result coin flip. hand, coin fair \\(p=0.5\\), least certainty result coin flip, variance maximum coin fair.Another application property election results (assuming 2 candidates, idea applies candidates). swing states race closer (\\(p\\) closer half), projections winner uncertainty need get data wait longer projections. states primarily vote one candidate (\\(p\\) closer 0 1), projections happen lot quicker projections less uncertainty.","code":"\np<-seq(0,1,by = 0.001) ##sequence of values for p\nBern_var<-p*(1-p) ##variance of Bernoulli\n##plot variance against p\nplot(p, Bern_var, ylab=\"Variance\", main=\"Variance of Bernoulli as p is Varied\")"},{"path":"discrete-random-variables.html","id":"binomial","chapter":"3 Discrete Random Variables","heading":"3.5.2 Binomial","text":"Suppose experiment follows Bernoulli distribution, perform experiment \\(n\\) times (sometimes called trials), time success probability \\(p\\). experiments independent . Let \\(X\\) denote number successes \\(n\\) trials. \\(X\\) follows binomial distribution parameters \\(n\\) \\(p\\) (number trials success probability). write \\(X \\sim Bin(n,p)\\) express \\(X\\) follows binomial distribution parameters \\(n\\) \\(p\\), \\(n>0\\) \\(0<p<1\\). PMF Binomial distribution written \\[\\begin{equation}\nP(X=k) = \\binom{n}{k} p^k (1-p)^{n-k}\n\\tag{3.11}\n\\end{equation}\\]\\(k=0,1,2, \\cdots, n\\), also support binomial distribution.equation (3.11), \\(\\binom{n}{k}\\) called binomial coefficient, number represents number combinations result \\(k\\) successes \\(n\\) trials. binomial coefficient can found using\\[\\begin{equation}\n\\binom{n}{k} =  \\frac{n!}{k! (n-k)!}.\n\\tag{3.12}\n\\end{equation}\\]\\(n!\\) called n-factorial, product positive integers less equal n. \\(n! = n \\times (n-1) \\times (n-2) \\times \\cdots \\times 1.\\) example \\(5! = 5 \\times 4 \\times 3 \\times 2 \\times 1 = 120\\), using R:Note: fairly common model, logistic regression model aggregated data, based binomial distribution. mentioned logistic regression earlier. difference two (without aggregated data) based structure data frame. interested differences, please read https://www.r-bloggers.com/2021/02/--run-logistic-regression--aggregate-data--r/.go back first example counting number heads three coin flips follows binomial distribution.coin flip either heads tails. two outcomes flip.success probability, probability heads, 0.5 flip. parameter fixed flip.result flip independent flips since flips affect outcome.number trials (flips example) \\(n=3\\) specified fixed value.Since four conditions met, number heads 3 coin flips can modeled using binomial distribution. let \\(x\\) denote number heads 3 coin clips, write \\(X \\sim Bin(3,0.5)\\).Suppose want calculate \\(P(X=2)\\) using equation (3.11):\\[\n\\begin{split}\nP(X=2) &= \\binom{3}{2} (0.5)^2 (0.5)^1\\\\\n       &= \\frac{3!}{2! 1!} (0.5)^2 (0.5)^1 \\\\\n       &= 3 \\times \\frac{1}{8} \\\\\n       &= \\frac{3}{8}.\n\\end{split}\n\\]example, binomial coefficient equals 3. indicates 3 combinations obtain 2 heads 3 coin flips. \\(P(X=2)\\) can written \\(P(HHT \\cup HTH \\cup THH)\\). Solving \\(P(HHT \\cup HTH \\cup THH)\\), \\[\n\\begin{split}\nP(HHT \\cup HTH \\cup THH) &= P(HHT) + P(HTH) + P(THH)\\\\\n       &= 0.5^3 + 0.5^3 + 0.5^3 \\\\\n       &= 3 \\times \\frac{1}{8} \\\\\n       &= \\frac{3}{8}.\n\\end{split}\n\\]\nsolved using basic probability rules previous module, without using PMF binomial distribution equation (3.11). course, PMF binomial distribution gets lot convenient \\(n\\) gets larger, number combinations sample space get lot larger.can also use R find \\(P(X=2)\\):","code":"\nfactorial(5)## [1] 120\ndbinom(2,3,0.5) ##specify values of k, n, p in this order## [1] 0.375"},{"path":"discrete-random-variables.html","id":"relationship-between-binomial-and-bernoulli","chapter":"3 Discrete Random Variables","heading":"3.5.2.1 Relationship Between Binomial and Bernoulli","text":"Looking description Bernoulli binomial distributions, may notice Bernoulli random variable special case binomial random variable \\(n=1\\), .e. 1 trial.binomial random variable also sometimes viewed sum \\(n\\) independent Bernoulli random variables, value \\(p\\).","code":""},{"path":"discrete-random-variables.html","id":"properties-of-binomial","chapter":"3 Discrete Random Variables","heading":"3.5.2.2 Properties of Binomial","text":"\\(X \\sim Bin(n,p)\\), \\[\\begin{equation}\nE(X) = np\n\\tag{3.13}\n\\end{equation}\\]\\[\\begin{equation}\nVar(X) = np(1-p).\n\\tag{3.14}\n\\end{equation}\\]results make sense note relationship binomial random variable Bernoulli random variable. Suppose random variables \\(Y_1, Y_2, \\cdots, Y_n\\) Bernoulli random variables parameter \\(p\\) independent. \\(Y = Y_1 + Y_2 + \\cdots + Y_n \\sim Bin(n,p)\\). Therefore, using linearity expectations equation (3.3), \\(E(Y) = E(Y_1) + E(Y_2) + \\cdots + E(Y_n) = np\\). Since \\(Y_1, Y_2, \\cdots, Y_n\\) independent, \\(Var(Y) = Var(Y_1) + Var(Y_2) + \\cdots + Var(Y_n) = np(1-p)\\).","code":""},{"path":"discrete-random-variables.html","id":"pmfs-of-binomial","chapter":"3 Discrete Random Variables","heading":"3.5.2.3 PMFs of Binomial","text":"take look PMFs binomials, \\(n=10\\) vary \\(p\\) 0.2, 0.5, 0.9, Figure 3.7:\nFigure 3.7: PMF X, n=10, p varied\nfigure 3.7, can see distribution binomial symmetric \\(p=0.5\\), middle values \\(k\\) higher probabilities, probabilities decrease go away middle. \\(p \\neq 0.5\\), see distribution gets skewed. success probability small, smaller number successes likelier, success probability large, larger number successes likelier, intuitive. probability success small, expect outcomes failures.","code":""},{"path":"discrete-random-variables.html","id":"poisson","chapter":"3 Discrete Random Variables","heading":"3.5.3 Poisson","text":"One common distribution used discrete random variables Poisson distribution. often used variable interest call count data (support non negative integers), example, number cars cross intersection day.random variable \\(X\\) follows Poisson distribution parameter \\(\\lambda\\), \\(\\lambda>0\\). Using mathematical notation, can write \\(X \\sim Pois(\\lambda)\\) express random variable \\(X\\) distributed Poisson parameter \\(p\\). PMF Poisson distribution written \\[\\begin{equation}\nP(X=k) = \\frac{e^{-\\lambda}\\lambda^k}{k!}\n\\tag{3.15}\n\\end{equation}\\]\\(k=0,1,2,\\cdots\\). \\(\\lambda\\) sometimes called rate parameter, related rate arrivals, example, number cross intersection period time.","code":""},{"path":"discrete-random-variables.html","id":"properties-of-poisson","chapter":"3 Discrete Random Variables","heading":"3.5.3.1 Properties of Poisson","text":"\\(X \\sim Pois(\\lambda)\\), \\[\\begin{equation}\nE(X) = \\lambda\n\\tag{3.16}\n\\end{equation}\\]\\[\\begin{equation}\nVar(X) = \\lambda.\n\\tag{3.17}\n\\end{equation}\\]imply larger values Poisson random variable associated larger variances. common feature count data. Consider number cars cross intersection one-hour time period. Consider average number cars rush hour, say 5 6pm. average number large, number lot smaller due inclement weather, number get lot larger convention occurring nearby. hand, consider average number cars 3 4am. average number small, likely small time, regardless weather conditions whether special events happening.Another interesting property Poisson distribution skewed \\(\\lambda\\) small, approaches bell-shaped distribution \\(\\lambda\\) gets bigger. Figure 3.8 displays density plots Poisson distributions \\(\\lambda\\) varied:\nFigure 3.8: PMF Poissons Rate Parameter Varied\n","code":"\n##calculate probability of Poisson with these values on the support\nx<-0:20\nlambda<-c(0.5, 1, 4, 10) ##try 4 different values of lambda\n\n##create PMFs of these 4 Poissons with different lambdas\npar(mfrow=c(2,2))\nfor (i in 1:4)\n  \n{\n  dens<-dpois(x,lambda[i])\n  plot(x, dens, type=\"l\", main=paste(\"Lambda is\", lambda[i]))\n}"},{"path":"discrete-random-variables.html","id":"poisson-approximation-to-binomial","chapter":"3 Discrete Random Variables","heading":"3.5.3.2 Poisson Approximation to Binomial","text":"\\(X \\sim Bin(n,p)\\), \\(n\\) large \\(p\\) small, PMF \\(X\\) can approximated Poisson distribution rate parameter \\(\\lambda = np\\). words, approximation works better \\(n\\) gets larger \\(np\\) gets smaller.several rules thumbs exist guide large \\(n\\) small \\(np\\) . National Institute Standards Technology (NIST) suggest \\(n \\geq 20\\) \\(p \\leq 0.05\\), \\(n \\geq 100\\), \\(np \\leq 10\\).One main using approximation, instead directly using binomial distribution, binomial coefficient can become computationally expensive compute \\(n\\) large.Consider example: company manufactures computer chips, 2 percent chips defective. quality control manager randomly samples 100 chips coming assembly line. probability 3 chips defective?Let \\(Y\\) denote number chips defective 100 chips.chip either defective . two outcomes chip.“success” probability 0.02 chip. probability assumed fixed chip.assume chip independent.number chips fixed \\(n=100\\).can model \\(Y \\sim Bin(100,0.02)\\), long assume chips independent. find \\(P(Y \\leq 3)\\), can:use binomial distribution, orapproximate using \\(Pois(2)\\), \\(\\lambda = np = 100 \\times 0.02\\).Notice values close .","code":"\n##set up binomial\nn<-100 \np<-0.02 \ny<-0:3 ##we want P(Y=0), P(Y=1), P(Y=2), P(Y=3)\nsum(dbinom(y,n,p))## [1] 0.8589616\n##Use Poisson to approx binomial\nlambda<-n*p\nsum(dpois(y,lambda))## [1] 0.8571235"},{"path":"discrete-random-variables.html","id":"Rdis","chapter":"3 Discrete Random Variables","heading":"3.6 Using R","text":"R built functions compute PMF, CDF, percentiles, well simulate data common distributions. start using random variable \\(Y\\) follows binomial distribution, \\(n=5, p = 0.3\\) example first. Note example support \\(Y\\) \\(\\{0,1,2,3,4,5 \\}\\).find \\(P(Y=2)\\), use:probability \\(Y\\) equal 2 0.3087.find \\(P(Y \\leq 2)\\), use:probability \\(Y\\) less equal 2 0.83692.find value support corresponds median (50th percentile), use:median binomial distribution 5 trials success probability 0.3 1.simulate 10 realizations (repetitions) \\(Y\\), use:outputs vector length 10. value represents result rep. first time ran binomial distribution \\(n=5, p=0.3\\), 1 5 success. second time run, 2 5 success, .Notice functions ended binom. just added different letter first, depending whether want PMF, CDF, percentile, random draw. letters d, p, q, r respectively.idea works distribution. example, find probability Poisson distribution rate parameter 2 equal 1, type:Thought questions: Suppose \\(Y \\sim Pois(1)\\).Find \\(P(Y \\leq 2)\\).Find 75th percentile \\(Y\\).Simulate 10,000 reps Y, find sample mean. sample mean close expected value?","code":"\ndbinom(2, 5, 0.3) ##supply the value of Y you want, then the parameters n and p in this order## [1] 0.3087\npbinom(2, 5, 0.3) ##supply the value of Y you want, then the parameters n and p in this order## [1] 0.83692\nqbinom(0.5, 5, 0.3) ##supply the value of the percentile you need, then the parameters n and p in this order## [1] 1\nset.seed(2) ##use set.seed() so we get the same random numbers each time the code is run\nrbinom(10, 5, 0.3) ##supply the number of simulated data you need, then the parameters n and p##  [1] 1 2 2 0 3 3 0 2 1 2\ndpois(1, 2) ##supply value of k, then parameter## [1] 0.2706706"},{"path":"continuous-random-variables.html","id":"continuous-random-variables","chapter":"4 Continuous Random Variables","heading":"4 Continuous Random Variables","text":"module based Introduction Probability (Blitzstein, Hwang), Chapters 5 6. can access book free https://stat110.hsites.harvard.edu/ (click Book). Please note cover additional topics, skip certain topics book. may skip Examples 5.1.6, 5.1.7, Proposition 5.2.3, Example 5.2.4, Sections 5.2.6, 5.2.7, Definition 5.3.7, Theorem 5.3.8, Example 5.4.7, Sections 5.5, 5.6, 5.7, Proposition 6.2.5, 6.2.6, Theorem 6.3.4, Sections 6.4 6.7 book.","code":""},{"path":"continuous-random-variables.html","id":"introduction","chapter":"4 Continuous Random Variables","heading":"4.1 Introduction","text":"previous module, learned discrete random variables. learned distributions can described PMFs CDFs, find expected values variances, well common distributions discrete random variables. learn counterparts dealing continuous random variables. concepts similar, computed can quite different.reminder:discrete random variable can take countable (finite infinite) number values.continuous random variable can take uncountable number values interval real numbers.example, height American adult continuous random variable, height can take value interval 40 100 inches. values 40 100 possible. list possible real numbers range list never ending.sample space associated continuous random variable difficult list, since takes uncountable number values. Using example heights American adults, real number 40 100 inches possible.different discrete random variable list sample space, support, find probability associated value support.Similar discrete random variables, want describe shape distribution, centrality, spread continuous random distribution idea probabilities associated different ranges values random variable.","code":""},{"path":"continuous-random-variables.html","id":"cumulative-distribution-functions-cdfs-1","chapter":"4 Continuous Random Variables","heading":"4.2 Cumulative Distribution Functions (CDFs)","text":"start talking cumulative distribution function, definition applies discrete continuous random variables. CDF random variable \\(X\\) \\(F_X(x) = P(X \\leq x)\\). difference lies CDF looks visually.Take look CDF discrete random variable CDF continuous random variable Figure 4.1:\nFigure 4.1: CDF Discrete RV vs CDF Continuous RV\nmentioned previous module, CDF discrete random variable called step function, jumps value support. hand, CDF continuous random variable increases smoothly sample space infinite.height CDF informs us percentile associated value random variable. Looking CDF continuous random variable Figure 4.1, height 0.5 random variable 0, value 0 corresponds 50th percentile distribution.technical definition continuous random variable : random variable continuous distribution CDF differentiable.discrete random variable fails definition since derivative undefined jumps.","code":""},{"path":"continuous-random-variables.html","id":"valid-cdfs-1","chapter":"4 Continuous Random Variables","heading":"4.2.1 Valid CDFs","text":"criteria valid CDF , matter random variable discrete continuous:non decreasing. means \\(x\\) gets larger, CDF either stays increases. Visually, graph CDF never decreases \\(x\\) increases.approach 1 \\(x\\) approaches infinity approach 0 \\(x\\) approaches negative infinity. Visually, graph CDF equal close 1 large values x, equal close 0 small values x.Thought question: Look CDFs example Figure 4.1, see satisfy criteria listed valid CDF.","code":""},{"path":"continuous-random-variables.html","id":"probability-density-functions-pdfs","chapter":"4 Continuous Random Variables","heading":"4.3 Probability Density Functions (PDFs)","text":"probability density function (PDF) continuous random variable analogous PMF discrete random variable.definition PDF continuous random variables following: continuous random variable \\(X\\) CDF \\(F_X(x)\\), PDF \\(X\\), \\(f_X(x)\\), derivative CDF, words, \\(f_X(x) = F_X^{\\prime}(x)\\). support \\(X\\) set \\(x\\) \\(f_X(x) >0\\).relationship PDF CDF continuous random variable \\(X\\) can expressed \\[\\begin{equation}\nF_X(x) = P(X \\leq x) = \\int_{-\\infty}^{x} f_X(x) dx.\n\\tag{4.1}\n\\end{equation}\\]take look example . Suppose continuous random variable \\(X\\) CDF PDF displayed , want find \\(P(X \\leq 1)\\):\nFigure 4.2: Probabilities CDF PDF\ncan find \\(P(X \\leq 1)\\) two different ways:CDF, find value 1 horizontal axis, read corresponding value vertical axis (blue lines). tells us \\(P(X \\leq 1) = 0.84\\).PDF, find area PDF \\(X \\leq 1\\). area corresponds shaded region blue, equal 0.84 performed integration per equation (4.1).Compare equation (4.1) equation (3.1) note similarities differences. discrete CDFs, sum PMF values less equal \\(x\\), whereas continuous CDFs, integrate, accumulate area, PDF values less equal \\(x\\). people view integral continuous version summation.equation (4.1), can generalize way find probability \\(P(<X<b)\\) continuous random variable \\(X\\):\\[\\begin{equation}\nP(<X<b) = F_X(b) - F_X() = \\int_{}^{b} f_X(x) dx.\n\\tag{4.2}\n\\end{equation}\\]words, find probability range values \\(X\\), just find area PDF range values. Going back example, want find \\(P(0<X<1)\\), find area PDF \\(0<X<1\\), like Figure 4.3 :\nFigure 4.3: Probabilities PDF\nmentioned, PDF continuous random variable analogous, exactly , PMF discrete random variable. One common misconception PDF tells us probability, example, value \\(f_X(2) = P(X=2)\\), \\(X\\) continuous. correct \\(X\\) discrete. fact, look equation (4.2) little closely, \\(P(X=c) = 0\\) \\(X\\) continuous \\(c\\) constant, since area PDF 0.","code":""},{"path":"continuous-random-variables.html","id":"valid-pdfs","chapter":"4 Continuous Random Variables","heading":"4.3.1 Valid PDFs","text":"PDF continuous random variable must satisfy following criteria:Non negative: \\(f_X(x) \\geq 0\\),Integrates 1: \\(\\int_{-\\infty}^{\\infty}f_X(x) dx = 1\\).","code":""},{"path":"continuous-random-variables.html","id":"pdfs-and-density-plots","chapter":"4 Continuous Random Variables","heading":"4.3.2 PDFs and Density Plots","text":"Recall Section 3.2.2, learned discrete random variables, PMF histogram related. PMF represents long-run proportion, histogram represents relative frequency based data. sample size gets larger, PMF match histogram.Similarly continuous random variables, PDF density plot related. PDF associated distribution known random variable, density plot estimated data, data follows known random variable, PDF match density plot sample size gets larger.go details density plots created end module, Section 4.6.1, still need cover bit concepts.","code":""},{"path":"continuous-random-variables.html","id":"summaries-of-a-distribution","chapter":"4 Continuous Random Variables","heading":"4.4 Summaries of a Distribution","text":"Next, talk common summaries associated distribution. involve measures centrality variance, covered . also talk couple measures: skewness kurtosis.","code":""},{"path":"continuous-random-variables.html","id":"expectations-1","chapter":"4 Continuous Random Variables","heading":"4.4.1 Expectations","text":"expected value continuous random variable \\(X\\) \\[\\begin{equation}\nE(X) = \\int_{-\\infty}^{\\infty} x f_X(x) dx.\n\\tag{4.3}\n\\end{equation}\\]Another common notation \\(E(X)\\) \\(\\mu\\), sometimes \\(\\mu_X\\) show writing mean random variable \\(X\\).compare equation (4.3) equation (3.2), notice use integral instead summation now working continuous random variables.interpretation expected values still : expectation random variable can interpreted long-run mean random variable, .e. able repeat experiment infinite number times, expectation random variable average result among experiments. still measure centrality random variable.linearity expectations still hold way, per equation (3.3). matter random variable discrete continuous.Law Unconscious Statistician (LOTUS) also still applies. continuous random variable \\(X\\), (unsurprisingly):\\[\\begin{equation}\nE(g(X)) = \\int_{-\\infty}^{\\infty} g(x) f_X(x).\n\\tag{4.4}\n\\end{equation}\\]Notice compare equation (4.4) discrete counterpart equation (3.4): just replaced summation integral.Thought question: Can guess write equation variance continuous random variable? Hint: variance discrete random variable given equation (3.5).","code":""},{"path":"continuous-random-variables.html","id":"median-1","chapter":"4 Continuous Random Variables","heading":"4.4.1.1 Median","text":"value \\(m\\) median random variable \\(X\\) \\(P(X \\leq c) \\geq \\frac{1}{2}\\) \\(P(X \\geq c) \\geq \\frac{1}{2}\\).Intuitively, median value \\(m\\) splits area PDF half (close half possible random variable discrete). Half area left \\(m\\), half area right \\(m\\).","code":""},{"path":"continuous-random-variables.html","id":"mode-1","chapter":"4 Continuous Random Variables","heading":"4.4.1.2 Mode","text":"continuous random variable \\(X\\), mode value \\(c\\) maximizes PDF: \\(f_X(c) \\geq f_X(x)\\) \\(x\\).discrete random variable \\(X\\), mode value \\(c\\) maximizes PMF: \\(P(X=c) \\geq P(X=x)\\) \\(x\\). Intuitively, mode commonly occurring value discrete random variable","code":""},{"path":"continuous-random-variables.html","id":"loss-functions","chapter":"4 Continuous Random Variables","heading":"4.4.1.3 Loss Functions","text":"goal statistical modeling use model make predictions. want able quantify quality prediction, prediction error. Suppose experiment can described random variable \\(X\\), want predict value next experiment. mean median natural guesses value next experiment.turns several ways quantify prediction error. usually called loss functions. Suppose predicted value denoted \\(x_{pred}\\). couple common loss functions :Mean squared error (MSE): \\(E(X-x_{pred})^2\\),Mean absolute error (MAE): \\(E|X-x_{pred}|\\).turns expected value \\(E(X)\\) minimizes MSE, median minimizes MAE. depending loss function suits analysis, use either mean median predictions. cover ideas detail later module (indeed later courses program).","code":""},{"path":"continuous-random-variables.html","id":"variance-1","chapter":"4 Continuous Random Variables","heading":"4.4.2 Variance","text":"variance continuous random variable \\(X\\) \\[\\begin{equation}\nVar(X) = \\int_{-\\infty}^{\\infty} (x-\\mu)^2 f_X(x) dx.\n\\tag{4.5}\n\\end{equation}\\]properties variance still Section 3.4.3.1. matter random variable discrete continuous. common notation used variance \\(\\sigma^2\\), sometimes \\(\\sigma_X^2\\) show variance random variable \\(X\\).","code":""},{"path":"continuous-random-variables.html","id":"moments","chapter":"4 Continuous Random Variables","heading":"4.4.3 Moments","text":"talking measures used describe distributions, cover terminology used measures. Suppose random variable \\(X\\).\\(n\\)th moment \\(X\\) \\(E(X^n)\\). expected value, mean, sometimes called first moment.\\(n\\)th central moment \\(X\\) \\(E((X-\\mu)^n)\\). variance sometimes called second central moment.\\(n\\) standardized moment \\(X\\) \\(E(\\frac{(X-\\mu)^n}{\\sigma})\\).","code":""},{"path":"continuous-random-variables.html","id":"skewness","chapter":"4 Continuous Random Variables","heading":"4.4.4 Skewness","text":"One measure used describe shape distribution skewness, measure symmetry (measure skewness). skew random variable \\(X\\) third standardized moment:\\[\\begin{equation}\nSkew(X) = E \\left(\\frac{(X-\\mu)^3}{\\sigma} \\right)\n\\tag{4.6}\n\\end{equation}\\]random variable \\(X\\) symmetric distribution mean \\(X - \\mu\\) distribution \\(\\mu - X\\). Fairly often, people just say \\(X\\) symmetric; almost always assumed symmetry mean.Intuitively, symmetry means PDF \\(X\\) left mean mirror image PDF \\(X\\) right mean. look couple examples Figure 4.4:\nFigure 4.4: PDFs Symmetric RV vs Skewed RV\nblue vertical lines indicate mean distributions. Notice mirror image first plot, second plot.distribution symmetric, can say distribution asymmetric, skewed. values \\(Skew(X)\\) associated different shapes :\\(Skew(X) = 0\\): \\(X\\) symmetric.\\(Skew(X) > 0\\): \\(X\\) right skewed.\\(Skew(X) < 0\\): \\(X\\) left skewed.","code":""},{"path":"continuous-random-variables.html","id":"kurtosis","chapter":"4 Continuous Random Variables","heading":"4.4.5 Kurtosis","text":"One measure deals tail behavior distribution. Visually, tails PDF associated probabilities extreme values random variable. distribution heavy tailed means extreme values (ends) likely occur. Tail behavior important consideration risk management finance: e.g. heavy left tail PDF mean financial crisis. Figure 4.5 shows example heavy tailed distribution (blue), compared Gaussian distribution (black). talk Gaussian distribution next subsection.\nFigure 4.5: PDF Heavy Tailed Distribution\ncommon measure tail behavior Kurtosis. kurtosis random variable \\(X\\) shifted fourth standardized moment:\\[\\begin{equation}\nKurt(X) = E \\left(\\frac{(X-\\mu)^4}{\\sigma} \\right) - 3.\n\\tag{4.7}\n\\end{equation}\\]reason subtracting (shifting ) 3 Gaussian distribution (commonly used distribution continuous random variables) kurtosis 0. Note: authors call equation (4.7) excess kurtosis kurtosis subtract 3.values \\(Kurt(X)\\) associated tail behaviors :\\(Kurt(X) = 0\\): \\(X\\) similar tails Gaussian distribution.\\(Kurt(X) > 0\\): \\(X\\) heavier tails compared Gaussian distribution (extreme values likely).\\(Kurt(X) < 0\\): \\(X\\) smaller tails compared Gaussian distribution (extreme values less likely).","code":""},{"path":"continuous-random-variables.html","id":"common-continuous-random-variables","chapter":"4 Continuous Random Variables","heading":"4.5 Common Continuous Random Variables","text":"Next, introduce commonly used distributions may used continuous random variables. number common statistical models (example, linear regression) based distributions.","code":""},{"path":"continuous-random-variables.html","id":"uniform","chapter":"4 Continuous Random Variables","heading":"4.5.1 Uniform","text":"random variable follows uniform distribution interval \\((,b)\\) completely random number \\(\\) \\(b\\). Notionally, upper case \\(U\\) usually used denote uniform random variable. \\(U\\) said uniform distribution interval \\((,b)\\), denoted \\(U \\sim(,b)\\), PDF \\[\\begin{equation}\nf_X(x) = \\begin{cases}\n  \\frac{1}{b-} & \\text{} <x<b \\\\\n  0 & \\text{otherwise }.\n\\end{cases}\n\\tag{4.8}\n\\end{equation}\\]Note parameters \\(,b\\) also help define support uniform distribution. Figure 4.6 displays plot PDF \\(U(,b)\\):\nFigure 4.6: PDF U(,b). Picture https://en.wikipedia.org/wiki/Continuous_uniform_distribution\nThought question: Can verify valid PDF?Figure 4.7 displays plot CDF \\(U(,b)\\):\nFigure 4.7: CDF U(,b). Picture https://en.wikipedia.org/wiki/Continuous_uniform_distribution\nproperties uniform distribution:mean \\(E(U) = \\frac{+b}{2}\\).variance \\(Var(U) = \\frac{(b-)^2}{12}\\).skewness 0, symmetric.kurtosis -\\(\\frac{6}{5}\\), tails heavy compared Gaussian distribution.Thought question: Can see uniform distribution symmetric? Can see tails heavy?support uniform distribution 0 1, standard uniform distribution. talk importance standard uniform distribution next subsection.","code":""},{"path":"continuous-random-variables.html","id":"universality-of-uniform","chapter":"4 Continuous Random Variables","heading":"4.5.1.1 Universality of Uniform","text":"turns can construct random variable continuous distribution based standard uniform distribution. fact used simulate random numbers continuous distributions. fact called Universality Uniform: Let \\(F_X(x)\\) denote CDF continuous random variable \\(X\\), :Let \\(U \\sim U(0,1)\\) \\(X = F^{-1}(U)\\). \\(X\\) random variable CDF \\(F_X(x)\\).\\(F_X(X) \\sim U(0,1)\\).give insight means, look example. Another continuous distribution called standard logistic distribution, denote \\(X\\). CDF \\[\nF_X(x) = \\frac{e^x}{1+e^x}.\n\\]\nLet \\(U \\sim U(0,1)\\). first part universality uniform informs us inverse CDF standard logistic \\(F_X^{-1}(U) \\sim X\\), invert \\(F_X(x)\\) get inverse \\(F_X^{-1}(x)\\). done setting CDF \\(X\\) equal \\(u\\), .e. let \\(u = \\frac{e^x}{1+e^x}\\), solving \\(x\\):\\[\n\\begin{split}\nu + u e^x &= e^x\\\\\n\\implies u &= e^x (1-u) \\\\\n\\implies e^x &= \\frac{u}{1-u} \\\\\n\\implies x &= \\log (\\frac{u}{1-u}).\n\\end{split}\n\\]Therefore \\(F^{-1}(u) = \\log (\\frac{u}{1-u})\\) \\(F^{-1}(U) = \\log (\\frac{U}{1-U})\\). Therefore \\(\\log (\\frac{U}{1-U})\\) follows standard logistic distribution.Let us use simulations show going . First, simulate 10,000 reps standard uniform distribution, invert values using \\(\\log (\\frac{u}{1-u})\\), create density plot \\(\\log (\\frac{u}{1-u})\\). steps shown Figure 4.8 :\nFigure 4.8: Uniform Logistic\nFigure 4.8:first plot shows density plot 10,000 reps standard normal. close PDF standard uniform.second plot shows density plot inverting 10,000 reps standard normal, .e. \\(F^{-1}(u) = \\log (\\frac{u}{1-u})\\).third plot shows PDF standard logistic. Notice similar looks second plot.see \\(\\log (\\frac{U}{1-U})\\) follows standard logistic distribution.second part universality uniform informs us \\(X\\) follows standard logistic distribution, \\(F(X) = \\frac{e^X}{1 + e^X} \\sim U(0,1)\\)., can see purpose universality uniform:part 1, can simulate reps distribution, long know CDF. software use may able simulate reps particular distribution, can write code simulate reps distribution based standard uniform.part 2, can convert random variable unknown distribution one known: standard uniform.","code":"\nset.seed(4)\n\nreps<-10000 ##number of reps\nu<-runif(reps) ##simulate standard uniform\ninvert<- log(u/(1-u)) ##invert based on F inverse. These should now follow standard logistic\n\npar(mfrow=c(1,3))\nplot(density(u), main=\"Density Plot from 10,000 U's\")\nplot(density(invert), main=\"Density Plot after Inverting\", xlim=c(-6,6))\ncurve(dlogis, from = -7, to = 7, main = \"PDF for Logistic\", ylab=\"Density\", xlab=\"\")"},{"path":"continuous-random-variables.html","id":"normal","chapter":"4 Continuous Random Variables","heading":"4.5.2 Normal","text":"Another widely used distribution continuous random variables normal, Gaussian distribution. distribution symmetric bell-shaped. probably important distribution statistics data science due central limit theorem. define theorem later module, loosely speaking, says take average bunch random variables, average approximate normal distribution, even random variables individually normal.lot questions wish answer based averages. exampleDoes implementation certain technologies class improve test scores students, average?male Gentoo penguins heavier female counterparts, average?replacing traffic lights roundabout reduce number traffic accidents, average?central limit theorem implies even test scores, weights Gentoo penguins, number traffic accidents follow normal distribution, average values approximate normal distribution.","code":""},{"path":"continuous-random-variables.html","id":"standard-normal","chapter":"4 Continuous Random Variables","heading":"4.5.2.1 Standard Normal","text":"First, talk standard normal distribution, normal distributions can viewed variations standard normal. standard normal distribution mean 0 variance 1. usually denoted \\(Z\\). can also write \\(Z \\sim N(0,1)\\) say \\(Z\\) normally distributed mean 0 variance 1. PDF standard normal distribution :\\[\\begin{equation}\n\\phi(z) = \\frac{1}{\\sqrt{2 \\pi}} e^{-z^2/2}.\n\\tag{4.9}\n\\end{equation}\\]Notice constant \\(\\frac{1}{\\sqrt{2 \\pi}}\\) equation (4.9). presence needed make PDF valid, since PDF must integrate 1. constants called normalizing constants.Figure 4.9 displays PDF:\nFigure 4.9: PDF Standard Normal\nFigure 4.9, can see following properties standard normal distribution (apply normal distribution):PDF symmetric mean. Figure 4.9, PDF symmetric 0, .e. \\(\\phi(-z) = \\phi(z)\\).implies tail areas also symmetric. example, \\(P(Z \\leq -2) = P(Z \\geq 2)\\).skew 0, since symmetric.actually closed-formed equation CDF standard normal (normal distribution). write \\(\\Phi(z) = P(Z \\leq z) = \\int_{\\infty}^z \\phi(z) dz\\) express CDF standard normal.Notice special letters \\(Z, \\phi, \\Phi\\) denote standard normal distribution. indication often used warrant notation.","code":"\ncurve(dnorm, from = -4, to = 4, main = \"PDF for Z\", ylab=\"Density\", xlab=\"\")"},{"path":"continuous-random-variables.html","id":"from-standard-normal-to-other-normals","chapter":"4 Continuous Random Variables","heading":"4.5.2.2 From Standard Normal to Other Normals","text":"\\(Z \\sim N(0,1)\\), \\(X = \\mu + \\sigma Z \\sim N(\\mu, \\sigma^2)\\). words, \\(Z\\) standard normal, \\(X = \\mu + \\sigma Z\\) follows normal distribution mean \\(\\mu\\) variance \\(\\sigma^2\\). parameters normal distribution mean \\(\\mu\\) variance \\(\\sigma^2\\).Note authors say parameters mean \\(\\mu\\) standard deviation \\(\\sigma\\) instead, careful reading notation associated normal distributions various sources. example, \\(N(0,2)\\) class book means normal distribution mean 0 variance 2, authors, \\(N(0,2)\\) means normal distribution mean 0 standard deviation 2. Indeed, functions R use alternate parameterization, need careful.Thought question: Can use linearity expectations explain \\(X\\) mean \\(\\mu\\)? Can use properties variance Section 3.4.3.1 explain \\(X\\) variance \\(\\sigma^2\\)?Notice started standard normal \\(Z\\), transformed \\(Z\\) multiplying \\(\\sigma\\) adding \\(\\mu\\) get normal distribution. transformation called location-scale transformation, sometimes shifting scaling. scale changes since multiply constant \\(\\sigma\\); location transformed since mean changes 0 \\(\\mu\\).can also reverse transformation state following: \\(X \\sim N(\\mu, \\sigma^2)\\), \\(Z = \\frac{X-\\mu}{\\sigma} \\sim N(0,1)\\). start \\(X \\sim N(\\mu, \\sigma^2)\\), can transform \\(X\\) subtracting , dividing \\(\\sigma\\), obtain \\(Z\\). particular transformation called standardization:\\[\\begin{equation}\nZ = \\frac{X-\\mu}{\\sigma}.\n\\tag{4.10}\n\\end{equation}\\]PDF normal distribution \\(X \\sim N(\\mu, \\sigma^2)\\) \\[\\begin{equation}\nf_X(x) = \\frac{1}{\\sigma \\sqrt{2 \\pi}} \\exp \\left(-\\frac{(x - \\mu)^2}{2 \\sigma^2} \\right).\n\\tag{4.11}\n\\end{equation}\\]Thought question: Compare equations (4.11) (4.9). Can see equation (4.9) can derived equation (4.11)?","code":""},{"path":"continuous-random-variables.html","id":"rule","chapter":"4 Continuous Random Variables","heading":"4.5.2.3 68-95-99.7% Rule","text":"following property holds normal distribution, often called 68-99-99.7% rule. normal distribution \\(X \\sim N(\\mu, \\sigma^2)\\):\\(P(\\mu - \\sigma < X < \\mu + \\sigma) \\approx 0.68\\),\\(P(\\mu - 2\\sigma < X < \\mu + 2\\sigma) \\approx 0.95\\),\\(P(\\mu - 3\\sigma < X < \\mu + 3\\sigma) \\approx 0.997\\).mean normal distribution:68% observed values fall within 1 standard deviation mean,95% observed values fall within 2 standard deviations mean, andAbout 99.7% observed values fall within 3 standard deviations mean.last statement basis term six sigma used manufacturing, since range virtually data points fall within range six sigma wide (assuming follow normal distribution). Visually, rule shown Figure 4.10 applied standard normal:\nFigure 4.10: 68-95-99.7 Rule\nwork first statement, 68% observed values fall within 1 standard deviation mean normal distribution. use R help us verify rule standard normal:Thought question: tweak code verify two statements associated 68-95-99.7% rule?","code":"\nupper1<-pnorm(1) ## what is percentile associated with Z=1 (i.e. 1 standard deviation above mean)\nlower1<-pnorm(-1) ## what is percentile associated with Z=-1 (i.e. 1 standard deviation below mean)\nupper1-lower1 ## find proportion in between 1 SD above and below mean.## [1] 0.6826895"},{"path":"continuous-random-variables.html","id":"using-r","chapter":"4 Continuous Random Variables","heading":"4.6 Using R","text":"R built functions compute density, CDF, percentiles, well simulate data common distributions. start random variable \\(Y \\sim N(1, 9)\\) example.find \\(f_Y(2)\\), use:density \\(f_Y(2)\\) 0.1257944. Note: R, normal distribution parameterized mean standard deviation, different set notes book, uses mean variance.find \\(P(Y \\leq 2)\\), use:probability \\(Y\\) less equal 2 0.6305587.Alternatively, can standardize normal distribution, use standard normal. standardization, per equation (4.10), gives us\\[\nz = \\frac{2-1}{3} = \\frac{1}{3},\n\\]\\[\n\\begin{split}\nP(Y \\leq 2) &= P(\\frac{Y-\\mu}{\\sigma} \\leq \\frac{2-1}{3}) \\\\\n            &= P(Z \\leq \\frac{1}{3}) \\\\\n            &= \\Phi(\\frac{1}{3})\n\\end{split}\n\\]can found usingwhich gives answer pnorm(2,1,3).find value support corresponds 90th percentile, use:90th percentile \\(Y \\sim N(1,3)\\) 4.844655.want use standard normal, find 90th percentile:apply location scale transformationwhich answer qnorm(0.9,1,3).simulate 10 draws (repetitions) \\(Y\\), use:outputs vector length 10. value represents result rep. first value drawn \\(Y \\sim N(1,3)\\) -1.6907436, second value drawn 1.5545476 .Just like Section 3.6, notice functions ended norm. just added different letter first, depending whether want density (analogous PDF), CDF, percentile, random draw. letters d, p, q, r respectively.One thing note: supply mean standard deviation, example type rnorm(10), R assume want use standard normal distribution, rnorm(10) draw 10 random numbers standard normal.","code":"\ndnorm(2, 1, 3) ##supply the value of Y you want, then the parameters mu and sigma## [1] 0.1257944\npnorm(2, 1, 3) ##supply the value of Y you want, then the parameters mu and sigma## [1] 0.6305587\npnorm(1/3) ##don't supply mu and sigma means you want to use standard normal## [1] 0.6305587\nqnorm(0.9, 1, 3) ##supply the value of the percentile you need, then the parameters mu and sigma## [1] 4.844655\nqnorm(0.9)## [1] 1.281552\nqnorm(0.9)*3 + 1 ##multiply by sigma, then add mu## [1] 4.844655\nset.seed(2) ##use set.seed() so we get the same random numbers each time the code is run\nrnorm(10, 1, 3) ##supply the number of simulated data you need, then the parameters mu and sigma##  [1] -1.6907436  1.5545476  5.7635360 -2.3911270  0.7592447  1.3972609\n##  [7]  3.1238642  0.2809059  6.9534218  0.5836390"},{"path":"continuous-random-variables.html","id":"KDE","chapter":"4 Continuous Random Variables","heading":"4.6.1 Density Plots and Kernel Density Estimation","text":"now ready talk density plots, like ones Figure 4.8 created. Recall difference density plots PDFs:plot PDF describes distribution known random variable.density plot based data, used describe distribution data. data may may follow commonly known random variable. , plot PDF density plot match gather data.Proportions found way, finding area PDF density plot appropriate range support.Suppose \\(n\\) observed values unknown random variable \\(X\\): \\(x_1, x_2, \\cdots, x_n\\).density \\(f\\) \\(X\\) unknown want estimate data. estimate density \\(f\\), use kernel density estimator:\\[\\begin{equation}\n\\hat{f}_h(x) = \\frac{1}{nh} \\sum_{=1}^n K \\left( \\frac{x-x_i}{h}\\right ),\n\\tag{4.12}\n\\end{equation}\\]\\(K\\) kernel \\(h\\) smoothing parameter, often called bandwidth. Looking equation (4.12), KDE can viewed weighted average relative likelihood observing particular value, values nearer specific value support receiving higher weight.kernel can viewed weighting function, weights following shape distribution user specifies (usually symmetric). Common kernel functions shapes displayed Figure 4.11:\nFigure 4.11: Common Kernals. Picture adapted https://tgstewart.cloud/compprob/kde.html\nhorizontal axis kernel can viewed distance value data point specific value support, mid point horizontal axis represents distance 0.Looking normal kernel, nearest values receive highest weight, values away receive less weight.uniform kernel, values within certain distance receive weight, values beyond certain distance receive weight.Epanechnikov (parabolic) kernel mix : values beyond certain distance receive weight, values within certain distance receive weight roughly inversely proportional distance.\\(h\\) smoothing parameter analogous bin width histograms. Larger values result smoother looking density plots.Let us go back old example. use loan50 dataset openintro package. data originally consist thousands loans made Lending Club platform, randomly select 50 loans. Let us study interest rate loans 50 applicants received.\nFigure 4.12: Density Plot 50 Interest Rates\nactually uses KDE default settings: kernel normal, bandwidth based Silverman’s rule thumb.change , add kernel adjust argument using density() function, example, use Epanechnikov kernal twice default bandwidth:\nFigure 4.13: Density Plot 50 Interest Rates, Epanechnikov Kernel, Twice Bandwidth\n","code":"\nlibrary(openintro)\n\n##create object for data\nData<-loan50\n\n##create density plot using default\nplot(density(Data$interest_rate), main=\"Density Plot of Interest Rates\")\n##create density plot using different settings\nplot(density(Data$interest_rate, kernel = \"epanechnikov\", adjust = 2), \n     main=\"Density Plot of Interest Rates\")"},{"path":"continuous-random-variables.html","id":"density-plots-and-histograms","chapter":"4 Continuous Random Variables","heading":"4.6.2 Density Plots and Histograms","text":"Section 1.2.3, mentioned density plots can viewed smoothed versions histogram. create histogram interest rates, overlay density plot blue, per Figure 4.14 :\nFigure 4.14: Histogram Density Plot 50 Interest Rates\n","code":"\nhist(Data$interest_rate, prob = TRUE, main = \"Histogram with Density Plot\", xlab=\"Interest Rates\")\n\n##create density plot using default\nlines(density(Data$interest_rate), col=\"blue\")"},{"path":"continuous-random-variables.html","id":"numerical-summaries","chapter":"4 Continuous Random Variables","heading":"4.6.3 Numerical Summaries","text":"Equations (4.3), (4.5), (4.6), (4.7) used obtain mean, variance, skewness, kurtosis known distribution random variable. calculate quantities based sample observed data, \\(x_1, x_2, \\cdots, x_n\\), use:\\[\\begin{equation}\n\\bar{x} =  \\frac{1}{n} \\sum_{=1}^n x_i,\n\\tag{4.13}\n\\end{equation}\\]\\[\\begin{equation}\ns_X^2 =  \\frac{1}{n-1} \\sum_{=1}^n (x_i - \\bar{x})^2,\n\\tag{4.14}\n\\end{equation}\\]\\(\\bar{x}\\) \\(s_x^2\\) denote sample mean variance respectively. sample skewness sample kurtosis \\[\\begin{equation}\n\\text{sample skewness } =  \\frac{\\frac{1}{n} \\sum_{=1}^n (x_i - \\bar{x})^3}{s_X^3},\n\\tag{4.15}\n\\end{equation}\\]\\[\\begin{equation}\n\\text{sample kurtosis } =  \\frac{\\frac{1}{n} \\sum_{=1}^n (x_i - \\bar{x})^4}{s_X^4} - 3.\n\\tag{4.16}\n\\end{equation}\\]functions mean(), var(), skewness(), kurtosis() compute quantities R. latter two functions come moments package sure install load prior using .data also right skewed heavy tailed.","code":"\nmean(Data$interest_rate) ##mean## [1] 11.5672\nvar(Data$interest_rate) ##variance## [1] 25.52387\nlibrary(moments)\nmoments::skewness(Data$interest_rate) ##greater than 0## [1] 1.102193\nmoments::kurtosis(Data$interest_rate) ##greater than 0## [1] 3.651631"}]
