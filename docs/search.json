[{"path":"index.html","id":"preface","chapter":"Preface","heading":"Preface","text":"examples preface based OpenIntro Statistics (Diez, Ceytinka-Rundel, Barr), Chapter 9.4 9.5, provide background information. can access book free https://www.openintro.org/book/os/main goal using data science understand data. Broadly speaking, involve building statistical model predicting, estimating response variable based one predictors. models used wide variety fields finance, medicine, public policy, sports, . look couple examples.","code":""},{"path":"index.html","id":"examples","chapter":"Preface","heading":"0.1 Examples","text":"","code":""},{"path":"index.html","id":"example-1-mario-kart-auction-prices","chapter":"Preface","heading":"0.1.1 Example 1: Mario Kart Auction Prices","text":"first example, look Ebay auctions video game called Mario Kart played Nintendo Wii. want predict price auction based whether game new , whether auction’s main photo stock photo, duration auction days, number Wii wheels included auction.model can use example linear regression model:Generally speaking, linear regression equation takes following form:\\[\n\\hat{y} = \\hat{\\beta}_0 + \\hat{\\beta}_1 x_1 + \\hat{\\beta}_2 x_2 + \\cdots + \\hat{\\beta}_k x_k\n\\]\\(\\hat{y}\\) denotes predicted value response variable, price action example, \\(x_1, x_2, \\cdots, x_k\\) denote values predictors. example, : \\(x_1\\) whether game new , \\(x_2\\) whether auction’s main photo stock photo, \\(x_3\\) duration auction days, \\(x_4\\) number Wii wheels included auction. \\(\\hat{\\beta}_0, \\hat{\\beta}_1, \\cdots, \\hat{\\beta}_k\\) represent estimated regression parameters. know values , can easily plug values predictors obtain predicted price auction.Fitting model R, obtain estimated regression parameters::\\[\n\\hat{y} = 43.5201 - 2.5816 x_1 - 6.7542 x_2 + 0.3788 x_3 + 9.9476 x_4\n\\]auction Mario Kart game used, uses stock photo, listed 2 days, comes 0 wheels, predicted price \\(\\hat{y} = 43.5201 - 2.5816 - 6.7542 + 0.3788 \\times 2 = 34.94\\) 35 dollars.","code":"\nlibrary(openintro)\n\nData<-mariokart\n##fit model\nresult<-lm(total_pr~cond+stock_photo+duration+wheels, data=Data)\n##get estimated regression parameters\nresult## \n## Call:\n## lm(formula = total_pr ~ cond + stock_photo + duration + wheels, \n##     data = Data)\n## \n## Coefficients:\n##    (Intercept)        condused  stock_photoyes        duration          wheels  \n##        43.5201         -2.5816         -6.7542          0.3788          9.9476"},{"path":"index.html","id":"eg2","chapter":"Preface","heading":"0.1.2 Example 2: Job Application Callback Rates","text":"example, look data experiment sought evaluate effect race gender job application callback rates. experiment, researchers created fake resumes job postings Boston Chicago see resumes resulted callback. fake resumes included relevant information applicant’s educational attainment, many year’s experience applicant well first last name. names fake resume meant imply applicant’s race gender. two races considered (Black White) two genders considered (Make Female) experiment.Prior experiment, researchers conducted surveys check racial gender associations names fake resumes; names passed certain threshold surveys included experiment.model can used example logistic regression modelGenerally speaking, logistic regression equation takes following form\\[\n\\log (\\frac{\\hat{\\pi}}{1-\\hat{\\pi}}) = \\hat{\\beta}_0 + \\hat{\\beta}_1 x_1 + \\hat{\\beta}_2 x_2 + \\cdots + \\hat{\\beta}_k x_k\n\\]\\(\\hat{\\pi}\\) denotes predicted probability applicant receives call back. \\(x_1, x_2, \\cdots, x_k\\) denote values predictors. example, : \\(x_1\\) city job posting located , \\(x_2\\) whether applicant college degree , \\(x_3\\) experience applicant, \\(x_4\\) associated race applicant, \\(x_5\\) associated gender applicant. Similar linear regression, \\(\\hat{\\beta}_0, \\hat{\\beta}_1, \\cdots, \\hat{\\beta}_k\\) represent estimated regression parameters. know values , can easily plug values predictors obtain predicted probability applicant characteristics receive callback.Fitting model R, obtain estimated regression parametersso \\[\n\\log (\\frac{\\hat{\\pi}}{1-\\hat{\\pi}}) = -2.63974 - 0.39206 x_1 - 0.0655 x_2 + 0.03152 x_3 + 0.44299 x_4 - 0.22814 x_5\n\\]applicant Boston, college degree, 10 years experience name associated Black male, logistic regression equation becomes \\(\\log (\\frac{\\hat{\\pi}}{1-\\hat{\\pi}}) = -2.63974 - 0.0655 + 0.03152 \\times 10 - 0.22814 = -2.61818\\). little bit algebra solve, get \\(\\hat{\\pi} = 0.06797751\\). applicant 6.8 percent chance receiving callback.","code":"\nData2<-resume\n##fit model\nresult2<-glm(received_callback~job_city + college_degree+years_experience+race+gender, family=\"binomial\", data=Data2)\n##get estimated regression parameters\nresult2## \n## Call:  glm(formula = received_callback ~ job_city + college_degree + \n##     years_experience + race + gender, family = \"binomial\", data = Data2)\n## \n## Coefficients:\n##      (Intercept)   job_cityChicago    college_degree  years_experience  \n##         -2.63974          -0.39206          -0.06550           0.03152  \n##        racewhite           genderm  \n##          0.44299          -0.22814  \n## \n## Degrees of Freedom: 4869 Total (i.e. Null);  4864 Residual\n## Null Deviance:       2727 \n## Residual Deviance: 2680  AIC: 2692"},{"path":"index.html","id":"how-were-estimated-parameters-calculated","chapter":"Preface","heading":"0.2 How were Estimated Parameters Calculated?","text":"two examples, notice used R functions, supplied names variables, R functions generated values estimated parameters? One thing learn functions actually calculate numbers. turns calculations based foundational concepts associated measures uncertainty, probability, expected values. learning concepts class.want know calculations performed? understand intuition logic behind models built. becomes lot easier work models understand logic (example, know models can used used, know steps take notice data certain characteristics, etc), instead memorizing bunch steps.presenting models data people, people may occasionally questions methods models. trust model? trust numbers seem come black box?Notice used two different models, linear regression logistic regression, examples 1 2. use models? swapped type model used examples? answer actually . One main considerations deciding model use identify response variable quantitative categorical. learn linear regression model works response variable quantitative, logistic regression model works response variable categorical.","code":""},{"path":"index.html","id":"the-course-understanding-uncertainty","chapter":"Preface","heading":"0.3 The Course: Understanding Uncertainty","text":"mentioned previous section, learning foundational concepts associated measures uncertainty, probability, expected values. concepts help explain intuition statistical models built.end course, apply concepts revisit linear regression logistic regression models. two widely used models used data science, relatively easier understand explain. modern methods (learn future classes) decision trees neural networks can viewed extensions linear logistic regression models.","code":""},{"path":"descriptive.html","id":"descriptive","chapter":"1 Descriptive Statistics","heading":"1 Descriptive Statistics","text":"module based OpenIntro Statistics (Diez, Ceytinka-Rundel, Barr), Chapter 2.1. can access book free https://www.openintro.org/book/os/ Please note cover additional topics, skip certain topics book.","code":""},{"path":"descriptive.html","id":"uncertainty-with-data","chapter":"1 Descriptive Statistics","heading":"1.1 Uncertainty with Data","text":"analyzing data, always going degree uncertainty, randomness lot phenomena observe world. event random individual outcomes event unpredictable. example, weight next baby born local hospital. Without knowing information biological parents, high degree uncertainty try predict baby’s weight. Even know detailed information biological parents (example tall), may feel confident predicting baby likely heavier average, certain prediction.end hand, event deterministic can predict individual outcomes event certainty. example, know length cube 2 inches, know sure volume \\(2^3 = 8\\) cubic inches, based rules mathematics. volume cube length 2 inches always going 8 cubic inches, volume deterministic.Thought question: think data see real life. Write . data random deterministic?explore tools help us quantify uncertainty data. module, explore fairly standard tools used describe data give us idea degree uncertainty data. describing data quantitative, usually describe following: shape distribution, average typical value, spread uncertainty.","code":""},{"path":"descriptive.html","id":"visualizing-data","chapter":"1 Descriptive Statistics","heading":"1.2 Visualizing Data","text":"Data visualization representation information form pictures. Imagine access weights newborn babies local hospital. Examining numerical value time consuming. instead, can use visualizations give us idea values weights. example, weights newborns common? proportion babies dangerously low weights (may indicate health risks)? Good data visualizations can give us information fairly quickly. Next, explore common visualizations used quantitative (numerical) variables.","code":""},{"path":"descriptive.html","id":"dot-plots","chapter":"1 Descriptive Statistics","heading":"1.2.1 Dot Plots","text":"start dot plot, basic visualization quantitative variable. use loan50 dataset openintro package. data originally consist thousands loans made Lending Club platform, randomly select 50 loans. Let us study interest rate loans 50 applicants received.simplicity, round numerical values interest rates nearest whole number:can create corresponding dot plot, per Figure 1.1:\nFigure 1.1: Dot Plot 50 Interest Rates (rounded)\nNotice 1 black dot corresponds interest rate 20 (presumably percent), one applicant rounded interest rate 20 percent. 8 black dots correspond interest rate 10 percent, 8 applicants rounded interest rate 10 percent. interest rates 10 percent much commonly occurring interest rate 20 percent. can use height, number dots, help us glean often value certain interest rate occurs. Based dotplot, interest rates 5 11 percent common, higher values less common.Note: get torn details code produce dot plot. chosen present dot plot way highlight use , without getting bogged details can produced. using dot plots class.","code":"\nlibrary(tidyverse)\nlibrary(openintro)\n\n##create object for data\nData<-loan50\n##round interest rate to whole number\nData<- Data%>%\n  mutate(r_int_rate = round(interest_rate))\n##dotplot\nggplot(Data,aes(x=r_int_rate))+\n  geom_dotplot(binwidth=1)+\n  theme(\n    axis.text.y = element_blank(),  # Remove y-axis labels\n    axis.title.y = element_blank(), # Remove y-axis title\n    axis.ticks.y = element_blank()  # Remove y-axis ticks\n  )+ \n  labs(x=\"Interest Rates (Rounded)\")"},{"path":"descriptive.html","id":"histograms","chapter":"1 Descriptive Statistics","heading":"1.2.2 Histograms","text":"turns dot plots often useful large data sets, provide general idea visualizations larger data sets work. height dots inform us frequency values occurring.visualization commonly used larger data sets histogram. Instead displaying common value variable exists, think values belonging bin values. example, can create bin contains interest rates 5 7.5 percent, another bin containing interest rates 7.5 10 percent, . things note histograms:convention, values lie exactly boundary bin belong lower bin. example, interest rate exactly 12.5 percent belong bin 10 12.5 percent, bin 12.5 15 percent.convention, values lie exactly boundary bin belong lower bin. example, interest rate exactly 12.5 percent belong bin 10 12.5 percent, bin 12.5 15 percent.bin width. example, width 2.5.bin width. example, width 2.5.create histogram (using original interest rates) , per Figure 1.2:\nFigure 1.2: Historgram 50 Interest Rates\nSimilar dot plot Figure 1.1, height histogram inform us values commonly occurring. can see histogram interest rates 5 10 percent common, much loans interest rates greater 20 percent. say certainty randomly selected loan applicant interest rate 5 10 percent interest rate greater 20 percent.","code":"\n##set up sequence to specify the bins\ns25<-seq(5,27.5,2.5)\n\nggplot(Data,aes(x=interest_rate))+\n  geom_histogram(breaks=s25,fill=\"blue\",color=\"orange\")+\n  labs(x=\"Interest Rate\", title=\"Histogram of Interest Rates\")"},{"path":"descriptive.html","id":"shapes-of-distribution","chapter":"1 Descriptive Statistics","heading":"1.2.2.1 Shapes of Distribution","text":"Histograms can also give us idea shape distribution interest rates. histogram Figure 1.2, loans less 15 percent, small number loans greater 20 percent. can say greater certainty loan interest rate less 15 percent. data tail right histogram, shape said right-skewed. variable said right-skewed, large values variable much less common small values variable; smaller values likely occur.histogram reverse characteristic, .e. data tail left instead, shape said left-skewed. implies small values variable much less common large values variable; larger values likely occur.histogram reverse characteristic, .e. data tail left instead, shape said left-skewed. implies small values variable much less common large values variable; larger values likely occur.Histograms tail similarly directions called symmetric. Large small values variable equally likely.Histograms tail similarly directions called symmetric. Large small values variable equally likely.Histograms peak middle, tail sides symmetic, also bell-shaped, normal distribution. Note: turns one assumptions linear regression response variable follow normal distribution. may seem restrictive, however, see later modules assumption particularly crucial circumstances.Histograms peak middle, tail sides symmetic, also bell-shaped, normal distribution. Note: turns one assumptions linear regression response variable follow normal distribution. may seem restrictive, however, see later modules assumption particularly crucial circumstances.Thought question: Can think real life variables symmetric, right-skewed, left-skewed distributions? Feel free search internet examples.","code":""},{"path":"descriptive.html","id":"considerations-with-histograms","chapter":"1 Descriptive Statistics","heading":"1.2.2.2 Considerations with Histograms","text":"interest rate example, may noticed made specific choice width bins created histograms. turns width bins can impact shape histogram, potentially, interpret histogram.Consider creating histogram bin width 0.5, instead 2.5, per Figure 1.3:\nFigure 1.3: Historgram 50 Interest Rates, Bin Width 0.5\nComparing Figure 1.3 Figure 1.2, note following:Visually, histogram looks jagged smaller bin width, whereas histogram looks smoother larger bin width.Visually, histogram looks jagged smaller bin width, whereas histogram looks smoother larger bin width.Smaller bin widths may preferred need information smaller ranges interest rates. However, can difficult write general trends.Smaller bin widths may preferred need information smaller ranges interest rates. However, can difficult write general trends.Larger bin widths may useful trying look general trends interest rates.Larger bin widths may useful trying look general trends interest rates.Thought question: happens create histogram bin width large?","code":"\n##set up sequence to specify the bins. width now 0.5\ns05<-seq(5,27.5,0.5)\n\nggplot(Data,aes(x=interest_rate))+\n  geom_histogram(breaks=s05,fill=\"blue\",color=\"orange\")+\n  labs(x=\"Interest Rate\", title=\"Histogram of Interest Rates\")"},{"path":"descriptive.html","id":"densplots","chapter":"1 Descriptive Statistics","heading":"1.2.3 Density Plots","text":"Another visualization quantitative variable density plot. density plot can viewed smoothed version histogram. can use heights inform us values common. create density plot interest rates Figure 1.4:\nFigure 1.4: Density Plot 50 Interest Rates\nBased Figure 1.4, see low interest rates (5 12.5 percent) much common high interest rates (higher 20 percent). things note interpreting density plots:area density plot always equals 1.find proportion interest rates two values, example 10 15 percent, integrate density plot range, .e. \\(\\int_{10}^{15} f(x) dx\\), \\(f(x)\\) mathematical equation describes density plot. learn equation detail later module.values vertical axis equal probabilities (common misconception).density plot found using method called kernel density estimation (KDE). details KDE Section 4.6.1 need cover quite bit material .","code":"\n##density plot\nplot(density(Data$interest_rate), main=\"Density Plot of Interest Rates\")"},{"path":"descriptive.html","id":"considerations-with-density-plots","chapter":"1 Descriptive Statistics","heading":"1.2.3.1 Considerations with Density Plots","text":"Similar bins histograms, density plots affected bandwidth. Larger bandwidths lead smoother density plots, smaller bandwidths lead jagged density plots. create density plot uses bandwidth twice default Figure 1.5 :\nFigure 1.5: Density Plot 50 Interest Rates Larger Bandwidth\nNotice Figure 1.5 little peak interest rates 15 20 (existed Figures 1.4 also 1.2) longer exists. Using bandwidths large can smooth peaks.Thought question: bin widths histograms bandwidths density plots related?","code":"\nplot(density(Data$interest_rate, adjust=2), main=\"Density Plot of Interest Rates, with Bandwidth Twice the Default\")"},{"path":"descriptive.html","id":"ordered-statistics","chapter":"1 Descriptive Statistics","heading":"1.3 Ordered Statistics","text":"idea behind ordered statistics pretty self-explanatory: take numerical variable, order values smallest largest. Going back example interest rates 50 loan applicants, let \\(X\\) denote interest rate. \\(x_{(1)}\\) denote interest rate smallest, \\(x_{(2)}\\) denotes second smallest interest rate, \\(x_{(50)}\\) denotes largest interest rate sample 50.","code":""},{"path":"descriptive.html","id":"quantiles","chapter":"1 Descriptive Statistics","heading":"1.3.1 Quantiles","text":"Quantiles partition range numerical data continuous intervals (groups) (nearly) equal proportions. Common quantiles names:Quartiles: 4 groupsPercentiles: 100 groupsWe go quartiles detail.","code":""},{"path":"descriptive.html","id":"quart","chapter":"1 Descriptive Statistics","heading":"1.3.1.1 Quartiles","text":"Quartiles divide data 4 groups, group (nearly) equal number observations. three quartiles, denoted \\(Q_1, Q_2, Q_3\\).first group values negative infinity \\(Q_1\\).second group values negative \\(Q_1\\) \\(Q_2\\).third group values negative \\(Q_2\\) \\(Q_3\\).fourth group values negative \\(Q_3\\) infinity.\\(Q_2\\), sometimes called second quartile, easiest value find. also called median data. Going back interest rates 50 loan applicants. Using ordered statistics, median middle observation. Since even number observations, two middle observations, \\(x_{(25)}\\) \\(x_{(26)}\\). situation, median average two middle observations. Using R, find median :roughly half interest rates less 9.93 percent, roughly half interest rates greater 9.93 percent. might also recognize another term median: 50th percentile, 50 percent interest rates less 9.93.find middle observation(s) based sample size \\(n\\):\\(n\\) even, 2 middle observations position \\(\\frac{n}{2}\\) \\(\\frac{n}{2} + 1\\) ordered statistics.\\(n\\) odd, middle observation position \\(\\frac{n}{2} + 0.5\\) ordered statistics.\\(Q_1\\) \\(Q_3\\) (also called first third quartiles) found together, finding \\(Q_2\\). Note \\(Q_2\\) divides data two groups. Using interest rates example, one group contains \\(x_{(1)}, \\cdots, x_{(25)}\\), another group contains \\(x_{(26)}, \\cdots, x_{(50)}\\). \\(Q_1\\) median first group, \\(Q_3\\) median second group. 50 loan applicants:\\(Q_1\\) \\(x_{(13)}\\), \\(Q_3\\) \\(x_{(38)}\\).find values R, type:\\(Q_1\\) 7.96 percent, \\(Q_3\\) 14.08 percent. turns \\(Q_1\\) also 25th percentile, \\(Q_3\\) also 75th percentile, definition.Remember wrote following earlier:first group values negative infinity \\(Q_1\\). quarter observations interest rates less 7.96 percent.second group values negative \\(Q_1\\) \\(Q_2\\). quarter observations interest rates 7.96 9.93 percent.third group values negative \\(Q_2\\) \\(Q_3\\). quarter observations interest rates 9.93 14.08 percent.fourth group values negative \\(Q_3\\) infinity. quarter observations interest rates 14.08 percent.Note: may notice used type = 1 inside quantile() function. Using type = 1 gives values first third quartiles based method just described. actually several ways find quantiles, may result slightly differing values, although generally meet definition \\(Q_1\\) 25th percentile, \\(Q_3\\) 75th percentile.","code":"\nmedian(Data$interest_rate)## [1] 9.93\nquantile(Data$interest_rate, prob=c(0.25,0.75), type = 1)##   25%   75% \n##  7.96 14.08"},{"path":"descriptive.html","id":"percentiles","chapter":"1 Descriptive Statistics","heading":"1.3.1.2 Percentiles","text":"Another common quantile percentile. general k-th percentile value data point \\(k\\) percent observations found. earlier example, said \\(Q_3\\) interest rates 14.08 percent, also 75th percentile. 75 percent interest rates less 14.08 percent.","code":""},{"path":"descriptive.html","id":"box-plots","chapter":"1 Descriptive Statistics","heading":"1.3.2 Box Plots","text":"Another visualization used summarize quantitative data box plot. box plot summarizes 5-number summary. 5 numbers minimum, \\(Q_1, Q_2, Q_3\\), maximum. Using interest rate data, box plot shown Figure 1.6:\nFigure 1.6: Box Plot Interest Rates\npeople call box plot box whisker plot.boundaries box represent \\(Q_1\\) \\(Q_3\\).thick line box represents median.two whiskers either side box extend minimum maximum, outliers exist. outliers exist, whiskers extend minimum maximum values outliers.Generally, one quantitative variable, outlier observation whose numerical value far away rest data. words, lot smaller larger relative rest data.50 loans, two loan applicants interest rates around 25 percent flagged lot larger rest loans, reasonable since loans lot smaller 20 percent.go details outliers determined box plots. interested, can read Chapter 2.1.5 OpenIntro Statistics (Diez, Ceytinka-Rundel, Barr). Generally, working one variable, outliers observations lot larger smaller rest observations.Notice much large values (\\(Q_3\\) maximum) median, compared distance small values (\\(Q_1\\) minimum) median. indicates distribution interest rates right-skewed. Compare boxplot interest rates Figure 1.6 corresponding histogram (Figure 1.2) density plot (Figure 1.4).Thought question: can sketch box plot represents variable left-skewed? variable symmetric?","code":"\n##box plot\nggplot(Data,aes(y=interest_rate))+\n  geom_boxplot()+\n  labs(y=\"Interest Rate\", title=\"Box Plot of Interest Rates\")"},{"path":"descriptive.html","id":"ecdf","chapter":"1 Descriptive Statistics","heading":"1.3.3 Empirical Cumulative Distribution Function","text":"previous sections, can see use histograms, density plots, box plots inform us proportion observations take certain values, values data correspond certain percentiles. However, limited quartiles percentile using box plots, need find areas density plot (using integration, trivial task), add frequencies histogram (can time consuming).plot can easily give us values variable correspond percentiles empirical cumulative distribution function (ECDF) plot.Let \\(X\\) denote random variable, observed \\(n\\) observations \\(X\\) denoted \\(x_1, \\cdots, x_n\\). Let \\(x_{(1)}, \\cdots x_{(n)}\\) denote ordered statstics \\(n\\) observations. ECDF, denoted \\(\\hat{F}_n(x)\\) proportion sample observations less equal value \\(x\\) random variable. Mathematically, ECDF :\\[\n\\hat{F}_n(x) =\n  \\begin{cases}\n   0, & \\text{} x < x_{(1)} \\\\\n   \\frac{k}{n},       & \\text{} x_{(k)} \\leq x < x_{(k+1)}, k = 1, \\cdots, n-1\\\\\n   1, & \\text{} x \\geq x_{(n)}.\n  \\end{cases}\n\\]\nshall use simple toy example illustrate ECDF constructed. Suppose ask 5 people many times go gym (least 20 minutes) typical work week. answers : 3, 0, 1, 5, 3. random variable \\(X\\) many times person goes gym least 20 minutes, ordered statistics \\(x_{(1)} = 0, x_{(2)} = 1, x_{(3)} = 3, x_{(4)} = 3, x_{(5)} = 5\\). Using mathematical definition ECDF, :\\(\\hat{F}_n(x) = 0\\) \\(x < x_{(1)} = 0\\).\\(\\hat{F}_n(x) = \\frac{1}{5}\\) \\(0 \\leq x < x_{(2)} = 1\\).\\(\\hat{F}_n(x) = \\frac{2}{5}\\) \\(1 \\leq x < x_{(3)} = 3\\).\\(\\hat{F}_n(x) = \\frac{4}{5}\\) \\(3 \\leq x < x_{(5)} = 5\\). value special example since two observations \\(x=3\\).\\(\\hat{F}_n(x) = 1\\) \\(x \\geq 5\\).corresponding ECDF plot shown Figure 1.7:\nFigure 1.7: ECDF Plot Toy Example\ncan easily find percentiles plot, example, 40th percentile equal 1, going gym week. 20 percent observations go gym less 1 time week. video explains construction ECDF:Next, create ECDF plot interest rates 50 loan applicants.\nFigure 1.8: ECDF Plot Interest Rates\noverlaid horizontal line 80th percentile, can read horizontal axis corresponds interest rate 17 percent. 80 percent loan applicants interest rate less 17 percent.Thought question: try using histogram density plot interest rates (Figures 1.2 1.4) find interest rate corresponds 80th percentile. easy perform?","code":"\n##toy data\ny<-c(3, 0, 1, 5, 3)\n##ECDF plot\nplot(ecdf(y), main = \"ECDF for Toy Example\")\nplot(ecdf(Data$interest_rate), main = \"ECDF Plot of Interest Rates\")\nabline(h=0.8)"},{"path":"descriptive.html","id":"measures-of-centrality","chapter":"1 Descriptive Statistics","heading":"1.4 Measures of Centrality","text":"far, used visualizations summarize shape distribution quantitative variable. Next, look common measures centrality. Loosely speaking, measures centrality measures describe average typical value quantitative variable. common measures centrality mean, median, mode.","code":""},{"path":"descriptive.html","id":"mean","chapter":"1 Descriptive Statistics","heading":"1.4.1 Mean","text":"sample mean simply average value variable sample. sample mean random variable \\(X\\) denoted \\(\\bar{x}\\), found :\\[\\begin{equation}\n\\bar{x} = \\frac{\\sum_{=1}^n x_i}{n}.\n\\tag{1.1}\n\\end{equation}\\], toy example 5 people often go gym week, sample mean \\(\\bar{x} = \\frac{3+0+1+5+3}{5} = 2.4\\).","code":""},{"path":"descriptive.html","id":"median","chapter":"1 Descriptive Statistics","heading":"1.4.2 Median","text":"went find median section 1.3.1.1. median value middle observation ordered statistics. also called \\(Q_2\\), second quartile, 50th percentile, approximately 50 percent observations values smaller median., toy example 5 people often go gym week, sample median \\(x_{(3)} = 3\\). 50 percent people went gym less 3 times week.","code":""},{"path":"descriptive.html","id":"mode","chapter":"1 Descriptive Statistics","heading":"1.4.3 Mode","text":"Another measure mode. Mathematically speaking, mode commonly occurring value data. toy example, mode 3, since 3 occurs twice occurs often data.","code":""},{"path":"descriptive.html","id":"considerations","chapter":"1 Descriptive Statistics","heading":"1.4.4 Considerations","text":"things consider using measures centrality:mean measure people comfortable , however, caution needs used variable skewed, extreme outliers drastically alter value mean. Using toy example gym, suppose person visits gym visits 50 times, instead 5. numerical value sample mean explodes, give good representation central value many visits gym person makes week. mean fine variable symmetric.mean measure people comfortable , however, caution needs used variable skewed, extreme outliers drastically alter value mean. Using toy example gym, suppose person visits gym visits 50 times, instead 5. numerical value sample mean explodes, give good representation central value many visits gym person makes week. mean fine variable symmetric.median measure recommended skewed distributions, since order associated ordered statistics influenced extreme outliers. Using gym example, previous bullet point, median unaffected.median measure recommended skewed distributions, since order associated ordered statistics influenced extreme outliers. Using gym example, previous bullet point, median unaffected.mean larger median indication distribution right-skewed. Using interest rate example, :mean larger median indication distribution right-skewed. Using interest rate example, :consistent right skew saw histogram density plot Figures 1.2 1.4. Conversely, left-skewed distribution usually mean smaller median. symmetric distribution typically similar values mean median.mean considered sensitive measure, since numerical value can drastically affected outliers. median considered robust measure, since numerical value resistant less affected outliers.mean considered sensitive measure, since numerical value can drastically affected outliers. median considered robust measure, since numerical value resistant less affected outliers.mathematical definition mode can difficult use variables continuous, since likely observations value variable continuous. instance, mode typically refers bin histogram tallest. , using histogram Figure 1.2 interest rates, mode 7.5 10 percent.mathematical definition mode can difficult use variables continuous, since likely observations value variable continuous. instance, mode typically refers bin histogram tallest. , using histogram Figure 1.2 interest rates, mode 7.5 10 percent.","code":"\nmean(Data$interest_rate)## [1] 11.5672\nmedian(Data$interest_rate)## [1] 9.93"},{"path":"descriptive.html","id":"measures-of-spread","chapter":"1 Descriptive Statistics","heading":"1.5 Measures of Spread","text":"previous sections, learned summarizing features quantitative variable, using visualizations summarize shape, using measures centrality describe average typical values variable. One feature can summarize spread, associated values quantitative variable. Measures spread considered way measure uncertainty. Data larger spread uncertainty.","code":""},{"path":"descriptive.html","id":"variance-and-standard-deviation","chapter":"1 Descriptive Statistics","heading":"1.5.1 Variance and Standard Deviation","text":"One measure spread variance. sample variance random variable \\(X\\) denoted \\(s^2\\), sometimes \\(s_x^2\\), found :\\[\\begin{equation}\ns^2 = \\frac{\\sum_{=1}^n (x_i-\\bar{x})^2}{n-1}.\n\\tag{1.2}\n\\end{equation}\\]variance can interpreted approximate average squared distance observations mean. formula equation (1.2) may look bit complicated, let us use toy example asked 5 people often go gym workweek. answers : 3, 0, 1, 5, 3, earlier found sample mean \\(\\bar{x} = 2.4\\). calculate sample variance:\\[\n\\begin{split}\ns^2 &= \\frac{\\sum_{=1}^n (x_i-\\bar{x})^2}{n-1}\\\\\n&= \\frac{(3-2.4)^2 + (0-2.4)^2 + (1-2.4)^2 + (5-2.4)^2 + (3-2.4)^2}{5-1} \\\\\n&= 3.8\n\\end{split}\n\\]Notice numerator equation (1.2): take difference observed value sample mean, square differences, add squared differences. divide \\(n-1\\), rather \\(n\\), hence sample variance approximate averaged squared distance observations mean. nuance mathematics divide \\(n-1\\) instead \\(n\\), may intuitive . turns dividing \\(n-1\\) makes sample variance unbiased estimator true variance population (denoted \\(\\sigma^2\\)) reliable divided \\(n\\). go detail later module covering additional concepts.video explains calculation sample variance:Larger values sample variance indicate observations generally away sample mean, indicating larger spread, higher degree uncertainty future values.Thought question: mean sample variance set observations 0? indicate little () uncertainty set observations?Another related measure sample standard deviation, square root sample variance. Similar variance, larger values indicated spread data.","code":""},{"path":"descriptive.html","id":"interquartile-range","chapter":"1 Descriptive Statistics","heading":"1.5.2 Interquartile Range","text":"Another measure spread interquartile range (IQR), difference third first queartiles,\\[\\begin{equation}\nIQR = Q_3 - Q_1.\n\\tag{1.3}\n\\end{equation}\\]IQR considered robust measure spread, sample variance standard deviations considered sensitive.","code":""},{"path":"probability.html","id":"probability","chapter":"2 Probability","heading":"2 Probability","text":"module based Introduction Probability (Blitzstein, Hwang), Chapters 1 2. can access book free https://stat110.hsites.harvard.edu/ (click Book). Please note cover additional topics, skip certain topics book. may skip: Sections 1.4, 1.5, Theorem 1.6.3, Examples 1.6.4, 2.4.5, 2.5.12, 2.7.3 book.","code":""},{"path":"probability.html","id":"introduction-to-probability","chapter":"2 Probability","heading":"2.1 Introduction to Probability","text":"way quantifying uncertainty probability. Think statements: “100% certain rain next hour” “50% certain rain next hour”. percentages used reflect degree certainty event happening. first statement reflects certainty; second reflects uncertainty statement implies belief equally likely rain . module, learn basic concepts probability.","code":""},{"path":"probability.html","id":"why-study-probability","chapter":"2 Probability","heading":"2.1.1 Why Study Probability?","text":"book (Section 1.1) lists 10 different applications probability, many applications. go far say anything deals data also deal probability.","code":""},{"path":"probability.html","id":"frequentiest-and-bayesian-view-of-probability","chapter":"2 Probability","heading":"2.1.2 Frequentiest and Bayesian View of Probability","text":"couple viewpoints interpret probability: frequentist Bayesian. Consider statement “flip fair coin, coin 50% chance landing heads”.frequentist viewpoint views probability relative frequency associated event repeated infinite number times. interpret 50% probability : flip coin many many times, 50% times result coin landing heads.frequentist viewpoint views probability relative frequency associated event repeated infinite number times. interpret 50% probability : flip coin many many times, 50% times result coin landing heads.Bayesian viewpoint views probability measure belief, certainty, event happen. interpret 50% probability : heads tails equally likely occur coin flip.Bayesian viewpoint views probability measure belief, certainty, event happen. interpret 50% probability : heads tails equally likely occur coin flip.coin flip example, interpretations reasonable. However, instances, frequentist interpretation may interpretable repeat event many times. example, earlier statement rain: “50% certain rain next hour”. Whether rain next hour repeatable event, frequentist interpretation makes less sense .","code":""},{"path":"probability.html","id":"key-concepts-in-probability","chapter":"2 Probability","heading":"2.2 Key Concepts in Probability","text":"section, cover basic terminology foundational ideas probability.","code":""},{"path":"probability.html","id":"sample-space","chapter":"2 Probability","heading":"2.2.1 Sample Space","text":"sample space experiment, denoted \\(S\\), set possible outcomes experiment.rest module, use following example: consider standard deck 52 cards, draw one card random. card drawn? sample space experiment can viewed list 52 cards, per Figure 2.1 .\nFigure 2.1: Sample Space Drawing One Card Standard Deck. Picture https://en.wikipedia.org/wiki/Standard_52-card_deck\ndefinition sample space may appear elementary, writing sample space almost always first step performing probability calculations.","code":""},{"path":"probability.html","id":"event","chapter":"2 Probability","heading":"2.2.2 Event","text":"event subset sample space, usually denoted upper case letter. example, let \\(\\) denote event draw card black suit (spades clubs), let \\(B\\) denote event draw picture card (Jack, Queen, King). Events \\(\\) \\(B\\) shown Figures Figure 2.2 Figure 2.3 .\nFigure 2.2: Event \\(\\) (Blue)\n\nFigure 2.3: Event \\(B\\) (gold)\nsample space experiment can finite infinite. card example, sample space finite since can actually write possible outcomes. number possible outcomes infinite (.e. write entire list possible outcomes), sample space infinite.assign probability event. probability event \\(\\) happening \\(P()\\). outcome sample space equally likely finite sample space, probability event number outcomes belonging event divided number outcomes sample space. Using card example, \\(P() = \\frac{26}{52} = \\frac{1}{2}\\) \\(P(B) = \\frac{12}{52} = \\frac{3}{13}\\).","code":""},{"path":"probability.html","id":"complements","chapter":"2 Probability","heading":"2.2.3 Complements","text":"complement event set outcomes belong event. example, complement \\(\\), denoted \\(^c\\), drawing card red suit (hearts diamonds). One way think complements complement event event happening. Loking Figure 2.2, cards outlined blue. example, \\(P(^c) = \\frac{26}{52} = \\frac{1}{2}\\).Thought question: probability drawing non picture card?examples, might realize probability associated complement event can found subtracting probability event 1, .e.\\[\\begin{equation}\nP(^c) = 1 - P().\n\\tag{2.1}\n\\end{equation}\\]Sometimes, calculation probability complement event much less tedious probability event. instance, equation (2.1) useful.","code":""},{"path":"probability.html","id":"unions","chapter":"2 Probability","heading":"2.2.4 Unions","text":"union events least one events happen. example, union events \\(\\) \\(B\\), denoted \\(\\cup B\\), event card drawn either black suit, picture card, black suit picture card. reflected Figure 2.4.\nFigure 2.4: Union , B (blue gold, blue gold)\nfind \\(P(\\cup B)\\), can refer Figure 2.4 just count number outcomes belong either event \\(\\) (black suit) event \\(B\\) (picture card), find \\(\\frac{32}{52}\\).union \\(\\) \\(B\\) can viewed event either event \\(\\) \\(B\\) () happens.","code":""},{"path":"probability.html","id":"intersections","chapter":"2 Probability","heading":"2.2.5 Intersections","text":"intersection events events happen. Using example, intersection events \\(\\) \\(B\\) denoted \\(\\cap B\\), event card drawn black suit picture card. Using Figure 2.4, outcomes belonging \\(\\cap B\\) cards outlined blue gold. probability \\(P(\\cap B) = \\frac{6}{52}\\).","code":""},{"path":"probability.html","id":"addition-rule","chapter":"2 Probability","heading":"2.2.6 Addition Rule","text":"common mistake can made calculating \\(P(\\cup B)\\) just add probabilities individual event, mistake say probability \\(\\frac{26}{52} + \\frac{12}{52} = \\frac{38}{52}\\). problem approach outcomes belong events (black picture cards) get counted twice, want count . leads following formula calculating probabilities involving unions two events, sometimes called addition rule probability:\\[\\begin{equation}\nP(\\cup B) = P() + P(B) - P(\\cap B).\n\\tag{2.2}\n\\end{equation}\\]Using equation (2.2), \\(P(\\cup B) = \\frac{26}{52} + \\frac{12}{52} - \\frac{6}{32} = \\frac{32}{52}\\).video explains addition rule example bit detail:","code":""},{"path":"probability.html","id":"disjoint-or-mutually-exclusive-events","chapter":"2 Probability","heading":"2.2.7 Disjoint or Mutually Exclusive Events","text":"previous discussion leads idea disjoint, mutually exclusive events. Events disjoint happen simultaneously. card example, events \\(\\) \\(B\\) disjoint, since \\(\\) \\(B\\) can happen simultaneously, since card drawn can black picture card, e.g. draw king spades.Using Figure 2.4 visual example, can see events \\(\\) \\(B\\) disjoint since outcomes blue overlap outcomes gold.Suppose define another event, \\(C\\), denote card drawn Ace. events \\(B\\) \\(C\\) disjoint since card drawn picture card ace. definition disjoint events leads following: events disjoint, probability intersection 0.Using Figure 2.5 visual example, can see events \\(B\\) \\(C\\) disjoint since outcomes gold pink overlap.\nFigure 2.5: Events B, C (gold pink respectively)\nApplying idea equation (2.2), following disjoint events: disjoint events, probability least one event happening sum probabilities event.","code":""},{"path":"probability.html","id":"axioms-of-probability","chapter":"2 Probability","heading":"2.2.8 Axioms of Probability","text":"following called axioms probability, considered foundation properties associated probability:probability event, \\(E\\), non negative, .e. \\(P(E) \\geq 0\\).probability least one outcome sample space occurs 1, .e.\\(P(S) = 1\\).\\(A_1, A_2, \\cdots\\) disjoint events, \\[\nP(\\bigcup\\limits_{=1}^{\\infty} A_{}) = \\sum_{=1}^{\\infty} P(A_i).\n\\]\nwords, disjoint events, probability least one event happens sum individual probabilities.Note: writers list three axioms. book combines first two axioms 1, write two axioms.can easily see equations (2.1) (2.2) can derived axioms. Note equations axioms apply circumstances, regardless whether sample space finite .","code":""},{"path":"probability.html","id":"condprob","chapter":"2 Probability","heading":"2.3 Conditional Probability","text":"concept conditional probability appears almost statistical data science models. statistical models logistic regression, trying use observable data (called predictors, input variables, etc) model probabilities associated different values outcome random (called response variable, output variable, etc). observable data predictive outcome, probabilities associated outcome indicate greater certainty, observable data. Conditional probabilities allows us incorporate observable data, evidence, evaluating uncertainty random outcomes.Consider headed lunch, need decide want bring umbrella (assuming bring umbrella think going rain). working windowless basement internet, high degree uncertainty evaluating rain . However, look outside observe current weather conditions heading , likely higher degree certainty evaluating rain . Conditional probabilities allow us incorporate see prediction random event.use language probability denote example, let \\(R\\) denote event rain go lunch. working windowless basement internet, calculating \\(P(R)\\), probability rain go lunch. able incorporate current weather conditions, probability denoted \\(P(R|data)\\), data denotes current observe weather conditions. \\(P(R|data)\\) can read probability rain go lunch, given observed weather. example, can see \\(P(R)\\) \\(P(R|data)\\) different, since update probability given useful information. Notice \\(|\\) symbol inside probability. symbol implies working conditional probability, given observed information listed \\(|\\).","code":""},{"path":"probability.html","id":"def","chapter":"2 Probability","heading":"2.3.1 Definition","text":"\\(X\\) \\(Y\\) events, \\(P(X)>0\\), conditional probability \\(Y\\) given \\(X\\), denoted \\(P(Y|X)\\), \\[\\begin{equation}\nP(Y|X) = \\frac{P(Y \\cap X)}{P(X)}.\n\\tag{2.3}\n\\end{equation}\\]definition, want update probability \\(Y\\) happening, given observed \\(X\\). \\(X\\) can viewed observable data evidence want incorporate.Bayesian viewpoint probability, \\(P(Y)\\) called prior probability \\(Y\\) since reflects belief \\(Y\\) observing data. \\(P(Y|X)\\) called posterior probability \\(Y\\), reflects update belief \\(Y\\) incorporating observed data.Let us go back standard deck cards example. Let us find \\(P(B|)\\), probability card picture card, given know card black suit. Visually, can use definition conditional probability using Figure 2.6 .\nFigure 2.6: Events , given B\ntold card black suit, 26 possible outcomes consider, red cards eliminated crossed Figure 2.6. 26 outcomes, many picture cards? probability \\(P(B|)\\) \\(\\frac{6}{26}\\). Figure 2.6 represents frequentist viewpoint conditional probability: \\(P(B|)\\) represents long run proportion picture cards among cards black suits.can also apply equation (2.3): \\(P(B|) = \\frac{\\frac{6}{52}}{\\frac{1}{2}} = \\frac{6}{26}\\) gives answer.video explains conditional probability example bit detail:Thought question: work probability card drawn black suit, given know card picture card.can see example general \\(P(Y|X) \\neq P(X|Y)\\). informs us need extremely careful writing conditional probabilities interpreting , knowing one matters analysis. example, probability feel unwell given flu close 1, probability flu given feel unwell close 1 (since many things can make feel unwell). confusion regarding conditional probabilities sometimes called confusion inverse prosecutor’s fallacy. fallacy wrongly assumes probability fingerprint match given person innocent small, means probability person innocent given fingerprint match must also small. going fallacy detail, need cover concepts.","code":""},{"path":"probability.html","id":"multiplication-rule","chapter":"2 Probability","heading":"2.3.2 Multiplication Rule","text":"equation (2.3), multiplication rule probability:\\[\\begin{equation}\nP(Y \\cap X) = P(Y|X) \\times P(X) = P(X|Y) \\times P(Y).\n\\tag{2.4}\n\\end{equation}\\]multiplication rule useful finding probability multiple events happening, aespecially events happen sequentially. example, consider drawing two cards, without replacement, standard deck cards. Without replacement means drawing first card, returned deck, 51 cards remaining first draw. Let \\(D_1\\) \\(D_2\\) denote events first draw diamond suit second draw diamond suit respectively. want find probability cards drawn diamond suits. probability can written \\(P(D_1 \\cap D_2) = P(D_1) \\times P(D_2|D_1)  = \\frac{13}{52} \\times \\frac{12}{51}  = \\frac{156}{2652}\\).","code":""},{"path":"probability.html","id":"independent-events","chapter":"2 Probability","heading":"2.3.3 Independent Events","text":"Events independent knowledge whether one event happens change probability event happening. implies \\(X\\) \\(Y\\) independent events, definition conditional probability simplifies \\(P(Y|X) = P(Y)\\). Likewise \\(P(X|Y) = P(X)\\). Applying multiplication rule, following multiplication rule independent events\\[\\begin{equation}\nP(Y \\cap X) = P(Y) \\times P(X).\n\\tag{2.5}\n\\end{equation}\\]probability events happening just product probabilities individual event, events independent.Going back example standard deck cards, \\(\\) denotes event draw card black suit (spades clubs), \\(B\\) denotes event draw picture card (Jack, Queen, King). earlier found \\(P(B) = \\frac{12}{52}\\) \\(P(B|) = \\frac{6}{26}\\). Notice two probabilities numerically equal, informs us events independent. Knowing whether card black suit change probability card picture card. makes sense intuitively since proportion cars picture black red suits.","code":""},{"path":"probability.html","id":"bayes-rule","chapter":"2 Probability","heading":"2.3.4 Bayes’ Rule","text":"definition conditional probability equation (2.3) multiplication rule equation (2.4) give us Bayes’ rule\\[\\begin{equation}\nP(Y|X) = \\frac{P(X|Y)P(Y)}{P(X)}.\n\\tag{2.6}\n\\end{equation}\\]Bayes’ rule useful want find \\(P(Y|X)\\) information regarding \\(P(X|Y)\\) available. fairly popular model called linear discriminant analysis, models conditional probability using Bayes’ rule.","code":""},{"path":"probability.html","id":"odds","chapter":"2 Probability","heading":"2.3.5 Odds","text":"odds event \\(Y\\) \\[\\begin{equation}\nodds(Y) = \\frac{P(Y)}{P(Y^c)}.\n\\tag{2.7}\n\\end{equation}\\]may realize left hand side equation (2.7) equal left hand side logistic regression equation saw Section 0.1.2.Using equation (2.7), can switch odds probability easily\\[\\begin{equation}\nP(Y) = \\frac{odds(Y)}{1 + odds(Y)}.\n\\tag{2.8}\n\\end{equation}\\]","code":""},{"path":"probability.html","id":"odds-form-of-bayes-rule","chapter":"2 Probability","heading":"2.3.6 Odds Form of Bayes’ Rule","text":"Using Bayes’ rule equation (2.6) definition odds equation (2.7), odds form Bayes’ rule\\[\\begin{equation}\n\\frac{P(Y|X)}{P(Y^c|X)} = \\frac{P(X|Y)}{P(X|Y^c)} \\frac{P(Y)}{P(Y^c)}.\n\\tag{2.9}\n\\end{equation}\\]","code":""},{"path":"probability.html","id":"law-of-total-probability","chapter":"2 Probability","heading":"2.3.7 Law of Total Probability","text":"Let \\(Y_1, Y_2, \\cdots, Y_n\\) partition sample space (\\(Y_1, Y_2, \\cdots, Y_n\\) disjoint union sample space, \\(P(Y_i) > 0\\)) \\(\\). \\[\\begin{equation}\n\\begin{split}\nP(X) &= \\sum_{=1}^n P(X|Y_i) \\times P(Y_i)\\\\\n    &= P(X|Y_1) \\times P(Y_1) + P(X|Y_2) \\times P(Y_2) + \\cdots + P(X|Y_n) \\times P(Y_n).\n\\end{split}\n\\tag{2.10}\n\\end{equation}\\]law total probability informs us way find probability \\(X\\). can divide sample space disjoint sets \\(Y_i\\), find conditional probability \\(X\\) within set, take weighted sum conditional probabilities, weighted \\(P(Y_i)\\). useful conditional probability set easy obtain.law total probability equation (2.10) can applied denominator Bayes’ rule equation (2.6) following variation Bayes’ rule:\\[\\begin{equation}\nP(Y|X) = \\frac{P(X|Y)P(Y)}{\\sum_{=1}^n P(X|Y_i) \\times P(Y_i)}.\n\\tag{2.11}\n\\end{equation}\\]","code":""},{"path":"probability.html","id":"worked-example","chapter":"2 Probability","heading":"2.3.8 Worked Example","text":"consider worked example apply Bayes’ rule law total probability. Suppose email can divided three categories: \\(E_1\\) denotes spam email, \\(E_2\\) denotes important email, \\(E_3\\) denotes important email. email must belong one categories. Let \\(F\\) denote event email contains word “free”. past data, following information:\\(P(E_1) = 0.2, P(E_2) = 0.5, P(E_3) = 0.3\\).word “free” appears 99% spam email, \\(P(F|E_1) = 0.99\\).word “free” appears 10% important email, \\(P(F|E_2) = 0.1\\).word “free” appears 5% important email, \\(P(F|E_3) = 0.05\\).receive email word free. probability spam? want find \\(P(E_1|F)\\).","code":""},{"path":"probability.html","id":"approach-1-using-bayes-rule","chapter":"2 Probability","heading":"2.3.8.1 Approach 1: Using Bayes’ Rule","text":"Using equation (2.11), \\[\n\\begin{split}\nP(E_1|F) &= \\frac{P(E_1 \\cap F)}{P(F)}\\\\\n&= \\frac{P(F|E_1) \\times P(E_1)}{P(F|E_1) \\times P(E_1) + P(F|E_2) \\times P(E_2) + P(F|E_3) \\times P(E_3)} \\\\\n&= \\frac{0.99 \\times 0.2}{0.99 \\times 0.2 + 0.1 \\times 0.5 + 0.05 \\times 0.3}\\\\\n&= 0.7528517\n\\end{split}\n\\]video goes approach little bit detail:","code":""},{"path":"probability.html","id":"approach-2-using-tree-diagrams","chapter":"2 Probability","heading":"2.3.8.2 Approach 2: Using Tree Diagrams","text":"tree diagram useful finding conditional probabilities probabilities involving intersections. visual way displaying information hand, conditional probabilities disjoint sets probabilities disjoint set. toy example, disjoint sets type email receive, \\(E_1, E_2, E_3\\), conditional probabilities disjoint sets, .e. \\(P(F|E_1), P(F|E_2)\\) \\(P(F|E_3)\\). can put information visual first splitting sample space disjoint sets \\(E_1, E_2, E_3\\), splitting disjoint set whether email word “free” (\\(F\\)) (\\(F^c\\)). information displayed tree diagram Figure 2.7.\nFigure 2.7: Tree Diagram Email Example\nsplit represented branch, write corresponding probability branch. want find probability received email spam given contains word “free”, \\(P(E_1|F)\\), using definition conditional probability equation (2.3)\\[\nP(E_1|F) = \\frac{P(E_1 \\cap F)}{P(F)}.\n\\]Looking tree diagram Figure 2.7, can label branches lead numerator \\(P(E_1 \\cap F)\\), probability email spam contains word free. shown tree diagram Figure 2.8 highlighting corresponding branches blue.\nFigure 2.8: Tree Diagram Email Example, Branch Numerator Blue\n\\(P(E_1 \\cap F) = 0.2 \\times 0.99 = 0.198\\). need find denominator \\(P(F)\\). Looking Figure 2.7, can see three branches lead email containing word free: \\(P(E_1 \\cap F)\\) \\(P(E_2 \\cap F)\\) \\(P(E_3 \\cap F)\\). shown tree diagram Figure 2.9 highlighting corresponding branches gold.\nFigure 2.9: Tree Diagram Email Example, Branches Denominator Gold\nknow probability branch, add obtain denominator \\(P(F) = 0.2 \\times 0.99 + 0.5 \\times 0.1 + 0.3 \\times 0.05 = 0.263.\\) Putting pieces together, \\[\nP(E_1|F) = \\frac{P(E_1 \\cap F)}{P(F)} = \\frac{0.198}{0.263} = 0.7528517.\n\\]Note: compare intermediate calculations approach 2, end using calculations approach 1, without referring associated equations.video goes tree diagrams little bit detail:","code":""},{"path":"probability.html","id":"confusion-of-the-inverse","chapter":"2 Probability","heading":"2.4 Confusion of the Inverse","text":"now ready talk prosecutor’s fallacy, confusion inverse, earlier mention section 2.3.1. essence, confusion happens falsely equate \\(P(X|Y)\\) equal \\(P(Y|X)\\). fact, large value \\(P(X|Y)\\) necessarily imply \\(P(Y|X)\\) also large. term prosecutor’s fallacy confusion applied criminal trial, e.g. probability abusive relationship ends murder small, probability abuse relationship ended murder lot higher.go examples based real life.","code":""},{"path":"probability.html","id":"disease-diagnostics","chapter":"2 Probability","heading":"2.4.1 Disease Diagnostics","text":"Suppose testing patient rare disease, estimated prevalent 0.5% people. Suppose medical test disease accurate. can number definitions accuracy. disease diagnostics, couple measures sensitivity, proportion people disease test positive, specificity, proportion people without disease test negative. positive test indicates person disease. Suppose sensitivity specificity high: 0.95 0.9 respectively. Suppose patient tests positive, probability patient actually disease? Assume test always indicates positive negative.example, let \\(D\\) denote event patient disease, let + denote event patient tests positive test, - denote event patient tests negative test. Given information, \\(P(D) = 0.005\\).\\(P(+|D) = 0.95\\).\\(P(-|D^c) = 0.9\\).wish find \\(P(D|+)\\). Using Bayes rule Law Total probability, \\[\n\\begin{split}\nP(D|+) &= \\frac{P(D \\cap +)}{P(+)}\\\\\n&= \\frac{P(+|D) \\times P(D)}{P(+|D) \\times P(D) + P(+|D^c) \\times P(D^c)} \\\\\n&= \\frac{0.95 \\times 0.005}{0.95 \\times 0.005 + 0.1 \\times 0.995 }\\\\\n&= 0.04556355\n\\end{split}\n\\]small probability, patient highly unlikely actually rare disease. test high sensitivity \\(P(+|D) = 0.95\\), imply patient tests positive actually disease, since \\(P(D|+)\\) low. implication rare disease, positive test imply high probability disease, even test accurate.result make sense? Essentially, large proportion small population still numerically much smaller small proportion large population. disease rare, small population people disease, almost detected test. also extremely large population people without disease, even small proportion erroneously test positive still fairly large number. among positive tests, people disease. consider following table based population 20 thousand people, Table 2.1 :Table 2.1:  Hypothetical Table Based 20,000 PeopleLook first column Table 2.1, shows number people test positive. see large proportion diseased people detected, since relatively people disease, number small, 95. small proportion people disease test positive disease, small proportion large population results relatively larger number, 1990. people test positive, \\(95 + 1990 = 2085\\) actually disease. Therefore \\(P(D|+) = \\frac{95}{2085} = 0.04556355\\).can also explain result Bayes’ viewpoint probability. Without knowing information results test, prior probability \\(P(D) = 0.005\\). However, upon seeing person positive, updated posterior probability \\(P(D|+) = 0.04556355\\), increase 0.005 knew knowing. updated posterior probability 9 times prior. believe person likely disease upon viewing positive test, knew nothing test result. posterior probability still small since value depends two pieces information: prior \\(P(D)\\) sensitivity \\(P(+|D)\\). product values belong numerator calculating \\(P(D|+)\\). denominator \\(P(+|D) \\times P(D) + P(+|D^c) \\times P(D^c)\\). prior \\(P(D)\\) extremely low, \\(P(D^c)\\) extremely close 1, since person either disease disease. \\(P(D)\\) belong extremely low, numerator close 0, value denominator close \\(P(+|D^c) \\times P(D^c)\\), therefore \\(P(D|+)\\) small.Notice talking rare diseases? confusion inverse, thinking high sensitivity implies person likely disease test positive, applies rare diseases. disease prevalent, high sensitivity likely imply person disease test positive.take tests rare diseases? ? go test . turns test positive twice rare disease, probability disease increases lot tested tested positive.perform calculation, use odds form Bayes’ rule, per equation (2.9)\\[\n\\begin{split}\n\\frac{P(D|T_1 \\cap T_2)}{P(D^c|T_1 \\cap T_2)} &= \\frac{P(T_1 \\cap T_2 | D)}{P(T_1 \\cap T_2 | D^c)} \\frac{P(D)}{P(D^c)}\\\\\n&= \\frac{0.95^2}{0.1^2} \\frac{0.005}{0.995} \\\\\n&= 0.4535176\n\\end{split}\n\\]\\(T_1\\) \\(T_2\\) denote events person test positive first test second test respectively. also assume results test independent previous tests.odds disease given person positive twice 0.4535176. Therefore, using equation (2.8), corresponding probability disease given person tested positive twice \\(P(D|T_1 \\cap T_2) = \\frac{0.4535176}{1+0.4535176} = 0.3120138\\). See posterior probability increased two positive tests, 1 positive test.Thought question: perform calculations show posterior probability person disease person tests positive 3 tests 0.8116199.Thought question: notice certain pattern emerging performing calculations person undergoes tests? write either mathematical equation, even function R, allows us quickly compute probability person disease given person tested positive \\(k\\) times, \\(k\\) can denote non negative integer?","code":""},{"path":"probability.html","id":"prosecutors-fallacy","chapter":"2 Probability","heading":"2.4.2 Prosecutor’s Fallacy","text":"confusion inverse also called prosecutor’s fallacy (sometimes also called defense attorney’s fallacy depending side making mistake) occurs legal setting. Generally, confusion comes equating P(evidence|innocent) P(innocent|evidence).book provides discussion Section 2.8, examples 2.8.1 2.8.2.","code":""},{"path":"discrete-random-variables.html","id":"discrete-random-variables","chapter":"3 Discrete Random Variables","heading":"3 Discrete Random Variables","text":"module based Introduction Probability (Blitzstein, Hwang), Chapters 3 4. can access book free https://stat110.hsites.harvard.edu/ (click Book). Please note cover additional topics, skip certain topics book. may skip Sections 3.4, 3.9, Example 4.2.3, Section 4.3, Example 4.4.6, 4.4.7, Theorem 4.4.8, Example 4.4.9, 4.6.4, 4.7.4, 4.7.7, Section 4.9 book.","code":""},{"path":"discrete-random-variables.html","id":"random-variables","chapter":"3 Discrete Random Variables","heading":"3.1 Random Variables","text":"idea behind random variables simplify notation regarding probability, enable us summarize results experiments, make easier quantify uncertainty.","code":""},{"path":"discrete-random-variables.html","id":"example","chapter":"3 Discrete Random Variables","heading":"3.1.1 Example","text":"Consider flipping coin three times recording lands heads tails time. sample space experiment \\(S = \\{HHH, HHT, HTH, THH, HTT, THT, TTH, TTT\\}\\). Given outcome equally likely, probability associated outcome \\(\\frac{1}{8}\\).Suppose want find probability get exactly 2 heads 3 flips. express :\\(P(\\text{two heads three flips})\\), \\(P(HHT \\cup HTH \\cup THH)\\), \\(P()\\) \\(\\) denotes event getting two heads three flips.Another way define random variable \\(X\\) expresses event bit efficiently. Let \\(X\\) denote number heads three flips, another way write \\(P(X=2\\)). idea behind random variables: assign events number.","code":""},{"path":"discrete-random-variables.html","id":"definition","chapter":"3 Discrete Random Variables","heading":"3.1.2 Definition","text":"random variable (RV) function sample space real numbers.convention, denote random variables capital letters. Using 3 coin flip example, \\(X\\) 0, 1, 2, 3. assign number possible outcome sample space.Random variables provide numerical summaries experiment. can useful especially sample space complicated. Random variables can also used non numeric outcomes.","code":""},{"path":"discrete-random-variables.html","id":"discrete-vs-continuous","chapter":"3 Discrete Random Variables","heading":"3.1.3 Discrete Vs Continuous","text":"One key distinctions make random variables determine discrete continuous. way express probabilities random variables depends whether random variable discrete continuous.discrete random variable can take countable (finite infinite) number values.number heads 3 coin flips, \\(X\\) countable finite, since can actually list values can take \\(\\{0,1,2,3 \\}\\) 4 values. \\(X\\) must take one 4 numerical values; number outside list. discrete.random variable countable infinite can list values can take, list end. example, number people using crosswalk 10 year period take values \\(\\{0, 1, 2, 3, \\cdots \\}\\). number take infinite number values, values whole numbers occur. number people using crosswalk 10 year period discrete random variable.continuous random variable can take uncountable number values interval real numbers.example, height American adult continuous random variable, height can take value interval interval, say 40 100 inches. values 40 100 possible.module, focus discrete random variables.support discrete random variable \\(X\\) set values \\(X\\) can take \\(P(X = x) > 0\\), .e. set values non zero probability happening. Using 3 coin flips example, \\(X\\) number heads 3 coin slips, support \\(\\{0,1,2,3 \\}\\). support discrete random variables usually integers.Thought question: Can come examples discrete continuous random variables ? Feel free search internet examples well.","code":""},{"path":"discrete-random-variables.html","id":"probability-mass-functions-pmfs","chapter":"3 Discrete Random Variables","heading":"3.2 Probability Mass Functions (PMFs)","text":"use probability describe behavior random variables. called distribution random variable. distribution random variable specifies probabilities events associated random variable.discrete random variables, distribution specified probability mass function (PMF). PMF discrete random variable \\(X\\) function \\(P_X(x) = P(X=x)\\). positive \\(x\\) support \\(X\\), 0 otherwise.Note: notation random variables, capital letters \\(X\\) denote random variables, lower case letters \\(x\\) denote actual numerical values. want find probability 2 heads 3 coin flips, write \\(P(X=2)\\), \\(x\\) 2 example.Going back example record number heads 3 coin flips, can write PMF random variable \\(X\\):\\(P_X(0) = P(X=0) = P(TTT) = \\frac{1}{8}\\),\\(P_X(1) = P(X=1) = P(HTT \\cup THT \\cup TTH) = \\frac{3}{8}\\),\\(P_X(2) = P(X=2) = P(HHT \\cup THH \\cup HTH) = \\frac{3}{8}\\),\\(P_X(3) = P(X=3) = P(HHH) = \\frac{1}{8}\\).Fairly often, PMF discrete random variable presented simple table like Table 3.1 :Table 3.1: PMF XOr PMF can represented using simple plot like one Figure 3.1:\nFigure 3.1: PMF X\nPMF provides list possible values random variable corresponding probabilities. words, PMF describes distribution relative frequencies outcome. experiment, observing 1 2 heads equally likely, occur three times often observing 0 3 heads. Observing 0 3 heads also equally likely.","code":"\n##support\nx<-0:3\n## PMF for each value in the support. \nPMFs<-c(1/8, 3/8, 3/8, 1/8)\n## create plot of PMF vs each value in support\nplot(x, PMFs, type=\"h\", main = \"PMF for X\", xlab=\"# of heads\", ylab=\"Probability\", ylim=c(0,1))"},{"path":"discrete-random-variables.html","id":"valid-pmfs","chapter":"3 Discrete Random Variables","heading":"3.2.1 Valid PMFs","text":"Consider discrete random variable \\(X\\) support \\(x_1, x_2, \\cdots\\). PMF \\(P_X(x)\\) \\(X\\) must satisfy:\\(P_X(x) > 0\\) \\(x = x_j\\), \\(P_X(x) = 0\\) otherwise.\\(\\sum_{j=1}^{\\infty} P_X(x_j) = 1\\).words, probabilities associated support greater 0, sum probabilities across whole support must add 1.Thought question: based Table 3.1, can see PMF \\(X\\) valid?","code":""},{"path":"discrete-random-variables.html","id":"pmfhist","chapter":"3 Discrete Random Variables","heading":"3.2.2 PMFs and Histograms","text":"Recall frequentist viewpoint probability, represents relative frequency associated event repeated infinite number times.Consider experiment flip coin 3 times count number heads. support random variable \\(X\\), number heads, \\(\\{0,1,2,3 \\}\\). Imagine performing experiment large number times. time perform experiment, record number heads. performed experiment one million times, recorded one million values number heads, value must support \\(X\\). create histogram one million values number heads, shape histogram close shape plot PMF Figure 3.1. Figure 3.2 shows resulting histogram performing experiment 1 million times.\nFigure 3.2: Histogram Experiment Performed 1 Million Times\ngeneral, PMF random variable match histogram long run.Note: just done use simulations repeat experiment large number times.","code":""},{"path":"discrete-random-variables.html","id":"cumulative-distribution-functions-cdfs","chapter":"3 Discrete Random Variables","heading":"3.3 Cumulative Distribution Functions (CDFs)","text":"Another function used describe distribution discrete random variables cumulative distribution function (CDF). CDF random variable \\(X\\) \\(F_X(x) = P(X \\leq x)\\). Notice unlike PMF, definition CDF applies discrete continuous random variables.Going back example record number heads 3 coin flips, can write CDF random variable \\(X\\):\\(F_X(0) = P(X \\leq 0) = P(X=0) = \\frac{1}{8}\\),\\(F_X(1) = P(X \\leq 1) = P(X=0) + P(X=1) = \\frac{1}{8} +  \\frac{3}{8} = \\frac{1}{2}\\),\\(F_X(2) = P(X \\leq 2) = P(X=0) + P(X=1) + P(X=2) = \\frac{1}{2} + \\frac{3}{8} = \\frac{7}{8}\\),\\(F_X(3) = P(X \\leq 3) = P(X=0) + P(X=1) + P(X=2) + P(X=3) = \\frac{7}{8} + \\frac{1}{8} = 1\\).Notice calculations based PMF. find \\(P(X \\leq x)\\), summed PDF values support less equal \\(x\\). Therefore, another way write CDF discrete random variable \\[\\begin{equation}\nF_X(x) = P(X \\leq x) = \\sum_{x_j \\leq x} P(X=x_j).\n\\tag{3.1}\n\\end{equation}\\]Fairly often, CDF discrete random variable presented simple table like Table 3.2 :Table 3.2: CDF XOr simple plot like Figure 3.3 :\nFigure 3.3: CDF X\nCDF discrete random variables always look like step function, increases discrete jumps value support. height jump corresponds PMF value support.Thought question: see similarities CDF empirical cumulative density function (ECDF) section 1.3.3?","code":""},{"path":"discrete-random-variables.html","id":"valid-cdfs","chapter":"3 Discrete Random Variables","heading":"3.3.1 Valid CDFs","text":"CDF \\(F_X(x)\\) \\(X\\) must:non decreasing. means \\(x\\) gets larger, CDF either stays increases. Visually, graph CDF never decreases \\(x\\) increases.approach 1 \\(x\\) approaches infinity approach 0 \\(x\\) approaches negative infinity. Visually, graph CDF equal close 1 large values x, equal close 0 small values x.Thought question: Look CDF example Figure 3.3, see satisfies criteria listed valid CDF.","code":""},{"path":"discrete-random-variables.html","id":"expectations","chapter":"3 Discrete Random Variables","heading":"3.4 Expectations","text":"previous section, see PMFs CDFs can used describe distribution random variable. PMF can viewed long-run version histogram, gives us idea shape distribution. Similar Section 1, also interested measures centrality spread random variables.measure centrality random variables expectation, expected value. expectation random variable can interpreted long-run mean random variable, .e. able repeat experiment infinite number times, expectation random variable average result among experiments.discrete random variable \\(X\\) support \\(x_1, x_2, \\cdots,\\), expected value, denoted \\(E(X)\\), \\[\\begin{equation}\nE(X) = \\sum_{j=1}^{\\infty} x_j P(X=x_j).\n\\tag{3.2}\n\\end{equation}\\]can use Table 3.1 example. find expected number heads 3 coin flips, using equation (3.2),\\[\n\\begin{split}\nE(X) &= 0 \\times \\frac{1}{8} + 1 \\times \\frac{3}{8} + 2 \\times \\frac{3}{8} + 3 \\times \\frac{1}{8}\\\\\n       &= 1.5\n\\end{split}\n\\]take product value support random variable corresponding probability, add products.can see another interpretation expected value random variable calculation: weighted average values random variable, weighted probabilities.Intuitively, expected value 1.5 make sense. flip coin 3 times, coin fair, expect half flips land heads, 1.5 flips land heads.View video detailed explanation calculate expected values:","code":""},{"path":"discrete-random-variables.html","id":"linearity-of-expectations","chapter":"3 Discrete Random Variables","heading":"3.4.1 Linearity of Expectations","text":"seen calculate expected value random variable \\(X\\) using equation (3.2). need PMF \\(X\\). Sometimes random variable can viewed sum (difference) random variables, involve multiplication / adding constant random variable. Consider scenarios:Suppose friend fisherman. Let \\(Y\\) random variable describing number fish catch workday, let \\(W\\) random variable describing number fish friend catches workday. can let \\(T = Y+W\\) random variable describing total number fish catch workday.Suppose friend fisherman. Let \\(Y\\) random variable describing number fish catch workday, let \\(W\\) random variable describing number fish friend catches workday. can let \\(T = Y+W\\) random variable describing total number fish catch workday.Suppose sell fish $10 friend sells fish $15. can let \\(R = 10Y + 15W\\) random variable describes revenue generate workday.Suppose sell fish $10 friend sells fish $15. can let \\(R = 10Y + 15W\\) random variable describes revenue generate workday.Suppose friend rent space market sell fish, costs $5 day rent space. can let \\(G = 10Y + 15W - 5\\) random variable describes gross income day.Suppose friend rent space market sell fish, costs $5 day rent space. can let \\(G = 10Y + 15W - 5\\) random variable describes gross income day.examples involve new random variables, \\(T, R, G\\) can based previously defined random variables, \\(Y, W\\). turns find expectations new random variables, need expectations previously defined random variables. need find PMFs \\(T, R\\) \\(S\\) apply equation (3.2).can done linearity expectations: Let \\(X\\) \\(Y\\) denote random variables, \\(,b,c\\) denote constants, \\[\\begin{equation}\nE(aX + + c) = aE(X) + (Y) + c.\n\\tag{3.3}\n\\end{equation}\\]Applying equation (3.3) fishing examples:\\(E(T) = R(Y + W) = E(Y) + E(W)\\),\\(E(R) = E(10Y + 15W) = 10E(Y) + 15E(W)\\),\\(E(G) = E(10Y + 15W - 5) = 10E(Y) + 15E(W) - 5\\).need find expected values total number fish, revenue generated, gross income expected values number fish us caught. need PMFs \\(T,R,G\\).","code":""},{"path":"discrete-random-variables.html","id":"visual-explanation","chapter":"3 Discrete Random Variables","heading":"3.4.1.1 Visual Explanation","text":"visual explanation equation (3.3) makes sense, go back previous example \\(X\\) denotes number heads 3 coin flips. Figure 3.1 displays PMF random variable. Let us create PMF new random variable \\(Y=2X\\) display Figure 3.4 :\nFigure 3.4: PMF X Y=2X\nNote red vertical lines represent expected value random variable, since PMFs symmetric, expected value lies right middle support. Comparing PMFs Figure 3.4, get \\(Y\\) multiplying \\(X\\) 2. support \\(Y\\) now \\(\\{0,2,4,6\\}\\) associated probabilities unchanged, heights probabilities vertical axis unchanged. Therefore, center, expected value, multiplied constant.Consider another random variable \\(W = X+3\\). create PMF \\(W\\) display Figure 3.5 :\nFigure 3.5: PMF X W=X+3\nNotice PMFs \\(X\\) \\(W\\) look almost exactly . difference every value support \\(X\\) shifted 3 units. probabilities stay , heights PMFs unchanged. every value shifted 3 units, expected value, long-run average, also gets shifted 3 units. Adding constant random variable shifts expected value accordingly.","code":"\n##support of X\nx<-0:3\n## PMF for each value in the support. \nPMFs<-c(1/8, 3/8, 3/8, 1/8)\nEX<-1.5\n\n##support of Y\ny<-2*x\n## PMF for each value in the support. \nPMFs<-c(1/8, 3/8, 3/8, 1/8)\nEY<-2*EX\n\npar(mfrow=c(2,1))\n\n## create plot of PMF vs each value in support\nplot(x, PMFs, type=\"h\", main = \"PMF for X\", xlab=\"X\", ylab=\"Probability\", ylim=c(0,1))\n##overlay a line representing EX in red\nabline(v=EX, col=\"red\")\n\n## create plot of PMF vs each value in support\nplot(y, PMFs, type=\"h\", main = \"PMF for Y\", xlab=\"Y\", ylab=\"Probability\", ylim=c(0,1))\n##overlay a line representing EY in red\nabline(v=EY, col=\"red\")\n##support of X\nx<-0:3\n## PMF for each value in the support. \nPMFs<-c(1/8, 3/8, 3/8, 1/8)\nEX<-1.5\n\n##support of W\nw<-x+3\n## PMF for each value in the support. \nPMFs<-c(1/8, 3/8, 3/8, 1/8)\nEW<-EX+3\n\npar(mfrow=c(2,1))\n\n## create plot of PMF vs each value in support\nplot(x, PMFs, type=\"h\", main = \"PMF for X\", xlab=\"X\", ylab=\"Probability\", ylim=c(0,1))\n##overlay a line representing EX in red\nabline(v=EX, col=\"red\")\n\n## create plot of PMF vs each value in support\nplot(w, PMFs, type=\"h\", main = \"PMF for w\", xlab=\"W\", ylab=\"Probability\", ylim=c(0,1))\n##overlay a line representing EW in red\nabline(v=EW, col=\"red\")"},{"path":"discrete-random-variables.html","id":"one-more-example","chapter":"3 Discrete Random Variables","heading":"3.4.1.2 One More Example","text":"look one example illustrate usefulness linearity expectations. Consider drunk man walk one-dimensional number line starts 0 position. step drunk man takes, either moves forward, backward, stays spot. steps forward probability \\(p_f\\), backward probability \\(p_b\\), stays spot probability \\(p_s\\), \\(p_f + p_b+p_s = 1\\). Let \\(Y\\) position number line drunk man 2 steps. expected position drunk man two steps, .e. \\(E(Y)\\)? Assume step independent.Using brute force, can find PMF \\(Y\\), find \\(E(Y)\\) using equation (3.2). First, need find sample space \\(Y\\). two steps, sample space \\(\\{-2,-1,0,1,2\\}\\). Next, need find probabilities associated outcome sample space.\\(Y=-2\\), man must move backward step. probability \\(P(Y=-2) = p_b^2\\).Likewise, \\(Y=2\\), man must move forward step. probability \\(P(Y=2) = p_f^2\\).\\(Y=-1\\), man stay first step, move back second, move back first step, stay second. probability \\(P(Y=-1) = p_s p_b + p_b p_s = 2p_b p_s\\).Likewise, \\(Y=1\\), man stay first step, move forward second, move forward first step, stay second. probability \\(P(Y=1) = p_s p_f + p_f p_s = 2p_f p_s\\).\\(Y=0\\), man move forward, backward, move backward forward, stay steps. \\(P(Y=0) = p_f p_b + p_b p_f + p_s^2 = p_s^2 + 2 p_b p_f\\).Using equation (3.2),\\[\n\\begin{split}\nE(Y) &= -2 \\times p_b^2 + -1 \\times 2p_b p_s + 0 \\times p_s^2 + 2 p_b p_f + 1 \\times 2p_f p_s + 2 \\times p_f^2 \\\\\n       &= 2 (p_f - p_b)\n\\end{split}\n\\]Note: skipped lot messy algebra get end result. Even skipping messy algebra, setting PMF quite bit work.Suppose use linearity expectations equation (3.3). Let \\(Y_1, Y_2\\) denote distance man moves step 1 2 respectively. \\(Y = Y_1 + Y_2\\). sample \\(Y_1\\) \\(Y_2\\) : \\(\\{-1,0,1\\}\\). \\(Y_1\\) \\(Y_2\\) following PMF:\\(P(Y_i = -1) = p_b\\)\\(P(Y_i = 0) = p_s\\)\\(P(Y_i = 1) = p_f\\)using equation (3.2),\\[\n\\begin{split}\nE(Y_i) &= -1 \\times p_b + 0 \\times p_s + 1 \\times p_f \\\\\n       &= p_f - p_b\n\\end{split}\n\\]therefore \\(E(Y) = E(Y_1 + Y_2) = E(Y_1) + E(Y_2) = 2(p_f - p_b)\\). approaches resulted answer, notice much simpler obtain solution using linearity expectations. Imagine wanted find expected position 500 steps? Writing sample space 500 steps extremely long.View video detailed explanation worked example:","code":""},{"path":"discrete-random-variables.html","id":"law-of-the-unconscious-statistician","chapter":"3 Discrete Random Variables","heading":"3.4.2 Law of the Unconscious Statistician","text":"Suppose PMF random variable \\(X\\), want find \\(E(g(X))\\), \\(g\\) function \\(X\\) (can think \\(g\\) transformation performed \\(X\\)). One idea find PMF \\(g(X)\\) use definition expectation equation (3.2). seen previous subsection finding sample space transforming random variable can challenging. turns can find \\(E(g(X))\\) based PMF \\(X\\), without find PMF \\(g(X)\\).done Law Unconscious Statistician (LOTUS). Let \\(X\\) discrete random variable support \\(\\{x_1, x_2, \\cdots \\}\\), \\(g\\) function applied \\(X\\), \\[\\begin{equation}\nE(g(X)) = \\sum_{=j}^{\\infty} g(x_j) P(X=x_j).\n\\tag{3.4}\n\\end{equation}\\]application LOTUS finding variance discrete random variable.","code":""},{"path":"discrete-random-variables.html","id":"variance","chapter":"3 Discrete Random Variables","heading":"3.4.3 Variance","text":"talked shape distribution discrete random variable, expected value. One measure interested spread associated distribution. One common measure variance random variable.variance random variable \\(X\\) \\[\\begin{equation}\nVar(X) = E[(X - EX)^2]\n\\tag{3.5}\n\\end{equation}\\]standard deviation random variable \\(X\\) squareroot variance\\[\\begin{equation}\nSD(X) = \\sqrt{Var(X)}.\n\\tag{3.6}\n\\end{equation}\\]Looking equation (3.5) little closely, can see natural interpretation variance random variable: average squared distance random variable mean, long-run. equivalent formula variance random variable \\[\\begin{equation}\nVar(X) = E(X^2) - (EX)^2.\n\\tag{3.7}\n\\end{equation}\\]Equation (3.7) easier work equation (3.5) performing actual calculations.Let us now go back original example, \\(X\\) denotes number heads 3 coin flips. Earlier, found PMF random variable, per Table 3.1, found expectation 1.5. find variance \\(X\\) using equation (3.7), find \\(E(X^2)\\) first using LOTUS equation (3.4)\\[\n\\begin{split}\nE(X^2) &= 0^2 \\times \\frac{1}{8} + 1^2 \\times \\frac{3}{8} + 2^2 \\times \\frac{3}{8} + 3^2 \\times \\frac{1}{8} \\\\\n       &= 3\n\\end{split}\n\\]\\(Var(x) = 3 - 1.5^2 = \\frac{3}{4}\\).Thought question: Try find \\(Var(X)\\) using equation (3.5) LOTUS. arrive answer steps may bit complicated.View video detailed explanation calculate variance discrete random variables using equations (3.7) (3.5):","code":""},{"path":"discrete-random-variables.html","id":"var-prop","chapter":"3 Discrete Random Variables","heading":"3.4.3.1 Properties of Variance","text":"Variance following properties:\\(Var(X+c) = Var(X)\\), \\(c\\) constant. make sense, since add constant random variable, shift \\(c\\) units. shown earlier Figure 3.5, expected value also gets shifted \\(c\\) units. Variance measures average squared distance variable mean. distance, squared distance, \\(X\\) mean unchanged.\\(Var(cX) = c^2 Var(X)\\). Look Figure 3.4, notice distance value support expected value gets multiplied 2 (since \\(Y=2X\\)). multiply random variable \\(c\\), distance value support expected value multiplied \\(c\\). Since variance measures squared distance, variance gets multiplied \\(c^2\\).\\(X\\) \\(Y\\) independent random variables, \\(Var(X+Y) = Var(X) + Var(Y)\\).","code":""},{"path":"discrete-random-variables.html","id":"common-discrete-random-variables","chapter":"3 Discrete Random Variables","heading":"3.5 Common Discrete Random Variables","text":"Next, introduce commonly used distributions may used discrete random variables. number common statistical models (example, logistic regression, Poisson regression) based distributions.","code":""},{"path":"discrete-random-variables.html","id":"bernoulli","chapter":"3 Discrete Random Variables","heading":"3.5.1 Bernoulli","text":"Bernoulli distribution might simplest discrete random variable. support random variable \\(\\{0,1\\}\\). words, value random variable follows Bernoulli distribution either 0 1. Bernoulli distribution also described parameter \\(p\\), probability random variable takes value 1.formally, random variable \\(X\\) follows Bernoulli distribution parameter \\(p\\) \\(P(X=1) = p\\) \\(P(X=0) = 1-p\\), \\(0<p<1\\). Using mathematical notation, can write \\(X \\sim Bern(p)\\) express random variable \\(X\\) distributed Bernoulli parameter \\(p\\). PMF Bernoulli distribution written \\[\\begin{equation}\nP(X=k) = p^k (1-p)^{1-k}\n\\tag{3.8}\n\\end{equation}\\]\\(k=0, 1\\).enough specify random variable follows Bernoulli distribution. need also clearly specify value parameter \\(p\\). Consider following two examples describe two different experiments:Suppose flip fair coin . Let \\(Y=1\\) coin lands heads, \\(Y=0\\) coin lands tails. \\(Y \\sim Bern(\\frac{1}{2})\\) example since coin fair.Suppose flip fair coin . Let \\(Y=1\\) coin lands heads, \\(Y=0\\) coin lands tails. \\(Y \\sim Bern(\\frac{1}{2})\\) example since coin fair.Suppose asked question given 5 multiple choices, 1 correct answer. idea topic, multiple choices help, guess. Let \\(W=1\\) answer correctly, \\(W=0\\) answer incorrectly. \\(W \\sim Bern(\\frac{1}{5})\\).Suppose asked question given 5 multiple choices, 1 correct answer. idea topic, multiple choices help, guess. Let \\(W=1\\) answer correctly, \\(W=0\\) answer incorrectly. \\(W \\sim Bern(\\frac{1}{5})\\).\\(P(Y=1)\\) \\(P(W=1)\\) examples.Fairly often, Bernoulli random variable, event results random variable coded 1 called success, event results random variable coded 0 called failure. setting, parameter \\(p\\) called success probability Bernoulli distribution. experiment Bernoulli distribution can called Bernoulli trial.go back second example section 0.1.2, modeling whether job applicant receives callback . example, let \\(V\\) random variable applicant receives callback, \\(V=1\\) denoting applicant received callback, \\(V=0\\) applicant receive callback. used logistic regression example. turns logistic regression used variable interest follows Bernoulli distribution.","code":""},{"path":"discrete-random-variables.html","id":"bernprop","chapter":"3 Discrete Random Variables","heading":"3.5.1.1 Properties of Bernoulli","text":"Consider \\(X\\) Bernoulli distribution parameter \\(p\\). expectation Bernoulli distribution \\[\\begin{equation}\nE(X) = p\n\\tag{3.9}\n\\end{equation}\\]variance \\[\\begin{equation}\nVar(X) = p(1-p).\n\\tag{3.10}\n\\end{equation}\\]Thought question: Use definition expectations discrete random variables, equation (3.2), PMF Bernoulli random variable, LOTUS prove equations (3.9) (3.10).expected value equal \\(p\\) Bernoulli distribution make sense. Remember support random variable 0 1, \\(P(X=1) = p\\). Using frequentist viewpoint, flip coin record heads tails, repeat coin flipping many times, record bunch 0s 1s represent result coin flips. average bunch 0s 1s just proportion 1s.equation variance Bernoulli distribution exhibits interesting intuitive behavior. Figure 3.6 shows variance behaves vary value \\(p\\):\nFigure 3.6: Variance Bernoulli\nNotice variance maximum \\(p=0.5\\), variance minimum (fact 0) \\(p=0\\) \\(p=1\\). biased coin always lands heads, every coin flip land heads exception. variability result, utmost certainty result coin flip. hand, coin fair \\(p=0.5\\), least certainty result coin flip, variance maximum coin fair.Another application property election results (assuming 2 candidates, idea applies candidates). swing states race closer (\\(p\\) closer half), projections winner uncertainty need get data wait longer projections. states primarily vote one candidate (\\(p\\) closer 0 1), projections happen lot quicker projections less uncertainty.","code":"\np<-seq(0,1,by = 0.001) ##sequence of values for p\nBern_var<-p*(1-p) ##variance of Bernoulli\n##plot variance against p\nplot(p, Bern_var, ylab=\"Variance\", main=\"Variance of Bernoulli as p is Varied\")"},{"path":"discrete-random-variables.html","id":"binomial","chapter":"3 Discrete Random Variables","heading":"3.5.2 Binomial","text":"Suppose experiment follows Bernoulli distribution, perform experiment \\(n\\) times (sometimes called trials), time success probability \\(p\\). experiments independent . Let \\(X\\) denote number successes \\(n\\) trials. \\(X\\) follows binomial distribution parameters \\(n\\) \\(p\\) (number trials success probability). write \\(X \\sim Bin(n,p)\\) express \\(X\\) follows binomial distribution parameters \\(n\\) \\(p\\), \\(n>0\\) \\(0<p<1\\). PMF Binomial distribution written \\[\\begin{equation}\nP(X=k) = \\binom{n}{k} p^k (1-p)^{n-k}\n\\tag{3.11}\n\\end{equation}\\]\\(k=0,1,2, \\cdots, n\\), also support binomial distribution.equation (3.11), \\(\\binom{n}{k}\\) called binomial coefficient, number combinations result \\(k\\) successes \\(n\\) trials. binomial coefficient can found using\\[\\begin{equation}\n\\binom{n}{k} =  \\frac{n!}{k! (n-k)!}.\n\\tag{3.12}\n\\end{equation}\\]\\(n!\\) called n-factorial, product positive integers less equal n. \\(n! = n \\times (n-1) \\times (n-2) \\times \\cdots \\times 1.\\) example \\(5! = 5 \\times 4 \\times 3 \\times 2 \\times 1 = 120\\), using R:Note: fairly common model, logistic regression model aggregated data, based binomial distribution. mentioned logistic regression earlier. difference two (without aggregated data) based structure data frame. interested differences, please read https://www.r-bloggers.com/2021/02/--run-logistic-regression--aggregate-data--r/.go back first example counting number heads three coin flips follows binomial distribution.coin flip either heads tails. two outcomes flip.success probability, probability heads, 0.5 flip. parameter fixed flip.result flip independent flips since flips affect outcome.number trials (flips example) \\(n=3\\) specified fixed value.Since four conditions met, number heads 3 coin flips can modeled using binomial distribution. let \\(x\\) denote number heads 3 coin clips, write \\(X \\sim Bin(3,0.5)\\).Suppose want calculate \\(P(X=2)\\) using equation (3.11):\\[\n\\begin{split}\nP(X=2) &= \\binom{3}{2} (0.5)^2 (0.5)^1\\\\\n       &= \\frac{3!}{2! 1!} (0.5)^2 (0.5)^1 \\\\\n       &= 3 \\times \\frac{1}{8} \\\\\n       &= \\frac{3}{8}.\n\\end{split}\n\\]example, binomial coefficient equals 3. indicates 3 combinations obtain 2 heads 3 coin flips. \\(P(X=2)\\) can written \\(P(HHT \\cup HTH \\cup THH)\\). Solving \\(P(HHT \\cup HTH \\cup THH)\\), \\[\n\\begin{split}\nP(HHT \\cup HTH \\cup THH) &= P(HHT) + P(HTH) + P(THH)\\\\\n       &= 0.5^3 + 0.5^3 + 0.5^3 \\\\\n       &= 3 \\times \\frac{1}{8} \\\\\n       &= \\frac{3}{8}.\n\\end{split}\n\\]\nsolved using basic probability rules previous module, without using PMF binomial distribution equation (3.11). course, PMF binomial distribution gets lot convenient \\(n\\) gets larger, number combinations sample space get lot larger.can also use R find \\(P(X=2)\\):","code":"\nfactorial(5)## [1] 120\ndbinom(2,3,0.5) ##specify values of k, n, p in this order## [1] 0.375"},{"path":"discrete-random-variables.html","id":"relationship-between-binomial-and-bernoulli","chapter":"3 Discrete Random Variables","heading":"3.5.2.1 Relationship Between Binomial and Bernoulli","text":"Looking description Bernoulli binomial distributions, may notice Bernoulli random variable special case binomial random variable \\(n=1\\), .e. 1 trial.binomial random variable also sometimes viewed sum \\(n\\) independent Bernoulli random variables, value \\(p\\).","code":""},{"path":"discrete-random-variables.html","id":"properties-of-binomial","chapter":"3 Discrete Random Variables","heading":"3.5.2.2 Properties of Binomial","text":"\\(X \\sim Bin(n,p)\\), \\[\\begin{equation}\nE(X) = np\n\\tag{3.13}\n\\end{equation}\\]\\[\\begin{equation}\nVar(X) = np(1-p).\n\\tag{3.14}\n\\end{equation}\\]results make sense note relationship binomial random variable Bernoulli random variable. Suppose random variables \\(Y_1, Y_2, \\cdots, Y_n\\) Bernoulli random variables parameter \\(p\\) independent. \\(Y = Y_1 + Y_2 + \\cdots + Y_n \\sim Bin(n,p)\\). Therefore, using linearity expectations equation (3.3), \\(E(Y) = E(Y_1) + E(Y_2) + \\cdots + E(Y_n) = np\\). Since \\(Y_1, Y_2, \\cdots, Y_n\\) independent, \\(Var(Y) = Var(Y_1) + Var(Y_2) + \\cdots + Var(Y_n) = np(1-p)\\).","code":""},{"path":"discrete-random-variables.html","id":"pmfs-of-binomial","chapter":"3 Discrete Random Variables","heading":"3.5.2.3 PMFs of Binomial","text":"take look PMFs binomials, \\(n=10\\) vary \\(p\\) 0.2, 0.5, 0.9, Figure 3.7:\nFigure 3.7: PMF X, n=10, p varied\nfigure 3.7, can see distribution binomial symmetric \\(p=0.5\\), middle values \\(k\\) higher probabilities, probabilities decrease go away middle. \\(p \\neq 0.5\\), see distribution gets skewed. success probability small, smaller number successes likelier, success probability large, larger number successes likelier, intuitive. probability success small, expect outcomes failures.","code":""},{"path":"discrete-random-variables.html","id":"poisson","chapter":"3 Discrete Random Variables","heading":"3.5.3 Poisson","text":"One common distribution used discrete random variables Poisson distribution. often used variable interest call count data (support non negative integers), example, number cars cross intersection day.random variable \\(X\\) follows Poisson distribution parameter \\(\\lambda\\), \\(\\lambda>0\\). Using mathematical notation, can write \\(X \\sim Pois(\\lambda)\\) express random variable \\(X\\) distributed Poisson parameter \\(p\\). PMF Poisson distribution written \\[\\begin{equation}\nP(X=k) = \\frac{e^{-\\lambda}\\lambda^k}{k!}\n\\tag{3.15}\n\\end{equation}\\]\\(k=0,1,2,\\cdots\\). \\(\\lambda\\) sometimes called rate parameter, related rate arrivals, example, number cross intersection period time.","code":""},{"path":"discrete-random-variables.html","id":"properties-of-poisson","chapter":"3 Discrete Random Variables","heading":"3.5.3.1 Properties of Poisson","text":"\\(X \\sim Pois(\\lambda)\\), \\[\\begin{equation}\nE(X) = \\lambda\n\\tag{3.16}\n\\end{equation}\\]\\[\\begin{equation}\nVar(X) = \\lambda.\n\\tag{3.17}\n\\end{equation}\\]imply larger values Poisson random variable associated larger variances. common feature count data. Consider number cars cross intersection one-hour time period. Consider average number cars rush hour, say 5 6pm. average number large, number lot smaller due inclement weather, number get lot larger convention occurring nearby. hand, consider average number cars 3 4am. average number small, likely small time, regardless weather conditions whether special events happening.Another interesting property Poisson distribution skewed \\(\\lambda\\) small, approaches bell-shaped distribution \\(\\lambda\\) gets bigger. Figure 3.8 displays density plots Poisson distributions \\(\\lambda\\) varied:\nFigure 3.8: PMF Poissons Rate Parameter Varied\n","code":"\n##calculate probability of Poisson with these values on the support\nx<-0:20\nlambda<-c(0.5, 1, 4, 10) ##try 4 different values of lambda\n\n##create PMFs of these 4 Poissons with different lambdas\npar(mfrow=c(2,2))\nfor (i in 1:4)\n  \n{\n  dens<-dpois(x,lambda[i])\n  plot(x, dens, type=\"l\", main=paste(\"Lambda is\", lambda[i]))\n}"},{"path":"discrete-random-variables.html","id":"poisson-approximation-to-binomial","chapter":"3 Discrete Random Variables","heading":"3.5.3.2 Poisson Approximation to Binomial","text":"\\(X \\sim Bin(n,p)\\), \\(n\\) large \\(p\\) small, PMF \\(X\\) can approximated Poisson distribution rate parameter \\(\\lambda = np\\). words, approximation works better \\(n\\) gets larger \\(np\\) gets smaller.several rules thumbs exist guide large \\(n\\) small \\(np\\) . National Institute Standards Technology (NIST) suggest \\(n \\geq 20\\) \\(p \\leq 0.05\\), \\(n \\geq 100\\), \\(np \\leq 10\\).One main using approximation, instead directly using binomial distribution, binomial coefficient can become computationally expensive compute \\(n\\) large.Consider example: company manufactures computer chips, 2 percent chips defective. quality control manager randomly samples 100 chips coming assembly line. probability 3 chips defective?Let \\(Y\\) denote number chips defective 100 chips.chip either defective . two outcomes chip.“success” probability 0.02 chip. probability assumed fixed chip.assume chip independent.number chips fixed \\(n=100\\).can model \\(Y \\sim Bin(100,0.02)\\), long assume chips independent. find \\(P(Y \\leq 3)\\), can:use binomial distribution, orapproximate using \\(Pois(2)\\), \\(\\lambda = np = 100 \\times 0.02\\).Notice values close .","code":"\n##set up binomial\nn<-100 \np<-0.02 \ny<-0:3 ##we want P(Y=0), P(Y=1), P(Y=2), P(Y=3)\nsum(dbinom(y,n,p))## [1] 0.8589616\n##Use Poisson to approx binomial\nlambda<-n*p\nsum(dpois(y,lambda))## [1] 0.8571235"},{"path":"discrete-random-variables.html","id":"Rdis","chapter":"3 Discrete Random Variables","heading":"3.6 Using R","text":"R built functions compute PMF, CDF, percentiles, well simulate data common distributions. start using random variable \\(Y\\) follows binomial distribution, \\(n=5, p = 0.3\\) example first. Note example support \\(Y\\) \\(\\{0,1,2,3,4,5 \\}\\).find \\(P(Y=2)\\), use:probability \\(Y\\) equal 2 0.3087.find \\(P(Y \\leq 2)\\), use:probability \\(Y\\) less equal 2 0.83692.find value support corresponds median (50th percentile), use:median binomial distribution 5 trials success probability 0.3 1.simulate 10 realizations (replications) \\(Y\\), use:outputs vector length 10. value represents result rep. first time ran binomial distribution \\(n=5, p=0.3\\), 1 5 success. second time run, 2 5 success, .Notice functions ended binom. just added different letter first, depending whether want PMF, CDF, percentile, random draw. letters d, p, q, r respectively.idea works distribution. example, find probability Poisson distribution rate parameter 2 equal 1, type:Thought questions: Suppose \\(Y \\sim Pois(1)\\).Find \\(P(Y \\leq 2)\\).Find 75th percentile \\(Y\\).Simulate 10,000 reps Y, find sample mean. sample mean close expected value?","code":"\ndbinom(2, 5, 0.3) ##supply the value of Y you want, then the parameters n and p in this order## [1] 0.3087\npbinom(2, 5, 0.3) ##supply the value of Y you want, then the parameters n and p in this order## [1] 0.83692\nqbinom(0.5, 5, 0.3) ##supply the value of the percentile you need, then the parameters n and p in this order## [1] 1\nset.seed(2) ##use set.seed() so we get the same random numbers each time the code is run\nrbinom(10, 5, 0.3) ##supply the number of simulated data you need, then the parameters n and p##  [1] 1 2 2 0 3 3 0 2 1 2\ndpois(1, 2) ##supply value of k, then parameter## [1] 0.2706706"},{"path":"continuous-random-variables.html","id":"continuous-random-variables","chapter":"4 Continuous Random Variables","heading":"4 Continuous Random Variables","text":"module based Introduction Probability (Blitzstein, Hwang), Chapters 5 6. can access book free https://stat110.hsites.harvard.edu/ (click Book). Please note cover additional topics, skip certain topics book. may skip Examples 5.1.6, 5.1.7, Proposition 5.2.3, Example 5.2.4, Sections 5.2.6, 5.2.7, Definition 5.3.7, Theorem 5.3.8, Example 5.4.7, Sections 5.5, 5.6, 5.7, Proposition 6.2.5, 6.2.6, Theorem 6.3.4, Sections 6.4 6.7 book.","code":""},{"path":"continuous-random-variables.html","id":"introduction","chapter":"4 Continuous Random Variables","heading":"4.1 Introduction","text":"previous module, learned discrete random variables. learned distributions can described PMFs CDFs, find expected values variances, well common distributions discrete random variables. learn counterparts dealing continuous random variables. concepts similar, computed can quite different.reminder:discrete random variable can take countable (finite infinite) number values.continuous random variable can take uncountable number values interval real numbers.example, height American adult continuous random variable, height can take value interval 40 100 inches. values 40 100 possible. list possible real numbers range list never ending.sample space associated continuous random variable difficult list, since takes uncountable number values. Using example heights American adults, real number 40 100 inches possible.different discrete random variable list sample space, support, find probability associated value support.Similar discrete random variables, want describe shape distribution, centrality, spread continuous random distribution idea probabilities associated different ranges values random variable.","code":""},{"path":"continuous-random-variables.html","id":"cumulative-distribution-functions-cdfs-1","chapter":"4 Continuous Random Variables","heading":"4.2 Cumulative Distribution Functions (CDFs)","text":"start talking cumulative distribution function, definition applies discrete continuous random variables. CDF random variable \\(X\\) \\(F_X(x) = P(X \\leq x)\\). difference lies CDF looks visually.Take look CDF discrete random variable CDF continuous random variable Figure 4.1:\nFigure 4.1: CDF Discrete RV vs CDF Continuous RV\nmentioned previous module, CDF discrete random variable called step function, jumps value support. hand, CDF continuous random variable increases smoothly sample space infinite.height CDF informs us percentile associated value random variable. Looking CDF continuous random variable Figure 4.1, height 0.5 random variable 0, value 0 corresponds 50th percentile distribution.technical definition continuous random variable : random variable continuous distribution CDF differentiable.discrete random variable fails definition since derivative undefined jumps.","code":""},{"path":"continuous-random-variables.html","id":"valid-cdfs-1","chapter":"4 Continuous Random Variables","heading":"4.2.1 Valid CDFs","text":"criteria valid CDF , matter random variable discrete continuous:non decreasing. means \\(x\\) gets larger, CDF either stays increases. Visually, graph CDF never decreases \\(x\\) increases.approach 1 \\(x\\) approaches infinity approach 0 \\(x\\) approaches negative infinity. Visually, graph CDF equal close 1 large values x, equal close 0 small values x.Thought question: Look CDFs example Figure 4.1, see satisfy criteria listed valid CDF.","code":""},{"path":"continuous-random-variables.html","id":"probability-density-functions-pdfs","chapter":"4 Continuous Random Variables","heading":"4.3 Probability Density Functions (PDFs)","text":"probability density function (PDF) continuous random variable analogous PMF discrete random variable.definition PDF continuous random variables following: continuous random variable \\(X\\) CDF \\(F_X(x)\\), PDF \\(X\\), \\(f_X(x)\\), derivative CDF, words, \\(f_X(x) = F_X^{\\prime}(x)\\). support \\(X\\) set \\(x\\) \\(f_X(x) >0\\).relationship PDF CDF continuous random variable \\(X\\) can expressed \\[\\begin{equation}\nF_X(x) = P(X \\leq x) = \\int_{-\\infty}^{x} f_X(x) dx.\n\\tag{4.1}\n\\end{equation}\\]take look example . Suppose continuous random variable \\(X\\) CDF PDF displayed , want find \\(P(X \\leq 1)\\):\nFigure 4.2: Probabilities CDF PDF\ncan find \\(P(X \\leq 1)\\) two different ways:CDF, find value 1 horizontal axis, read corresponding value vertical axis (blue lines). tells us \\(P(X \\leq 1) = 0.84\\).PDF, find area PDF \\(X \\leq 1\\). area corresponds shaded region blue, equal 0.84 performed integration per equation (4.1).Compare equation (4.1) equation (3.1) note similarities differences CDFs continuous discrete random variables. discrete CDFs, sum PMF values less equal \\(x\\), whereas continuous CDFs, integrate, accumulate area, PDF values less equal \\(x\\). people view integral continuous version summation.equation (4.1), can generalize way find probability \\(P(<X<b)\\) continuous random variable \\(X\\):\\[\\begin{equation}\nP(<X<b) = F_X(b) - F_X() = \\int_{}^{b} f_X(x) dx.\n\\tag{4.2}\n\\end{equation}\\]words, find probability range values \\(X\\), just find area PDF range values. Going back example, want find \\(P(0<X<1)\\), find area PDF \\(0<X<1\\), like Figure 4.3 :\nFigure 4.3: Probabilities PDF\nmentioned, PDF continuous random variable analogous, exactly , PMF discrete random variable. One common misconception PDF tells us probability, example, value \\(f_X(2) = P(X=2)\\), \\(X\\) continuous. correct \\(X\\) discrete. fact, look equation (4.2) little closely, \\(P(X=c) = 0\\) \\(X\\) continuous \\(c\\) constant, since area PDF 0.","code":""},{"path":"continuous-random-variables.html","id":"valid-pdfs","chapter":"4 Continuous Random Variables","heading":"4.3.1 Valid PDFs","text":"PDF continuous random variable must satisfy following criteria:Non negative: \\(f_X(x) \\geq 0\\),Integrates 1: \\(\\int_{-\\infty}^{\\infty}f_X(x) dx = 1\\).","code":""},{"path":"continuous-random-variables.html","id":"pdfs-and-density-plots","chapter":"4 Continuous Random Variables","heading":"4.3.2 PDFs and Density Plots","text":"Recall Section 3.2.2, learned discrete random variables, PMF histogram related. PMF represents long-run proportion, histogram represents relative frequency based data. sample size gets larger, PMF match histogram.Similarly continuous random variables, PDF density plot related. PDF associated distribution known random variable, density plot estimated data, data follows known random variable, PDF match density plot sample size gets larger.go details density plots created end module, Section 4.6.1, still need cover concepts.","code":""},{"path":"continuous-random-variables.html","id":"summaries-of-a-distribution","chapter":"4 Continuous Random Variables","heading":"4.4 Summaries of a Distribution","text":"Next, talk common summaries associated distribution. involve measures centrality variance, covered . also talk couple measures: skewness kurtosis.","code":""},{"path":"continuous-random-variables.html","id":"expectations-1","chapter":"4 Continuous Random Variables","heading":"4.4.1 Expectations","text":"expected value continuous random variable \\(X\\) \\[\\begin{equation}\nE(X) = \\int_{-\\infty}^{\\infty} x f_X(x) dx.\n\\tag{4.3}\n\\end{equation}\\]Another common notation \\(E(X)\\) \\(\\mu\\), sometimes \\(\\mu_X\\) show writing mean random variable \\(X\\).compare equation (4.3) equation (3.2), notice use integral instead summation now working continuous random variables.interpretation expected values still : expectation random variable can interpreted long-run mean random variable, .e. able repeat experiment infinite number times, expectation random variable average result among experiments. still measure centrality random variable.linearity expectations still hold way, per equation (3.3). matter random variable discrete continuous.Law Unconscious Statistician (LOTUS) also still applies. continuous random variable \\(X\\), (unsurprisingly):\\[\\begin{equation}\nE(g(X)) = \\int_{-\\infty}^{\\infty} g(x) f_X(x) dx.\n\\tag{4.4}\n\\end{equation}\\]Notice compare equation (4.4) discrete counterpart equation (3.4): just replaced summation integral.Thought question: Can guess write equation variance continuous random variable? Hint: variance discrete random variable given equation (3.5).","code":""},{"path":"continuous-random-variables.html","id":"median-1","chapter":"4 Continuous Random Variables","heading":"4.4.1.1 Median","text":"value \\(m\\) median random variable \\(X\\) \\(P(X \\leq c) \\geq \\frac{1}{2}\\) \\(P(X \\geq c) \\geq \\frac{1}{2}\\).Intuitively, median value \\(m\\) splits area PDF half (close half possible random variable discrete). Half area left \\(m\\), half area right \\(m\\).","code":""},{"path":"continuous-random-variables.html","id":"mode-1","chapter":"4 Continuous Random Variables","heading":"4.4.1.2 Mode","text":"continuous random variable \\(X\\), mode value \\(c\\) maximizes PDF: \\(f_X(c) \\geq f_X(x)\\) \\(x\\).discrete random variable \\(X\\), mode value \\(c\\) maximizes PMF: \\(P(X=c) \\geq P(X=x)\\) \\(x\\). Intuitively, mode commonly occurring value discrete random variable","code":""},{"path":"continuous-random-variables.html","id":"lossfunc","chapter":"4 Continuous Random Variables","heading":"4.4.1.3 Loss Functions","text":"goal statistical modeling use model make predictions. want able quantify quality prediction, prediction error. Suppose experiment can described random variable \\(X\\), want predict value next experiment. mean median natural guesses value next experiment.turns several ways quantify prediction error. usually called loss functions. Suppose predicted value denoted \\(x_{pred}\\). couple common loss functions :Mean squared error (MSE): \\(E(X-x_{pred})^2\\),Mean absolute error (MAE): \\(E|X-x_{pred}|\\).turns expected value \\(E(X)\\) minimizes MSE, median minimizes MAE. depending loss function suits analysis, use either mean median predictions. cover ideas detail later module (indeed later courses program).","code":""},{"path":"continuous-random-variables.html","id":"variance-1","chapter":"4 Continuous Random Variables","heading":"4.4.2 Variance","text":"variance continuous random variable \\(X\\) \\[\\begin{equation}\nVar(X) = \\int_{-\\infty}^{\\infty} (x-\\mu)^2 f_X(x) dx.\n\\tag{4.5}\n\\end{equation}\\]properties variance still Section 3.4.3.1. matter random variable discrete continuous. common notation used variance \\(\\sigma^2\\), sometimes \\(\\sigma_X^2\\) show variance random variable \\(X\\).","code":""},{"path":"continuous-random-variables.html","id":"moments","chapter":"4 Continuous Random Variables","heading":"4.4.3 Moments","text":"talking measures used describe continuous distributions, cover terminology used measures. Suppose random variable \\(X\\).\\(k\\)th moment \\(X\\) \\(E(X^k)\\). expected value, mean, sometimes called first moment.\\(k\\)th central moment \\(X\\) \\(E((X-\\mu)^k)\\). variance sometimes called second central moment.\\(k\\) standardized moment \\(X\\) \\(E(\\frac{(X-\\mu)^k}{\\sigma})\\).","code":""},{"path":"continuous-random-variables.html","id":"skewness","chapter":"4 Continuous Random Variables","heading":"4.4.4 Skewness","text":"One measure used describe shape distribution skewness, measure symmetry (measure skewness). skew random variable \\(X\\) third standardized moment:\\[\\begin{equation}\nSkew(X) = E \\left(\\frac{(X-\\mu)^3}{\\sigma} \\right)\n\\tag{4.6}\n\\end{equation}\\]random variable \\(X\\) symmetric distribution mean \\(X - \\mu\\) distribution \\(\\mu - X\\). Fairly often, people just say \\(X\\) symmetric; almost always assumed symmetry mean.Intuitively, symmetry means PDF \\(X\\) left mean mirror image PDF \\(X\\) right mean. look couple examples Figure 4.4:\nFigure 4.4: PDFs Symmetric RV vs Skewed RV\nblue vertical lines indicate mean distributions. Notice mirror image first plot, second plot.distribution symmetric, can say distribution asymmetric, skewed. values \\(Skew(X)\\) associated different shapes :\\(Skew(X) = 0\\): \\(X\\) symmetric.\\(Skew(X) > 0\\): \\(X\\) right (positively) skewed.\\(Skew(X) < 0\\): \\(X\\) left (negatively) skewed.","code":""},{"path":"continuous-random-variables.html","id":"kurtosis","chapter":"4 Continuous Random Variables","heading":"4.4.5 Kurtosis","text":"One measure deals tail behavior distribution. Visually, tails PDF associated probabilities extreme values random variable. distribution heavy tailed means extreme values (ends) likely occur. Tail behavior important consideration risk management finance: e.g. heavy left tail PDF mean financial crisis. Figure 4.5 shows example heavy tailed distribution (blue), compared Gaussian distribution (black). talk Gaussian distribution next subsection.\nFigure 4.5: PDF Heavy Tailed Distribution\ncommon measure tail behavior Kurtosis. kurtosis random variable \\(X\\) shifted fourth standardized moment:\\[\\begin{equation}\nKurt(X) = E \\left(\\frac{(X-\\mu)^4}{\\sigma} \\right) - 3.\n\\tag{4.7}\n\\end{equation}\\]reason subtracting (shifting ) 3 Gaussian distribution (commonly used distribution continuous random variables) kurtosis 0. Note: authors call equation (4.7) excess kurtosis kurtosis subtract 3.values \\(Kurt(X)\\) associated tail behaviors :\\(Kurt(X) = 0\\): \\(X\\) similar tails Gaussian distribution.\\(Kurt(X) > 0\\): \\(X\\) heavier tails compared Gaussian distribution (extreme values likely).\\(Kurt(X) < 0\\): \\(X\\) smaller tails compared Gaussian distribution (extreme values less likely).","code":""},{"path":"continuous-random-variables.html","id":"common-continuous-random-variables","chapter":"4 Continuous Random Variables","heading":"4.5 Common Continuous Random Variables","text":"Next, introduce commonly used distributions may used continuous random variables. number common statistical models (example, linear regression) based distributions.","code":""},{"path":"continuous-random-variables.html","id":"uniform","chapter":"4 Continuous Random Variables","heading":"4.5.1 Uniform","text":"random variable follows uniform distribution interval \\((,b)\\) completely random number \\(\\) \\(b\\). Notionally, upper case \\(U\\) usually used denote uniform random variable. \\(U\\) said uniform distribution interval \\((,b)\\), denoted \\(U \\sim(,b)\\), PDF \\[\\begin{equation}\nf_X(x) = \\begin{cases}\n  \\frac{1}{b-} & \\text{} <x<b \\\\\n  0 & \\text{otherwise }.\n\\end{cases}\n\\tag{4.8}\n\\end{equation}\\]Note parameters \\(,b\\) also help define support uniform distribution. Figure 4.6 displays plot PDF \\(U(,b)\\):\nFigure 4.6: PDF U(,b). Picture https://en.wikipedia.org/wiki/Continuous_uniform_distribution\nThought question: Can verify valid PDF?Figure 4.7 displays plot CDF \\(U(,b)\\):\nFigure 4.7: CDF U(,b). Picture https://en.wikipedia.org/wiki/Continuous_uniform_distribution\nproperties uniform distribution:mean \\(E(U) = \\frac{+b}{2}\\).variance \\(Var(U) = \\frac{(b-)^2}{12}\\).skewness 0, symmetric.kurtosis -\\(\\frac{6}{5}\\), tails heavy compared Gaussian distribution.Thought question: Can see uniform distribution symmetric? Can see tails heavy?support uniform distribution 0 1, standard uniform distribution. talk importance standard uniform distribution next subsection.","code":""},{"path":"continuous-random-variables.html","id":"universality-of-uniform","chapter":"4 Continuous Random Variables","heading":"4.5.1.1 Universality of Uniform","text":"turns can construct random variable continuous distribution based standard uniform distribution. fact used simulate random numbers continuous distributions. fact called Universality Uniform: Let \\(F_X(x)\\) denote CDF continuous random variable \\(X\\), :Let \\(U \\sim U(0,1)\\) \\(X = F^{-1}(U)\\). \\(X\\) random variable CDF \\(F_X(x)\\).\\(F_X(X) \\sim U(0,1)\\).give insight means, look example. Another continuous distribution called standard logistic distribution, denote \\(X\\). CDF \\[\nF_X(x) = \\frac{e^x}{1+e^x}.\n\\]\nLet \\(U \\sim U(0,1)\\). first part universality uniform informs us inverse CDF standard logistic \\(F_X^{-1}(U) \\sim X\\), invert \\(F_X(x)\\) get inverse \\(F_X^{-1}(x)\\). done setting CDF \\(X\\) equal \\(u\\), .e. let \\(u = \\frac{e^x}{1+e^x}\\), solving \\(x\\):\\[\n\\begin{split}\nu + u e^x &= e^x\\\\\n\\implies u &= e^x (1-u) \\\\\n\\implies e^x &= \\frac{u}{1-u} \\\\\n\\implies x &= \\log (\\frac{u}{1-u}).\n\\end{split}\n\\]Therefore \\(F^{-1}(u) = \\log (\\frac{u}{1-u})\\) \\(F^{-1}(U) = \\log (\\frac{U}{1-U})\\). Therefore \\(\\log (\\frac{U}{1-U})\\) follows standard logistic distribution.Let us use simulations show going . First, simulate 10,000 reps standard uniform distribution, invert values using \\(\\log (\\frac{u}{1-u})\\), create density plot \\(\\log (\\frac{u}{1-u})\\). steps shown Figure 4.8 :\nFigure 4.8: Uniform Logistic\nFigure 4.8:first plot shows density plot 10,000 reps standard normal. close PDF standard uniform.second plot shows density plot inverting 10,000 reps standard normal, .e. \\(F^{-1}(u) = \\log (\\frac{u}{1-u})\\).third plot shows PDF standard logistic. Notice similar looks second plot.see \\(\\log (\\frac{U}{1-U})\\) follows standard logistic distribution.View video detailed explanation example:second part universality uniform informs us \\(X\\) follows standard logistic distribution, \\(F(X) = \\frac{e^X}{1 + e^X} \\sim U(0,1)\\)., can see purpose universality uniform:part 1, can simulate reps distribution, long know CDF. software use may able simulate reps particular distribution, can write code simulate reps distribution based standard uniform.part 2, can convert random variable unknown distribution one known: standard uniform.","code":"\nset.seed(4)\n\nreps<-10000 ##number of reps\nu<-runif(reps) ##simulate standard uniform\ninvert<- log(u/(1-u)) ##invert based on F inverse. These should now follow standard logistic\n\npar(mfrow=c(1,3))\nplot(density(u), main=\"Density Plot from 10,000 U's\")\nplot(density(invert), main=\"Density Plot after Inverting\", xlim=c(-6,6))\ncurve(dlogis, from = -7, to = 7, main = \"PDF for Logistic\", ylab=\"Density\", xlab=\"\")"},{"path":"continuous-random-variables.html","id":"normdist","chapter":"4 Continuous Random Variables","heading":"4.5.2 Normal","text":"Another widely used distribution continuous random variables normal, Gaussian distribution. distribution symmetric bell-shaped. probably important distribution statistics data science due Central Limit Theorem. define theorem later module, loosely speaking, says take average bunch random variables, average approximate normal distribution, even random variables individually normal.lot questions wish answer based averages. exampleDoes implementation certain technologies class improve test scores students, average?male Gentoo penguins heavier female counterparts, average?replacing traffic lights roundabout reduce number traffic accidents, average?central limit theorem implies even test scores, weights Gentoo penguins, number traffic accidents follow normal distribution, average values approximate normal distribution.","code":""},{"path":"continuous-random-variables.html","id":"standard-normal","chapter":"4 Continuous Random Variables","heading":"4.5.2.1 Standard Normal","text":"First, talk standard normal distribution, normal distributions can viewed variations standard normal. standard normal distribution mean 0 variance 1. usually denoted \\(Z\\). can also write \\(Z \\sim N(0,1)\\) say \\(Z\\) normally distributed mean 0 variance 1. PDF standard normal distribution :\\[\\begin{equation}\n\\phi(z) = \\frac{1}{\\sqrt{2 \\pi}} e^{-z^2/2}.\n\\tag{4.9}\n\\end{equation}\\]Notice constant \\(\\frac{1}{\\sqrt{2 \\pi}}\\) equation (4.9). presence needed make PDF valid, since PDF must integrate 1. constants called normalizing constants.Figure 4.9 displays PDF:\nFigure 4.9: PDF Standard Normal\nFigure 4.9, can see following properties standard normal distribution (apply normal distribution):PDF symmetric mean. Figure 4.9, PDF symmetric 0, .e. \\(\\phi(-z) = \\phi(z)\\).implies tail areas also symmetric. example, \\(P(Z \\leq -2) = P(Z \\geq 2)\\).skew 0, since symmetric.actually closed-formed equation CDF standard normal (normal distribution). write \\(\\Phi(z) = P(Z \\leq z) = \\int_{\\infty}^z \\phi(z) dz\\) express CDF standard normal.Notice special letters \\(Z, \\phi, \\Phi\\) denote standard normal distribution. indication often used warrant notation.","code":"\ncurve(dnorm, from = -4, to = 4, main = \"PDF for Z\", ylab=\"Density\", xlab=\"\")"},{"path":"continuous-random-variables.html","id":"norm","chapter":"4 Continuous Random Variables","heading":"4.5.2.2 From Standard Normal to Other Normals","text":"\\(Z \\sim N(0,1)\\), \\(X = \\mu + \\sigma Z \\sim N(\\mu, \\sigma^2)\\). words, \\(Z\\) standard normal, \\(X = \\mu + \\sigma Z\\) follows normal distribution mean \\(\\mu\\) variance \\(\\sigma^2\\). parameters normal distribution mean \\(\\mu\\) variance \\(\\sigma^2\\).Note authors say parameters mean \\(\\mu\\) standard deviation \\(\\sigma\\) instead, careful reading notation associated normal distributions various sources. example, \\(N(0,2)\\) class book means normal distribution mean 0 variance 2, authors, \\(N(0,2)\\) means normal distribution mean 0 standard deviation 2. Indeed, functions R use alternate parameterization, need careful.Thought question: Can use linearity expectations explain \\(X\\) mean \\(\\mu\\)? Can use properties variance Section 3.4.3.1 explain \\(X\\) variance \\(\\sigma^2\\)?Notice started standard normal \\(Z\\), transformed \\(Z\\) multiplying \\(\\sigma\\) adding \\(\\mu\\) get normal distribution. transformation called location-scale transformation, shifting scaling. scale changes since multiply constant \\(\\sigma\\); location transformed since mean changes 0 \\(\\mu\\).can also reverse transformation state following: \\(X \\sim N(\\mu, \\sigma^2)\\), \\(Z = \\frac{X-\\mu}{\\sigma} \\sim N(0,1)\\). start \\(X \\sim N(\\mu, \\sigma^2)\\), can transform \\(X\\) subtracting , dividing \\(\\sigma\\), obtain \\(Z\\). particular transformation called standardization:\\[\\begin{equation}\nZ = \\frac{X-\\mu}{\\sigma}.\n\\tag{4.10}\n\\end{equation}\\]PDF normal distribution \\(X \\sim N(\\mu, \\sigma^2)\\) \\[\\begin{equation}\nf_X(x) = \\frac{1}{\\sigma \\sqrt{2 \\pi}} \\exp \\left(-\\frac{(x - \\mu)^2}{2 \\sigma^2} \\right).\n\\tag{4.11}\n\\end{equation}\\]Thought question: Compare equations (4.11) (4.9). Can see equation (4.9) can derived equation (4.11)?","code":""},{"path":"continuous-random-variables.html","id":"rulenorm","chapter":"4 Continuous Random Variables","heading":"4.5.2.3 68-95-99.7% Rule","text":"following property holds normal distribution, often called 68-99-99.7% rule. normal distribution \\(X \\sim N(\\mu, \\sigma^2)\\):\\(P(\\mu - \\sigma < X < \\mu + \\sigma) \\approx 0.68\\),\\(P(\\mu - 2\\sigma < X < \\mu + 2\\sigma) \\approx 0.95\\),\\(P(\\mu - 3\\sigma < X < \\mu + 3\\sigma) \\approx 0.997\\).mean normal distribution:68% observed values fall within 1 standard deviation mean,95% observed values fall within 2 standard deviations mean, andAbout 99.7% observed values fall within 3 standard deviations mean.last statement basis term six sigma used manufacturing, since virtually data points fall within range six sigma wide (assuming follow normal distribution). Visually, rule shown Figure 4.10 applied standard normal:\nFigure 4.10: 68-95-99.7 Rule\nwork first statement, 68% observed values fall within 1 standard deviation mean normal distribution. use R help us verify rule standard normal:Thought question: tweak code verify two statements associated 68-95-99.7% rule?","code":"\nupper1<-pnorm(1) ## what is percentile associated with Z=1 (i.e. 1 standard deviation above mean)\nlower1<-pnorm(-1) ## what is percentile associated with Z=-1 (i.e. 1 standard deviation below mean)\nupper1-lower1 ## find proportion in between 1 SD above and below mean.## [1] 0.6826895"},{"path":"continuous-random-variables.html","id":"usingR","chapter":"4 Continuous Random Variables","heading":"4.6 Using R","text":"R built functions compute density, CDF, percentiles, well simulate data common distributions. start random variable \\(Y \\sim N(1, 9)\\) example.find \\(f_Y(2)\\), use:density \\(f_Y(2)\\) 0.1257944. Note: R, normal distribution parameterized mean standard deviation, different set notes book, uses mean variance.find \\(P(Y \\leq 2)\\), use:probability \\(Y\\) less equal 2 0.6305587.Alternatively, can standardize normal distribution, use standard normal. standardization, per equation (4.10), gives us\\[\nz = \\frac{2-1}{3} = \\frac{1}{3},\n\\]\\[\n\\begin{split}\nP(Y \\leq 2) &= P(\\frac{Y-\\mu}{\\sigma} \\leq \\frac{2-1}{3}) \\\\\n            &= P(Z \\leq \\frac{1}{3}) \\\\\n            &= \\Phi(\\frac{1}{3})\n\\end{split}\n\\]can found usingwhich gives answer pnorm(2,1,3).View video detailed explanation example:find value support corresponds 90th percentile, use:90th percentile \\(Y \\sim N(1,9)\\) 4.844655.want use standard normal, find 90th percentile:apply location scale transformationwhich answer qnorm(0.9,1,3).simulate 10 draws (replicates) \\(Y\\), use:outputs vector length 10. value represents result rep. first value drawn \\(Y \\sim N(1,9)\\) -1.6907436, second value drawn 1.5545476 .Just like Section 3.6, notice functions ended norm. just added different letter first, depending whether want density (analogous PDF), CDF, percentile, random draw. letters d, p, q, r respectively.One thing note: supply mean standard deviation, example type rnorm(10), R assume want use standard normal distribution, rnorm(10) draw 10 random numbers standard normal.","code":"\ndnorm(2, 1, 3) ##supply the value of Y you want, then the parameters mu and sigma## [1] 0.1257944\npnorm(2, 1, 3) ##supply the value of Y you want, then the parameters mu and sigma## [1] 0.6305587\npnorm(1/3) ##don't supply mu and sigma means you want to use standard normal## [1] 0.6305587\nqnorm(0.9, 1, 3) ##supply the value of the percentile you need, then the parameters mu and sigma## [1] 4.844655\nqnorm(0.9)## [1] 1.281552\nqnorm(0.9)*3 + 1 ##multiply by sigma, then add mu## [1] 4.844655\nset.seed(2) ##use set.seed() so we get the same random numbers each time the code is run\nrnorm(10, 1, 3) ##supply the number of simulated data you need, then the parameters mu and sigma##  [1] -1.6907436  1.5545476  5.7635360 -2.3911270  0.7592447  1.3972609\n##  [7]  3.1238642  0.2809059  6.9534218  0.5836390"},{"path":"continuous-random-variables.html","id":"KDE","chapter":"4 Continuous Random Variables","heading":"4.6.1 Density Plots and Kernel Density Estimation","text":"now ready talk density plots, like ones Figure 4.8 created. Recall difference density plots PDFs:plot PDF describes distribution known random variable.density plot based data, used describe distribution data. data may may follow commonly known random variable. , plot PDF density plot match gather data.Proportions found way, finding area PDF density plot appropriate range support.Suppose \\(n\\) observed values unknown random variable \\(X\\): \\(x_1, x_2, \\cdots, x_n\\).density \\(f\\) \\(X\\) unknown want estimate data. estimate density \\(f\\), use kernel density estimator:\\[\\begin{equation}\n\\hat{f}_h(x) = \\frac{1}{nh} \\sum_{=1}^n K \\left( \\frac{x-x_i}{h}\\right ),\n\\tag{4.12}\n\\end{equation}\\]\\(K\\) kernel \\(h\\) smoothing parameter, often called bandwidth. Looking equation (4.12), KDE can viewed weighted average relative likelihood observing particular value.kernel can viewed weighting function, weights following shape distribution user specifies (usually symmetric). Common kernel functions shapes displayed Figure 4.11:\nFigure 4.11: Common Kernals. Picture adapted https://tgstewart.cloud/compprob/kde.html\nhorizontal axis kernel can viewed distance value data point specific value support, mid point horizontal axis represents distance 0.Looking normal kernel, nearest values receive highest weight, values away receive less weight.uniform kernel, values within certain distance receive weight, values beyond certain distance receive weight.Epanechnikov (parabolic) kernel mix : values beyond certain distance receive weight, values within certain distance receive weight roughly inversely proportional distance.\\(h\\) smoothing parameter analogous bin width histograms. Larger values result smoother looking density plots.Let us go back old example. use loan50 dataset openintro package. data originally consist thousands loans made Lending Club platform, randomly select 50 loans. Let us study interest rate loans 50 applicants received.\nFigure 4.12: Density Plot 50 Interest Rates\nuses KDE default settings: kernel normal, bandwidth based Silverman’s rule thumb.change , add kernel adjust argument using density() function, example, use Epanechnikov kernal twice default bandwidth:\nFigure 4.13: Density Plot 50 Interest Rates, Epanechnikov Kernel, Twice Bandwidth\ndensity plot Figure 4.13 looks smoother density plot Figure 4.12.","code":"\nlibrary(openintro)\n\n##create object for data\nData<-loan50\n\n##create density plot using default\nplot(density(Data$interest_rate), main=\"Density Plot of Interest Rates\")\n##create density plot using different settings\nplot(density(Data$interest_rate, kernel = \"epanechnikov\", adjust = 2), \n     main=\"Density Plot of Interest Rates\")"},{"path":"continuous-random-variables.html","id":"density-plots-and-histograms","chapter":"4 Continuous Random Variables","heading":"4.6.2 Density Plots and Histograms","text":"Section 1.2.3, mentioned density plots can viewed smoothed versions histogram. create histogram interest rates, overlay density plot blue, per Figure 4.14 :\nFigure 4.14: Histogram Density Plot 50 Interest Rates\nNotice density plot approximates histogram.","code":"\nhist(Data$interest_rate, prob = TRUE, main = \"Histogram with Density Plot\", xlab=\"Interest Rates\")\n\n##create density plot using default\nlines(density(Data$interest_rate), col=\"blue\")"},{"path":"continuous-random-variables.html","id":"numerical-summaries","chapter":"4 Continuous Random Variables","heading":"4.6.3 Numerical Summaries","text":"Equations (4.3), (4.5), (4.6), (4.7) used obtain mean, variance, skewness, kurtosis known distribution random variable. calculate quantities based sample observed data, \\(x_1, x_2, \\cdots, x_n\\), use:\\[\\begin{equation}\n\\bar{x} =  \\frac{1}{n} \\sum_{=1}^n x_i,\n\\tag{4.13}\n\\end{equation}\\]\\[\\begin{equation}\ns_X^2 =  \\frac{1}{n-1} \\sum_{=1}^n (x_i - \\bar{x})^2,\n\\tag{4.14}\n\\end{equation}\\]\\(\\bar{x}\\) \\(s_x^2\\) denote sample mean variance respectively. sample skewness sample kurtosis \\[\\begin{equation}\n\\text{sample skewness } =  \\frac{\\frac{1}{n} \\sum_{=1}^n (x_i - \\bar{x})^3}{s_X^3},\n\\tag{4.15}\n\\end{equation}\\]\\[\\begin{equation}\n\\text{sample kurtosis } =  \\frac{\\frac{1}{n} \\sum_{=1}^n (x_i - \\bar{x})^4}{s_X^4} - 3.\n\\tag{4.16}\n\\end{equation}\\]functions mean(), var(), skewness(), kurtosis() compute quantities R. latter two functions come moments package sure install load prior using .data also right skewed heavy tailed.","code":"\nmean(Data$interest_rate) ##mean## [1] 11.5672\nvar(Data$interest_rate) ##variance## [1] 25.52387\nlibrary(moments)\nmoments::skewness(Data$interest_rate) ##greater than 0## [1] 1.102193\nmoments::kurtosis(Data$interest_rate) ##greater than 0## [1] 3.651631"},{"path":"joint-distributions.html","id":"joint-distributions","chapter":"5 Joint Distributions","heading":"5 Joint Distributions","text":"module based Introduction Probability (Blitzstein, Hwang), Chapters 7 9. can access book free https://stat110.hsites.harvard.edu/ (click Book). Please note cover additional topics, skip certain topics book. may skip Story 7.1.9, Theorems 7.1.10 7.1.12, Examples 7.1.23 7.1.26, Section 7.2, Examples 7.3.6 7.3.8, 7.4.8 (parts d f ), Definition 7.5.6, Examples 9.1.8 9.1.10, Example 9.2.5, Theorem 9.3.2, Example 9.3.3, Theorems 9.3.7 9.3.9, Sections 9.4 9.6 book.","code":""},{"path":"joint-distributions.html","id":"introduction-1","chapter":"5 Joint Distributions","heading":"5.1 Introduction","text":"previous two modules, learned summarize distribution individual random variables. now ready extend concepts modules apply slightly different setting, analyzing multiple variables related . extremely common want analyze relationship least two variables. book lists examples, :Public policy: increasing expenditure infrastructure impact economic development?Education: smaller class sizes higher teacher pay impact student learning outcomes?Marketing: design website influence probability customer purchasing item?module consider variables jointly, words, relate . lot concepts CDF, PDF, PMF, expectations, variances, analogous versions considering variables jointly.","code":""},{"path":"joint-distributions.html","id":"joint-distributions-for-discrete-rvs","chapter":"5 Joint Distributions","heading":"5.2 Joint Distributions for Discrete RVs","text":"start discrete random variables, move continuous random variables. keep things simple, use two random variables explain concepts. concepts can generalized number random variables.Recall single discrete random variable \\(X\\), use PMF inform us support \\(X\\) probability associated value support. said PMF informs us distribution random variable \\(X\\).now two discrete random variables, \\(X\\) \\(Y\\). joint distribution \\(X\\) \\(Y\\) provides probability associated possible combination \\(X\\) \\(Y\\). joint PMF \\(X\\) \\(Y\\) \\[\\begin{equation}\np_{X,Y}(x,y) = P(X=x, Y=y).\n\\tag{5.1}\n\\end{equation}\\]Equation (5.1) can read probability random variables \\(X\\) \\(Y\\) equal \\(x\\) \\(y\\) respectively. Recall upper case letters usually used denote random variables, lower case letters usually used placeholder actual numerical value random variable take.Joint distributions sometimes called multivariate distributions. looking distribution one random variable, distribution can called univariate distribution.Joint PMFs can displayed via table, like Table 5.1 . example, consider study time, \\(X\\), related grades, \\(Y\\), \\(X=1\\) studying 0 5 hours week,\\(X=2\\) studying 6 10 hours week, \\(X=3\\) studying 10 hours week.\\(Y=1\\) denotes getting ,\\(Y=2\\) denotes getting B, \\(Y=3\\) denotes getting C lower.Table 5.1:  Example Joint PMF Study Time (\\(X\\)) Grades (\\(Y\\))also write joint PMF :\\(P(X=1, Y=1) = 0.05\\)\\(P(X=1, Y=2) = 0.05\\)\\(P(X=1, Y=3) = 0.10\\)\\(P(X=2, Y=1) = 0.15\\)\\(P(X=2, Y=2) = 0.20\\)\\(P(X=2, Y=3) = 0.05\\)\\(P(X=3, Y=1) = 0.30\\)\\(P(X=3, Y=2) = 0.10\\)\\(P(X=3, Y=3) = 0\\)Just like PMFs single discrete random variable must sum 1 PMF must non negative, joint PMFs discrete random variables must sum 1 individual PMF must non negative valid.Thought question: Can verify joint PMF Table 5.1 valid?joint CDF discrete random variables \\(X\\) \\(Y\\) \\[\\begin{equation}\nF_{X,Y}(x,y) = P(X \\leq x, Y \\leq y).\n\\tag{5.2}\n\\end{equation}\\]Thought question: Compare equation (5.2) univariate counterpart equation (3.1). Can see similarities differences?","code":""},{"path":"joint-distributions.html","id":"marginal-distributions-for-discrete-rvs","chapter":"5 Joint Distributions","heading":"5.2.1 Marginal Distributions for Discrete RVs","text":"joint distribution \\(X\\) \\(Y\\), can get distribution individual random variable. call marginal distribution, unconditional distribution, \\(X\\) \\(Y\\). marginal distribution \\(X\\) gives us information distribution \\(X\\), without taking \\(Y\\) consideration. get marginal PMF \\(X\\) joint PMF \\(X\\) \\(Y\\):\\[\\begin{equation}\nP(X=x) = \\sum_y P(X=x, Y=y).\n\\tag{5.3}\n\\end{equation}\\]Note summation performed support \\(Y\\). go back Table 5.1 example. Suppose want find marginal distribution study times, \\(X\\). Applying equation (5.3):\\[\n\\begin{split}\nP(X=1) &= \\sum_y P(X=1, Y=y)\\\\\n&= P(X=1, Y=1) + P(X=1, Y=2) + P(X=1, Y=3) \\\\\n&= 0.05 + 0.05 + 0.10\\\\\n&= 0.2,\n\\end{split}\n\\]\\[\n\\begin{split}\nP(X=2) &= \\sum_y P(X=2, Y=y)\\\\\n&= P(X=2, Y=1) + P(X=2, Y=2) + P(X=2, Y=3) \\\\\n&= 0.15 + 0.20 + 0.05\\\\\n&= 0.4,\n\\end{split}\n\\]\\[\n\\begin{split}\nP(X=3) &= \\sum_y P(X=3, Y=y)\\\\\n&= P(X=3, Y=1) + P(X=3, Y=2) + P(X=3, Y=3) \\\\\n&= 0.30 + 0.10 + 0\\\\\n&= 0.4.\n\\end{split}\n\\]can add information Table 5.1, create Table 5.2Table 5.2:  Example Joint PMF Study Time (\\(X\\)) Grades (\\(Y\\)), Marginal PMF Study TimeNotice just added probabilities column get marginal PMF \\(X\\), write probabilities margin table (hence term marginal PMF).may notice marginal PMF \\(X\\) ends just PMF \\(X\\). term marginal used imply PMF derived joint PMF, even information .Thought question: Can see equation (5.3) based Law Total Probability equation (2.10)?View video detailed explanation deriving marginal PMF \\(X\\):Likewise, obtain marginal PMF \\(Y\\) joint PMF \\(X\\) \\(Y\\):\\[\\begin{equation}\nP(X=x) = \\sum_x P(X=x, Y=y).\n\\tag{5.4}\n\\end{equation}\\]summation now performed support \\(X\\).Thought question: Can verify marginal PMF grades displayed Table 5.3 ?Table 5.3:  Example Joint PMF Study Time (\\(X\\)) Grades (\\(Y\\)), Marginal PMF Study Time Study Time","code":""},{"path":"joint-distributions.html","id":"conddist","chapter":"5 Joint Distributions","heading":"5.2.2 Conditional Distributions for Discrete RVs","text":"may need update distribution one variables based observed value variable, need distribution one variables based specific value variable. leads conditional PMF.Suppose want update distribution \\(Y\\) based observed value \\(X=x\\), want distribution \\(Y\\) observations \\(X=x\\) (words, \\(X\\) equal specific value \\(x\\)). \\(X\\) \\(Y\\) discrete, conditional PMF \\(Y\\) given \\(X=x\\) :\\[\\begin{equation}\nP(Y=y|X=x) = \\frac{P(X=x, Y=y)}{P(X=x)}.\n\\tag{5.5}\n\\end{equation}\\]conditional PMF \\(Y\\) given \\(X=x\\) essentially joint PMF \\(X\\) \\(Y\\) divided marginal PMF \\(X\\). Note conditional PMF \\(Y\\) given \\(X=x\\) viewed function value \\(x\\) fixed.go back Table 5.1 example find conditional PMFs. Suppose want find distribution grades students study little (0 5 hours per week). want conditional PMF \\(Y\\) given \\(X=1\\). Applying equation (5.5) Table 5.3, \\[\n\\begin{split}\nP(Y=1|X=1) &= \\frac{P(X=1, Y=1)}{P(X=1)}\\\\\n&= \\frac{0.05}{0.2} \\\\\n&= 0.25,\n\\end{split}\n\\]\\[\n\\begin{split}\nP(Y=2|X=1) &= \\frac{P(X=1, Y=2)}{P(X=1)}\\\\\n&= \\frac{0.05}{0.2} \\\\\n&= 0.25,\n\\end{split}\n\\]\\[\n\\begin{split}\nP(Y=3|X=1) &= \\frac{P(X=1, Y=3)}{P(X=1)}\\\\\n&= \\frac{0.10}{0.2} \\\\\n&= 0.5.\n\\end{split}\n\\]\nfrequentist interpretation values among students studied little, 50% chance getting C lower, 25% chance getting B, 25% chance getting .Bayesian interpretation values know student studied little, student 50% chance getting C lower, 25% chance getting B, 25% chance getting .Thought question: Can show conditional PMF \\(Y\\) given \\(X=3\\) based Table 5.3 \\(P(Y=1|X=3) = 0.75, P(Y=2|X=3) = 0.25, P(Y=3|X=3) = 0\\)?find conditional PMF \\(X\\) given \\(Y=y\\):\\[\\begin{equation}\nP(X=x|Y=y) = \\frac{P(X=x, Y=y)}{P(Y=y)}.\n\\tag{5.6}\n\\end{equation}\\]Thought question: Can show conditional PMF \\(X\\) given \\(Y=1\\) based Table 5.3 \\(P(X=1|Y=1) = 0.1, P(X=2|Y=1) = 0.3, P(X=3|Y=1) = 0.6\\)?","code":""},{"path":"joint-distributions.html","id":"bayes-rule-1","chapter":"5 Joint Distributions","heading":"5.2.3 Bayes’ Rule","text":"can apply Bayes’ Rule alternative way finding conditional PMF \\(Y\\) given \\(X=x\\). Equation (5.5) can written :\\[\\begin{equation}\nP(Y=y|X=x) = \\frac{P(X=x|Y=y) P(Y=y)}{P(X=x)}.\n\\tag{5.7}\n\\end{equation}\\]","code":""},{"path":"joint-distributions.html","id":"law-of-total-probability-1","chapter":"5 Joint Distributions","heading":"5.2.4 Law of Total Probability","text":"can apply law total probability denominator equations (5.5) (5.7), .e. \\(P(X=x) = \\sum_y P(X=x|Y=y) P(Y=y)\\), conditional PMF \\(Y\\) given \\(X=x\\) can also written \\[\\begin{equation}\nP(Y=y|X=x) = \\frac{P(X=x|Y=y) P(Y=y)}{\\sum_y P(X=x|Y=y) P(Y=y)}.\n\\tag{5.8}\n\\end{equation}\\]","code":""},{"path":"joint-distributions.html","id":"indepdence-of-discrete-rvs","chapter":"5 Joint Distributions","heading":"5.2.5 Indepdence of Discrete RVs","text":"notion whether two random variables independent (also called dependent) whether random variables association, words, changing value one random variable affect distribution ?\\(X\\) \\(Y\\) discrete random variables, independent , values support \\(X\\) \\(Y\\):\\[\\begin{equation}\nP(X=x, Y=y) = P(X=x) P(Y=y).\n\\tag{5.9}\n\\end{equation}\\]equivalent condition values support \\(X\\) \\(Y\\):\\[\\begin{equation}\nP(Y=y | X=x) = P(Y=y),\n\\tag{5.10}\n\\end{equation}\\]\\[\\begin{equation}\nP(X=x | Y=y) = P(X=x),\n\\tag{5.11}\n\\end{equation}\\]show \\(X\\) \\(Y\\) independent, need show one equations (5.9), (5.10), (5.11) true values support \\(X\\) \\(Y\\). show \\(X\\) \\(Y\\) dependent, need show one equations (5.9), (5.10), (5.11) false just one value \\(X\\) \\(Y\\).Equations (5.10) (5.11) pretty intuitive. equations say conditional distribution one variable, given , marginal distribution variable. means distribution variable influenced knowlege variable.first equation (5.9) informs us discrete variables independent, joint PMF equal product marginal PMFs.go back study time grades example shown Table 5.3. Study time grades dependent (independent) since \\(P(Y=1|X=1) = 0.25\\) \\(P(Y=1) = 0.5\\). equal study time grades independent. usually easier prove condition met providing counterexample: find one specific example condition false.study time grades independent, needed show \\(P(Y=1|X=x) = P(Y=1)\\) \\(X=1,2,3\\), \\(P(Y=2|X=x) = P(Y=2)\\) \\(X=1,2,3\\), \\(P(Y=3|X=x) = P(Y=3)\\) \\(X=1,2,3\\). usually tedious prove condition met show condition met circumstances.often, knowing context random variables helps. Since expect students study get better grades, expect variables dependent, know just need provide counterexample.","code":""},{"path":"joint-distributions.html","id":"joint-marginal-conditional-distributions-for-continuous-rvs","chapter":"5 Joint Distributions","heading":"5.3 Joint, Marginal, Conditional Distributions for Continuous RVs","text":"Recall previous modules CDFs PDFs continuous random variable similar CDFs PMFs discrete random variables. continuous versions generally found swapping summations integrals. general idea applies joint distributions random variables continuous.Now suppose \\(X\\) \\(Y\\) denote random variables continuous. required joint CDF \\(F_{X,Y}(x,y) = P(X \\leq x, Y \\leq y)\\) differentiable respect \\(x\\) \\(y\\). joint PDF partial derivative joint CDF respect \\(x\\) \\(y\\): \\(f_{X,Y}(x,y) = \\frac{\\partial^2}{\\partial x \\partial y} F_{X,Y}(x,y)\\).Similar univariate PDFs, joint PDFs valid, require :\\(f_{X,Y}(x,y) \\geq 0\\) \\(\\int_{-\\infty}^\\infty \\int_{-\\infty}^\\infty f_{X,Y}(x,y)  dx dy = 1\\).find probabilities, example \\(P(<X<b, c<Y<d)\\), integrate joint PDF two-dimensional region, .e. \\(\\int_{c}^d \\int_{}^b f_{X,Y}(x,y) dx dy\\).marginal PDF \\(X\\) can found integrating joint PDF support \\(Y\\):\\[\\begin{equation}\nf_X(x) = \\int_{-\\infty}^\\infty f_{X,Y}(x,y) dy.\n\\tag{5.12}\n\\end{equation}\\]conditional PDF \\(Y\\) given \\(X=x\\) \\[\\begin{equation}\nf_{Y|X}(y|x) = \\frac{f_{X,Y}(x,y)}{f_X(x)}\n\\tag{5.13}\n\\end{equation}\\]Bayes’ rule continuous random variables \\[\\begin{equation}\nf_{Y|X}(y|x) = \\frac{f_{X|Y}(x|y) f_Y(y)}{f_X(x)}\n\\tag{5.14}\n\\end{equation}\\]law total probability \\[\\begin{equation}\nf_X(x) = \\int_{-\\infty}^\\infty f_{X|Y}(x|y) f_Y(y) dy.\n\\tag{5.15}\n\\end{equation}\\]Continuous random variables \\(X\\) \\(Y\\) independent values \\(x\\) \\(y\\):\\[\\begin{equation}\nF_{X,Y} (x,y) = F_X(x) F_Y(y)\n\\tag{5.16}\n\\end{equation}\\]\\[\\begin{equation}\nf_{X,Y} (x,y) = f_X(x) f_Y(y)\n\\tag{5.17}\n\\end{equation}\\]\\[\\begin{equation}\nf_{Y|X} (y|x) = f_Y(y)\n\\tag{5.18}\n\\end{equation}\\]\\[\\begin{equation}\nf_{X|Y} (x|y) = f_X(x).\n\\tag{5.19}\n\\end{equation}\\]","code":""},{"path":"joint-distributions.html","id":"covariance-and-correlation","chapter":"5 Joint Distributions","heading":"5.4 Covariance and Correlation","text":"previous modules, used summaries mean, variance, skewness, kurtosis provide insight distribution single random variable. multiple random variables, one question random variables related . Summaries used quantify linear relationship two quantitative random variables covariance correlation.Generally speaking, two random variables positive covariance correlation increase decrease together, .e. \\(X\\) increases, \\(Y\\) also generally increases; \\(X\\) decreases, \\(Y\\) also generally decreases.Two random variables negative covariance correlation move opposite direction, .e. \\(X\\) increases, \\(Y\\) generally decreases; \\(X\\) decreases, \\(Y\\) generally increases. Figure 5.1 displays ideas visually scatter plots. scatter plot left shows example pair random variables positive covariance, scatter plot right shows example pair random variables negative covariance.\nFigure 5.1: Positive Covariance (Left), Negative Covariance (Right)\nOne thing note: covariance correlations can calculated random variables long quantitative, least one categorical. concept increasing random variable categorical make intuitive sense, example, random variable denotes color eyes, increasing color eyes mean?","code":""},{"path":"joint-distributions.html","id":"id_5-cov","chapter":"5 Joint Distributions","heading":"5.4.1 Covariance","text":"now define covariance. covariance random variables \\(X\\) \\(Y\\) \\[\\begin{equation}\nCov(X,Y) = E\\left[(X- \\mu_X)(Y - \\mu_Y) \\right].\n\\tag{5.20}\n\\end{equation}\\]Looking equation (5.20), see \\(X\\) \\(Y\\) generally move direction, \\(X - \\mu_x\\) \\(Y - \\mu_y\\) either positive negative, therefore product positive, average. \\(X\\) \\(Y\\) generally move opposite directions, \\(X - \\mu_x\\) \\(Y - \\mu_y\\) opposite signs, therefore product negative, average.key properties covariance:\\(Cov(X,X) = Var(X)\\).covariance random variable variance.\\(Cov(X,Y) = Cov(Y,X)\\). covariance \\(X\\) \\(Y\\) covariance \\(Y\\) \\(X\\).\\(Cov(X,c) = 0\\) constant \\(c\\). Since constant move, relationship \\(X\\).\\(Cov(aX,Y) = Cov(X,Y)\\) constant \\(\\). implies covariance affected units \\(X\\) \\(Y\\).\\(Var(X + Y) = Var(X) + Var(Y) + 2 Cov(X,Y)\\).\\(X\\) \\(Y\\) independent, \\(Cov(X,Y) = 0\\).However, \\(Cov(X,Y) = 0\\) mean \\(X\\) \\(Y\\) independent. common misconception. Remember covariance measures linear relationship. relationship \\(X\\) \\(Y\\) non linear, instances, covariance used. Figure 5.2 provides example . figure, \\(X\\) \\(Y\\) quadratic relationship, clearly independent, yet covariance virtually 0.\nFigure 5.2: Covariance Non Linear Relationship\nSuppose two vectors observed data, size \\(n\\): \\(X = (x_1, x_2, \\cdots, x_n)\\) \\(Y = (y_1, y_2, \\cdots, y_n)\\). sample covariance \\[\\begin{equation}\ns_{x,y} = \\frac{\\sum_{=1}^n (x_i - \\bar{x})(y_i - \\bar{y})}{n-1}\n\\tag{5.21}\n\\end{equation}\\]noted earlier covariance affected units variables. Suppose one variable measured meters, convert become centimeters. value covariance get multiplied 100. People find easier interpret measure depend units. correlation comes : unitless version covariance.View video visual explanation sample covariance positive linear relationship positive:","code":"\nx<-seq(-1,1, by=0.01)\ny<-x^2\n\n##note from plot that X & Y do not have a linear relationship\nplot(x,y, xlab=\"X\", ylab=\"Y\")\ncov(x,y) ##covariance is virtually 0## [1] 1.19967e-17"},{"path":"joint-distributions.html","id":"correlation","chapter":"5 Joint Distributions","heading":"5.4.2 Correlation","text":"correlation random variables \\(X\\) \\(Y\\) \\[\\begin{equation}\n\\rho = Corr(X,Y) = \\frac{Cov(X,Y)}{\\sqrt{Var(X) Var(Y)}}.\n\\tag{5.22}\n\\end{equation}\\]sample correlation two vectors observed data, size \\(n\\): \\(X = (x_1, x_2, \\cdots, x_n)\\) \\(Y = (y_1, y_2, \\cdots, y_n)\\), \\[\\begin{equation}\nr = \\frac{\\sum_{=1}^n (x_i - \\bar{x})(y_i - \\bar{y})}{\\sqrt{\\sum_{=1}^n (x_i - \\bar{x})^2 \\sum_{=1}^n (y_i - \\bar{y})^2}}\n\\tag{5.23}\n\\end{equation}\\]key properties correlation:bounded -1 1.Values closer -1 1 indicate stronger linear relationship.Values closer 0 indicate weaker linear relationship.numerical value unchanged location / scale changes.\\(X\\) \\(Y\\) independent, \\(Corr(X,Y) = 0\\).However, \\(Corr(X,Y) = 0\\) mean \\(X\\) \\(Y\\) independent.Correlation used relationship \\(X\\) \\(Y\\) linear.Figure 5.3 shows examples scatterplots sample correlations. left plot, data points fall close straight line, negative, correlation close -1. middle plot linear relationship, see trend one variable increasing decreasing variable increases. right plot shows data points fairly close straight line, close left plot), correlation close 1 (-1).\nFigure 5.3: Strong Negative Correlation (Left), Correlation (Middle), Moderate Positive Correlation (Right)\n","code":""},{"path":"joint-distributions.html","id":"conditional-expectation","chapter":"5 Joint Distributions","heading":"5.5 Conditional Expectation","text":"Section 2.3 5.2.2, explored notion conditional probabilities conditional distributions, used :Updating probability distribution random variable \\(Y\\), observing certain outcome another random variable \\(X\\), orRestricting probability distribution random variable \\(Y\\) certain value another random variable.represent Bayesian frequentist viewpoints conditional probability conditional distribution.turns similar idea applies expected value random variable. Recall expected value random variable long-run average, words, average value observing random variable infinite number times.conditional expectation random variable long run-average:observing certain outcome another variable event, orafter restricting attention cases another random variable fixed equal specific value.Fairly often, use statistical models predict response variable \\(Y\\) based predictor \\(X\\). Predictions values \\(Y\\) based observed values \\(X\\) usually use conditional expectation \\(Y\\) given \\(X\\). Given see predictor, long run average \\(Y\\) ends used predicted value response variable. basis statistical models.two slightly different notions conditional expectations:Conditional expectation random variable \\(Y\\) given event \\(\\). \\(\\) happened, expected value \\(Y\\)?Conditional expectation random variable \\(Y\\) given another random variable \\(X\\). fix value random variable \\(X\\) value support, expected value \\(Y\\)?second notion one usually used statistical models, cover first notion easier understand, help us understand second notion.","code":""},{"path":"joint-distributions.html","id":"conditional-expectation-given-event","chapter":"5 Joint Distributions","heading":"5.5.1 Conditional Expectation Given Event","text":"Recall expectation \\(E(Y)\\) random variable \\(Y\\) long-run average. \\(Y\\) discrete, take weighted average involving probabilities PMF \\(P(Y=y)\\). calculation conditional expectation \\(E(Y|)\\) \\(\\) event occurred simply replaces probabilities \\(P(Y=y)\\) conditional probabilities \\(P(Y=y|)\\). Therefore, discrete random variable \\(Y\\),\\[\\begin{equation}\nE(Y|) = \\sum_y y P(Y=y|)\n\\tag{5.24}\n\\end{equation}\\]sum support \\(Y\\). Notice summing product support corresponding conditional probability, whereas find \\(E(Y)\\), sum product support corresponding unconditional probability.\\(Y\\) continuous, use conditional PDF instead:\\[\\begin{equation}\nE(Y|) = \\int_{-\\infty}^{\\infty} y f(y|) dy.\n\\tag{5.25}\n\\end{equation}\\]key understand intuition behind conditional expectations, use simulation approximate (approximation works better use simulated data). Simulation represents frequentist viewpoint conditional expectation. code following:Generate 100 values \\(X\\) uniformly support \\(\\{1,2,3,4\\}\\).Simulate \\(Y\\) using \\(Y = 10 + X + \\epsilon\\) \\(\\epsilon \\sim N(0,1)\\).Represent values scatter plot, also overlay line represents sample mean \\(Y\\), estimates \\(E(Y)\\). simply average value y-axis 100 data points. plot left Figure 5.4 .Represent values scatter plot, use blue denote event \\(\\) \\(X=1\\). line represents sample mean \\(Y\\), blue data points (.e. \\(X=1\\)), overlaid. value estimates \\(E(Y|)\\) \\(E(Y|X=1)\\). plot right Figure 5.4 . calculating sample mean, completed disregarded black data points \\(X\\) 1.\nFigure 5.4: Comparison E(Y) E(Y|X=1)\n, can interpret conditional expectation \\(E(Y|)\\) long-run average \\(Y\\) () \\(\\) happened. long-run average \\(Y\\) certain condition met.View video detailed explanation simulation:","code":"\nset.seed(40)\nn<-100 ##100 data points\n\n##generate X\nx<-c(rep(1,n/4), rep(2,n/4), rep(3,n/4), rep(4,n/4)) \n\n##simulate Y\ny<- 10 + x + rnorm(n)\n\npar(mfrow=c(1,2))\nplot(x,y, main=\"Estimated E(Y) Overlaid\")\nabline(h=mean(y)) ##add line to represent est E(Y)\n\nplot(x,y, col = ifelse(x == 1,'blue', 'black'), pch = 19, main=\"Estimated E(Y|X=1) Overlaid\" )\nabline(h=mean(y[x=1]), col=\"blue\") ##add line to represent est E(Y|X=1)"},{"path":"joint-distributions.html","id":"conditional-expectation-given-random-variable","chapter":"5 Joint Distributions","heading":"5.5.2 Conditional Expectation Given Random Variable","text":"conditional expectation \\(Y\\) given random variable \\(X\\) slightly different. simulated example previous subsection, set \\(X\\) specific value. Now, consider long-run average \\(Y\\) value, instead specific value, support \\(X\\).One way think consider \\(E(Y|X=x)\\), \\(x\\) value support \\(X\\). \\(Y\\) discrete, conditional expectation :\\[\\begin{equation}\nE(Y|) = \\sum_y y P(Y=y|X=x)\n\\tag{5.26}\n\\end{equation}\\]sum support \\(Y\\).\\(Y\\) continuous:\\[\\begin{equation}\nE(Y|) = \\int_{-\\infty}^{\\infty} y f(y|x) dy.\n\\tag{5.27}\n\\end{equation}\\]go back simulated example previous subsection explain \\(E(Y|X=x)\\) represents. Recall support \\(X\\) \\(\\{1,2,3,4\\}\\) \\(Y = 10 + X + \\epsilon\\) \\(\\epsilon \\sim N(0,1)\\). \\[\n\\begin{split}\nE(Y|X=x) &= E(10 + X + \\epsilon | X=x)\\\\\n&= E(10 + x + \\epsilon) \\\\\n&= E(10) + E(x) + E(\\epsilon) \\\\\n&= 10 + x + 0 \\\\\n&= 10 + x.\n\\end{split}\n\\]brief explanation step:go line 1 line 2, subbed \\(x\\) \\(X\\), since setting \\(X=x\\).go line 2 line 3, apply linearity expectations.go line 3 4, \\(E(c)=c\\) constant. case, fixing \\(x\\) value support constant, \\(E(\\epsilon) = 0\\) since \\(\\epsilon \\sim N(0,1)\\).\\(E(Y|X) = 10 + X\\). means :\\(X=1\\), \\(E(Y|X=1) = 11\\),\\(X=2\\), \\(E(Y|X=1) = 12\\),\\(X=3\\), \\(E(Y|X=1) = 13\\), andWhen \\(X=4\\), \\(E(Y|X=1) = 14\\).Note: set \\(Y = 10 + X + \\epsilon\\) \\(\\epsilon \\sim N(0,1)\\) simulation. follows framework linear regression sets \\(Y = \\beta_0 + \\beta_1 X + \\epsilon\\) \\(\\epsilon \\sim N(0,\\sigma^2)\\), .e. \\(\\epsilon\\) normal mean 0 variance fixed value. conditional expectation given \\(X\\) ends prediction \\(Y\\) minimizes mean squared error linear regression.View video detailed explanation example:","code":""},{"path":"joint-distributions.html","id":"common-multivariate-distributions","chapter":"5 Joint Distributions","heading":"5.6 Common Multivariate Distributions","text":"now cover two common multivariate distributions: multinomial distribution multivariate normal distribution discrete continuous random variables respectively.","code":""},{"path":"joint-distributions.html","id":"multinomial","chapter":"5 Joint Distributions","heading":"5.6.1 Multinomial","text":"multinomial distribution can viewed generalization binomial distribution higher dimensions. Recall binomial distribution, carry \\(n\\) trials, trial record whether success failure, words, two outcomes trial. multinomial distribution differs can two outcomes trial. example, randomly select \\(n\\) adults ask political affiliation. affiliation Democrat, Republican, party, affiliation, four possible outcomes categories person.set multinomial distribution follows:\\(n\\) independent trials, trial belongs one \\(k\\) categories.trial belongs category \\(j\\) probability \\(p_j\\), \\(p_j\\) non negative \\(\\sum_{j=1}^k p_j = 1\\), .e. sum one.Let \\(X_1\\) denote number trials belonging category 1, \\(X_2\\) denote number trials belonging category 2, . \\(X_1 + \\cdots X_k = n\\).say \\(\\boldsymbol{X} = (X_1, \\cdots, X_k)\\) said multinomial distribution parameters \\(n\\) \\(\\boldsymbol{p} = (p_1, \\cdots, p_k)\\). can written \\(\\boldsymbol{X} \\sim Mult_k(n, \\boldsymbol{p})\\).Note vectors \\(\\boldsymbol{X}\\) \\(\\boldsymbol{p}\\) written bold. Vectors matrices commonly written using bold distinguish scalars, bold. \\(\\boldsymbol{X}\\) example call random vector, vector random variables \\(X_1, \\cdots, X_k\\).\\(\\boldsymbol{X} \\sim Mult_k(n, \\boldsymbol{p})\\), PMF \\[\\begin{equation}\nP(X_1 = n_1, \\cdots, X_k = n_k) = \\frac{n!}{n_{1}! \\cdots n_{k}!} p_1^{n_1} \\cdots p_k^{n_k},\n\\tag{5.28}\n\\end{equation}\\]\\(n_1 + \\cdots + n_k = n\\).Let us use toy example. Going back political affiliations. Suppose among American voters, 28% identify Democrats, 29% identify Republicans, 10% identify affiliations, 33% independents. Let \\(X_1, X_2, X_3, X_4\\) denote number Democrats, Republicans, others, independents. joint distribution \\(X_1, X_2, X_3, X_4\\) \\(\\boldsymbol{X} = (X_1, X_2, X_3, X_4) \\sim Mult_4(0.28, 0.29, 0.1, 0.33)\\).Suppose want find probability sample 10 voters, 2 Democrats, 3 Republicans, 1 another affiliation, 4 Independents:\\[\n\\begin{split}\nP(X_1 = 2, X_2 = 3, X_3 = 1, X_4 = 4) &= \\frac{10!}{2! 3! 1!4!} 0.28^{2} 0.29^{3} 0.1^1 0.33^4\\\\\n&= 0.02857172.\n\\end{split}\n\\]use","code":"\ndmultinom(c(2,3,1,4), prob=c(0.28,0.29,0.1,0.33)) ##specify X1, X2, X3, X4, then p1,p2,p3, p4## [1] 0.02857172"},{"path":"joint-distributions.html","id":"multinomial-marginals","chapter":"5 Joint Distributions","heading":"5.6.1.1 Multinomial Marginals","text":"marginals multinomial binomial. \\(\\boldsymbol{X} \\sim Mult_k(n, \\boldsymbol{p})\\), \\(X_j \\sim Bin(n, p_j)\\).Going back toy example American voters, means \\(X_1 \\sim Bin(n,0.28), X_2 \\sim Bin(n,0.29), X_3 \\sim Bin(n, 0.1), X_4 \\sim Bin(n,0.33)\\). Hopefully example makes sense. look \\(X_1,\\) looking number voters democrats . proportion Democrats still remains , proportion Republicans, affiliations, independents sum individual proportions, 1 minus proportion Democrats.","code":""},{"path":"joint-distributions.html","id":"multinomial-lumping","chapter":"5 Joint Distributions","heading":"5.6.1.2 Multinomial Lumping","text":"discrete categorical variables, can common want lump (merge, collapse, combine) categories together. \\(\\boldsymbol{X} \\sim Mult_k(n, \\boldsymbol{p})\\), \\(X_i + X_j \\sim Bin(n, p_i + p_j)\\). decide merge categories 1 2, \\((X_1 + X_2, X_3, \\cdots, X_k) \\sim Mult_{k-1}(n, (p_1 + p_2, p_3, \\cdots, p_k))\\).go back toy example. Suppose consider Democrats Republicans major parties, may wish combine everyone else one category: affiliations independents. can define using new random variable \\(\\boldsymbol{Y} = (X_1, X_2, X_3+X_4) \\sim Mult_3(n,(0.29,0.29,0.43)\\). Note now 3 categories instead 4. proportion lumped category sum individual proportions.","code":""},{"path":"joint-distributions.html","id":"multinomial-covariance","chapter":"5 Joint Distributions","heading":"5.6.1.3 Multinomial Covariance","text":"\\(\\boldsymbol{X} \\sim Mult_k(n, \\boldsymbol{p})\\) \\(\\boldsymbol{p} = (p_1,p_2, \\cdots, p_k)\\). covariance two distinct components \\(X_i\\) \\(X_j\\) \\[\\begin{equation}\nCov(X_i, X_j) = -n p_i p_j,\n\\tag{5.29}\n\\end{equation}\\]\\(\\neq j\\). book provides nice proof, Theorem 7.4.6, interested.Looking (5.29), notice covariance two distinct components negative (since probabilities non negative). means numerical values \\(X_i\\) \\(X_j\\) go opposite directions. make intuitive sense since \\(n = X_1 + \\cdots + X_k\\) fixed, \\(X_i\\) large, \\(X_j\\) small since \\(n\\) fixed. extreme example \\(X_i = n\\), \\(X_j\\) must 0.go back toy example. Suppose want find correlation \\(X_1\\) \\(X_2\\), number Democrats Republicans sample size \\(n\\). Note \\(X_1 \\sim Bin(n,0.28), X_2 \\sim Bin(n,0.29)\\), \\(Cov(X_1,X_2) = -n \\times 0.28 \\times 0.29 = 0.0812n\\),\\[\n\\begin{split}\nCorr(X_1,X_2) &= \\frac{Cov(X_1,X_2)}{\\sqrt{Var(X_1) Var(X_2)}}\\\\\n&= \\frac{-n p_1 p_2}{\\sqrt{n p_1 (1-p_1) n p_2 (1-p_2)}} \\\\\n&= -\\sqrt{\\frac{p_1 p_2}{(1-p_1)(1-p_2)}} \\\\\n&= -\\sqrt{\\frac{0.28 \\times 0.29}{(1-0.28)(1-0.29)}} \\\\\n&= -0.3985498.\n\\end{split}\n\\]","code":""},{"path":"joint-distributions.html","id":"conditional-multinomial","chapter":"5 Joint Distributions","heading":"5.6.1.4 Conditional Multinomial","text":"Sometimes, observed data multinomial distribution, wish update distribution. Suppose \\(\\boldsymbol{X} \\sim Mult_k(n, \\boldsymbol{p})\\), observed \\(X_1 = n_1\\), \\((X_2, \\cdots, X_k)|X_1 = n_1 \\sim Mult_{k-1}(n-n_1, (p_2^{\\prime}, \\cdots, p_k^{\\prime}))\\) \\(p_j^{\\prime} = \\frac{p_j}{p_2 + \\cdots + p_k}\\).","code":""},{"path":"joint-distributions.html","id":"multivariate-normal","chapter":"5 Joint Distributions","heading":"5.6.2 Multivariate Normal","text":"multivariate normal (MVN) distribution can viewed generalization normal distribution higher dimensions. Just like univariate normal distribution, central limit theorem also applies higher dimensions.\\(k\\)-dimensional random vector \\(\\boldsymbol{X} = (X_1, \\cdots, X_k)\\) said MVN distribution every linear combination \\(X_j\\) normal. means \\(t_1 X_1 + \\cdots + t_k X_k\\) normally distributed constants \\(t_1, \\cdots, t_k\\). \\(k=2\\), MVN often called bivariate normal.Section 4.5.2.2, mentioned parameters normal distribution mean \\(\\mu\\) variance \\(\\sigma^2\\). idea generalized MVN \\(\\boldsymbol{X} = (X_1, \\cdots, X_k)\\). parameters :mean vector \\((\\mu_1, \\cdots, \\mu_k)\\) \\(\\mu_j = E(X_j)\\). vector length \\(k\\) entry expected value component.mean vector \\((\\mu_1, \\cdots, \\mu_k)\\) \\(\\mu_j = E(X_j)\\). vector length \\(k\\) entry expected value component.covariance matrix. \\(k \\times k\\) matrix \\((,j)\\)th entry (.e. row \\(\\), column \\(j\\)) covariance \\(X_i\\) \\(X_j\\). implies diagonal entries give variance component (since \\(Cov(X_i, X_i) = Var(X_i)\\)), covariance matrix symmetric (since \\(Cov(X_i, X_j) = Cov(X_j, X_i)\\)).covariance matrix. \\(k \\times k\\) matrix \\((,j)\\)th entry (.e. row \\(\\), column \\(j\\)) covariance \\(X_i\\) \\(X_j\\). implies diagonal entries give variance component (since \\(Cov(X_i, X_i) = Var(X_i)\\)), covariance matrix symmetric (since \\(Cov(X_i, X_j) = Cov(X_j, X_i)\\)).example, suppose \\(\\boldsymbol{X} = (X_1, X_2, X_3)\\) MVN mean vector \\((5, 2, 8)\\) covariance matrix\\[\n\\begin{pmatrix}\n3 & 1.5 & 2.5\\\\\n1.5 & 2 & 4.2 \\\\\n2.5 & 4.2 & 1\n\\end{pmatrix},\n\\]\\(E(X_1) = 5, E(X_2) = 2, E(X_3) = 8\\),\\(Var(X_1) = 3, Var(X_2) = 2, Var(X_3) = 1\\),\\(Cov(X_1, X_2) = Cov(X_2, X_1) = 1.5\\),\\(Cov(X_1, X_3) = Cov(X_3, X_1) = 2.5\\), \\(Cov(X_2, X_3) = Cov(X_3, X_2) = 4.2\\).properties MVN distribution:\\(\\boldsymbol{X} = (X_1, \\cdots, X_k)\\) MVN, marginal distribution \\(X_j\\) normal, can set \\(t_j =1\\) constants 0.\\(\\boldsymbol{X} = (X_1, \\cdots, X_k)\\) MVN, marginal distribution \\(X_j\\) normal, can set \\(t_j =1\\) constants 0.However, converse necessarily true. \\(X_1, \\cdots, X_k\\) normal, \\((X_1, \\cdots, X_k)\\) necessarily MVN.However, converse necessarily true. \\(X_1, \\cdots, X_k\\) normal, \\((X_1, \\cdots, X_k)\\) necessarily MVN.\\((X_1, \\cdots, X_k)\\) MVN, subvector, e.g. \\((X_i, X_j)\\) bivariate normal.\\((X_1, \\cdots, X_k)\\) MVN, subvector, e.g. \\((X_i, X_j)\\) bivariate normal.\\(\\boldsymbol{X} = (X_1, \\cdots, X_k)\\) \\(\\boldsymbol{Y} = (Y_1, \\cdots, Y_m)\\) MVN \\(\\boldsymbol{X}\\) independent \\(\\boldsymbol{Y}\\), \\(\\boldsymbol{W} = (X_1, \\cdots, X_k, Y_1, \\cdots, Y_m)\\) MVN.\\(\\boldsymbol{X} = (X_1, \\cdots, X_k)\\) \\(\\boldsymbol{Y} = (Y_1, \\cdots, Y_m)\\) MVN \\(\\boldsymbol{X}\\) independent \\(\\boldsymbol{Y}\\), \\(\\boldsymbol{W} = (X_1, \\cdots, X_k, Y_1, \\cdots, Y_m)\\) MVN.Within MVN random vector, uncorrelated implies independence. \\(\\boldsymbol{X}\\) MVN \\(\\boldsymbol{X} = (\\boldsymbol{X_1, X_2})\\) \\(\\boldsymbol{X_1}\\) \\(\\boldsymbol{X_2}\\) subvectors, every component \\(\\boldsymbol{X_1}\\) uncorrelated every component \\(\\boldsymbol{X_2}\\), \\(\\boldsymbol{X_1}\\) \\(\\boldsymbol{X_2}\\) independent.Within MVN random vector, uncorrelated implies independence. \\(\\boldsymbol{X}\\) MVN \\(\\boldsymbol{X} = (\\boldsymbol{X_1, X_2})\\) \\(\\boldsymbol{X_1}\\) \\(\\boldsymbol{X_2}\\) subvectors, every component \\(\\boldsymbol{X_1}\\) uncorrelated every component \\(\\boldsymbol{X_2}\\), \\(\\boldsymbol{X_1}\\) \\(\\boldsymbol{X_2}\\) independent.","code":""},{"path":"joint-distributions.html","id":"simulations","chapter":"5 Joint Distributions","heading":"5.6.2.1 Simulations","text":"can use simulations verify first property. simulation, following:Simulate 5000 draws MVN distribution mean vector \\((1,2,5)\\) covariance matrix\\[\n\\begin{pmatrix}\n1 & 0.5 & 0.6\\\\\n0.5 & 2 & 0.2 \\\\\n0.6 & 0.2 & 4\n\\end{pmatrix}.\n\\]Assess component \\(X_1, X_2, X_3\\) normally distributed using Shapiro-Wilk test normality.\nnull hypothesis variable follows normal distribution, alternative hypothesis variable follow normal distribution.\nrejecting null hypothesis means variable inconsistent normal distribution, rejecting means evidence variable inconsistent normal distribution.\nrecord p-value test \\(X_1, X_2, X_3\\).\nAssess component \\(X_1, X_2, X_3\\) normally distributed using Shapiro-Wilk test normality.null hypothesis variable follows normal distribution, alternative hypothesis variable follow normal distribution.rejecting null hypothesis means variable inconsistent normal distribution, rejecting means evidence variable inconsistent normal distribution.record p-value test \\(X_1, X_2, X_3\\).Repeat previous 2 steps total 10 thousand reps.Repeat previous 2 steps total 10 thousand reps.Count proportion reps Shapiro-Wilk test rejected null hypothesis significance level 0.05 \\(X_1, X_2, X_3\\).\nproperty correct, expect close 5% p-values (wrongly) reject null hypothesis, since tests conducted 0.05 significance level.\nCount proportion reps Shapiro-Wilk test rejected null hypothesis significance level 0.05 \\(X_1, X_2, X_3\\).property correct, expect close 5% p-values (wrongly) reject null hypothesis, since tests conducted 0.05 significance level.Since close 5% hypothesis test rejected null hypothesis, appears component consistent normal distribution. (accurately, evidence say component normal.) appear \\(\\boldsymbol{X} = (X_1, \\cdots, X_k)\\) MVN, marginal distribution \\(X_j\\) normal. simulation provide evidence property.Note: done called Monte Carlo simulation, often used research verify theorems. may involved research, writing code run simulations good way understand theorems applied. cover Monte Carlo simulations detail later module.","code":"\nlibrary(mvtnorm) ##package to simulate from MVN\n\nreps<-1000 ## how many reps\npvalsx1<-pvalsx2<-pvalsx3<-array(0,reps) ##initialize an array to store the pvalues from each test at each rep\nsiglevel<-0.05 ##sig level\nn<-5000 ##number of draws for each rep\n\nmu_vector<-c(1,2,5) ##mean vector\n\n##set up covariance matrix\nsig12<-0.5\nsig13<-0.6\nsig23<-0.2\ncov_mat<-matrix(c(1,sig12,sig13,sig12,2,sig23,sig13,sig23,4), nrow=3, ncol=3)\n\n##set.seed so you can replicate my result.\nset.seed(30)\n\n##run steps 1 and 2 for 10 000 times\nfor (i in 1:reps)\n  \n{\n\n  data<-rmvnorm(n, mu_vector, cov_mat)\n  \n  x1<-data[,1] ##extract X1\n  x2<-data[,2] ##extract X2\n  x3<-data[,3] ##extract X3\n  \n  ##store pvalue from Shapiro-Wilk test from each component\n  pvalsx1[i]<-shapiro.test(x1)$p.value \n  pvalsx2[i]<-shapiro.test(x2)$p.value\n  pvalsx3[i]<-shapiro.test(x3)$p.value \n  \n}\n\n##proportion of tests that wrongly reject the null\nsum(pvalsx1<siglevel)/reps ##for X1## [1] 0.054\nsum(pvalsx2<siglevel)/reps ##for X2## [1] 0.037\nsum(pvalsx3<siglevel)/reps ##for X3## [1] 0.047"},{"path":"inequalities-limit-theorems-and-simulations.html","id":"inequalities-limit-theorems-and-simulations","chapter":"6 Inequalities, Limit Theorems, and Simulations","heading":"6 Inequalities, Limit Theorems, and Simulations","text":"module based Introduction Probability (Blitzstein, Hwang), Chapter 10. can access book free https://stat110.hsites.harvard.edu/ (click Book). Please note cover additional topics, skip certain topics book. may skip Example 10.1.3, 10.1.4, 10.1.7 10.1.9, Theorem 10.1.12, Example 10.2.5, 10.2.6, 10.3.7, Section 10.4 book.","code":""},{"path":"inequalities-limit-theorems-and-simulations.html","id":"introduction-2","chapter":"6 Inequalities, Limit Theorems, and Simulations","heading":"6.1 Introduction","text":"can difficult calculate probabilities expected values, example, PDF distribution unknown, integral difficult work . may notice used simulations approximate probabilities expected values examples previous modules. improvement computing capabilities, simulations can now performed faster tool used . tools calculate difficult probabilities expected values include using inequalities bound probabilities (e.g. probability greater less certain value), approximating using known theorems. ’ll look three tools module.","code":""},{"path":"inequalities-limit-theorems-and-simulations.html","id":"inequalities","chapter":"6 Inequalities, Limit Theorems, and Simulations","heading":"6.2 Inequalities","text":"probability expected value difficult calculate, may easier find bound via inequality. usually means can guarantee certain probability expected value within certain range values, narrows possible values exact answer. example, instead able calculate probability certain event, may able show probability 0.1, know event unlikely happen. cover couple well-known inequalities probability.","code":""},{"path":"inequalities-limit-theorems-and-simulations.html","id":"cauchy-schwartz-inequality","chapter":"6 Inequalities, Limit Theorems, and Simulations","heading":"6.2.1 Cauchy-Schwartz Inequality","text":"Cauchy-Schwarz inequality one famous inequalities mathematics many applications. context probability, written : random variables \\(X\\) \\(Y\\) finite variances\\[\\begin{equation}\n|E(XY)| \\leq \\sqrt{E(X^2)E(Y^2)}.\n\\tag{6.1}\n\\end{equation}\\]Next, use Cauchy-Schwartz inequality prove couple properties stated earlier modules:Cauchy-Schwartz inequality can used show correlation two random variables finite variances must -1 1. quick proof follows: apply equation (6.1) centered random variables \\(X - \\mu_X\\) \\(Y - \\mu_Y\\):\\[\n\\begin{split}\n|E[(X - \\mu_X)(Y - \\mu_Y)]| & \\leq \\sqrt{E[(X - \\mu_X)^2] E[(Y - \\mu_Y)^2]} \\\\\n\\implies |Cov(X,Y)| & \\leq \\sqrt{Var(X) Var(Y)} \\\\\n\\implies |Corr(X,Y)| & \\leq 1.\n\\end{split}\n\\]View video detailed explanation proof:Cauchy-Schwarz inequality can also used show variance random variable non negative. quick proof follows: apply equation (6.1) random variable \\(X\\) constant 1:\\[\n\\begin{split}\n|E(X)| & \\leq \\sqrt{E(X^2)E(1^2)}. \\\\\n\\implies |E(X)| & \\leq \\sqrt{E(X^2)} \\\\\n\\implies E(X)^2 & \\leq E(X^2) \\\\\n\\implies 0 & \\leq E(X^2) - E(X)^2 = Var(X).\n\\end{split}\n\\]\nView video detailed explanation proof:Note: One place may seen Cauchy-Schwarz inequality proof triangle inequality geometry.","code":""},{"path":"inequalities-limit-theorems-and-simulations.html","id":"jensens-inequality","chapter":"6 Inequalities, Limit Theorems, and Simulations","heading":"6.2.2 Jensen’s Inequality","text":"may noticed previous modules, written transforming random variable. One way transforming random variable scale change, words, value random variable multiplied constant. can happen change units variable. example want convert random variable based weight kilograms pounds. \\(X\\) \\(Y\\) denote weight kilograms pounds respectively, can write \\(Y = 2.2X\\). know expected value \\(X\\), can easily find expected value \\(Y\\) multiplying \\(E(X)\\) 2.2. fairly intuitive based linearity expectations using equation (3.3).stating Jensen’s inequality, cover couple concepts: linear vs non linear transformations, convex vs concave functions.","code":""},{"path":"inequalities-limit-theorems-and-simulations.html","id":"linnonlin","chapter":"6 Inequalities, Limit Theorems, and Simulations","heading":"6.2.2.1 Linear and Non Linear Transformations","text":"way think transformations write \\(Y = g(X)\\), \\(g\\) function describes transformation. kilograms pounds example, \\(g\\) exactly 2.2, \\(Y = 2.2X\\). transformation linear transformation since graph \\(Y = 2.2X\\) straight line. example, \\(E(Y) = E(2.2X) = 2.2E(X)\\).use non linear transformation? popular non linear transformation log transformation. used random variable right skewed (happens pretty often real data, wages, since people make really high wages vast majority people wages lower end). Expected values often used statistical models predictions; however, know mean may best measure centrality skewed data. One way transform right skewed data become less skewed log transform data. example, \\(Y = \\log(X)\\), \\(g(x) = \\log(x)\\). know expected value original variable, \\(E(X)\\), can easily find expected value \\(Y\\)? Can write \\(E(Y) = E(\\log(X)) = \\log E(X)\\)? actually incorrect. turns operations work non linear transformations, .e. \\(g\\) non linear, \\(E(g(X))\\) necessarily equal \\(g(E(X))\\). log transformation linear since graph \\(Y = \\log(X)\\) straight line.Let us use toy example show . Suppose roll fair six-sided die, let \\(X\\) denote number dots die shows. game, get win money based result roll, specifically twice result. Let \\(D\\) denote winnings game, \\(D = 2X\\). Since know \\(E(X) = 3.5\\), means expected winnings game \\(E(D) = E(2X) = 2E(X) = 7\\), since linear transformation . code verifies :Now suppose winnings now defined squared number dots die shows. Let \\(T\\) denote new winnings, \\(T = X^2\\). Since non linear transformation, \\(E(T) = E(X^2)\\) may equal \\(E(X)^2\\):example, see \\(E(T) > E(X)^2\\), words, \\(E(g(X)) > g(E(X))\\), \\(g(x) = x^2\\). \\(E(g(X)) > g(E(X))\\) always non linear function \\(g\\)? turns always case.summarize:\\(g\\) linear, \\(E(g(X)) = g(E(X))\\), can use linearity expectations.\\(g\\) non linear, \\(E(g(X)) \\neq g(E(X))\\).","code":"\nX<-c(1,2,3,4,5,6) ##support for X\n\nD<-2* X ##winnings\n\nmean(X) ##EX since die is fair## [1] 3.5\nmean(D) ##Expected winnings. This is equal to 2 times mean(X)## [1] 7\nX<-c(1,2,3,4,5,6) ##support for X\n\nT<-X^2 ##winnings\n\nmean(T) ##Expected winnings. ## [1] 15.16667\nmean(X)^2 ##not equal## [1] 12.25"},{"path":"inequalities-limit-theorems-and-simulations.html","id":"convex-and-concave-functions","chapter":"6 Inequalities, Limit Theorems, and Simulations","heading":"6.2.2.2 Convex and Concave Functions","text":"example , instance \\(E(g(X)) \\neq g(E(X))\\). direction inequality depends whether function \\(g\\) convex concave. couple ways decide function convex concave:Using derivatives:\nfunction \\(g(x)\\) convex second derivative non negative, .e. \\(g^{\\prime \\prime}(x) \\geq 0\\) domain. domain set values \\(x\\) \\(g(x)\\) defined.\nfunction \\(g(x)\\) concave second derivative non positive, .e. \\(g^{\\prime \\prime}(x) \\leq 0\\) domain.\nfunction \\(g(x)\\) convex second derivative non negative, .e. \\(g^{\\prime \\prime}(x) \\geq 0\\) domain. domain set values \\(x\\) \\(g(x)\\) defined.function \\(g(x)\\) concave second derivative non positive, .e. \\(g^{\\prime \\prime}(x) \\leq 0\\) domain.Using visuals:\nfunction \\(g(x)\\) convex every line segment joining two points graph never graph.\nfunction \\(g(x)\\) concave every line segment joining two points graph never graph.\nfunction \\(g(x)\\) convex every line segment joining two points graph never graph.function \\(g(x)\\) concave every line segment joining two points graph never graph.now look couple functions see convex concave:\\(g(x) = \\log(x)\\) concave function.\nsecond derivative \\(g^{\\prime \\prime}(x) = -\\frac{1}{x^2}\\). Note domain \\(\\log(x)\\) positive real numbers (undefined \\(x \\leq 0\\)), second derivative always negative.\ncan also look graph \\(y = \\log(x)\\), draw line segments join two points graph. lines never graph. Figure 6.1 shows example one line segment, can see line segment joins two points graph never graph.\nsecond derivative \\(g^{\\prime \\prime}(x) = -\\frac{1}{x^2}\\). Note domain \\(\\log(x)\\) positive real numbers (undefined \\(x \\leq 0\\)), second derivative always negative.can also look graph \\(y = \\log(x)\\), draw line segments join two points graph. lines never graph. Figure 6.1 shows example one line segment, can see line segment joins two points graph never graph.\nFigure 6.1: Example Concave Function\n\\(g(x) = x^2\\) convex function.\nsecond derivative \\(g^{\\prime \\prime}(x) = 2\\), always positive.\ncan also look graph \\(y = x^2\\), draw line segments join two points graph. lines never graph. Figure 6.2 shows example one line segment, can see line segment joins two points graph never velow graph.\nsecond derivative \\(g^{\\prime \\prime}(x) = 2\\), always positive.can also look graph \\(y = x^2\\), draw line segments join two points graph. lines never graph. Figure 6.2 shows example one line segment, can see line segment joins two points graph never velow graph.\nFigure 6.2: Example Convex Function\nThought question: Consider function \\(g(x) = \\frac{1}{x}\\), .e. inverse function. Can explain function convex \\(x>0\\) concave \\(x<0\\)?","code":""},{"path":"inequalities-limit-theorems-and-simulations.html","id":"jensens-inequality-1","chapter":"6 Inequalities, Limit Theorems, and Simulations","heading":"6.2.2.3 Jensen’s Inequality","text":"now ready state Jensen’s inequality. Let \\(X\\) denote random variable. \\(g\\) convex, \\(E(g(X)) \\geq g(E(X))\\). \\(g\\) concave, \\(E(g(X)) \\leq g(E(X))\\).equality holds \\(g\\) linear function. turns linear functions convex concave. book goes simple proof Jensen’s inequality worth reading. Next, apply Jensen’s inequality examples:apply Jensen’s inequality toy example Section 6.2.2.1. reminder, suppose roll fair six-sided die, let \\(X\\) denote number dots die shows. winnings defined squared number dots die shows. Let \\(T\\) denote new winnings, \\(T = X^2\\), \\(g(x) = x^2\\) function representing non linear transformation. established quadratic function convex, Jensen’s inequality tells us \\(E(g(X)) \\geq g(E(X))\\), .e. \\(E(T) > E(X)^2\\) showed code.apply Jensen’s inequality toy example Section 6.2.2.1. reminder, suppose roll fair six-sided die, let \\(X\\) denote number dots die shows. winnings defined squared number dots die shows. Let \\(T\\) denote new winnings, \\(T = X^2\\), \\(g(x) = x^2\\) function representing non linear transformation. established quadratic function convex, Jensen’s inequality tells us \\(E(g(X)) \\geq g(E(X))\\), .e. \\(E(T) > E(X)^2\\) showed code.mentioned Section 6.2.2.1, log transformation often applied make data right skewed less skewed, popular methods linear regression, tree based methods, \\(K\\) nearest neighbors can used (methods can sensitive outliers since based conditional expectations conditional means). often happens log transformation applied variable interest, model fit, prediction made log transformed variable using conditional expectations, exponential applied predicted value convert back original variable. Jensen’s inequality tells us exponential average log variable greater average variable, model estimates.mentioned Section 6.2.2.1, log transformation often applied make data right skewed less skewed, popular methods linear regression, tree based methods, \\(K\\) nearest neighbors can used (methods can sensitive outliers since based conditional expectations conditional means). often happens log transformation applied variable interest, model fit, prediction made log transformed variable using conditional expectations, exponential applied predicted value convert back original variable. Jensen’s inequality tells us exponential average log variable greater average variable, model estimates.Jensen’s inequality can also used show sample standard deviation biased estimator population standard deviation, appears counter intuitive, since sample variance unbiased estimator population variance, .e. \\(E(s^2) = \\sigma^2\\), \\(E(s) \\neq \\sigma\\). quick proof isJensen’s inequality can also used show sample standard deviation biased estimator population standard deviation, appears counter intuitive, since sample variance unbiased estimator population variance, .e. \\(E(s^2) = \\sigma^2\\), \\(E(s) \\neq \\sigma\\). quick proof \\[\nE(s) = E(\\sqrt{s^2}) \\leq \\sqrt{E(s^2)} = \\sigma.\n\\]sample standard deviation underestimates population standard deviation. However, bias tends small sample size large. cover ideas relating unbiased estimators future module detail.","code":""},{"path":"inequalities-limit-theorems-and-simulations.html","id":"chebyshevs-inequality","chapter":"6 Inequalities, Limit Theorems, and Simulations","heading":"6.2.3 Chebyshev’s Inequality","text":"common inequality used probability Chebyshev’s inequality. provides upper bound probability random variable least certain distance mean. Let \\(X\\) random variable mean \\(\\mu\\) variance \\(\\sigma^2\\). \\(>0\\),\\[\\begin{equation}\nP(|X-\\mu| \\geq ) \\leq \\frac{\\sigma^2}{^2}.\n\\tag{6.2}\n\\end{equation}\\]alternative way expressing Chebyshev’s inequality let \\(= c \\sigma\\) equation (6.2), can interpreted providing upper bound probability random variable least \\(c\\) standard deviations mean:\\[\\begin{equation}\nP(|X-\\mu| \\geq c \\sigma) \\leq \\frac{\\sigma^2}{c^2 \\sigma^2} = \\frac{1}{c^2}.\n\\tag{6.3}\n\\end{equation}\\]Using equation (6.3), can say following upper bond probability random variable least 1, 2, 3 standard deviations mean:\\(c=1\\),\\[\nP(|X-\\mu| \\geq \\sigma) \\leq \\frac{1}{1^2} = 1.\n\\]\ninforms us probability random variable least one standard deviation mean 1. upper bound informative setting since know probabilities greater 1.\\(c=2\\),\\[\nP(|X-\\mu| \\geq 2\\sigma) \\leq \\frac{1}{2^2} = 0.25.\n\\]informs us probability random variable least two standard deviations mean 0.25. words, 25% chance random variable least 2 standard deviations mean, less 75% chance random variable within 2 standard deviations mean, since \\(P(|X-\\mu| \\leq 2\\sigma)\\) complement \\(P(|X-\\mu| \\geq 2\\sigma)\\).\\(c=3\\),\\[\nP(|X-\\mu| \\geq 3\\sigma) \\leq \\frac{1}{3^2} = \\frac{1}{9}.\n\\]11.11% chance random variable least 3 standard deviations mean, less 88.89% chance random variable within 3 standard deviations mean.Thought question: Can explain results consistent 68-99-99.7% rule normal distributions, stated Section 4.5.2.3?Notice Chebyshev’s inequality can applied distribution, can used provide bounds data can spread . flexible 68-99-99.7% rule normal distributions can applied distribution, bounds exact inequality. can trade-relaxing assumptions accuracy results.","code":""},{"path":"inequalities-limit-theorems-and-simulations.html","id":"limits","chapter":"6 Inequalities, Limit Theorems, and Simulations","heading":"6.3 Limit Theorems","text":"previous subsection, used inequalities provide bounds probabilities expectations may difficult calculate. Another way handing difficult calculations use approximations distribution random variable, instead exact distribution random variable. Generally speaking, approximations work better data (.e. sample size larger). approximations covered two important limit theorems: Law Large Numbers Central Limit Theorem. theorems approximate distribution sample mean ..d. (independent identically distributed) random variables sample size gets larger.Note: idea ..d. random variables implies observed value random variable independent , observed value come random variable. example, let \\(X\\) denote number dots roll 6-sided fair die, let \\(X_1, X_2\\) denote value first second roll respectively. \\(X_1\\) \\(X_2\\) ..d. since outcomes first second roll influence , independent. \\(X_1\\) \\(X_2\\) identically distributed follow distribution, \\(Mult_6(1, (1/6, 1/6, 1/6, 1/6, 1/6, 1/6))\\).rest section, Section 6.3, assume ..d. \\(X_1, \\cdots, X_n\\) finite mean \\(\\mu\\) finite variance \\(\\sigma^2\\). positive integers \\(n\\) (.e. possible sample size), define sample mean \\(\\bar{X}_n = \\frac{X_1 + \\cdots + X_n}{n}\\). can easily derive expected value variance sample mean using properties expectations variances. expected value \\[\\begin{equation}\n\\begin{split}\nE(\\bar{X}_n) &= E(\\frac{X_1 + \\cdots + X_n}{n}) \\\\\n             &= \\frac{1}{n}E(X_1 + \\cdots + X_n) \\\\\n             &= \\frac{1}{n} (E(X_1) + \\cdots + E(X_n)) \\\\\n             &= \\frac{1}{n} (\\mu + \\cdots + \\mu) \\\\\n             &= \\mu.\n\\end{split}\n\\tag{6.4}\n\\end{equation}\\]variance \\[\\begin{equation}\n\\begin{split}\nVar(\\bar{X}_n) &= Var(\\frac{X_1 + \\cdots + X_n}{n}) \\\\\n             &= \\frac{1}{n^2}Var(X_1 + \\cdots + X_n) \\\\\n             &= \\frac{1}{n^2} (Var(X_1) + \\cdots + Var(X_n)) \\\\\n             &= \\frac{1}{n^2} (\\sigma^2 + \\cdots + \\sigma^2) \\\\\n             &= \\frac{\\sigma^2}{n}.\n\\end{split}\n\\tag{6.5}\n\\end{equation}\\]View video detailed explanation results:Equation (6.4) informs us long-run average sample means equal population mean. can imagine taken different random samples size \\(n\\) population, random sample find sample mean, average sample means. average equals population mean \\(\\mu\\). code provides demonstration steps:simulate random sample \\(X_1, \\cdots, X_{500}\\) ..d. standard normal.Compute sample mean store .Repeat previous steps total 10 thousand reps.Find average 10 thousand sample means.Equation (6.5) informs us calculate variance sample means. can imagine taken different random samples size \\(n\\) population, random sample find sample mean, find variance sample means. variance original random variable divided \\(n\\). means sample size gets larger, variance sample means get smaller, words, sample means tend get closer population mean. re run code also find variance sample means.","code":"\nreps<- 10000 ##take 10000 random samples. This value should be large\nn<-500 ##sample size for each random sample\nxbar<-array(0,reps) ##store the sample mean for each random sample\n\nset.seed(90)\n\nfor (i in 1:reps)\n  \n{\n  \n  xbar[i]<-mean(rnorm(n)) ##find and store sample mean for each random sample\n  \n}\n\nmean(xbar) ##average the sample means. This should be close to 0. ## [1] -0.0001034368\nvar(xbar) ##variance of sample means. This should be close to 1/500, since n=500. ## [1] 0.001979948"},{"path":"inequalities-limit-theorems-and-simulations.html","id":"law-of-large-numbers","chapter":"6 Inequalities, Limit Theorems, and Simulations","heading":"6.3.1 Law of Large Numbers","text":"Law Large Numbers (LLN) states \\(n\\) gets larger approaches infinity, sample mean \\(\\bar{X}_n\\) converges true mean \\(\\mu\\). implies sample mean tends get closer population mean larger sample sizes. key word tends , guarantee sample mean always gets closer population mean whenever \\(n\\) gets larger, generally . explains tend trust results larger sample sizes.Another implication LLN can use simulations verify theoretical results, since results usually require us simulate data based large number independent replications.LLN product equations (6.4) (6.5). Equation (6.5) informs us \\(n\\) gets larger, variance sample mean gets smaller. Equation (6.4) informs us sample mean unbiased, .e. long run average equal true mean. Collectively, inform us \\(n\\) gets larger, sample mean likely closer true mean.use example illustrate LLN, comes flipping fair coin. Let \\(X\\) denote whether coin lands heads tails, let \\(X=1\\) heads \\(X=0\\) tails. can say \\(X \\sim Bern(0.5)\\) since coin fair. Imagine flipping coin \\(n\\) times, record outcome flip, \\(X_1, \\cdots, X_n\\) denote outcome flip. know \\(E(X) = 0.5\\) since \\(X \\sim Bern(0.5)\\). LLN informs us \\(\\bar{X}_1, \\cdots, \\bar{X}_n\\) usually get closer 0.5 \\(n\\) increases. words, value sample proportion flip get usually closer 0.5 flips. code simulates example \\(n=500\\), Figure 6.3 shows sample proportions get closer 0.5, general, \\(n\\) increases.\nFigure 6.3: LLN Example 2\nView video detailed explanation code:Note: set.seed() used can reproduce results exactly. However, observation sample mean tends get closer true mean \\(n\\) increases happen regardless set.seed() used, even set.seed() used.Note: LLN actually comes two versions, Weak Law Large Numbers (WLLN), Strong Law Large Numbers (SLLN). book goes detail definitions differences. written gives intuitive explanation LLN implies.","code":"\nn<-500 ##make this big, but not too big otherwise picture is difficult to see\n\nset.seed(23)\n\nX<-rbinom(n,1,0.5) ##simulate 500 flips of fair coin\n\ntotals<-cumsum(X) ##count total number of heads after each flip\nindex<-1:n\nprops<-totals/index ##find proportion of heads after each flip\n\n##create visual. LLN says that as n gets larger, the value of the sample proportion tends to get closer to 0.5\nplot(props, type=\"l\", main=\"Prop vs Sample Size\", ylab=\"Proportion\", xlab=\"n\")\nabline(h=0.5, col=\"blue\") ##overlay 0.5 for easy comparison"},{"path":"inequalities-limit-theorems-and-simulations.html","id":"misconceptions-with-lln","chapter":"6 Inequalities, Limit Theorems, and Simulations","heading":"6.3.1.1 Misconceptions with LLN","text":"One key idea LLN sample mean tends get closer true mean \\(n\\) gets larger. key words “tends” “\\(n\\) gets larger”.misunderstanding LLN gambler’s fallacy, erroneously believes sample mean must “self correct” get closer population mean small increments \\(n\\).Using example flipping fair coin. gambler’s fallacy erroneously thinks :proportion heads close 0.5, even small \\(n\\).results subsequent flips self correct, .e. proportion heads get closer 0.5 next flip. example, first 5 flips heads, next flip “due” tails since proportion get closer 0.5 next flip.convergence 0.5 comes flipping coin many times.","code":""},{"path":"inequalities-limit-theorems-and-simulations.html","id":"CLT","chapter":"6 Inequalities, Limit Theorems, and Simulations","heading":"6.3.2 Central Limit Theorem","text":"LLN informs us sample mean converges true mean. Statistical theory informs us expected value variance sample mean equations (6.4) (6.5). inform us distribution \\(\\bar{X}_n\\). Central Limit Theorem (CLT) comes .CLT states sample size gets larger tends infinity, distribution \\(\\bar{X}_n\\) standardization approaches standard normal distribution, .e.\\[\\begin{equation}\n\\sqrt{n} \\left(\\frac{\\bar{X}_n \\ - \\mu}{\\sigma} \\right) \\N(0,1).\n\\tag{6.6}\n\\end{equation}\\]CLT called asymptotic result, informs us limiting distribution \\(\\bar{X}_n\\) \\(n\\) gets larger tends infinity. CLT implies approximation \\(n\\) large enough. large \\(n\\), distribution \\(\\bar{X}_n\\) approximately \\(N(\\mu, \\frac{\\sigma^2}{n})\\).implication CLT even data follow normal distribution, average value data can approximated normal distribution sample size large enough. mentioned Section 4.5.2, lot questions research deal averages.consider hypothetical situation. Suppose waiting time customers calling customer service lunch time known mean 600 seconds standard deviation 30 seconds. company decides cut costs reduces staffing call center, claims wait times affected negatively. customers convinced otherwise. researchers obtains wait times 500 customers call lunch time staffing reduced. sample mean wait times customers 700 seconds. Can data used counter company’s claim wait times affected?One possible calculation assume company correct, wait times changed, average. , sample means approximately normal, mean 600 variance \\(\\frac{30^2}{\\sqrt{500}}\\), .e. \\(\\bar{X}_{500} \\sim N(600, \\frac{30^2}{\\sqrt{500}})\\). calculate \\(P(\\bar{X}_{500} \\geq 700)\\), probability sample mean equal greater 700 seconds. Using R, probability 0.0065, small, indicating data inconsistent company’s claim.CLT traditionally associated distribution sample mean \\(\\bar{X}_n\\). can applied sum well, due properties expectations variances. Let \\(T_n = X_1 + \\cdots + X_n = n \\bar{X}_n\\) denote sum \\(n\\) ..d. random variables. CLT says , large \\(n\\), distribution \\(T_n\\) approximately \\(N(n\\mu, n\\sigma^2)\\).","code":"\n1-pnorm(700, 600, 30^2/sqrt(500))## [1] 0.006486311"},{"path":"inequalities-limit-theorems-and-simulations.html","id":"considerCLT","chapter":"6 Inequalities, Limit Theorems, and Simulations","heading":"6.3.2.1 Considerations with CLT","text":"One question raised CLT used large sample size \\(n\\) approximation accurate? suggestions plentiful (usually along lines sample size least 25 30), fixed answer question. depends distribution \\(X\\). general, skewed \\(X\\) , \\(n\\) needs larger approximation work. hand, \\(X\\) already normal, distribution \\(\\bar{X}_n\\) exactly \\(N(\\mu, \\frac{\\sigma^2}{n})\\) sample size \\(n\\). look couple examples based different distributions.\\(X\\) standard normal. code following:simulate \\(n\\) draws \\(X\\) \\(n = 1\\).obtain distribution \\(\\bar{X}_n\\) value \\(n\\), repeat previous step total 10 thousand reps, produce histogram 10 thousand values \\(\\bar{X}_n\\).Repeat previous two steps, different values \\(n\\). use \\(n= 5, 30, 100\\) wellWe expect histograms \\(\\bar{X}_n\\) look normal values \\(n\\) used.\nFigure 6.4: Distribution Sample Means X N(0,1), n varied\nFigure 6.4 displays histograms simulation, matches expect CLT. Since \\(X\\) normal, \\(\\bar{X}_n\\) follows normal distribution value \\(n\\).\\(X\\) Poisson parameter 1. skewed distribution. use code mimics previous example, difference data simulated \\(Pois(1)\\) instead standard normal. \\(n\\) small, expect distribution \\(\\bar{X}_n\\) look normal. \\(n\\) gets larger, expect distribution \\(\\bar{X}_n\\) look normal.\nFigure 6.5: Distribution Sample Means X Pois(1), n varied\nFigure 6.5 displays histograms simulation, matches expect. \\(n\\) 1 5, histograms clearly normal, CLT approximation work well. \\(n=30\\) histogram looks approximately normal, \\(n=100\\), histogram looks even closer normal distribution.","code":""},{"path":"inequalities-limit-theorems-and-simulations.html","id":"monte-carlo-simulations","chapter":"6 Inequalities, Limit Theorems, and Simulations","heading":"6.4 Monte Carlo Simulations","text":"may noticed used simulations earlier sections module (previous modules) help explain certain concepts. simulations called Monte Carlo methods, Monte Carlo simulations. idea behind Monte Carlo methods used repeated random sampling (repeated, mean repeated large number times) estimate features data, usually probabilities expected values. Monte Carlo methods used following purposes:probability expectation complicated work hand. Recall finding probabilities expectations hand involve summations integrals, becomes obvious working summations especially integrals can get onerous.probability expectation complicated work hand. Recall finding probabilities expectations hand involve summations integrals, becomes obvious working summations especially integrals can get onerous.verify theoretical results involving probability expectations. lot theory proved using mathematics, academic papers include Monte Carlo simulations verify theoretical results. done verify LLN CLT previous subsection (circumstances).verify theoretical results involving probability expectations. lot theory proved using mathematics, academic papers include Monte Carlo simulations verify theoretical results. done verify LLN CLT previous subsection (circumstances).help confirm understand meaning theoretical results. way code matches theory understand theory.help confirm understand meaning theoretical results. way code matches theory understand theory.","code":""},{"path":"inequalities-limit-theorems-and-simulations.html","id":"monte-carlo-methods-for-expected-values","chapter":"6 Inequalities, Limit Theorems, and Simulations","heading":"6.4.1 Monte Carlo Methods for Expected Values","text":"Suppose want find expectation continuous random variable \\(X\\), \\(E(g(X))\\), \\(g\\) function. LOTUS says need use equation (4.4), .e. \\(E(g(X)) = \\int_{-\\infty}^{\\infty} g(x) f_X(x).\\) Monte Carlo methods avoid integration simulating \\(X_1, \\cdots, X_M\\) \\(X\\) estimate \\(E(g(X))\\) sample mean \\(g(X)\\), \\(\\frac{1}{M} \\sum_{=1}^M g(X_i)\\). LLN tells us \\(M\\) gets larger, sample mean converges \\(E(g(X))\\).Monte Carlo methods replace integral (summation) simulating random variable repeatedly many times. use simple example illustrate idea.Let \\(X\\) standard normal distribution. Suppose want find value \\(E(X^2)\\). try find using LOTUS, need find \\(\\int_{-\\infty}^{\\infty} x^2 \\frac{1}{\\sqrt{2 \\pi}} e^{-x^2/2} dx\\). Instead working integral hand, carry Monte Carlo simulation steps:Simulate \\(M\\) random values standard normal, \\(M\\) large.Calculate \\(X_1^2, \\cdots, X_M^2\\).Find sample average \\(X_1^2, \\cdots, X_M^2\\).Since \\(X\\) standard normal, know \\(E(X) = 0\\) \\(Var(X) = E(X^2) - E(X)^2 = E(X^2) = 1\\). see simulation estimated value \\(E(X^2)\\) pretty close theoretical value.","code":"\nset.seed(5)\n\nreps<-10000 ## this is M\n\nXs<-rnorm(reps) ##generate M values of X\n\nsquared.values<-Xs^2 ##square each X\n\nmean(squared.values) ##sample average of squared values. ## [1] 1.024546\n##when reps is large, this sample mean should be close to the true E(X^2), which is 1"},{"path":"inequalities-limit-theorems-and-simulations.html","id":"monte-carlo-methods-for-probabilities","chapter":"6 Inequalities, Limit Theorems, and Simulations","heading":"6.4.2 Monte Carlo Methods for Probabilities","text":"Suppose want find probability random variable satisfies event \\(E\\), \\(P(E)\\). perform summation integral find probability, estimate probability using Monte Carlo methods. simulate \\(X_1, \\cdots, X_M\\) \\(X\\), \\(M\\) large, words, simulate large number replicates \\(X\\). count many \\(X_i\\)s correspond event \\(E\\) happening, divide number \\(M\\), number replicates. use example illustrate idea.Let \\(X\\) standard normal distribution. Suppose want find probability \\(P(X^2 > 1)\\). carry Monte Carlo simulation steps:Simulate \\(M\\) random values standard normal, \\(M\\) large.Calculate \\(X_1^2, \\cdots, X_M^2\\).Count number times \\(X_i^2\\) greater 1.Divide number \\(M\\) estimate probability, since probability can interpreted long-run proportion.see estimated probability \\(P(X^2 > 1)\\) close theoretical probability.","code":"\nset.seed(5)\n\nreps<-10000 ## this is M\n\nXs<-rnorm(reps) ##generate M values of X\n\nsquared.values<-Xs^2 ##square each X\n\nsum(squared.values>1)/reps ##count the number of times X^2 is greater than 1, and divide by M## [1] 0.3178\n##when reps is large, this proportion should be close to\n1-pchisq(1, df=1)## [1] 0.3173105\n##it turns out that squaring a standard normal gives a chi-squared distribution with 1 df."},{"path":"inequalities-limit-theorems-and-simulations.html","id":"monte-carlo-methods-for-other-purposes","chapter":"6 Inequalities, Limit Theorems, and Simulations","heading":"6.4.3 Monte Carlo Methods for Other Purposes","text":"Monte Carlo methods exclusively used estimating expected values probabilities. versatile can used number purposes, long need repeated random sampling.fun example pretty famous uses Monte Carlo simulations estimate value \\(\\pi\\). can consider following hypothetical dart throwing experiment , based figure :\nFigure 6.6: Board Dart Throwing Experiment\nexperiment works way:dart always land square, equal probability landing spot square.\ncan let \\(X \\sim U(-1,1)\\) represent position dart x-axis circle Figure 6.6.\ncan let \\(Y \\sim U(-1,1)\\) represent position dart y-axis circle Figure 6.6.\ncan let \\(X \\sim U(-1,1)\\) represent position dart x-axis circle Figure 6.6.can let \\(Y \\sim U(-1,1)\\) represent position dart y-axis circle Figure 6.6.throw large number darts. dart, see lands circle .\nassess dart lies circle, assess whether \\(x_i^2 + y_i^2 \\leq 1\\) dart \\(\\). condition met, know dart \\(\\) lies circle, , lies outside circle.\nassess dart lies circle, assess whether \\(x_i^2 + y_i^2 \\leq 1\\) dart \\(\\). condition met, know dart \\(\\) lies circle, , lies outside circle.stands reason \\(\\frac{\\text{Area circle}}{\\text{square}} = \\frac{\\pi}{4} \\approx \\frac{\\text{Number darts landing circle}}{\\text{Number darts thrown}}\\).Therefore, throwing large number darts, \\(\\pi \\approx 4 \\times \\frac{\\text{Number darts landing circle}}{\\text{Number darts thrown}}.\\)code carries experiment 10 thousand reps (10 thousand dart throws):see estimated value \\(\\pi\\) using Monte Carlo simulation close true value.","code":"\nreps<-10000 ##number of dart throws\n\ncount<-0 ##counter that keeps track of number of throws inside circle\n\nset.seed(222)\n\nfor (i in 1:reps) {\n\nx<-runif(1,min=-1, max=1) ##simulate landing spot on x axis\ny<-runif(1,min=-1, max=1) ##simulate landing spot on y axis\n\n  if (x^2 + y^2 <= 1){\n    count <- count+1 ##counter adds 1 if dart lands in circle\n  }\n\n}\n\n##estimate pi. should be close to real value of pi. Gets closer if we throw more darts\ncount/reps * 4 ## [1] 3.1436"},{"path":"inequalities-limit-theorems-and-simulations.html","id":"considerations-with-monte-carlo-methods","chapter":"6 Inequalities, Limit Theorems, and Simulations","heading":"6.4.4 Considerations with Monte Carlo Methods","text":"examples , compared estimated values using Monte Carlo methods real values, see methods work. However, know real values, two questions come mind:many replicates need? know estimated values converge true values increase number replicates. Using replicates make simulation run longer computer.many replicates need? know estimated values converge true values increase number replicates. Using replicates make simulation run longer computer.Related previous question, close close enough? know estimated value simulation close enough truth? way knowing true value unknown.Related previous question, close close enough? know estimated value simulation close enough truth? way knowing true value unknown.","code":""},{"path":"inequalities-limit-theorems-and-simulations.html","id":"set.seed-in-r","chapter":"6 Inequalities, Limit Theorems, and Simulations","heading":"6.4.4.1 set.seed() in R","text":"may noticed provided simulations, use function set.seed() input number. enable others replicate exact results, someone wants verify code.Monte Carlo simulations, generating numbers randomly. set seed set.seed() certain number, ensure random numbers generated time code run.go details R generates random numbers, random number generation whole field .terms running examples, can choose copy code exclude line set.seed(). still observe estimated values simulations close true values.","code":""},{"path":"est.html","id":"est","chapter":"7 Estimation","heading":"7 Estimation","text":"module based Introduction Probability Data Science (Chan), Chapter 8.1 8.2. can access book free https://probability4datascience.com. Please note cover additional topics, skip certain topics book. may skip Section 8.1.3, 8.1.4, 8.1.6 book.","code":""},{"path":"est.html","id":"introduction-3","chapter":"7 Estimation","heading":"7.1 Introduction","text":"consider building models based data . Many models based distribution, example, linear regression model based normal distribution, logistic regression model based Bernoulli distribution. Recall distributions specified parameters: mean \\(\\mu\\) variance \\(\\sigma^2\\) normal distribution, success probability \\(p\\) Bernoulli distribution. value parameters almost always unknown real life. module deals estimation: estimate values parameters, well quantify level uncertainty estimated values, given data .","code":""},{"path":"est.html","id":"big-picture-idea-with-estimation","chapter":"7 Estimation","heading":"7.1.1 Big Picture Idea with Estimation","text":"Consider simple scenario. want find distribution associated systolic blood pressure American adults. able achieve goal, get systolic blood pressure every single American adult. usually feasible researchers unlikely time money interview every single American adult. Instead, representative sample American adults obtained, example, 750 randomly selected American adults interviewed. can create density plots, histograms, compute mean, median, variance, skewness, summaries may interest, based 750 American adults.","code":""},{"path":"est.html","id":"population-vs-sample","chapter":"7 Estimation","heading":"7.1.1.1 Population Vs Sample","text":"scenario illustrates concepts terms fundamental estimation. study, must clear population interest, sample.population (sometimes called population interest) entire set individuals, objects, events study interested . scenario described , population () American adults.sample set individuals, objects, events data . scenario described , sample 750 randomly selected American adults.Ideally, sample representative population. representative sample often achieved simple random sample, unit population chance selected sample. module, assume representative sample. Note: may feel obtaining simple random sample may difficult. get discussion sampling (sometimes called survey sampling), field statistics handles obtain representative samples, calculations adjusted sample representative. still lot research done survey sampling.","code":""},{"path":"est.html","id":"variables-observations","chapter":"7 Estimation","heading":"7.1.1.2 Variables & Observations","text":"variable characteristic attribute individuals, objects, events make population sample. scenario, variable systolic blood pressure American adults. can use notation random variables describe variables. example, can let \\(X\\) denote systolic blood pressure American adult, writing \\(P(X>200)\\) means want find probability American adult systolic blood pressure greater 200 mmHg .observation individual person, object event collect data . scenario, observation single American adult sample 750.One way think variables observations spreadsheet. Typically, row represents observation column represents variable. Figure 7.1 displays example, based described scenario. row represents observation, .e. single American adult sample, column represents variable, systolic blood pressure.\nFigure 7.1: Example Data Spreadsheet\n","code":""},{"path":"est.html","id":"parameter-vs-estimator","chapter":"7 Estimation","heading":"7.1.1.3 Parameter Vs Estimator","text":"Now made distinction population sample, ready define parameters estimators.parameter numerical summary associated population. scenario described , example population parameter population mean systolic blood pressure American adults.estimator numerical summary associated samples. estimator typically used estimate parameter. scenario described , estimator population mean systolic blood pressure American adults average systolic blood pressure sample. sample mean estimator population mean.estimated value, estimate, actual value estimator based sample. scenario described , suppose average systolic blood pressure 750 American adults 140 mmHg. say estimated value mean systolic blood pressure American adults 140 mmHg.parameter number associated population, estimator number associated sample. differences parameters estimators:value parameters unknown, can actually calculate numerical values estimators.value parameters considered fixed (one population), numerical values estimators can vary obtain multiple random samples sample size. Using scenario , suppose obtain second representative sample 750 American adults. average systolic blood pressure second sample likely different average systolic blood pressure first sample. illustrates variance, uncertainty, associated estimators due random sampling. uncertainty focusing section.Whenever propose estimator parameter, want assess “good” estimator . situations, obvious choice estimator, example, using sample mean, \\(\\bar{x} = \\frac{\\sum x_i}{n}\\) estimate population mean. instances, choice may obvious. example, use sample variance \\(s^2 = \\frac{\\sum (x_i - \\bar{x})^2}{n-1}\\) estimator population variance, \\(\\frac{\\sum (x_i - \\bar{x})^2}{n}\\)? cover measures used assess estimator: bias, variance, mean-squared error.also cover couple methods estimating parameters: method moments, method maximum likelihood. notice use probability rules methods.sum estimation: use data sample estimate unknown characteristics population, can answer questions regarding variables population, well provide measure uncertainty answers.","code":""},{"path":"est.html","id":"method-of-moments-estimation","chapter":"7 Estimation","heading":"7.2 Method of Moments Estimation","text":"cover couple methods estimation. first method method moments. intuitive method, although lacks certain ideal properties. defining method, recall define terms.Section 4.4.3, defined moments. reminder, random variable \\(X\\), \\(k\\)th moment \\(E(X^k)\\), can found using LOTUS: \\(\\int_{-\\infty}^{\\infty} x^k f_X(x) dx\\).Suppose observe random sample \\(x_1, \\cdots, x_n\\) comes \\(X\\). \\(k\\)th sample moment \\(M_k = \\frac{1}{n} \\sum_{=1}^n x_i^k\\).Using definitions,1st moment \\(E(X) = \\mu_x\\), population mean \\(X\\).1st sample moment \\(M_1 = \\frac{1}{n} \\sum_{=1}^n x_i = \\bar{x}\\), sample mean.2nd moment \\(E(X^2)\\).2nd sample moment \\(M_2 = \\frac{1}{n} \\sum_{=1}^n x_i^2\\)..method moments estimation : Let \\(X\\) random variable distribution depending parameters \\(\\theta_1, \\cdots, \\theta_m\\). method moments (MOM) estimates \\(\\hat{\\theta}_1, \\cdots, \\hat{\\theta}_m\\) found equating first \\(m\\) sample moments corresponding first \\(m\\) moments solving \\(\\theta_1, \\cdots, \\theta_m\\).might noticed method moments based Law Large Numbers.Note: convention, parameters typically denoted Greek letters, estimators denoted hat symbol corresponding letter.Let us look couple examples:Suppose coin know fair . two outcomes flip, heads tails. flip independent flips. Let \\(X_i\\) denote whether \\(\\)th flip lands heads, \\(X_i = 1\\) heads \\(X_i = 0\\) tails. can see \\(X_i \\sim Bern(p)\\), \\(p\\) probability lands heads. Derive MOM estimate \\(p\\).Bernoulli distribution 1 parameter, \\(p\\), using method moments, need equate first sample moment first moment.first moment \\(E(X_i) = p\\), since \\(X_i \\sim Bern(p)\\).first sample moment \\(M_1 = \\frac{1}{n} \\sum_{=1}^n x_i = \\bar{x}\\).Set \\(E(X_i) = M_1\\), .e. \\(\\hat{p} = \\bar{x}\\). Since \\(X_i = 1\\) heads \\(X_i = 0\\) tails, \\(\\frac{1}{n} \\sum_{=1}^n x_i = \\bar{x}\\) actually represents proportion flips land heads, based \\(n\\) flips. actually sample proportion.MOM estimate problem \\(\\hat{p}\\), proportion \\(n\\) flips land heads. result fairly intuitive. flip coin large number times, 70% flips land heads, sample proportion \\(\\hat{p} = 0.7\\), estimated value \\(p\\), success probability 0.7.Birth weights newborn babies typically follow normal distribution. data births Baystate Medical Center Springfield, MA, 1986. Assuming births hospital representative births New England 1986, derive MOM estimates \\(\\mu\\) \\(\\sigma^2\\), mean variance distribution birth weights New England 1986.normal distribution 2 parameters, \\(\\mu\\) \\(\\sigma^2\\), need equate first two sample moments first two moments. Let \\(X\\) denote birth weights New England 1986, \\(X \\sim N(\\mu, \\sigma^2)\\).first moment \\(E(X) = \\mu\\), since \\(X \\sim N(\\mu, \\sigma^2)\\).first sample moment \\(M_1 = \\frac{1}{n} \\sum_{=1}^n x_i = \\bar{x}\\).set \\(E(X) = M_1\\), .e. \\(\\hat{\\mu} = \\bar{x}\\). just sample average birth weights Baystate Medical Center 1986.second moment \\(E(X^2)\\). know since \\(X \\sim N(\\mu, \\sigma^2)\\).\\[\n\\begin{split}\nVar(X) &= E(X^2) - E(X)^2\\\\\n\\implies E(X^2) &= Var(X) + E(X)^2 \\\\\n\\implies E(X^2) &= \\sigma^2 + \\mu^2\n\\end{split}\n\\]second sample moment \\(M_2 = \\frac{1}{n} \\sum_{=1}^n x_i^2\\).set \\(E(X^2) = M_2\\), .e. \\(\\hat{\\sigma^2} + \\hat{\\mu}^2 = \\frac{1}{n} \\sum_{=1}^n x_i^2\\). Since earlier found \\(\\hat{\\mu} = \\bar{x}\\), get \\(\\hat{\\sigma^2}  = \\frac{1}{n} \\sum_{=1}^n x_i^2 - \\bar{x}^2 = \\frac{1}{n} \\sum_{=1}^n (x_i - \\bar{x})^2\\).Therefore, MOM estimates \\(\\mu\\) \\(\\sigma^2\\) \\(\\hat{\\mu} = \\bar{x}\\) \\(\\hat{\\sigma^2}  = \\frac{1}{n} \\sum_{=1}^n (x_i - \\bar{x})^2\\) respectively.View video detailed explanation deriving MOM estimates normal distribution :now use MOM estimates data set birth weights Baystate Medical Center 1986. well established literature birth weights babies follow normal distribution. quick check Shapiro-Wilk’s test normality shows contradiction, proceed finding estimates \\(\\mu\\) \\(\\sigma^2\\). produce density plot birth weights, overlay curve corresponds normal distribution \\(\\hat{\\mu} = \\bar{x}\\) \\(\\hat{\\sigma^2}  = \\frac{1}{n} \\sum_{=1}^n (x_i - \\bar{x})^2\\). normal curve pretty close density plot, appears reasonable say birth weights follow normal distribution mean 2944.587 (grams) variance 528940 (grams-squared).\nFigure 7.2: Density Plot Birth Weights. Normal Curve (Blue) Parameters Estimated MOM Overlaid\n","code":"\nlibrary(MASS)\ndata<-MASS::birthwt ##dataset comes from MASS package\n\nshapiro.test(data$bwt) ##check for normality## \n##  Shapiro-Wilk normality test\n## \n## data:  data$bwt\n## W = 0.99244, p-value = 0.4353\nmu<-mean(data$bwt) ##MOM estimate for mu\nmu## [1] 2944.587\nsigma2<-mean((data$bwt-mu)^2) ##MOM estimate for sigma2\nsigma2## [1] 528940\n##create density plot for data, and overlay Normal curve with parameters estimated by MOM\nplot(density(data$bwt), main=\"\", ylim=c(0,6e-04))\ncurve(dnorm(x, mean=mu, sd=sqrt(sigma2)), \n      col=\"blue\", lwd=2, add=TRUE)"},{"path":"est.html","id":"alternative-form-of-method-of-moments-estimation","chapter":"7 Estimation","heading":"7.2.1 Alternative Form of Method of Moments Estimation","text":"Section 4.4.3, defined central moments. reminder, random variable \\(X\\), \\(k\\)th central moment \\(E((X-\\mu)^k)\\).Suppose observe random sample \\(x_1, \\cdots, x_n\\) comes \\(X\\). \\(k\\)th sample central moment \\(M_k^* = \\frac{1}{n} \\sum_{=1}^n (x_i - \\bar{x})^k\\).alternative form method moments estimation : Let \\(X\\) random variable distribution depending parameters \\(\\theta_1, \\cdots, \\theta_m\\). method moments (MOM) estimates \\(\\hat{\\theta}_1, \\cdots, \\hat{\\theta}_m\\) found equating first sample moment first moments, equating subsequent sample central moments corresponding central moments, solving \\(\\theta_1, \\cdots, \\theta_m\\).alternative form often easier work , since 2nd central moment actually variance \\(X\\).go back 2nd example previous subsection, trying find estimates \\(\\mu\\) \\(\\sigma^2\\) normal distribution.first moment \\(E(X) = \\mu\\), since \\(X \\sim N(\\mu, \\sigma^2)\\).first sample moment \\(M_1 = \\frac{1}{n} \\sum_{=1}^n x_i = \\bar{x}\\).set \\(E(X) = M_1\\), .e. \\(\\hat{\\mu} = \\bar{x}\\). just sample average birth weights Baystate Medical Center 1986.second central moment \\(Var(x) = E[(X-\\mu)^2] = \\sigma^2\\).second sample central moment \\(M_2^* = \\frac{1}{n} \\sum_{=1}^n (x_i - \\bar{x})^2\\).set \\(Var(X) = M_2^*\\) .e. \\(\\hat{\\sigma^2} = \\frac{1}{n} \\sum_{=1}^n (x_i - \\bar{x})^2\\). compare solution solution previous subsection, exactly .idea behind method moments estimation fairly intuitive; however, drawbacks talk introducing another method estimation next section, method maximum likelihood.","code":""},{"path":"est.html","id":"method-of-maximum-likelihood-estimation","chapter":"7 Estimation","heading":"7.3 Method of Maximum Likelihood Estimation","text":"method maximum likelihood workhorse statistics data science since widely used estimating models. preferred method moments built upon stronger theoretical framework, estimators tend desirable properties. guaranteed see method maximum likelihood future.name suggests, method estimating parameters maximizing likelihood. go idea behind likelihood next.","code":""},{"path":"est.html","id":"likelihood-function","chapter":"7 Estimation","heading":"7.3.1 Likelihood Function","text":"Suppose \\(n\\) observations, denoted vector \\(\\boldsymbol{x} = (x_1, \\cdots, x_n)^{T}\\). can use PDF generalize distribution observations, \\(f_{\\boldsymbol{X}}(\\boldsymbol{x})\\). joint PDF variables.seen PDFs (hence joint PDFs) always described parameters (e.g. Normal distribution \\(\\mu, \\sigma^2\\), Bernoulli \\(p\\)). section, let \\(\\boldsymbol{\\theta}\\) denote parameters PDF. example, working multivariate normal distribution, \\(\\boldsymbol{\\theta} = (\\boldsymbol{\\mu}, \\boldsymbol{\\Sigma})\\), mean vector covariance matrix.write \\(f_{\\boldsymbol{X}}(\\boldsymbol{x}; \\boldsymbol{\\theta})\\)\nexpress PDF random vector \\(\\boldsymbol{X}\\) parameter \\(\\boldsymbol{\\theta}\\). PDF function two items:first item vector \\(\\boldsymbol{x} = (x_1, \\cdots, x_n)^{T}\\), basically vector observed data. previous modules, expressed PDFs function observed data, since calculate PDF \\(\\boldsymbol{X} = \\boldsymbol{x}\\). estimation, vector observed data actually fixed something given data set.first item vector \\(\\boldsymbol{x} = (x_1, \\cdots, x_n)^{T}\\), basically vector observed data. previous modules, expressed PDFs function observed data, since calculate PDF \\(\\boldsymbol{X} = \\boldsymbol{x}\\). estimation, vector observed data actually fixed something given data set.second item parameter \\(\\boldsymbol{\\theta}\\). Estimating parameter focus estimation. general idea maximum likelihood find value \\(\\boldsymbol{\\theta}\\) “best explains” “consistent” observed values data \\(\\boldsymbol{x}\\). maximize \\(f_{\\boldsymbol{X}}(\\boldsymbol{x}; \\boldsymbol{\\theta})\\) achieve goal.second item parameter \\(\\boldsymbol{\\theta}\\). Estimating parameter focus estimation. general idea maximum likelihood find value \\(\\boldsymbol{\\theta}\\) “best explains” “consistent” observed values data \\(\\boldsymbol{x}\\). maximize \\(f_{\\boldsymbol{X}}(\\boldsymbol{x}; \\boldsymbol{\\theta})\\) achieve goal.likelihood function PDF, written way shifts emphasis parameters. likelihood function denoted \\(L(\\boldsymbol{\\theta} | \\boldsymbol{x})\\) defined \\(f_{\\boldsymbol{X}}(\\boldsymbol{x})\\).Note: likelihood function viewed function \\(\\boldsymbol{\\theta}\\), shape changes depending values observed data \\(\\boldsymbol{x}\\).simplify calculations involving likelihood function, make assumption observations \\(\\boldsymbol{x}\\) independent come identical distribution PDF \\(f_X(x)\\), words, observations ..d. (independent identically distributed).Given ..d. random variables \\(X_1, \\cdots, X_n\\), PDF \\(f_X(x)\\), likelihood function \\[\\begin{equation}\nL(\\boldsymbol{\\theta} | \\boldsymbol{x} ) = \\prod_i^n f_X(x; \\boldsymbol{\\theta}).\n\\tag{7.1}\n\\end{equation}\\]maximizing likelihood function, often log transform likelihood function first, maximize log transformed likelihood function. log transformed likelihood function called log-likelihood function, \\[\\begin{equation}\n\\ell(\\boldsymbol{\\theta} | \\boldsymbol{x}) = \\log L(\\boldsymbol{x} | \\boldsymbol{\\theta}) = \\sum_{=1}^n \\log f_X(x; \\boldsymbol{\\theta}).\n\\tag{7.2}\n\\end{equation}\\]turns maximizing log-likelihood function often easier computationally maximizing likelihood function.logarithm monotonic increasing function (never decreases), maximizing log transformed function equivalent maximizing original function. Next, look write likelihood log-likelihood functions couple examples.","code":""},{"path":"est.html","id":"example-1-bernoulli","chapter":"7 Estimation","heading":"7.3.1.1 Example 1: Bernoulli","text":"Let \\(X_1, \\cdots, X_n\\) ..d. \\(Bern(p)\\). Find corresponding likelihood log-likelihood functions.equation (3.8), PMF \\(X \\sim Bern(p)\\) \\(f_X(x) = p^x (1-p)^{1-x}\\), support \\(x\\) \\(\\{0,1\\}\\).likelihood function, per equation (7.1), becomes\\[\nL(p | \\boldsymbol{x} ) = \\prod_{=1}^n f_X(x_i; p) = \\prod_{=1}^n p^{x_i} (1-p)^{1-x_i}.\n\\]log-likelihood function, per equation (7.2), becomes\\[\n\\begin{split}\n\\ell (p | \\boldsymbol{x}) &= \\sum_{=1}^n \\log f_X(x_i;p) \\\\\n                          &= \\sum_{=1}^n \\log \\left( p^{x_i} (1-p)^{1-x_i} \\right) \\\\\n                          &= \\sum_{=1}^n x_i \\log p + (1-x_i) \\log (1-p) \\\\\n                          &= \\log p \\left(\\sum_{=1}^n x_i \\right) + \\log (1-p) \\left( n - \\sum_{=1}^n x_i \\right).\n\\end{split}\n\\]","code":""},{"path":"est.html","id":"example-1-continued-visualizing-likelihood-and-log-likelihood-functions","chapter":"7 Estimation","heading":"7.3.1.2 Example 1 Continued: Visualizing Likelihood and Log-Likelihood Functions","text":"mentioned likelihood log-likelihood functions, \\(L(\\boldsymbol{\\theta} | \\boldsymbol{x} )\\) \\(\\ell(\\boldsymbol{\\theta} | \\boldsymbol{x} )\\), functions parameters \\(\\boldsymbol{\\theta}\\) observed data \\(\\boldsymbol{x}\\). typically view functions observing data \\(\\boldsymbol{x}\\).Suppose trying estimate proportion college students use passphrases university email account. randomly select 20 students ask use passphrases university email account. Let \\(x_i\\) denote response student \\(\\), \\(x_i = 1\\) student \\(\\) uses passphrases \\(x_i=0\\) otherwise. can say \\(X \\sim Bern(p)\\) \\(p\\) denotes proportion college students use passphrases university email account. sample 20 students, 7 said use passphrases university email account. example 1, know likelihood function now \\[\n\\begin{split}\nL(p | \\boldsymbol{x} ) &= \\prod_{=1}^n p^{x_i} (1-p)^{1-x_i} \\\\\n                       &= p^7 (1-p)^{13}\n\\end{split}\n\\]log-likelihood function becomes\\[\n\\begin{split}\n\\ell (p | \\boldsymbol{x}) &= \\log p \\left(\\sum_{=1}^n x_i \\right) + \\log (1-p) \\left( n - \\sum_{=1}^n x_i \\right)\\\\\n                          &= 7 \\log p + 13 \\log(1-p).\n\\end{split}\n\\]\ncan create plots \\(L(p | \\boldsymbol{x} )\\) \\(\\ell(p | \\boldsymbol{x} )\\) based observed data, vary value \\(p\\) 0 1 (support \\(p\\)). plots displayed Figure 7.3 \nFigure 7.3: Likelihood (left) Log-Likelihood (right) Functions Bernoulli, n=20, 7 Yeses\nplots Figure 7.3 show us likelihood log-likelihood functions behave, given data (.e. 7 20 students said use passphrases), vary value parameter \\(p\\). note value \\(p\\) maximized likelihood log-likelihood functions 0.35. idea behind method maximum likelihood estimation: value parameter maximizes likelihood log-likelihood functions? regular language, value parameter best explains data?Note: value 0.35 corresponds sample proportion students use passphrases. make intuitive sense 7 20 students sample say use passphrases, say best estimate proportion college students use passphrases university email 0.35.View video provides bit detail finding visualizing likelihood log-likelihood functions Bernoulli distribution:","code":"\n##function to compute likelihood function \nbern_like<-function(x, p) ##supply vector of data, and value of success probability\n  \n{\n  \n  n<-length(x) ##sample size\n  S<-sum(x) \n  \n  likelihood<-p^S * (1-p)^(n-S) ##formula for likelihood function from Example 1\n  return(likelihood)\n  \n}\n\n##function to compute loglikelihood function\n\nbern_loglike<-function(x,p)\n  \n{\n  \n  n<-length(x) ##sample size\n  S<-sum(x) \n  \n  loglike<- S*log(p) + (n-S)*log(1-p) ##formula for log-likelihood function from Example 1\n  return(loglike) \n  \n}\n\n##our \"data\" according to described scenario\ndata<-c(rep(1,7), rep(0,13)) \n##vary the value of p, from 0 to 1, in increments of 0.01\nprops<-seq(0,1,by=0.01) \n\npar(mfrow=c(1,2))\n##plot the likelihood function on y axis, against the value of p\nplot(props, bern_like(data, props), type=\"l\", xlab=\"p\", ylab=\"Likelihood Function\")\n##overlay line on x-axis that corresponds to max value for likelihood function\nabline(v=props[which.max(bern_like(data, props))], col=\"blue\")\n##plot the loglikelihood function on y axis, against the value of p\nplot(props, bern_loglike(data, props), type=\"l\", xlab=\"p\", ylab=\"Log-Likelihood Function\")\n##overlay line on x-axis that corresponds to max value for loglikelihood function\nabline(v=props[which.max(bern_loglike(data, props))], col=\"blue\")\n## what value of p had maximum value of likelihood\nprops[which.max(bern_like(data, props))]## [1] 0.35\n## what value of p had maximum value of loglikelihood\nprops[which.max(bern_loglike(data, props))]## [1] 0.35"},{"path":"est.html","id":"example-2-normal","chapter":"7 Estimation","heading":"7.3.1.3 Example 2: Normal","text":"Let \\(X_1, \\cdots, X_n\\) ..d. \\(N(\\mu, \\sigma^2)\\). Find corresponding likelihood log-likelihood functions.equation (4.11), PDF \\(X \\sim N(\\mu, \\sigma^2)\\) \\(f_X(x) =  \\frac{1}{\\sigma \\sqrt{2 \\pi}} \\exp \\left(-\\frac{(x - \\mu)^2}{2 \\sigma^2} \\right)\\), support \\(x\\) real numbers.likelihood function, per equation (7.1), becomes\\[\nL(\\mu, \\sigma^2 | \\boldsymbol{x} ) = \\prod_{=1}^n f_X(x_i; \\mu, \\sigma^2) = \\prod_{=1}^n \\frac{1}{\\sigma \\sqrt{2 \\pi}} \\exp \\left(-\\frac{(x_i - \\mu)^2}{2 \\sigma^2} \\right).\n\\]log-likelihood function, per equation (7.2), becomes\\[\n\\begin{split}\n\\ell (\\mu, \\sigma^2 | \\boldsymbol{x}) &= \\sum_{=1}^n \\log f_X(x_i; \\boldsymbol{\\theta}) \\\\\n                          &= \\sum_{=1}^n \\log \\left\\{ \\frac{1}{\\sigma \\sqrt{2 \\pi}} \\exp \\left(-\\frac{(x_i - \\mu)^2}{2 \\sigma^2} \\right) \\right\\} \\\\\n                          &= \\sum_{=1}^n \\left\\{-\\frac{1}{2} \\log (2 \\pi \\sigma^2) -\\frac{(x_i - \\mu)^2}{2 \\sigma^2} \\right\\} \\\\\n                          &= -\\frac{n}{2} \\log (2 \\pi \\sigma^2) - \\frac{1}{2 \\sigma^2} \\sum_{=1}^n (x_i - \\mu)^2.\n\\end{split}\n\\]View video provides bit detail finding visualizing likelihood log-likelihood functions normal distribution:","code":""},{"path":"est.html","id":"maximum-likelihood-estimation","chapter":"7 Estimation","heading":"7.3.2 Maximum Likelihood Estimation","text":"now ready formally define method maximimum likelihood estimation. maximum likelihood (ML) estimates \\(\\hat{\\theta}_1, \\cdots, \\hat{\\theta}_m\\) parameters \\(\\theta_1, \\cdots, \\theta_m\\) found maximizing likelihood function \\(L(\\boldsymbol{\\theta} | \\boldsymbol{x} )\\).Remember following finding ML estimates:values \\(\\boldsymbol{x}\\) considered fixed given data.varying values parameters \\(\\boldsymbol{\\theta}\\) finding specific values \\(\\boldsymbol{\\theta}\\) maximize likelihood function.choose maximize log-likelihood function instead. often easier work log-likelihood function, likelihood functions often exponents (powers) , makes maximizing complicated. solution .re-visit Examples 1 2 previous subsection.","code":""},{"path":"est.html","id":"example-1-bernoulli-1","chapter":"7 Estimation","heading":"7.3.2.1 Example 1: Bernoulli","text":"Let \\(X_1, \\cdots, X_n\\) ..d. \\(Bern(p)\\). Find ML estimate \\(p\\).work log-likelihood function, found \\[\n\\ell (p | \\boldsymbol{x}) = \\log p \\left(\\sum_{=1}^n x_i \\right) + \\log (1-p) \\left( n - \\sum_{=1}^n x_i \\right).\n\\]find ML estimate \\(p\\), need find value \\(p\\) maximizes \\(\\ell (p | \\boldsymbol{x})\\). previous subsection, provided visual way represent log-likelihood function \\(p\\) varied, find value \\(p\\) corresponded peak graph. visual approach works specific scenario 7 20 students say yes. Can generalize ML estimate Bernoulli distribution?can easily maximize function taking first derivative setting 0. take first derivative \\(\\ell (p | \\boldsymbol{x})\\) respect parameter \\(p\\):\\[\n\\begin{split}\n\\frac{d}{d p}\\ell (p | \\boldsymbol{x}) &= \\frac{d}{d p} \\left\\{ \\log p \\left(\\sum_{=1}^n x_i \\right) + \\log (1-p) \\left( n - \\sum_{=1}^n x_i \\right) \\right\\}\\\\\n                          &= \\frac{\\sum_{=1}^n x_i}{p} - \\frac{n - \\sum_{=1}^n x_i}{1-p}\n\\end{split}\n\\]\nset 0:\\[\n\\begin{split}\n\\frac{\\sum_{=1}^n x_i}{p} - \\frac{n - \\sum_{=1}^n x_i}{1-p} &= 0\\\\\n                                               \\implies p     &= \\frac{\\sum_{=1}^n x_i}{n}\n\\end{split}\n\\]ML estimate \\(p\\) \\(\\hat{p}_{ML} = \\frac{\\sum_{=1}^n x_i}{n}\\). just sample proportion observed data \\(x_i = 1\\). result means data comes Bernoulli distribution, sample proportion data “success” ML estimate success probability \\(p\\).go back example 7 20 students say use passphrases, ML estimate \\(p\\) \\(\\hat{p}_{ML} = \\frac{7}{20} = 0.35\\), matches result obtained viewed log-likelihood function visually Figure 7.3.View video provides bit detail deriving ML estimates Bernoulli distribution:Thought question: Play around code produced Figure 7.3. Change vector data (anything ’d like). find value \\(p\\) maximizes likelihood log-likelihood functions always \\(\\hat{p}_{ML} = \\frac{\\sum_{=1}^n x_i}{n}\\), sample proportion “success” sample.","code":""},{"path":"est.html","id":"example-2-normal-1","chapter":"7 Estimation","heading":"7.3.2.2 Example 2: Normal","text":"Let \\(X_1, \\cdots, X_n\\) ..d. \\(N(\\mu, \\sigma^2)\\). Find ML estimates \\(\\mu\\) \\(\\sigma^2\\)., work log-likelihood function, found \\[\n\\ell (\\mu, \\sigma^2 | \\boldsymbol{x}) = -\\frac{n}{2} \\log (2 \\pi \\sigma^2) - \\frac{1}{2 \\sigma^2} \\sum_{=1}^n (x_i - \\mu)^2.\n\\]Notice two parameters \\(\\mu\\) \\(\\sigma^2\\), take partial derivatives \\(\\ell (\\mu, \\sigma^2 | \\boldsymbol{x})\\) respect parameter:Partial derivative respect \\(\\mu\\):\\[\n\\begin{split}\n\\frac{d}{d \\mu}\\ell (\\mu, \\sigma^2 | \\boldsymbol{x}) &= \\frac{d}{d \\mu} \\left\\{ -\\frac{n}{2} \\log (2 \\pi \\sigma^2) - \\frac{1}{2 \\sigma^2} \\sum_{=1}^n (x_i - \\mu)^2 \\right \\} \\\\\n                                                     &= \\frac{1}{\\sigma^2} \\sum_{=1}^n (x_i - \\mu).\n\\end{split}\n\\]Partial derivative respect \\(\\sigma^2\\):\\[\n\\begin{split}\n\\frac{d}{d \\sigma^2}\\ell (\\mu, \\sigma^2 | \\boldsymbol{x}) &= \\frac{d}{d \\sigma^2} \\left\\{ -\\frac{n}{2} \\log (2 \\pi \\sigma^2) - \\frac{1}{2 \\sigma^2} \\sum_{=1}^n (x_i - \\mu)^2 \\right \\} \\\\\n                                                     &= -\\frac{n}{2}\\frac{2\\pi}{2 \\pi \\sigma^2} + \\frac{1}{2 \\sigma^4} \\sum_{=1}^n (x_i - \\mu)^2 \\\\\n                                                     &= -\\frac{n}{2 \\sigma^2} + \\frac{1}{2 \\sigma^4} \\sum_{=1}^n (x_i - \\mu)^2\n\\end{split}\n\\]\nset partial derivatives 0:\\[\n\\begin{split}\n\\frac{d}{d \\mu}\\ell (\\mu, \\sigma^2 | \\boldsymbol{x}) &= 0 \\\\\n\\implies \\frac{1}{\\sigma^2} \\sum_{=1}^n (x_i - \\mu) &= 0 \\\\\n\\implies \\mu  &= \\frac{\\sum_{=1}^n x_i}{n} = \\bar{x}.\n\\end{split}\n\\]\nML estimate \\(\\mu\\) \\(\\hat{\\mu_{ML}} = \\bar{x}\\), .e. sample mean.\\[\n\\begin{split}\n\\frac{d}{d \\sigma^2}\\ell (\\mu, \\sigma^2 | \\boldsymbol{x}) &= 0 \\\\\n\\implies -\\frac{n}{2 \\sigma^2} + \\frac{1}{2 \\sigma^4} \\sum_{=1}^n (x_i - \\mu)^2 &= 0 \\\\\n\\implies \\sigma^2 &= \\frac{\\sum_{=1}^n (x_i - \\mu)^2}{n}.\n\\end{split}\n\\]ML estimate \\(\\sigma^2\\) \\(\\hat{\\sigma_{ML}^2} = \\frac{\\sum_{=1}^n (x_i - \\mu)^2}{n}\\). Note normally calculate sample variance, per equation (1.2): \\(\\frac{\\sum_{=1}^n (x_i-\\bar{x})^2}{n-1}\\).View video provides bit detail deriving ML estimates normal distribution:","code":""},{"path":"est.html","id":"calculating-maximum-likelihood-estimates","chapter":"7 Estimation","heading":"7.3.2.3 Calculating Maximum Likelihood Estimates","text":"examples worked , found values maximum likelihood estimates using couple approaches, one using plot likelihood function different values parameter, based observed data, another using calculus, setting derivative log-likelihood function 0.times approaches may feasible, example, closed form solution first derivative. instances, numerical methods used. Numerical methods typically algorithms perform complex computations approximate mathematical result. go details algorithms, numerical methods still active area research.","code":""},{"path":"est.html","id":"estprops","chapter":"7 Estimation","heading":"7.4 Properties of Estimators","text":"learned couple different methods estimate parameters. two methods common, methods estimate parameters. One question assess estimates “good” ? define concepts used asessment.","code":""},{"path":"est.html","id":"estimators-vs-estimates","chapter":"7 Estimation","heading":"7.4.1 Estimators VS Estimates","text":"wrote estimators estimates earlier module, quick reminder :estimator numerical summary associated samples.estimator numerical summary associated samples.estimated value, estimate, actual value estimator based sample.estimated value, estimate, actual value estimator based sample.previously said ML estimate found maximizing likelihood function, .e. \\(\\hat{\\theta}_{ML}(\\boldsymbol{x})\\) value \\(\\theta\\) maximizes \\(L(\\theta| \\boldsymbol{x})\\). write \\(\\hat{\\theta}_{ML}(\\boldsymbol{x})\\) emphasize ML estimate function observed data \\(\\boldsymbol{x} = (x_1,\\cdots, x_n)^T\\). , ML estimate sample mean, write \\(\\hat{\\theta}_{ML}(\\boldsymbol{x}) = \\frac{\\sum_{=1}^n x_i}{n}\\). value calculated based observed data.\ncan also view sample mean random variable, especially want analyze uncertainty associated sample mean. words, distribution sample mean, obtained many different random samples calculated sample mean random sample? viewing sample mean random variable, write \\(\\hat{\\Theta}_{ML}(\\boldsymbol{X}) = \\frac{\\sum_{=1}^n X_i}{n}\\), call \\(\\hat{\\Theta}_{ML}\\) ML estimator parameter \\(\\theta\\).Note: consider one parameter \\(\\theta\\) subsection, simplify notation introduction \\(\\Theta\\). ideas can applied number parameters.ML estimators just one kind estimators. can find estimators ways (method moments, method). estimator function uses data calculates number data can denoted \\(\\hat{\\Theta}(\\boldsymbol{X})\\). call \\(\\hat{\\Theta}\\) estimator \\(\\theta\\).Consider \\(X_1, \\cdots, X_n\\) ..d. Normal unknown mean \\(\\mu\\) variance \\(\\sigma^2\\) known. can define two estimators \\(\\mu\\) \\(\\hat{\\Theta}_1(\\boldsymbol{X}) = \\frac{\\sum_{=1}^n X_i}{n}\\) \\(\\hat{\\Theta}_2(\\boldsymbol{X}) = X_1 + 2\\). first estimator takes average value \\(n\\) data points. second estimator uses first data point adds 2 . first estimator ML estimator, second estimator .can see free define estimators various ways. question now evaluate whether estimator “good” . metrics evaluate estimators.","code":""},{"path":"est.html","id":"bias","chapter":"7 Estimation","heading":"7.4.2 Bias","text":"One metric used evaluate estimators consider long-run average estimator. estimator unbiased long run average estimator equal true value parameter. Mathematically, estimator \\(\\hat{\\Theta}\\) unbiased \\(E(\\hat{\\Theta}) = \\theta\\). definition unbiased estimator, definition bias estimator:\\[\\begin{equation}\nBias(\\hat{\\Theta}) = E(\\hat{\\Theta}) - \\theta.\n\\tag{7.3}\n\\end{equation}\\], value estimator vary sample sample, estimator unbiased equal true parameter, average long-run.go back example previous subsection. Consider \\(X_1, \\cdots, X_n\\) ..d. Normal unknown mean \\(\\mu\\) variance \\(\\sigma^2\\) known. can define two estimators \\(\\mu\\) \\(\\hat{\\Theta}_1(\\boldsymbol{X}) = \\frac{\\sum_{=1}^n X_i}{n}\\) \\(\\hat{\\Theta}_2(\\boldsymbol{X}) = X_1 + 2\\). Assess whether \\(\\hat{\\Theta}_1(\\boldsymbol{X})\\) \\(\\hat{\\Theta}_2(\\boldsymbol{X})\\) unbiased .mathematical way answering question evaluate expected value estimators:\n\\(E(\\hat{\\Theta}_1) = \\frac{1}{n}E(\\sum_{=1}^n X_i) = \\frac{1}{n}\\sum_{=1}^n E(X_i) = \\frac{1}{n}(\\mu + \\cdots + \\mu) = \\mu\\), unbiased.\n\\(E(\\hat{\\Theta}_2)  = E(X_1 + 2) = E(X_1) + 2 = \\mu +2\\) equal \\(\\mu\\), biased.\nmathematical way answering question evaluate expected value estimators:\\(E(\\hat{\\Theta}_1) = \\frac{1}{n}E(\\sum_{=1}^n X_i) = \\frac{1}{n}\\sum_{=1}^n E(X_i) = \\frac{1}{n}(\\mu + \\cdots + \\mu) = \\mu\\), unbiased.\\(E(\\hat{\\Theta}_1) = \\frac{1}{n}E(\\sum_{=1}^n X_i) = \\frac{1}{n}\\sum_{=1}^n E(X_i) = \\frac{1}{n}(\\mu + \\cdots + \\mu) = \\mu\\), unbiased.\\(E(\\hat{\\Theta}_2)  = E(X_1 + 2) = E(X_1) + 2 = \\mu +2\\) equal \\(\\mu\\), biased.\\(E(\\hat{\\Theta}_2)  = E(X_1 + 2) = E(X_1) + 2 = \\mu +2\\) equal \\(\\mu\\), biased.can also use Monte Carlo simulations show results. simulation following:\ncode simulates \\(X_1, \\cdots, X_{n}\\) ..d. known distribution, \\(n\\) fixed. code , used standard normal, \\(n=100\\).\nGenerate large number replicates (used 10 thousand replicates) \\(X_1, \\cdots, X_{n}\\).\nreplicate \\(X_1, \\cdots, X_{n}\\), calculate \\(\\hat{\\Theta}_1(\\boldsymbol{X}) = \\frac{\\sum_{=1}^n X_i}{n}\\) \\(\\hat{\\Theta}_2(\\boldsymbol{X}) = X_1 + 2\\).\nfind average \\(\\hat{\\Theta}_1(\\boldsymbol{X})\\) \\(\\hat{\\Theta}_2(\\boldsymbol{X})\\) across replicates.\ncan also use Monte Carlo simulations show results. simulation following:code simulates \\(X_1, \\cdots, X_{n}\\) ..d. known distribution, \\(n\\) fixed. code , used standard normal, \\(n=100\\).Generate large number replicates (used 10 thousand replicates) \\(X_1, \\cdots, X_{n}\\).replicate \\(X_1, \\cdots, X_{n}\\), calculate \\(\\hat{\\Theta}_1(\\boldsymbol{X}) = \\frac{\\sum_{=1}^n X_i}{n}\\) \\(\\hat{\\Theta}_2(\\boldsymbol{X}) = X_1 + 2\\).find average \\(\\hat{\\Theta}_1(\\boldsymbol{X})\\) \\(\\hat{\\Theta}_2(\\boldsymbol{X})\\) across replicates.estimator unbiased, average Monte Carlo simulation close 0, since true mean standard normal 0.\nFigure 7.4: Dist Theta1 (left) Theta2 (right)\nFigure 7.4 displays distribution 10 thousand \\(\\hat{\\Theta}_1(\\boldsymbol{X})\\)s \\(\\hat{\\Theta}_2(\\boldsymbol{X})\\)s. Visually, estimator unbiased “middle” equal value parameter, 0. can see clearly case \\(\\hat{\\Theta}_2(\\boldsymbol{X})\\) biased.results MC simulation matches math. shown \\(\\hat{\\Theta}_1(\\boldsymbol{X}) = \\frac{\\sum_{=1}^n X_i}{n}\\) unbiased estimator \\(\\mu\\), whereas \\(\\hat{\\Theta}_2(\\boldsymbol{X}) = X_1 + 2\\) biased estimator., based bias, sample mean better estimator population mean using first data point adding 2 .bias deals centrality estimator, .e. estimator equal true parameter, average. seen previous modules, concerned measures centrality, also measures uncertainty. example, likely observe random sample estimated value far true parameter?","code":"\nreps<-10000\n\nest1<-est2<-array(0,reps) ##array to store est1 & est2 from each rep\n\nn<-100\n\nset.seed(7)\n\nfor (i in 1:reps)\n  \n{\n  \n  X<-rnorm(n) ##Xi iid N(0,1) with n=100\n  \n  est1[i]<-mean(X) ##est 1\n  est2[i]<-X[1] + 2 ##est 2\n  \n}\n\nmean(est1) ##should be close to 0, indicating sample mean is unbiased## [1] -0.0009093132\nmean(est2) ##should not be close to 0, indicating first obs + 2 is biased## [1] 2.003234\n##create density plots to show distribution for est1 and est 2, and overlay line to show true value of mu\npar(mfrow=c(1,2))\nplot(density(est1), main=\"Density Plot for Theta1\")\nabline(v=0, col=\"blue\")\nplot(density(est2), main=\"Density Plot for Theta2\")\nabline(v=0, col=\"blue\")"},{"path":"est.html","id":"standard-error-and-variance","chapter":"7 Estimation","heading":"7.4.3 Standard Error and Variance","text":"turns concept variance can also applied estimators, can viewed random variables, just individual data points. Estimators smaller variances smaller degree uncertainty: value estimators change much random sample random sample.can go back example previous subsection. Consider \\(X_1, \\cdots, X_n\\) ..d. Normal unknown mean \\(\\mu\\) variance \\(\\sigma^2\\) known. defined two estimators \\(\\mu\\) \\(\\hat{\\Theta}_1(\\boldsymbol{X}) = \\frac{\\sum_{=1}^n X_i}{n}\\) \\(\\hat{\\Theta}_2(\\boldsymbol{X}) = X_1 + 2\\). Derive variance estimators.mathematical way answering question evaluate variance estimators:\\(Var(\\hat{\\Theta}_1) = \\frac{1}{n^2}Var(\\sum_{=1}^n X_i) = \\frac{1}{n^2} \\sum_{=1}^n Var(X_i) = \\frac{1}{n^2} (\\sigma^2 + \\cdots + \\sigma^2) = \\frac{\\sigma^2}{n}\\).\\(Var(\\hat{\\Theta}_1) = \\frac{1}{n^2}Var(\\sum_{=1}^n X_i) = \\frac{1}{n^2} \\sum_{=1}^n Var(X_i) = \\frac{1}{n^2} (\\sigma^2 + \\cdots + \\sigma^2) = \\frac{\\sigma^2}{n}\\).\\(Var(\\hat{\\Theta}_2) = Var(X_1 + 2) = Var(X_1) = \\sigma^2\\).\\(Var(\\hat{\\Theta}_2) = Var(X_1 + 2) = Var(X_1) = \\sigma^2\\).also separate term used evaluating variance estimator: standard error estimator. essentially standard deviation estimator, .e. \\(SE(\\hat{\\Theta}) = \\sqrt{Var(\\hat{\\Theta})}\\). Going back example: \\(SE(\\hat{\\Theta}_1) = \\frac{\\sigma}{\\sqrt{n}}\\) \\(SE(\\hat{\\Theta}_2) = \\sigma\\).Note: term standard error applied estimators. used finding standard deviation data points.can also use Monte Carlo simulation previous subsection estimate values:results reflected Figure 7.4. Notice scale x-axis plot \\(\\hat{\\Theta}_2\\) (right) much larger, \\(\\hat{\\Theta}_2\\) larger variability \\(\\hat{\\Theta}_1\\), .e. uncertain value \\(\\hat{\\Theta}_2\\), since deviate .clear now estimators smaller standard errors (variances) desired. , based standard error, sample mean better estimator population mean using first data point adding 2 .","code":"\nvar(est1) ##should be close to 1/100, since n=1000## [1] 0.009956095\nvar(est2) ##should be close to 1## [1] 0.9988228\nsd(est1) ##should be close to 1/10## [1] 0.09978023\nsd(est2) ##should be close to 1## [1] 0.9994112"},{"path":"est.html","id":"consistency","chapter":"7 Estimation","heading":"7.4.3.1 Consistency","text":"associated concept variance standard errors estimators consistency. definition consistency estimators fairly technical, give broad overview concept.Notice denoted estimator \\(\\hat{\\Theta}(\\boldsymbol{X})\\), \\(\\boldsymbol{X} = (X_1, \\cdots, X_n)^T\\). stands reason behavior \\(\\hat{\\Theta}(\\boldsymbol{X})\\) may change number data points \\(n\\) changes. use notation \\(\\hat{\\Theta}_n\\) denote estimator based \\(n\\) data points, emphasize focusing estimator changes \\(n\\) changes.estimator consistent \\(\\hat{\\Theta}_n\\) gets closer approaches true value \\(\\theta\\) \\(n\\) gets larger approaches infinity. means sample size \\(n\\) gets larger, estimator tends get closer true value parameter.go back example previous subsection. Consider \\(X_1, \\cdots, X_n\\) ..d. Normal unknown mean \\(\\mu\\) variance \\(\\sigma^2\\) known. defined two estimators \\(\\mu\\) \\(\\hat{\\Theta}_1(\\boldsymbol{X}) = \\frac{\\sum_{=1}^n X_i}{n}\\) \\(\\hat{\\Theta}_2(\\boldsymbol{X}) = X_1 + 2\\). Assess whether \\(\\hat{\\Theta}_1(\\boldsymbol{X})\\) \\(\\hat{\\Theta}_2(\\boldsymbol{X})\\) consistent .mathematical way answering question involves mathematical concepts get . general way assessing estimator consistent see variance shrinks towards zero \\(n\\) gets larger goes infinity.\nearlier showed \\(Var(\\hat{\\Theta}_1) = \\frac{\\sigma^2}{n}\\), shrinks towards zero \\(n\\) gets larger, consistent.\nearlier showed \\(Var(\\hat{\\Theta}_2) = \\sigma^2\\), strink towards zero \\(n\\) gets larger, consistent.\nmathematical way answering question involves mathematical concepts get . general way assessing estimator consistent see variance shrinks towards zero \\(n\\) gets larger goes infinity.earlier showed \\(Var(\\hat{\\Theta}_1) = \\frac{\\sigma^2}{n}\\), shrinks towards zero \\(n\\) gets larger, consistent.earlier showed \\(Var(\\hat{\\Theta}_1) = \\frac{\\sigma^2}{n}\\), shrinks towards zero \\(n\\) gets larger, consistent.earlier showed \\(Var(\\hat{\\Theta}_2) = \\sigma^2\\), strink towards zero \\(n\\) gets larger, consistent.earlier showed \\(Var(\\hat{\\Theta}_2) = \\sigma^2\\), strink towards zero \\(n\\) gets larger, consistent.can also use Monte Carlo simulations show results:\ncode simulates \\(X_1, \\cdots, X_{n}\\) ..d. known distribution, \\(n\\) varied small values large values. code , used standard normal, \\(n=10, 100, 1000\\).\nvalue \\(n\\), generate large number replicates (used 10 thousand replicates) \\(X_1, \\cdots, X_{n}\\).\nreplicate \\(X_1, \\cdots, X_{n}\\), calculate \\(\\hat{\\Theta}_1(\\boldsymbol{X}) = \\frac{\\sum_{=1}^n X_i}{n}\\) \\(\\hat{\\Theta}_2(\\boldsymbol{X}) = X_1 + 2\\).\nproduce density plots \\(\\hat{\\Theta}_1\\) \\(\\hat{\\Theta}_2\\) \\(n=10, 100, 1000\\).\ncan also use Monte Carlo simulations show results:code simulates \\(X_1, \\cdots, X_{n}\\) ..d. known distribution, \\(n\\) varied small values large values. code , used standard normal, \\(n=10, 100, 1000\\).value \\(n\\), generate large number replicates (used 10 thousand replicates) \\(X_1, \\cdots, X_{n}\\).replicate \\(X_1, \\cdots, X_{n}\\), calculate \\(\\hat{\\Theta}_1(\\boldsymbol{X}) = \\frac{\\sum_{=1}^n X_i}{n}\\) \\(\\hat{\\Theta}_2(\\boldsymbol{X}) = X_1 + 2\\).produce density plots \\(\\hat{\\Theta}_1\\) \\(\\hat{\\Theta}_2\\) \\(n=10, 100, 1000\\).estimator consistent, notice spread density plot get narrower \\(n\\) gets larger.\nFigure 7.5: Dist Theta1 (left) Theta2 (right) n Varied\nFigure 7.5 displays distribution 10 thousand \\(\\hat{\\Theta}_1(\\boldsymbol{X})\\)s \\(\\hat{\\Theta}_2(\\boldsymbol{X})\\)s \\(n=10, 100, 1000\\). Visually, estimator consistent density plot becomes narrower \\(n\\) gets larger. see left plot, \\(\\hat{\\Theta}_1(\\boldsymbol{X})\\) consistent, see right plot, \\(\\hat{\\Theta}_2(\\boldsymbol{X})\\) consistent.Note unbiased estimators consistent estimators two different concepts. estimator unbiased inconsistent, biased consistent.Thought question: Suppose \\(X_1, \\cdots, X_n\\) ..d. standard normal. Consider estimator \\(\\mu\\), \\(\\hat{\\Theta}_3 = X_1\\), estimator \\(\\sigma^2\\), \\(\\hat{\\Theta}_4 = \\frac{1}{n} \\sum_{=1}^n (X_i - \\mu)^2\\). book says \\(\\hat{\\Theta}_3\\) unbiased inconsistent estimator \\(\\mu\\), \\(\\hat{\\Theta}_4\\) biased consistent estimator \\(\\sigma^2\\). Can use Monte Carlo simulations verify claims?","code":"\nn<-c(10,100,1000)\n\nreps<-10000\n\nest1<-est2<-array(0,c(length(n),reps)) ##arrays to store est 1 and est 2 as n changes\n\nset.seed(50)\n\nfor (i in 1:length(n))\n  \n{\n  \n  for (j in 1:reps)\n    \n  {\n    \n    X<-rnorm(n[i])\n    est1[i,j]<-mean(X) ##est 1\n    est2[i,j]<-X[1]+2 ##est 2\n    \n  }\n  \n}\n\npar(mfrow=c(1,2))\n\n##find max value of density plots for est 1 so plots all show up complete\nmax_y1 <- max(density(est1[1,])$y, density(est1[2,])$y, density(est1[3,])$y) \n\nplot(density(est1[1,]), ylim=c(0, max_y1), main=\"Density Plot of Est1 with n Varied\")\nlines(density(est1[2,]), col=\"blue\")\nlines(density(est1[3,]), col=\"red\")\nlegend(\"topright\", legend = c(\"n=10\", \"n=100\", \"n=1000\"), col = c(\"black\",\"blue\", \"red\"), lty = 1, cex=0.7)\n\n##find max value of density plots for est 2 so plots all show up complete\nmax_y2 <- max(density(est2[1,])$y, density(est2[2,])$y, density(est2[3,])$y)\n\nplot(density(est2[1,]), ylim=c(0, max_y2), main=\"Density Plot of Est2 with n Varied\")\nlines(density(est2[2,]), col=\"blue\")\nlines(density(est2[3,]), col=\"red\")\nlegend(\"topright\", legend = c(\"n=10\", \"n=100\", \"n=1000\"), col = c(\"black\",\"blue\", \"red\"), lty = 1, cex=0.7)"},{"path":"est.html","id":"sampling-distribution","chapter":"7 Estimation","heading":"7.4.4 Sampling Distribution","text":"plots Figure 7.4 Figure 7.5 give rise idea distribution associated estimator. term , called sampling distribution estimator.estimators known distributions, example sample mean, \\(\\bar{X}\\). common estimators also known distributions, sample proportion estimator population proportion, sample variance estimator population variance, well ML estimators linear regression logistic regression models.sampling distribution estimator known follows well known distributions, can easily perform probability calculations involving estimators, example, likely observe sample mean 2 standard errors away true mean?However, estimators may known distributions, example, sample median estimator population median known sampling distribution. mean unable calculate probabilities associated estimators? turns exist methods called resampling methods allow us approximate calculations. cover bootstrap, commonly used resampling method, future module.","code":""},{"path":"est.html","id":"sampdistmean","chapter":"7 Estimation","heading":"7.4.4.1 Sampling Distribution of Sample Mean","text":"state sampling distribution sample mean, \\(\\bar{X}_n\\). couple conditions consider:\\(X_1, \\cdots, X_n\\) ..d. normal distribution finite mean \\(\\mu\\) finite variance \\(\\sigma^2\\). \\(\\bar{X}_n \\sim N(\\mu, \\frac{\\sigma^2}{n})\\). result based fact sum independent normals result normal distribution.means data originally normally distributed, sample mean normally distributed, regardless sample size.\\(X_1, \\cdots, X_n\\) ..d. distribution finite mean \\(\\mu\\) finite variance \\(\\sigma^2\\), \\(n\\) large enough, \\(\\bar{X}_n\\) approximately \\(N(\\mu, \\frac{\\sigma^2}{n})\\). based CLT section 6.3.2.means even data originally normally distributed, sample mean approximately normally distributed sample size large enough.apply sampling distribution sample mean left plot Figure 7.5, visually displays distribution sample mean \\(n=10, 100, 1000\\). data ..d. standard normal, meet first scenario know sample means follow normal distribution:\\(n=10\\), \\(\\bar{X}_n \\sim N(0, \\frac{1}{10}) = N(0, 0.1)\\).\\(n=100\\), \\(\\bar{X}_n \\sim N(0, \\frac{1}{100}) = N(0,0.01)\\).\\(n=1000\\), \\(\\bar{X}_n \\sim N(0, \\frac{1}{1000}) = N(0,0.001)\\).see sample size gets larger, variance sample mean decreases, less spread resulting density plot concentrated around mean. matches see left plot Figure 7.5.","code":""},{"path":"est.html","id":"mean-squared-error","chapter":"7 Estimation","heading":"7.4.5 Mean-Squared Error","text":"introduced mean-squared error (MSE) context evaluating prediction errors Section 4.4.1.3. MSE can also used evaluate estimator. context, MSE estimator \\(\\hat{\\Theta}\\) \\[\\begin{equation}\nMSE(\\hat{\\Theta}) = E\\left[(\\hat{\\Theta} - \\theta)^2 \\right].\n\\tag{7.4}\n\\end{equation}\\]MSE estimator can interpreted average squared difference estimator value parameter. turns MSE estimator related two metrics: bias estimator variance estimator:\\[\\begin{equation}\nMSE(\\hat{\\Theta}) = Var(\\hat{\\Theta}) + Bias(\\hat{\\Theta})^2.\n\\tag{7.5}\n\\end{equation}\\]words, MSE estimator equal variance estimator plus squared bias estimator. Equation (7.5) often called bias-variance decomposition MSE. estimator unbiased, equation Equation (7.5) tells us MSE estimator equal variance.MSE estimator suggests need consider bias variance estimator. People often think unbiased estimator “best”. However, unbiased estimator high variance. estimator high MSE. setting, may worth considering another estimator may biased, much smaller variance, resulting lower MSE. example can happen linear regression. Classical methods linear regression yield unbiased estimators, specific scenarios, estimators high variances. Another model, called ridge regression model, considers biased estimators may smaller variances, can result lower MSEs specific scenarios. learn models greater detail future class.","code":""},{"path":"est.html","id":"final-comments-on-estimation","chapter":"7 Estimation","heading":"7.5 Final Comments on Estimation","text":"covered method moments method maximum likelihood estimating parameters. methods also called parametric methods involve making assumption data follow well-known distribution unknown parameters, using data, assumed distribution, estimate numerical value unknown parameters.exist nonparametric methods estimation. actually seen one method (without calling nonparametric), kernel density estimation (KDE) Section 4.6.1. KDE used estimate PDF random variable can visualize distribution. look back KDE, notice made assumption distribution random variable. one fundamental differences parametric nonparametric estimation: former assumes distribution data, latter .","code":""},{"path":"est.html","id":"why-ml-estimators","chapter":"7 Estimation","heading":"7.5.1 Why ML Estimators?","text":"mentioned earlier ML estimators considered workhorse statistics data science likely common type estimator used. ML estimators properties:ML estimators consistent.ML estimators asymptotically Normal, .e. \\(\\frac{\\hat{\\Theta} - \\theta}{SE(\\hat{\\Theta})}\\) approximately standard normal \\(n\\) approaches infinity. also implies ML estimators asymptotically unbiased, .e. large \\(n\\), bias shrinks towards 0.ML estimators efficient. \\(n\\) approaches infinity, ML estimators lowest variance among unbiased estimators. However, smaller sample sizes, ML estimators may biased unbiased estimators smaller variances.ML estimators equivariant. \\(\\hat{\\Theta}\\) ML estimator \\(\\theta\\), \\(g(\\hat{\\Theta})\\) ML estimator \\(g(\\theta)\\).properties imply sample size large enough, estimates ML estimators highly likely close value parameter, ML estimators virtually unbiased, approximately normally distributed, smallest variance among possible unbiased estimators. properties exist sample size small.MOM estimators necessarily properties.properties require called regularity conditions. definitions get fairly technical beyond scope class. One conditions data ..d..","code":""},{"path":"confidence-intervals.html","id":"confidence-intervals","chapter":"8 Confidence Intervals","heading":"8 Confidence Intervals","text":"module based Introduction Probability Data Science (Chan), Chapter 9.1 9.2. can access book free https://probability4datascience.com. Please note cover additional topics, skip certain topics book. may skip Section ??? book.","code":""},{"path":"confidence-intervals.html","id":"introduction-4","chapter":"8 Confidence Intervals","heading":"8.1 Introduction","text":"Section 7, use data sample estimate parameters population. example, use sample mean systolic blood pressure 750 randomly selected American adults estimate mean systolic blood pressure American adults. also established estimators sample mean randomness . obtain another random sample 750 American adults, sample mean blood pressure sample likely different original random sample. uncertainty estimator due random sampling. also learned ways measure “well” estimator estimating parameter, bias, variance, standard error, mean-squared error estimator.section, introduce confidence intervals. Confidence intervals build ideas Section 7: estimators random can quantify uncertainty. purpose confidence interval provide range plausible values unknown population parameter, based sample. confidence interval provides estimated value parameter, also measure uncertainty associated estimation.first cover confidence intervals mean confidence intervals proportion, two basic confidence intervals. intervals based fact distribution corresponding estimators, sample mean sample proportion, known long certain conditions met. notice general ideas finding confidence intervals pretty similar; confidence intervals estimators known distributions constructed similarly. last subsection module, learn bootstrap, used distribution estimator unknown.","code":""},{"path":"confidence-intervals.html","id":"confidence-interval-for-the-mean","chapter":"8 Confidence Intervals","heading":"8.2 Confidence Interval for the Mean","text":"","code":""},{"path":"confidence-intervals.html","id":"randomness-of-estimators","chapter":"8 Confidence Intervals","heading":"8.2.1 Randomness of Estimators","text":"Suppose trying estimate mean systolic blood pressure American adults, using sample mean 750 randomly selected American adults. sample mean estimator population mean. want able report value estimator, well uncertainty estimator. way measure uncertainty estimator variance standard error estimator. Larger values indicate higher degree uncertainty, estimator larger variance means value estimator likely different among random samples.Monte Carlo simulations Section 7.4 show distribution associated estimator. start sample mean, since distribution known (see Section 7.4.4.1). talk confidence interval mean first, generalizing ideas estimators known distributions.","code":""},{"path":"confidence-intervals.html","id":"randomness-of-confidence-intervals","chapter":"8 Confidence Intervals","heading":"8.2.2 Randomness of Confidence Intervals","text":"confidence interval probability applied estimator \\(\\bar{X}_n\\). Instead focusing estimated value sample mean variance, construct confidence interval mean takes form:\\[\\begin{equation}\n= \\left(\\bar{X}_n - \\epsilon, \\bar{X}_n + \\epsilon \\right).\n\\tag{8.1}\n\\end{equation}\\]terminology associated intervals form equation (8.1):\\(\\epsilon\\) often called margin error. (Yes margin error often see reported elections polls). value function standard error estimator, gives measure uncertainty estimated value.\nRemember uncertainty measured uncertainty due random sampling, due sources uncertainty getting representative sample, people lying, etc. mentioned earlier modules, methods used handle issues belong field survey sampling, interesting active area research. get issues class.\n\\(\\epsilon\\) often called margin error. (Yes margin error often see reported elections polls). value function standard error estimator, gives measure uncertainty estimated value.Remember uncertainty measured uncertainty due random sampling, due sources uncertainty getting representative sample, people lying, etc. mentioned earlier modules, methods used handle issues belong field survey sampling, interesting active area research. get issues class.value \\(\\bar{X}_n - \\epsilon\\) often called lower bound confidence interval.value \\(\\bar{X}_n - \\epsilon\\) often called lower bound confidence interval.value \\(\\bar{X}_n + \\epsilon\\) often called upper bound confidence interval.value \\(\\bar{X}_n + \\epsilon\\) often called upper bound confidence interval.value \\(\\bar{X}_n\\) often called point estimate population mean.value \\(\\bar{X}_n\\) often called point estimate population mean.Equation (8.1) sometimes expressed \\[\\begin{equation}\n\\text{point estimate } \\pm \\text{ margin error}.\n\\tag{8.2}\n\\end{equation}\\]Given interval mean expressed equation (8.1), ask probability interval \\(\\) includes true value parameter \\(\\mu\\), .e. want evaluate\\[\\begin{equation}\nP(\\mu \\ ) = P(\\bar{X}_n - \\epsilon \\leq \\mu \\leq \\bar{X}_n + \\epsilon).\n\\tag{8.3}\n\\end{equation}\\]important bear mind since estimator, sample mean \\(\\bar{X}_n\\) random variable, also randomness interval \\(\\). numerical values lower upper bounds change different random sample, since value \\(\\bar{X}_n\\) change.idea interval \\(\\) random represented Figure 8.1 :\nFigure 8.1: Randomness Confidence Interval. Picture https://en.wikipedia.org/wiki/Confidence_interval\ndensity curve top Figure 8.1 represents PDF random variable, represents distribution variable population wish study.row dots represents values 10 randomly sampled data points PDF.colored lines row represent lower upper bounds 50% confidence interval calculated sampled data points row.colored dot center confidence interval represents \\(\\bar{x}\\) sampled data points row.intervals blue represent confidence intervals contain value \\(\\mu\\), intervals red represent confidence intervals contain value \\(\\mu\\).Figure 8.1, note 50% confidence intervals capture value \\(\\mu\\), probability per equation (8.3) 0.5. matches theory since confidence interval computed 50% confidence.create 95% confidence intervals row Figure 8.1, upper lower bounds intervals adjusted expect 19 20 intervals contain value \\(\\mu\\).illustration gives us interpretation probability associated confidence interval per equation (8.3): construct 95% confidence interval, 95% chance random interval \\(\\) contain true value parameter. words, 100 random samples construct 95% confidence intervals based sample, expect 95 intervals contain value parameter.idea probability random interval \\(\\) captures true parameter gives rise confidence level. confidence level confidence interval denoted \\(1-\\alpha\\), .e. construct interval 95% confidence, \\(\\alpha=0.05\\). Equation (8.3) can written \\[\\begin{equation}\nP(\\bar{X}_n - \\epsilon \\leq \\mu \\leq \\bar{X}_n + \\epsilon) = 1 - \\alpha.\n\\tag{8.4}\n\\end{equation}\\]say \\(\\) \\((1-\\alpha) \\times 100\\%\\) confidence interval, \\(\\) confidence interval confidence level \\((1-\\alpha) \\times 100\\%\\).Now established confidence intervals random definition confidence level, ready go details constructing confidence interval mean.","code":""},{"path":"confidence-intervals.html","id":"constructing-confidence-interval-for-the-mean","chapter":"8 Confidence Intervals","heading":"8.2.3 Constructing Confidence Interval for the Mean","text":"remind sampling distribution sample mean, \\(\\bar{X}_n\\), Section 7.4.4.1. couple conditions consider:\\(X_1, \\cdots, X_n\\) ..d. normal distribution finite mean \\(\\mu\\) finite variance \\(\\sigma^2\\). \\(\\bar{X}_n \\sim N(\\mu, \\frac{\\sigma^2}{n})\\).\\(X_1, \\cdots, X_n\\) ..d. normal distribution finite mean \\(\\mu\\) finite variance \\(\\sigma^2\\). \\(\\bar{X}_n \\sim N(\\mu, \\frac{\\sigma^2}{n})\\).\\(X_1, \\cdots, X_n\\) ..d. distribution finite mean \\(\\mu\\) finite variance \\(\\sigma^2\\), \\(n\\) large enough, \\(\\bar{X}_n\\) approximately \\(N(\\mu, \\frac{\\sigma^2}{n})\\).\\(X_1, \\cdots, X_n\\) ..d. distribution finite mean \\(\\mu\\) finite variance \\(\\sigma^2\\), \\(n\\) large enough, \\(\\bar{X}_n\\) approximately \\(N(\\mu, \\frac{\\sigma^2}{n})\\).either conditions met, distribution \\(\\bar{X}_n\\) standardization either standard normal approaches standard normal distribution, \\(\\frac{\\bar{X}_n - \\mu}{\\frac{\\sigma}{\\sqrt{n}}} = \\frac{\\bar{X}_n - \\mu}{\\sqrt{Var(\\bar{X})}} = \\frac{\\bar{X}_n - \\mu}{SE(\\bar{X}_n)}\\) either standard normal approximately standard normal \\(n\\) large enough.simplify notation pertains confidence interval mean, let \\(\\hat{Z} = \\frac{\\bar{X}_n - \\mu}{SE(\\bar{X})}\\), \\(\\hat{Z}\\) standard normal approximately standard normal. \\(\\hat{Z}\\) can called standardized version sample mean standardized score.","code":""},{"path":"confidence-intervals.html","id":"critical-value","chapter":"8 Confidence Intervals","heading":"8.2.3.1 Critical Value","text":"perform math operations equation (8.4) see can construct confidence interval mean:\\[\\begin{equation}\n\\begin{split}\nP(\\bar{X}_n - \\epsilon \\leq \\mu \\leq \\bar{X}_n + \\epsilon) &= 1 - \\alpha \\\\\n\\implies P(|\\bar{X}_n - \\mu| \\leq \\epsilon) &= 1 - \\alpha \\\\\n\\implies P \\left(|\\hat{Z}| = |\\frac{\\bar{X}_n - \\mu}{SE(\\bar{X}_n)}|  \\leq \\frac{\\epsilon}{SE(\\bar{X}_n)} = z^{*} \\right) &= 1 - \\alpha \\\\\n\\implies P(|\\hat{Z}| \\leq z^{*}) &= 1 - \\alpha \\\\\n\\implies P(-z^{*} \\leq \\hat{Z} \\leq z^{*}) &= 1 - \\alpha.\n\\end{split}\n\\tag{8.5}\n\\end{equation}\\]equation (8.5), \\(z^*\\) called critical value. can see related margin error, \\(\\epsilon\\): margin error critical value multiplied standard error estimator (case standard error sample mean since constructing confidence interval mean).words, equation (8.5) says want find critical value \\(z^*\\) probability standardized score \\(-z^*\\) \\(z^*\\) \\(1 - \\alpha\\). Visually, probability displayed Figure 8.2 \\(\\alpha=0.05\\). want find values horizontal axis blue shaded area corresponds value 0.95 (recall area PDF represents probability).\nFigure 8.2: Finding Critical Value 95% Confidence\ncontinue working equation (8.5) see obtain value \\(z^*\\), long either two conditions sampling distribution \\(\\bar{X}_n\\) known met:\\[\\begin{equation}\n\\begin{split}\nP(-z^{*} \\leq \\hat{Z} \\leq z^{*}) &= P(\\hat{Z} \\leq z^{*}) - P(\\hat{Z} \\leq -z^{*}) \\\\\n                                  &= \\Phi(z^{*}) - \\Phi(-z^{*}) = 1 - \\alpha.\n\\end{split}\n\\tag{8.6}\n\\end{equation}\\]\\(\\Phi(z) = P(\\hat{Z} \\leq z)\\) CDF standard normal. Due symmetry standard normal, \\(\\Phi(-z^{*}) = 1- \\Phi(z^{*})\\), sub equation (8.6) continue working solve \\(z^*\\):\\[\\begin{equation}\n\\begin{split}\nP(-z^{*} \\leq \\hat{Z} \\leq z^{*}) &= 2 \\Phi(z^*) - 1 = 1 - \\alpha \\\\\n\\implies \\Phi(z^*) &= 1 - \\frac{\\alpha}{2} \\\\\n\\implies z^* &= \\Phi^{-1} \\left(1 - \\frac{\\alpha}{2} \\right)\n\\end{split}\n\\tag{8.7}\n\\end{equation}\\]\\(z^*\\) found inverting CDF standard normal evaluated \\(1 - \\frac{\\alpha}{2}\\). quantity can easily found using R. example, 95% confidence, \\(\\alpha = 0.05\\), type:tells us critical value 1.96 95% confidence.Note: qnorm() function introduced bit detail Section 4.6, feel free go back review.Thought question: Can show critical value 96% confidence 2.054? Can show critical value 98% confidence 2.326?","code":"\nalpha<-0.05\nqnorm(1-alpha/2)## [1] 1.959964"},{"path":"confidence-intervals.html","id":"confidence-interval-for-the-mean-1","chapter":"8 Confidence Intervals","heading":"8.2.3.2 Confidence Interval for the Mean","text":"now ready put pieces together work confidence interval mean:\\[\\begin{equation}\n\\begin{split}\nP(-z^{*} \\leq \\hat{Z} \\leq z^{*}) &= P(-z^{*} \\leq \\frac{\\bar{X}_n - \\mu}{SE(\\bar{X}_n)} \\leq z^{*}) \\\\\n                                  &= P\\left(-z^{*}SE(\\bar{X}_n) \\leq \\bar{X}_n - \\mu \\leq z^{*}SE(\\bar{X}_n)\\right) \\\\\n                                  &= P\\left(\\bar{X}_n - z^{*}SE(\\bar{X}_n) \\leq \\mu \\leq \\bar{X}_n + z^{*}SE(\\bar{X}_n)\\right) \\\\\n                                  &= P\\left(\\bar{X}_n - z^{*} \\frac{\\sigma}{\\sqrt{n}} \\leq \\mu \\leq \\bar{X}_n + z^{*} \\frac{\\sigma}{\\sqrt{n}}\\right).\n\\end{split}\n\\tag{8.8}\n\\end{equation}\\]Therefore, \\((1-\\alpha) \\times 100\\%\\) confidence interval mean \\[\\begin{equation}\n\\left( \\bar{x}_n - z^{*} \\frac{\\sigma}{\\sqrt{n}}, \\bar{x}_n + z^{*} \\frac{\\sigma}{\\sqrt{n}} \\right).\n\\tag{8.9}\n\\end{equation}\\], formula equation (8.9) valid either two conditions Section 7.4.4.1 met, .e. either data originally normal, sample size large enough.several rules thumb exist assess “sample size large enough” (usually least 25 30 suggested). However, mentioned Section @(considerCLT), fixed answer question. depends distribution data. general, skewed data , \\(n\\) needs larger approximation work.","code":""},{"path":"confidence-intervals.html","id":"CP","chapter":"8 Confidence Intervals","heading":"8.2.3.3 Coverage Probability","text":"Figure 8.1 illustrates concept coverage probability. 20 random samples drawn, corresponding 50% confidence intervals constructed, find 10 20 intervals contained true value parameter. coverage probability 50%, since 10 20 intervals contained true value value parameter. matches confidence level 50%. coverage probability match confidence level, , distribution used sampling distribution estimator probably incorrect.general, confidence intervals constructed \\((1-\\alpha) \\times 100\\%\\) confidence, coverage probability confidence intervals \\((1-\\alpha) \\times 100\\%\\).run Monte Carlo simulation show coverage probability confidence intervals mean using equation (8.9). code following:Simulate \\(X_1, \\cdots, X_{10}\\) ..d. standard normal.Calculate 95% confidence interval mean using (8.9).Assess calculated confidence interval contains 0, since true mean 0 (simulated data standard normal)Repeat steps total 10 thousand replicates.Count number confidence intervals contain 0, divide number replicates. value estimates coverage probability.things correctly (correct use formula, correct distribution estimator), estimated coverage probability close confidence level 95%.estimated coverage probability based 10 thousand replicates 94.93%, close confidence level 95%. informs us distribution assumed sample mean correct.","code":"\nn<-10 ## sample size of each random sample\nalpha<-0.05\nreps<-10000 \n\nCIs<-array(0, c(reps,2)) ##store lower and upper bounds of CI\ncontain<-array(0, reps) ##store assessment if the true mean is contained within bounds of CI\n\nset.seed(100)\nfor (i in 1:reps)\n  \n{\n  \n  X<-rnorm(n) ##draw n data points from standard normal\n  ##calculate elements needed for CI of mean\n  xbar<-mean(X) ##sample mean\n  SE<-1/sqrt(n) ##SE of sample mean\n  crit<-qnorm(1-alpha/2) ##critical value\n  ME<-crit*SE ##margin of error\n  CIs[i,1]<-xbar-ME ##lower bound of CI\n  CIs[i,2]<-xbar+ME ##upper bound of CI\n  contain[i]<-CIs[i,1]<0 & CIs[i,2]>0 ##assess if CI contains 0, the true mean\n  \n}\n\n##find proportion of CIs from random samples that contain 0, should be close to 1 - alpha\nsum(contain)/reps ## [1] 0.9493"},{"path":"confidence-intervals.html","id":"worked-example-1","chapter":"8 Confidence Intervals","heading":"8.2.3.4 Worked Example","text":"basis extensive tests, yield point particular type mild steel reinforcing bar known normally distributed \\(\\sigma=100\\) pounds. composition bar slightly modified, modification believed affected either normality value \\(\\sigma\\). random sample 25 modified bars resulted sample average yield point 8439 pounds, compute 90% CI true average yield point modified bars.question, summarize information :\\(n = 25\\),\\(\\bar{x} = 8439\\),\\(\\sigma = 100\\),\\(\\alpha = 0.1\\), \\(z^*\\) found using qnorm(1-0.1/2) 1.644854.Since assuming distribution yield points normally distribution, sample means normally distributed regardless sample size, can proceed computing confidence interval true average yield point using equation (8.9):\\[\n\\left( 8439 - 1.644854 \\frac{100}{\\sqrt{25}} , 8439 + 1.644854 \\frac{100}{\\sqrt{25}} \\right).\n\\]\nWorking everything , get (8406.103, 8471.891).Interpreting CI: 90% probability random interval (8406.103, 8471.891) include true average yield point modified bars.else can say confidence interval?Values outside confidence interval considered “ruled ” plausible values parameter. wanted assess average yield point modified bars 8000 pounds, interval support claim, since value 8000 lies outside interval. can say data support claim average yield point modified bars 8000 pounds.Values outside confidence interval considered “ruled ” plausible values parameter. wanted assess average yield point modified bars 8000 pounds, interval support claim, since value 8000 lies outside interval. can say data support claim average yield point modified bars 8000 pounds.Values inside confidence interval considered plausible values parameter. value inside interval considered plausible. common mistake specify certain value interval, conclude parameter equal specific value. example, mistake say since value 8410 lies inside interval, interval supports claim average yield point modified bars 8410 pounds. values interval still considered plausible.\nsituation, say data support claim average yield point modified bars different 8410 pounds, since 8410 lies inside interval. rule value 8410.\nValues inside confidence interval considered plausible values parameter. value inside interval considered plausible. common mistake specify certain value interval, conclude parameter equal specific value. example, mistake say since value 8410 lies inside interval, interval supports claim average yield point modified bars 8410 pounds. values interval still considered plausible.situation, say data support claim average yield point modified bars different 8410 pounds, since 8410 lies inside interval. rule value 8410.","code":""},{"path":"confidence-intervals.html","id":"confidence-interval-for-the-mean-using-t-distribution","chapter":"8 Confidence Intervals","heading":"8.2.4 Confidence Interval for the Mean Using t Distribution","text":"may noticed calculate confidence interval mean using equation (8.9), involves knowing value \\(\\sigma^2\\), variance variable population, parameter. However, mentioned whole purpose estimation confidence intervals estimate value unknown parameters quantify uncertainty associated estimate. numerical value parameters rarely known! actually use equation (8.9) real life?solution fairly intuitive, use sample variance \\(s^2 = \\frac{1}{n-1} \\sum_{=1}^n (x_i - \\bar{x})^2\\) estimate \\(\\sigma^2\\). less intuitive distribution standardized version sample mean changes.earlier mentioned certain conditions met, \\(\\hat{Z} = \\frac{\\bar{X}_n - \\mu}{\\frac{\\sigma}{\\sqrt{n}}}\\) either standard normal approximately standard normal. estimate \\(\\sigma^2\\) \\(s^2\\) replace \\(\\sigma\\) \\(s\\) \\(\\hat{Z}\\), get new random variable\\[\\begin{equation}\nT =  \\frac{\\bar{X}_n - \\mu}{\\frac{s}{\\sqrt{n}}} = \\frac{\\bar{X}_n - \\mu}{SE(\\bar{X}_n)},\n\\tag{8.10}\n\\end{equation}\\]\\(SE(\\bar{X}_n)\\) now \\(\\frac{s}{\\sqrt{n}}\\). turns \\(T\\) follows another well-known distribution, called \\(t\\) distribution \\(n-1\\) degrees freedom. PDF \\(t\\) distribution pretty long needed course (can look ), take look plot PDF compare plot PDF standard normal, Figure 8.3 .\nFigure 8.3: Plot PDF Z t Distributions\nFigure 8.3, note things \\(t\\) distribution:centered 0, just like standard normal.also symmetric bell-shaped, just like standard normal.heavier tails standard normal. words, extreme values (large small) slightly higher probabilities occurring \\(t\\) distribution standard normal.degree freedom increases, \\(t\\) distribution gets closer standard normal. Notice blue curve closer standard normal curve black, red curve black curve. fact, one can show mathematically PDF \\(t\\) distribution converges PDF standard normal degree freedom increases infinity.fact working \\(t\\) distribution instead standard normal affect calculate confidence interval mean? equation (8.9) change? replace \\(\\sigma^2\\) \\(s^2\\) \\(z^*\\) \\(t^*\\). critical value now denoted \\(t^*\\) emphasize based \\(t\\) distribution.Therefore, \\((1-\\alpha) \\times 100\\%\\) confidence interval mean , \\(\\sigma^2\\) unknown, \\[\\begin{equation}\n\\left( \\bar{x}_n - t^{*} \\frac{s}{\\sqrt{n}}, \\bar{x}_n + t^{*} \\frac{s}{\\sqrt{n}} \\right).\n\\tag{8.11}\n\\end{equation}\\]find critical value \\(t\\) distribution 10 degrees freedom 95% confidence:, formula equation (8.11) valid either two conditions Section 7.4.4.1 met, .e. either data originally normal, sample size large enough.","code":"\n##plot PDF from -5 to 5\nx <- seq(-5, 5, length.out = 100)\n\n##plot the standard normal \ncurve(dnorm(x), from = -5, to = 5, lwd = 2,\n      ylab = \"Density\", xlab = \"x\", \n      main = \"Pdf of Standard Normal and t-Distribution\")\n\n##overlay the t-distribution with 1 and 10 degree of freedom\ncurve(dt(x, df = 1), from = -5, to = 5, col = \"red\", lwd = 2, add = TRUE)\ncurve(dt(x, df = 10), from = -5, to = 5, col = \"blue\", lwd = 2, add = TRUE)\nlegend(\"topright\", legend = c(\"Standard Normal\", \"t-Distribution (df=1)\", \"t-Distribution (df=10)\"),\n       col = c(\"black\", \"red\", \"blue\"), lty = 1, lwd = 2)\nalpha<-0.05\nqt(1-alpha/2, 10) ##supply percentile first, then value of df## [1] 2.228139"},{"path":"confidence-intervals.html","id":"worked-example-2","chapter":"8 Confidence Intervals","heading":"8.2.4.1 Worked Example","text":"sample 66 obese adults put low-carbohydrate diet year. average weight loss 11 lb \nstandard deviation 19 lb. Calculate 99% confidence interval true average weight loss. confidence interval provide support claim mean weight loss positive?question, summarize information :\\(n = 66\\),\\(df = n-1 = 65\\)\\(\\bar{x} = 11\\),\\(s = 19\\),\\(\\alpha = 0.01\\), \\(t^*\\) found using qt(1-0.01/2, 65) 2.653604.information distribution data. However, sample size 66, usually large enough use CLT, can use equation (8.11):\\[\n\\left( 11 - 2.653604 \\frac{19}{\\sqrt{66}} , 11 + 2.653604 \\frac{19}{\\sqrt{66}} \\right)\n\\]\ngives (4.793914, 17.20609). Since entire confidence interval lies 0, data support claim mean weight loss positive.","code":""},{"path":"confidence-intervals.html","id":"confidence-interval-for-mean-using-t-vs-using-z","chapter":"8 Confidence Intervals","heading":"8.2.4.2 Confidence Interval for Mean Using \\(t\\) VS Using \\(z\\)","text":"couple interesting things note critical values associated \\(t\\) distribution:level confidence, \\(t^*\\) never smaller \\(z^*\\).Thought question: Can give intuitive explanation ? Using Figure 8.3 may helpful.value \\(t^*\\) gets closer value \\(z^*\\) degree freedom gets larger.Recall margin error critical value multiplied standard error estimator. margin errors tend larger \\(\\sigma^2\\) unknown. make intuitive sense higher degree uncertainty since additional parameter estimate.Since margin error tends larger \\(\\sigma^2\\) unknown, means width confidence interval mean tends wider \\(\\sigma^2\\) unknown. width confidence interval difference upper lower bound, twice margin error.Thought question: Imagine running Monte Carlo simulation Section 8.2.3.3, now use sample variance instead population variance. Find coverage probabilities confidence interval use correct formula equation (8.11), use \\(z^*\\) critical value sample variance?","code":""},{"path":"confidence-intervals.html","id":"degrees-of-freedom","chapter":"8 Confidence Intervals","heading":"8.2.4.3 Degrees of Freedom","text":"intuitive explanation degrees freedom number independent pieces information can take numerical value, estimating parameterGenerally speaking, lose 1 degree freedom every equation must satisfied. context estimating population mean using sample mean, must always satisfy equation \\(\\bar{x} = \\frac{\\sum_{=1}^n x_i}{n}\\), lose 1 original set \\(n\\) observations, degree freedom \\(n-1\\) calculating CI mean.","code":""},{"path":"confidence-intervals.html","id":"factors-affecting-precision-of-confidence-intervals","chapter":"8 Confidence Intervals","heading":"8.2.5 Factors Affecting Precision of Confidence Intervals","text":"width confidence interval used measure precision. wider width indicates less precision higher degree uncertainty estimate. Narrower widths preferred, able narrow range plausible values unknown parameter ruling values. factors (within researcher’s control) affecting width confidence interval items used calculating margin error:level confidence, \\(1 - \\alpha\\): width increases level confidence increases.Though question: Can use Figure 8.2 help explain width increases level confidence increases?sample size, \\(n\\): \\(n\\) increases, width decreases. implies precision, lower degree uncertainty, larger sample sizes.factors whether \\(\\sigma^2\\) unknown , value variance, usually controllable researcher.","code":""},{"path":"confidence-intervals.html","id":"confidence-interval-for-the-proportion","chapter":"8 Confidence Intervals","heading":"8.3 Confidence Interval for the Proportion","text":"Next, go confidence interval proportion. another common confidence interval. sample proportion estimator population proportion.Proportions used summarize categorical variables, whereas means used summarize quantitative variables. One way decide variable categorical quantitative ask whether arithmetic operations make sense performed variable. operations make sense, variable quantitative, , variable categorical.measure systolic blood pressure sample American adults. variable quantitative, since answer numeric value arithmetic operations can applied. calculate average systolic blood pressure American adults. can work confidence interval mean, using equation (8.11).measure systolic blood pressure sample American adults. variable quantitative, since answer numeric value arithmetic operations can applied. calculate average systolic blood pressure American adults. can work confidence interval mean, using equation (8.11).ask sample voters whether support particular candidate. variable categorical, since answer yes , apply arithmetic operations answer. calculate proportion voters support candidate. need confidence interval proportion.ask sample voters whether support particular candidate. variable categorical, since answer yes , apply arithmetic operations answer. calculate proportion voters support candidate. need confidence interval proportion.","code":""},{"path":"confidence-intervals.html","id":"sampling-distribution-of-sample-proportions","chapter":"8 Confidence Intervals","heading":"8.3.1 Sampling Distribution of Sample Proportions","text":"can use Central Limit Theorem (CLT) approximate sampling distribution sample proportions. sketch derive sampling distribution provided.Let \\(X_1, \\cdots, X_n\\) ..d. Bernoulli success probability p. Using equations (3.9) (3.10), know \\(E(X_i) = p\\) \\(Var(X_i) - p(1-p)\\).Let \\(S = X_1 + \\cdots + X_n = \\sum_{=1}^n X_i\\) denote number successes sample size \\(n\\). Using properties expectations variances, know \\(E(S) = np\\) \\(Var(S) = np(1-p)\\).sample proportion, \\(\\hat{p}\\), just number successes divided sample size, \\(\\hat{p} = \\frac{S}{n} = \\frac{\\sum_{=1}^n X_i}{n} = \\bar{X}_n\\). Therefore, using property expectations variances, know \\(E(\\hat{p}) = p\\) \\(Var(\\hat{p}) = \\frac{p(1-p)}{n}\\).CLT informs us \\(n\\) large enough, \\(\\hat{p}\\) approximately \\(N\\left(p, \\frac{p(1-p)}{n}\\right)\\). Also, distribution \\(\\hat{p}\\) standardization approximately standard normal, .e. standardized score \\(\\hat{Z} = \\frac{\\hat{p}-p}{\\sqrt{\\frac{p(1-p)}{n}}}\\) approximately standard normal.","code":""},{"path":"confidence-intervals.html","id":"constructing-confidence-interval-for-the-proportion","chapter":"8 Confidence Intervals","heading":"8.3.2 Constructing Confidence Interval for the Proportion","text":"lot concepts math confidence interval mean carry confidence interval proportion. skip math confidence interval proportion (although able derive result following steps adjusting).\\((1-\\alpha) \\times 100\\%\\) confidence interval proportion \\[\\begin{equation}\n\\left( \\hat{p} - z^{*} \\sqrt{\\frac{\\hat{p}(1-\\hat{p})}{n}}, \\hat{p} + z^{*} \\sqrt{\\frac{\\hat{p}(1-\\hat{p})}{n}} \\right).\n\\tag{8.12}\n\\end{equation}\\]equation (8.12), can see thatThe point estimate \\(\\hat{p}\\).standard error \\(\\hat{p}\\) \\(SE(\\hat{p}) = \\sqrt{\\frac{\\hat{p}(1-\\hat{p})}{n}}\\).margin error \\(z^{*} \\sqrt{\\frac{\\hat{p}(1-\\hat{p})}{n}}\\).framework confidence intervals usually takes form given equation (8.2): add subtract margin error point estimate. margin error critical value multiplied standard error estimator. see confidence intervals mean proportion take framework, confidence intervals estimators.Note: equation (8.12) based proportions, percentages. calculating confidence interval proportion, reporters often convert values percentages multiplying hundred.equation (8.12), can also see factors affect width confidence interval proportion confidence interval mean.","code":""},{"path":"confidence-intervals.html","id":"conditions-for-confidence-interval-for-the-proportion","chapter":"8 Confidence Intervals","heading":"8.3.2.1 Conditions for Confidence Interval for the Proportion","text":"wrote \\(n\\) large enough, CLT informs us sample proportion \\(\\hat{p}\\) can approximated normal distribution. large large enough? , various rules thumb recommended, usually follow along lines needing least certain number successes, \\(n\\hat{p}\\), failures, \\(n(1-\\hat{p})\\) sample. Values 5 10 usually recommended. Just bear mind approximation works better number successes failures, \\(n\\hat{p}\\) \\(n(1-\\hat{p})\\), increases.","code":""},{"path":"confidence-intervals.html","id":"worked-example-3","chapter":"8 Confidence Intervals","heading":"8.3.2.2 Worked Example","text":"Texas College Tobacco Project survey administered 2016 found sample 5767 undergraduates ages 18–25, 525 said used electronic cigarettes least previous 30 days. Find 95% confidence interval proportion students population sampled used e-cigarettes previous 30 days. Also report margin error. data support claim 5% students population used e-cigarettes previous 30 days?question, summarize information :\\(n = 5767\\),\\(\\hat{p} = \\frac{525}{5767}\\),\\(\\alpha = 0.05\\), \\(z^*\\) found using qnorm(1-0.05/2) 1.959964.number “successes” 525 number “failures” \\(5767-525 = 5242\\). lot larger 10, can work sampling distribution \\(\\hat{p}\\) calculate confidence interval using equation (8.12):\\[\n\\left( \\frac{525}{5767} - 1.959964 \\sqrt{\\frac{\\frac{525}{5767}(1-\\frac{525}{5767})}{5767}} , \\frac{525}{5767} + 1.959964 \\sqrt{\\frac{\\frac{525}{5767}(1-\\frac{525}{5767})}{5767}} \\right)\n\\]gives (0.08361097, 0.09845943), margin error 0.007424228. data support claim 5% students population used e-cigarettes previous 30 days, since entire interval lies 0.05.95% probability random interval (0.08361097, 0.09845943) contains true proportion students population used e-cigarettes previous 30 days.","code":""},{"path":"confidence-intervals.html","id":"minimum-sample-size","chapter":"8 Confidence Intervals","heading":"8.3.3 Minimum Sample Size","text":"Fairly often, researchers want know sample size needed order achieve margin error specific value, want guarantee certain level precision report. Mathematically, want set \\(z^{*} \\sqrt{\\frac{\\hat{p}(1-\\hat{p})}{n}} \\leq M\\) \\(M\\) denotes value margin error needs guaranteed.know value \\(\\hat{p}\\) collecting data, mentioned Section 3.5.1.1 value \\(p(1-p)\\) maximized \\(p=\\frac{1}{2}\\). implies \\(SE(\\hat{p})\\) maximized \\(\\hat{p} = \\frac{1}{2}\\). long \\(M\\) satisfied \\(\\hat{p} = \\frac{1}{2}\\), \\(M\\) satisfied value \\(\\hat{p}\\). subbing \\(\\hat{p} = \\frac{1}{2}\\) \\(z^{*} \\sqrt{\\frac{\\hat{p}(1-\\hat{p})}{n}} \\leq M\\), \\[\\begin{equation}\n\\begin{split}\nz^{*} \\sqrt{\\frac{\\hat{p}(1-\\hat{p})}{n}} & \\leq M \\\\\n\\implies z^* \\sqrt{\\frac{\\frac{1}{2}\\frac{1}{2}}{n}}  & \\leq M \\\\\n\\implies \\left(\\frac{z^*}{2M}\\right)^2 & \\leq n.\n\\end{split}\n\\tag{8.13}\n\\end{equation}\\]Note: always sure round nearest whole number, working LHS inequality (8.13), guarantee margin error \\(M\\).","code":""},{"path":"confidence-intervals.html","id":"worked-example-4","chapter":"8 Confidence Intervals","heading":"8.3.3.1 Worked Example","text":"state legislator wishes survey residents district see proportion electorate aware position using state funds pay abortions. sample size necessary 95% CI p margin error 3 percentage points?Using inequality (8.13),\\[\n\\begin{split}\n\\left(\\frac{1.959964}{2 \\times 0.03}\\right)^2 \\leq n \\\\\n1067.072 \\leq n.\n\\end{split}\n\\]\nneed sample least 1068 residents guanrantee margin error 3 percentage points.","code":""},{"path":"confidence-intervals.html","id":"the-bootstrap","chapter":"8 Confidence Intervals","heading":"8.4 The Bootstrap","text":"","code":""},{"path":"confidence-intervals.html","id":"the-algorithm","chapter":"8 Confidence Intervals","heading":"8.4.1 The Algorithm","text":"","code":""},{"path":"confidence-intervals.html","id":"worked-example-5","chapter":"8 Confidence Intervals","heading":"8.4.2 Worked Example","text":"","code":""}]
