<!DOCTYPE html>
<html lang="en">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<title>Chapter 10 Linear Regression | Understanding Uncertainty Course Notes</title>
<meta name="author" content="Jeffrey Woo">
<meta name="description" content="10.1 Introduction There is a broad range of statistical methods available for us to learn about data. Broadly speaking, these methods can be classified as supervised and unsupervised. Supervised...">
<meta name="generator" content="bookdown 0.43 with bs4_book()">
<meta property="og:title" content="Chapter 10 Linear Regression | Understanding Uncertainty Course Notes">
<meta property="og:type" content="book">
<meta property="og:description" content="10.1 Introduction There is a broad range of statistical methods available for us to learn about data. Broadly speaking, these methods can be classified as supervised and unsupervised. Supervised...">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="Chapter 10 Linear Regression | Understanding Uncertainty Course Notes">
<meta name="twitter:description" content="10.1 Introduction There is a broad range of statistical methods available for us to learn about data. Broadly speaking, these methods can be classified as supervised and unsupervised. Supervised...">
<!-- JS --><script src="https://cdnjs.cloudflare.com/ajax/libs/clipboard.js/2.0.6/clipboard.min.js" integrity="sha256-inc5kl9MA1hkeYUt+EC3BhlIgyp/2jDIyBLS6k3UxPI=" crossorigin="anonymous"></script><script src="https://cdnjs.cloudflare.com/ajax/libs/fuse.js/6.4.6/fuse.js" integrity="sha512-zv6Ywkjyktsohkbp9bb45V6tEMoWhzFzXis+LrMehmJZZSys19Yxf1dopHx7WzIKxr5tK2dVcYmaCk2uqdjF4A==" crossorigin="anonymous"></script><script src="https://kit.fontawesome.com/6ecbd6c532.js" crossorigin="anonymous"></script><script src="libs/jquery-3.6.0/jquery-3.6.0.min.js"></script><meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<link href="libs/bootstrap-4.6.0/bootstrap.min.css" rel="stylesheet">
<script src="libs/bootstrap-4.6.0/bootstrap.bundle.min.js"></script><script src="libs/bs3compat-0.9.0/transition.js"></script><script src="libs/bs3compat-0.9.0/tabs.js"></script><script src="libs/bs3compat-0.9.0/bs3compat.js"></script><link href="libs/bs4_book-1.0.0/bs4_book.css" rel="stylesheet">
<script src="libs/bs4_book-1.0.0/bs4_book.js"></script><script src="https://cdnjs.cloudflare.com/ajax/libs/autocomplete.js/0.38.0/autocomplete.jquery.min.js" integrity="sha512-GU9ayf+66Xx2TmpxqJpliWbT5PiGYxpaG8rfnBEk1LL8l1KGkRShhngwdXK1UgqhAzWpZHSiYPc09/NwDQIGyg==" crossorigin="anonymous"></script><script src="https://cdnjs.cloudflare.com/ajax/libs/mark.js/8.11.1/mark.min.js" integrity="sha512-5CYOlHXGh6QpOFA/TeTylKLWfB3ftPsde7AnmhuitiTX4K5SqCLBeKro6sPS8ilsz1Q4NRx3v8Ko2IBiszzdww==" crossorigin="anonymous"></script><!-- CSS --><style type="text/css">
    
    div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
  </style>
</head>
<body data-spy="scroll" data-target="#toc">

<div class="container-fluid">
<div class="row">
  <header class="col-sm-12 col-lg-3 sidebar sidebar-book"><a class="sr-only sr-only-focusable" href="#content">Skip to main content</a>

    <div class="d-flex align-items-start justify-content-between">
      <h1>
        <a href="index.html" title="">Understanding Uncertainty Course Notes</a>
      </h1>
      <button class="btn btn-outline-primary d-lg-none ml-2 mt-1" type="button" data-toggle="collapse" data-target="#main-nav" aria-expanded="true" aria-controls="main-nav"><i class="fas fa-bars"></i><span class="sr-only">Show table of contents</span></button>
    </div>

    <div id="main-nav" class="collapse-lg">
      <form role="search">
        <input id="search" class="form-control" type="search" placeholder="Search" aria-label="Search">
</form>

      <nav aria-label="Table of contents"><h2>Table of contents</h2>
        <ul class="book-toc list-unstyled">
<li><a class="" href="index.html">Preface</a></li>
<li><a class="" href="descriptive.html"><span class="header-section-number">1</span> Descriptive Statistics</a></li>
<li><a class="" href="probability.html"><span class="header-section-number">2</span> Probability</a></li>
<li><a class="" href="discrete-random-variables.html"><span class="header-section-number">3</span> Discrete Random Variables</a></li>
<li><a class="" href="continuous-random-variables.html"><span class="header-section-number">4</span> Continuous Random Variables</a></li>
<li><a class="" href="joint-distributions.html"><span class="header-section-number">5</span> Joint Distributions</a></li>
<li><a class="" href="inequalities-limit-theorems-and-simulations.html"><span class="header-section-number">6</span> Inequalities, Limit Theorems, and Simulations</a></li>
<li><a class="" href="est.html"><span class="header-section-number">7</span> Estimation</a></li>
<li><a class="" href="confidence-intervals.html"><span class="header-section-number">8</span> Confidence Intervals</a></li>
<li><a class="" href="hypothesis-testing.html"><span class="header-section-number">9</span> Hypothesis Testing</a></li>
<li><a class="active" href="linear-regression.html"><span class="header-section-number">10</span> Linear Regression</a></li>
</ul>

        <div class="book-extra">
          
        </div>
      </nav>
</div>
  </header><main class="col-sm-12 col-md-9 col-lg-7" id="content"><div id="linear-regression" class="section level1" number="10">
<h1>
<span class="header-section-number">10</span> Linear Regression<a class="anchor" aria-label="anchor" href="#linear-regression"><i class="fas fa-link"></i></a>
</h1>
<div id="introduction-6" class="section level2" number="10.1">
<h2>
<span class="header-section-number">10.1</span> Introduction<a class="anchor" aria-label="anchor" href="#introduction-6"><i class="fas fa-link"></i></a>
</h2>
<p>There is a broad range of statistical methods available for us to learn about data. Broadly speaking, these methods can be classified as <strong>supervised</strong> and <strong>unsupervised</strong>. Supervised methods involve relating a response variable with predictors, whereas unsupervised methods do not make a distinction between response variables and predictors and are used to find structure or patterns in the data.</p>
<p>Supervised methods generally have two primary uses:</p>
<ol style="list-style-type: decimal">
<li>
<strong>Association</strong>: Quantify the relationship between variables. How does a change in the predictor variable change the value of the response variable?</li>
<li>
<strong>Prediction</strong>: Predict a future value of a response variable, using information from predictor variables.</li>
</ol>
<p>We always distinguish between a <strong>response variable</strong>, denoted by <span class="math inline">\(y\)</span>, and a <strong>predictor variable</strong>, denoted by <span class="math inline">\(x\)</span>. In most supervised methods, we say that the response variable can be approximated by some mathematical function, denoted by <span class="math inline">\(f\)</span>, of the predictor variable, i.e.</p>
<p><span class="math display">\[
y \approx f(x).
\]</span></p>
<p>Oftentimes, we write this relationship as</p>
<p><span class="math display">\[
y = f(x) + \epsilon,
\]</span></p>
<p>where <span class="math inline">\(\epsilon\)</span> denotes a <strong>random error term</strong>, with a mean of 0. The error term cannot be predicted based on the data we have.</p>
<p>There are various methods to estimate <span class="math inline">\(f\)</span>. Once we estimate <span class="math inline">\(f\)</span>, we can use our method for association and / or prediction.</p>
<p>In this module, we will introduce one of the most traditional supervised methods: <strong>linear regression</strong>. It is used when there is a single response variable that is quantitative. The predictors could be quantitative or categorical.</p>
<div id="motivation" class="section level3" number="10.1.1">
<h3>
<span class="header-section-number">10.1.1</span> Motivation<a class="anchor" aria-label="anchor" href="#motivation"><i class="fas fa-link"></i></a>
</h3>
<p>Why do we learn about linear regression?</p>
<ul>
<li><p>Linear regression is widely used in many fields, and, under certain conditions, does well in the two primary purposes of supervised methods: association and prediction. Other methods may be better at one of these purposes, but usually at the expense of the other purpose. The most important thing is to know what questions you have in order to select the right method that is best for your question.</p></li>
<li><p>Linear regression is fairly easy to interpret and explain to others who may want to know how the method works. Other methods are generally more complicated and can feel like a black-box when explaining to others, leading to less confidence in the method.</p></li>
<li><p>A lot of the ideas used in other methods can be viewed as extensions or variations of linear regression. Once you understand how linear regression works, it becomes easier to understand how other methods work.</p></li>
</ul>
</div>
<div id="toy-example" class="section level3" number="10.1.2">
<h3>
<span class="header-section-number">10.1.2</span> Toy Example<a class="anchor" aria-label="anchor" href="#toy-example"><i class="fas fa-link"></i></a>
</h3>
<p>The most common way of visualizing the relationship between one quantitative predictor variable and one quantitative response variable is with a scatter plot. In the simulated example below, we have data from 6000 UVa undergraduate students on the amount of time they spend studying in a week (in minutes), and how many courses they are taking in the semester (3 or 4 credit courses). Figure <a href="linear-regression.html#fig:10-scatter">10.1</a> displays the scatter plot.</p>
<div class="sourceCode" id="cb184"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="co">##create dataframe</span></span>
<span><span class="va">df</span><span class="op">&lt;-</span><span class="fu"><a href="https://rdrr.io/r/base/data.frame.html">data.frame</a></span><span class="op">(</span><span class="va">study</span>,<span class="va">courses</span><span class="op">)</span></span>
<span></span>
<span><span class="co">##fit regression</span></span>
<span><span class="va">result</span><span class="op">&lt;-</span><span class="fu"><a href="https://rdrr.io/r/stats/lm.html">lm</a></span><span class="op">(</span><span class="va">study</span><span class="op">~</span><span class="va">courses</span>, data<span class="op">=</span><span class="va">df</span><span class="op">)</span></span></code></pre></div>
<div class="sourceCode" id="cb185"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="co">##create scatterplot with regression line overlaid</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/graphics/plot.default.html">plot</a></span><span class="op">(</span><span class="va">df</span><span class="op">$</span><span class="va">courses</span>, <span class="va">df</span><span class="op">$</span><span class="va">study</span>, xlab<span class="op">=</span><span class="st">"# of Courses"</span>, ylab<span class="op">=</span><span class="st">"Study Time (Mins)"</span><span class="op">)</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/graphics/abline.html">abline</a></span><span class="op">(</span><span class="va">result</span><span class="op">)</span></span></code></pre></div>
<div class="figure">
<span style="display:block;" id="fig:10-scatter"></span>
<img src="bookdown-demo_files/figure-html/10-scatter-1.png" alt="Scatterplot of Study Time against Number of Courses Taken" width="672"><p class="caption">
Figure 10.1: Scatterplot of Study Time against Number of Courses Taken
</p>
</div>
<p>Figure <a href="linear-regression.html#fig:10-scatter">10.1</a> could help us with the following questions:</p>
<ul>
<li>Are study time and the number of courses taken related to one another?</li>
<li>How strong is this relationship?</li>
<li>Could we use the data to make a prediction for the study time of a student who is not in this scatter plot?</li>
</ul>
<p>These questions can be answered using linear regression.</p>
</div>
<div id="module-roadmap-8" class="section level3" number="10.1.3">
<h3>
<span class="header-section-number">10.1.3</span> Module Roadmap<a class="anchor" aria-label="anchor" href="#module-roadmap-8"><i class="fas fa-link"></i></a>
</h3>
<ul>
<li>Section <a href="linear-regression.html#SLR">10.2</a> sets up the linear regression model and covers the assumptions made.</li>
<li>Section <a href="linear-regression.html#estSLR">10.3</a> goes over how the linear regression model is estimated.</li>
<li>Section <a href="linear-regression.html#SLRinf">10.4</a> goes over the sampling distribution of the estimators in linear regression, as well as associated confidence intervals and hypothesis tests. These are the main tools used when we are focused on the primary goal of association.</li>
<li>Section <a href="linear-regression.html#SLRacc">10.5</a> goes over how we measure model accuracy in linear regression. These measures are used when we are focused on prediction.</li>
</ul>
</div>
</div>
<div id="SLR" class="section level2" number="10.2">
<h2>
<span class="header-section-number">10.2</span> Simple Linear Regression<a class="anchor" aria-label="anchor" href="#SLR"><i class="fas fa-link"></i></a>
</h2>
<p>For most of this module, we will keep things simple by only considering a single quantitative predictor. Such a regression is called <strong>simple linear regression (SLR)</strong> to emphasize that only one predictor is being considered. We will briefly touch on multiple linear regression (MLR) when multiple predictors are involved later in this module, and you will learn more about linear regression next semester.</p>
<div id="model-setup" class="section level3" number="10.2.1">
<h3>
<span class="header-section-number">10.2.1</span> Model Setup<a class="anchor" aria-label="anchor" href="#model-setup"><i class="fas fa-link"></i></a>
</h3>
<p>In SLR, the function <span class="math inline">\(f\)</span> that relates the predictor variable with the response variable is typically <span class="math inline">\(\beta_0 + \beta_1 x\)</span>. Mathematically, we express this as</p>
<p><span class="math display">\[
y \approx f(x) = \beta_0 + \beta_1 x,
\]</span></p>
<p>or in other words, that the response variable has an approximately linear relationship with the predictor variable. So the SLR model is written as</p>
<p><span class="math display" id="eq:10-SLRmod">\[\begin{equation}
y_i=\beta_0+\beta_{1}x_i + \epsilon_i,
\tag{10.1}
\end{equation}\]</span></p>
<p>where</p>
<ul>
<li>
<span class="math inline">\(y_i\)</span> denotes the value of the response variable for observation <span class="math inline">\(i\)</span>,</li>
<li>
<span class="math inline">\(x_i\)</span> denotes the value of the predictor for observation <span class="math inline">\(i\)</span>,</li>
<li>
<span class="math inline">\(\epsilon_i\)</span> denotes the value of the error for observation <span class="math inline">\(i\)</span>,</li>
<li>
<span class="math inline">\(\beta_0\)</span> and <span class="math inline">\(\beta_1\)</span> are parameters in the SLR model, and we want to estimate them. These parameters are sometimes called <strong>regression coefficients</strong>.</li>
<li>
<span class="math inline">\(\beta_1\)</span> is also called the <strong>slope</strong>.</li>
<li>
<span class="math inline">\(\beta_0\)</span> is also called the <strong>intercept</strong>.</li>
</ul>
<p>In linear regression, we make some assumptions about the error term <span class="math inline">\(\epsilon\)</span>:</p>
<p><span class="math display" id="eq:10-assumptions">\[\begin{equation}
\epsilon_1,\ldots,\epsilon_n \ i.i.d. \sim N(0,\sigma^2).
\tag{10.2}
\end{equation}\]</span></p>
<p>What these assumptions mean is that for each value of the predictor variable <span class="math inline">\(x\)</span>, the response variable:</p>
<ol style="list-style-type: decimal">
<li>follows a normal distribution,</li>
<li>with expected value equal to <span class="math inline">\(\beta_0+\beta_{1} x\)</span>, i.e.</li>
</ol>
<p><span class="math display" id="eq:10-SLR">\[\begin{equation}
E(Y|X=x) = \beta_0+\beta_{1} x
\tag{10.3}
\end{equation}\]</span></p>
<ol start="3" style="list-style-type: decimal">
<li>and variance equal to <span class="math inline">\(\sigma^2\)</span>.</li>
</ol>
<p>View the video below that explains how these are derived:</p>
<iframe width="560px" height="320px" allowfullscreen="true" allow="autoplay *" title="Module 10: Distribution of Y given X in Regression" src="https://virginiauniversity.instructuremedia.com/embed/445744ac-5422-496a-943e-6b2912130c4a" frameborder="0">
</iframe>
<p>In other words, the conditional distribution of <span class="math inline">\(Y|X=x\)</span> is <span class="math inline">\(N(\beta_0+\beta_{1} x, \sigma^2)\)</span>. Applying this to our study time example, it implies that:</p>
<ul>
<li>for students who take 3 courses, their study time follows a <span class="math inline">\(N(\beta_0 + 3\beta_1, \sigma^2)\)</span> distribution,</li>
<li>for students who take 4 courses, their study time follows a <span class="math inline">\(N(\beta_0 + 4\beta_1, \sigma^2)\)</span> distribution,</li>
<li>for students who take 5 courses, their study time follows a <span class="math inline">\(N(\beta_0 + 5\beta_1, \sigma^2)\)</span> distribution.</li>
</ul>
<p>So if we were to subset our dataframe into three subsets, one with students who take 3 courses, another subset for students who take 4 courses, and another subset for students who take 5 courses, and then create a density plot of study times for each subset, each density plot should follow a normal distribution, with different means, and the same spread.</p>
<p>Let us take a look at these density plots below in Figures <a href="linear-regression.html#fig:10-conddist3">10.2</a>, <a href="linear-regression.html#fig:10-conddist4">10.3</a>, and <a href="linear-regression.html#fig:10-conddist5">10.4</a> below:</p>
<div class="sourceCode" id="cb186"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="kw"><a href="https://rdrr.io/r/base/library.html">library</a></span><span class="op">(</span><span class="va"><a href="https://tidyverse.tidyverse.org">tidyverse</a></span><span class="op">)</span></span></code></pre></div>
<div class="sourceCode" id="cb187"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="co">##subset dataframe</span></span>
<span><span class="va">x.3</span><span class="op">&lt;-</span><span class="va">df</span><span class="op">[</span><span class="fu"><a href="https://rdrr.io/r/base/which.html">which</a></span><span class="op">(</span><span class="va">df</span><span class="op">$</span><span class="va">courses</span><span class="op">==</span><span class="fl">3</span><span class="op">)</span>,<span class="op">]</span></span>
<span><span class="co">##density plot of study time for students taking 3 courses</span></span>
<span><span class="fu"><a href="https://ggplot2.tidyverse.org/reference/ggplot.html">ggplot</a></span><span class="op">(</span><span class="va">x.3</span>,<span class="fu"><a href="https://ggplot2.tidyverse.org/reference/aes.html">aes</a></span><span class="op">(</span>x<span class="op">=</span><span class="va">study</span><span class="op">)</span><span class="op">)</span><span class="op">+</span></span>
<span>  <span class="fu"><a href="https://ggplot2.tidyverse.org/reference/geom_density.html">geom_density</a></span><span class="op">(</span><span class="op">)</span><span class="op">+</span></span>
<span>  <span class="fu"><a href="https://ggplot2.tidyverse.org/reference/labs.html">labs</a></span><span class="op">(</span>x<span class="op">=</span><span class="st">"Study Time (Mins)"</span>, title<span class="op">=</span><span class="st">"Dist of Study Times with 3 Courses"</span><span class="op">)</span></span></code></pre></div>
<div class="figure">
<span style="display:block;" id="fig:10-conddist3"></span>
<img src="bookdown-demo_files/figure-html/10-conddist3-1.png" alt="Distribution of Study Time for 3, 4, 5 Classes Taken" width="672"><p class="caption">
Figure 10.2: Distribution of Study Time for 3, 4, 5 Classes Taken
</p>
</div>
<div class="sourceCode" id="cb188"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="co">##subset dataframe</span></span>
<span><span class="va">x.4</span><span class="op">&lt;-</span><span class="va">df</span><span class="op">[</span><span class="fu"><a href="https://rdrr.io/r/base/which.html">which</a></span><span class="op">(</span><span class="va">df</span><span class="op">$</span><span class="va">courses</span><span class="op">==</span><span class="fl">4</span><span class="op">)</span>,<span class="op">]</span></span>
<span><span class="co">##density plot of study time for students taking 4 courses</span></span>
<span><span class="fu"><a href="https://ggplot2.tidyverse.org/reference/ggplot.html">ggplot</a></span><span class="op">(</span><span class="va">x.4</span>,<span class="fu"><a href="https://ggplot2.tidyverse.org/reference/aes.html">aes</a></span><span class="op">(</span>x<span class="op">=</span><span class="va">study</span><span class="op">)</span><span class="op">)</span><span class="op">+</span></span>
<span>  <span class="fu"><a href="https://ggplot2.tidyverse.org/reference/geom_density.html">geom_density</a></span><span class="op">(</span><span class="op">)</span><span class="op">+</span></span>
<span>  <span class="fu"><a href="https://ggplot2.tidyverse.org/reference/labs.html">labs</a></span><span class="op">(</span>x<span class="op">=</span><span class="st">"Study Time (Mins)"</span>, title<span class="op">=</span><span class="st">"Dist of Study Times with 4 Courses"</span><span class="op">)</span></span></code></pre></div>
<div class="figure">
<span style="display:block;" id="fig:10-conddist4"></span>
<img src="bookdown-demo_files/figure-html/10-conddist4-1.png" alt="Distribution of Study Time for 3, 4, 5 Classes Taken" width="672"><p class="caption">
Figure 10.3: Distribution of Study Time for 3, 4, 5 Classes Taken
</p>
</div>
<div class="sourceCode" id="cb189"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="co">##subset dataframe</span></span>
<span><span class="va">x.5</span><span class="op">&lt;-</span><span class="va">df</span><span class="op">[</span><span class="fu"><a href="https://rdrr.io/r/base/which.html">which</a></span><span class="op">(</span><span class="va">df</span><span class="op">$</span><span class="va">courses</span><span class="op">==</span><span class="fl">5</span><span class="op">)</span>,<span class="op">]</span></span>
<span><span class="co">##density plot of study time for students taking 5 courses</span></span>
<span><span class="fu"><a href="https://ggplot2.tidyverse.org/reference/ggplot.html">ggplot</a></span><span class="op">(</span><span class="va">x.5</span>,<span class="fu"><a href="https://ggplot2.tidyverse.org/reference/aes.html">aes</a></span><span class="op">(</span>x<span class="op">=</span><span class="va">study</span><span class="op">)</span><span class="op">)</span><span class="op">+</span></span>
<span>  <span class="fu"><a href="https://ggplot2.tidyverse.org/reference/geom_density.html">geom_density</a></span><span class="op">(</span><span class="op">)</span><span class="op">+</span></span>
<span>  <span class="fu"><a href="https://ggplot2.tidyverse.org/reference/labs.html">labs</a></span><span class="op">(</span>x<span class="op">=</span><span class="st">"Study Time (Mins)"</span>, title<span class="op">=</span><span class="st">"Dist of Study Times with 5 Courses"</span><span class="op">)</span></span></code></pre></div>
<div class="figure">
<span style="display:block;" id="fig:10-conddist5"></span>
<img src="bookdown-demo_files/figure-html/10-conddist5-1.png" alt="Distribution of Study Time for 3, 4, 5 Classes Taken" width="672"><p class="caption">
Figure 10.4: Distribution of Study Time for 3, 4, 5 Classes Taken
</p>
</div>
<p>Notice all of these plots are normal, with different means (centers), and similar spreads.</p>
<p>The notation on the left hand side of equation <a href="linear-regression.html#eq:10-SLR">(10.3)</a> denotes the <strong>expected value</strong> of the response variable, for a fixed value of the predictor variable. Therefore, the regression coefficients can be interpreted in the following manner:</p>
<ul>
<li>
<span class="math inline">\(\beta_1\)</span> denotes the change in the response variable, on average, when the predictor increases by one unit.</li>
<li>
<span class="math inline">\(\beta_0\)</span> denotes the mean of the response variable when the predictor is 0.</li>
</ul>
</div>
<div id="assessing-assumptions" class="section level3" number="10.2.2">
<h3>
<span class="header-section-number">10.2.2</span> Assessing Assumptions<a class="anchor" aria-label="anchor" href="#assessing-assumptions"><i class="fas fa-link"></i></a>
</h3>
<p>The assumptions for the error terms, <span class="math inline">\(\epsilon\)</span>, expressed in equation <a href="linear-regression.html#eq:10-assumptions">(10.2)</a>, can be re-stated with words as the following 4 assumptions:</p>
<ol style="list-style-type: decimal">
<li>For each value of the predictor, the errors have mean 0.</li>
</ol>
<ul>
<li>This implies that <span class="math inline">\(f(x) = \beta_0 + \beta_1 x\)</span> approximates the relationship between the variables well.</li>
<li>A scatter plot of the variables should show a linear relationship.</li>
<li>This is the most important assumption of the 4. If it is not met, predictions will be biased, in other words, predictions will systematically over- or under- predict the value of the response variable.</li>
</ul>
<p>The plots in Figure <a href="linear-regression.html#fig:10-ass1">10.5</a> are based on simulated data.</p>
<ul>
<li><p>The scatter plot shown in Figure <a href="linear-regression.html#fig:10-ass1">10.5</a>(a) is an example of when this assumption is met. As we move from left to right on the plot, the data points are generally evenly scattered on both sides of the regression line that is overlaid.</p></li>
<li><p>The scatter plot shown in Figure <a href="linear-regression.html#fig:10-ass1">10.5</a>(b) is an example of when this assumption is not met. As we move from left to right on the plot in Figure <a href="linear-regression.html#fig:10-ass1">10.5</a>(b), the data points are generally not evenly scattered on both sides of the regression line that is overlaid. The shape of the plots look more like a curve rather than a straight line.</p></li>
</ul>
<div class="figure">
<span style="display:block;" id="fig:10-ass1"></span>
<img src="images/10-ass1.jpg" alt="Assumption 1 Assessment"><p class="caption">
Figure 10.5: Assumption 1 Assessment
</p>
</div>
<ol start="2" style="list-style-type: decimal">
<li>For each value of the predictor, the errors have variance denoted by <span class="math inline">\(\sigma^2\)</span>.</li>
</ol>
<ul>
<li>This implies that in a scatter plot, the vertical variation of data points around the regression equation has the same magnitude everywhere.</li>
<li>If this assumption is not met, hypothesis tests and confidence intervals from the linear regression will be unreliable.</li>
</ul>
<p>The plots in Figure <a href="linear-regression.html#fig:10-ass2">10.6</a> are based on simulated data.</p>
<ul>
<li><p>The scatter plot shown in Figure <a href="linear-regression.html#fig:10-ass2">10.6</a>(a) is an example of when this assumption is met (this figure is actually the same as Figure <a href="linear-regression.html#fig:10-ass1">10.5</a>(a), so the data that produced these plots satisfy both assumptions). As we move from left to right on the plot, the vertical variation of the data points about the regression line is approximately constant.</p></li>
<li><p>The scatter plot shown in Figure <a href="linear-regression.html#fig:10-ass2">10.6</a>(b) is an example of when this assumption is not met. As we move from left to right on the plot in Figure <a href="linear-regression.html#fig:10-ass2">10.6</a>(b), the vertical variation of the data points about the regression line becomes larger as the value of the response variable gets larger, so the variance is not constant.</p></li>
</ul>
<div class="figure">
<span style="display:block;" id="fig:10-ass2"></span>
<img src="images/10-ass2.jpg" alt="Assumption 2 Assessment"><p class="caption">
Figure 10.6: Assumption 2 Assessment
</p>
</div>
<ol start="3" style="list-style-type: decimal">
<li>The errors are independent.</li>
</ol>
<ul>
<li>This implies that the observations are independent. This is usually a by-product of how the observations were sampled. So knowing the data collection method will help assess whether this assumption is met.</li>
<li>If this assumption is not met, hypothesis tests and confidence intervals from the linear regression will be unreliable.</li>
</ul>
<ol start="4" style="list-style-type: decimal">
<li>The errors are normally distributed.</li>
</ol>
<ul>
<li>This is considered the least important of the 4 assumptions, especially with large sample sizes. This is due to the Central Limit Theorem. In linear regression, we write the conditional expectation (or mean) of the response variable to be equal to <span class="math inline">\(f(x) = \beta_0 + \beta_1 x\)</span>, so hypothesis tests and confidence intervals from the linear regression are likely to be reliable.</li>
</ul>
<p><em>Thought question</em>: Look at the scatter plot of the toy example in Figure <a href="linear-regression.html#fig:10-scatter">10.1</a>. Can you explain why this scatter plot shows the first two assumptions are met for linear regression?</p>
<p>View the video below for more explanation on how to assess assumptions 1 and 2 with a scatter plot of the data:</p>
<iframe width="560px" height="320px" allowfullscreen="true" allow="autoplay *" title="Module 10: Assessing Assumptions in Linear Regression" src="https://virginiauniversity.instructuremedia.com/embed/6cc805a1-4274-4693-91de-c359d90e4e44" frameborder="0">
</iframe>
</div>
</div>
<div id="estSLR" class="section level2" number="10.3">
<h2>
<span class="header-section-number">10.3</span> Estimating Regression Coefficients<a class="anchor" aria-label="anchor" href="#estSLR"><i class="fas fa-link"></i></a>
</h2>
<p>There are two methods in estimating the regression coefficients: the method of <strong>least squares</strong> and the method of <strong>maximum likelihood</strong>. For large sample sizes, these methods give similar results.</p>
<p>We will go over the method of least squares first, since this is the method that is usually used to explain to new learners and is conceptually easier to understand.</p>
<div id="method-of-least-squares" class="section level3" number="10.3.1">
<h3>
<span class="header-section-number">10.3.1</span> Method of Least Squares<a class="anchor" aria-label="anchor" href="#method-of-least-squares"><i class="fas fa-link"></i></a>
</h3>
<p>From equations <a href="linear-regression.html#eq:10-SLR">(10.3)</a> and <a href="linear-regression.html#eq:10-SLRmod">(10.1)</a>, we notice that we have to estimate the regression coefficients <span class="math inline">\(\beta_0, \beta_1\)</span>. We are unable to obtain numerical values of these parameters as we do not have data from the entire population. So what we do is use the data from our sample to estimate these parameters. We estimate <span class="math inline">\(\beta_0,\beta_1\)</span> using <span class="math inline">\(\hat{\beta}_0,\hat{\beta}_1\)</span> based on a sample of observations <span class="math inline">\((x_i,y_i)\)</span> of size <span class="math inline">\(n\)</span>.</p>
<p>Following equations <a href="linear-regression.html#eq:10-SLR">(10.3)</a> and <a href="linear-regression.html#eq:10-SLRmod">(10.1)</a>, the sample versions are</p>
<p><span class="math display" id="eq:10-fitted">\[\begin{equation}
\hat{y}=\hat{\beta}_0+\hat{\beta}_1 x
\tag{10.4}
\end{equation}\]</span></p>
<p>and</p>
<p><span class="math display" id="eq:10-fitted-model">\[\begin{equation}
y=\hat{\beta}_0+\hat{\beta}_1 x + e
\tag{10.5}
\end{equation}\]</span></p>
<p>respectively. Eqaution <a href="linear-regression.html#eq:10-fitted">(10.4)</a> is called the <strong>fitted line</strong>. The line that was overlaid in the scatter plot for the toy example in Figure <a href="linear-regression.html#fig:10-scatter">10.1</a> represents the fitted line. Equation <a href="linear-regression.html#eq:10-fitted-model">(10.5)</a> is called the <strong>estimated SLR model</strong>.</p>
<p><span class="math inline">\(\hat{\beta}_1,\hat{\beta}_0\)</span> are the estimators for <span class="math inline">\(\beta_1,\beta_0\)</span> respectively. These estimators can be interpreted in the following manner:</p>
<ul>
<li>
<span class="math inline">\(\hat{\beta}_1\)</span> denotes the change in the predicted <span class="math inline">\(y\)</span> when <span class="math inline">\(x\)</span> increases by 1 unit. Alternatively, it estimates the change in <span class="math inline">\(y\)</span>, on average, when <span class="math inline">\(x\)</span> increases by 1 unit.</li>
<li>
<span class="math inline">\(\hat{\beta}_0\)</span> denotes the predicted <span class="math inline">\(y\)</span> when <span class="math inline">\(x=0\)</span>. Alternatively, it estimates the average of <span class="math inline">\(y\)</span> when <span class="math inline">\(x=0\)</span>.</li>
</ul>
<p>In equation <a href="linear-regression.html#eq:10-fitted-model">(10.5)</a>, notice we use <span class="math inline">\(e\)</span> to denote the <strong>residual</strong>, or in other words, the “error” in the sample.</p>
<p>From equations <a href="linear-regression.html#eq:10-fitted">(10.4)</a> and <a href="linear-regression.html#eq:10-fitted-model">(10.5)</a>, we have the following quantities that we can compute:</p>
<ul>
<li>Fitted values or predicted values:</li>
</ul>
<p><span class="math display" id="eq:10-fits">\[\begin{equation}
\hat{y} = \hat{\beta}_0+\hat{\beta}_1 x.
\tag{10.6}
\end{equation}\]</span></p>
<p>The <strong>fitted values</strong> are the predicted values of the response variable when the predictor is equal to some specific value. Visually, the fitted line represents each fitted value as we vary the value of the predictor.</p>
<ul>
<li>Residuals:</li>
</ul>
<p><span class="math display" id="eq:10-res">\[\begin{equation}
e_i = y_i-\hat{y}_i.
\tag{10.7}
\end{equation}\]</span></p>
<p>The <strong>residuals</strong> are the differences between the actual values of the response variable and their corresponding predicted values based on the fitted line. Visually, a residual is the vertical distance of a data point in the scatter plot from the fitted line, as shown in Figure <a href="linear-regression.html#fig:10-res">10.7</a> below:</p>
<div class="figure">
<span style="display:block;" id="fig:10-res"></span>
<img src="images/10-residuals.png" alt="Example of Residuals. Picture from https://www.statology.org/residuals/"><p class="caption">
Figure 10.7: Example of Residuals. Picture from <a href="https://www.statology.org/residuals/" class="uri">https://www.statology.org/residuals/</a>
</p>
</div>
<ul>
<li>
<strong>Sum of Squared Residuals</strong>:</li>
</ul>
<p><span class="math display" id="eq:10-SSres">\[\begin{equation}
SS_{res} =  \sum\limits_{i=1}^n(y_i-\hat{y}_i)^2.
\tag{10.8}
\end{equation}\]</span></p>
<p>We compute the estimated coefficients <span class="math inline">\(\hat{\beta}_1,\hat{\beta}_0\)</span> using the <strong>method of least squares</strong>, i.e. choose the numerical values of <span class="math inline">\(\hat{\beta}_1,\hat{\beta}_0\)</span> that minimize <span class="math inline">\(SS_{res}\)</span> as given in equation <a href="linear-regression.html#eq:10-SSres">(10.8)</a>. We find the line which minimizes the sum of all the squared residuals on the scatter plot, hence the name method of least squares.</p>
<p>By minimizing <span class="math inline">\(SS_{res}\)</span> with respect to <span class="math inline">\(\hat{\beta}_0\)</span> and <span class="math inline">\(\hat{\beta}_1\)</span>, the estimated coefficients in the simple linear regression equation are</p>
<p><span class="math display" id="eq:10-b1">\[\begin{equation}
\hat{\beta}_1 = \frac{\sum\limits_{i=1}^n(x_i-\bar{x})(y_i-\bar{y})}{\sum\limits_{i=1}^n(x_i-\bar{x})^2}
\tag{10.9}
\end{equation}\]</span></p>
<p>and</p>
<p><span class="math display" id="eq:10-b0">\[\begin{equation}
\hat{\beta}_0 = \bar{y}- \hat{\beta}_1 \bar{x}
\tag{10.10}
\end{equation}\]</span></p>
<p><span class="math inline">\(\hat{\beta}_1, \hat{\beta}_0\)</span> are called <strong>least squares estimators</strong>, to emphasize that these values are found by minimizing <span class="math inline">\(SS_{res}\)</span>.</p>
<p>The minimization of <span class="math inline">\(SS_{res}\)</span> with respect to <span class="math inline">\(\hat{\beta}_0\)</span> and <span class="math inline">\(\hat{\beta}_1\)</span> is done by taking the partial derivatives of <a href="linear-regression.html#eq:10-SSres">(10.8)</a> with respect to <span class="math inline">\(\hat{\beta}_1\)</span> and <span class="math inline">\(\hat{\beta}_0\)</span>, setting these two partial derivatives equal to 0, and solving these two equations for <span class="math inline">\(\hat{\beta}_1\)</span> and <span class="math inline">\(\hat{\beta}_0\)</span>.</p>
<p>View the video below for more explanation on the method of least squares:</p>
<iframe width="560px" height="320px" allowfullscreen="true" allow="autoplay *" title="Module 10: Method of Least Squares" src="https://virginiauniversity.instructuremedia.com/embed/129b20c0-b2fa-4bfb-a8e5-e810840cfb95" frameborder="0">
</iframe>
<p>Let’s take a look at the estimated coefficients for our study time example:</p>
<div class="sourceCode" id="cb190"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="co">##fit regression</span></span>
<span><span class="va">result</span><span class="op">&lt;-</span><span class="fu"><a href="https://rdrr.io/r/stats/lm.html">lm</a></span><span class="op">(</span><span class="va">study</span><span class="op">~</span><span class="va">courses</span>, data<span class="op">=</span><span class="va">df</span><span class="op">)</span> <span class="co">##supply y, then x, and specify dataframe via data</span></span>
<span><span class="co">##print out the estimated coefficients</span></span>
<span><span class="va">result</span></span></code></pre></div>
<pre><code>## 
## Call:
## lm(formula = study ~ courses, data = df)
## 
## Coefficients:
## (Intercept)      courses  
##       58.45       120.39</code></pre>
<p>From our sample of 6000 students, we have</p>
<ul>
<li>
<span class="math inline">\(\hat{\beta}_1\)</span> = 120.3930985. The predicted study time increases by 120.3930985 minutes for each additional course taken.</li>
<li>
<span class="math inline">\(\hat{\beta}_0\)</span> = 58.4482853. The predicted study time is 58.4482853 when no courses are taken. Notice this value does not make sense, as a student cannot be taking 0 courses. If you look at our data, the number of courses taken is 3, 4, or 5. So we should only use our regression when <span class="math inline">\(3 \leq x \leq 5\)</span>. We cannot use it for values of <span class="math inline">\(x\)</span> outside the range of our data. Making predictions of the response variable for predictors outside the range of the data is called <strong>extrapolation</strong> and should not be done.</li>
</ul>
<p><em>Thought question</em>: The response variable for the toy example is study time, in minutes. Suppose we convert these values to hours by dividing by 60. How will the numerical value of the estimated coefficients change? How will the interpretation of the estimated coefficients change?</p>
<p>Note: I often get a question about why we minimize <span class="math inline">\(SS_{res}\)</span> and not some other quantity such as minimizing the sum of the absolute value of the residuals <span class="math inline">\(\sum_{i=1}^n |y_i - \hat{y}_i|\)</span>? We will state a famous theorem here without proving it. <strong>Gauss Markov Theorem</strong>: Under assumptions for a regression model (as stated in equation <a href="linear-regression.html#eq:10-assumptions">(10.2)</a>), the least squares estimators <span class="math inline">\(\hat{\beta}_1\)</span> and <span class="math inline">\(\hat{\beta}_0\)</span> are unbiased and have minimum variance among all unbiased linear estimators. This means that any other way of deriving unbiased estimators will result in the estimators having larger variance than least squares estimators.</p>
</div>
<div id="method-of-maximum-likelihood" class="section level3" number="10.3.2">
<h3>
<span class="header-section-number">10.3.2</span> Method of Maximum Likelihood<a class="anchor" aria-label="anchor" href="#method-of-maximum-likelihood"><i class="fas fa-link"></i></a>
</h3>
<p>We will give a brief overview of maximum likelihood estimation is carried out for simple linear regression. We had earlier written that the conditional distribution of <span class="math inline">\(Y|X=x\)</span> is <span class="math inline">\(N(\beta_0+\beta_{1} x, \sigma^2)\)</span>. We know the PDF of any normal distribution takes the form in equation <a href="continuous-random-variables.html#eq:4-pdfNormal">(4.11)</a>, which implies that we can write the PDF of this distribution as</p>
<p><span class="math display" id="eq:10-SLRpdf">\[\begin{equation}
f_{Y|X}(y|x) = \frac{1}{\sigma \sqrt{2 \pi}} \exp \left(-\frac{(y - (\beta_0+\beta_{1} x))^2}{2 \sigma^2} \right).
\tag{10.11}
\end{equation}\]</span></p>
<p>We can then write the corresponding log-likelihood function, using equation <a href="est.html#eq:7-loglike">(7.2)</a>, as</p>
<p><span class="math display" id="eq:10-SLRloglike">\[\begin{equation}
\ell(\beta_0, \beta_1, \sigma^2 | \boldsymbol{y}, \boldsymbol{x}) = -\frac{n}{2} \log(2\pi) - n \log(\sigma) - \frac{1}{2 \sigma^2} \sum_{i=1}^n \left(y_i - (\beta_0 + \beta_1 x_i) \right)^2.
\tag{10.12}
\end{equation}\]</span></p>
<p>We then take the partial derivatives of equation <a href="linear-regression.html#eq:10-SLRloglike">(10.12)</a> with respect to <span class="math inline">\(\hat{\beta}_1\)</span> and <span class="math inline">\(\hat{\beta}_0\)</span>, set these two partial derivatives equal to 0, and solve these two equations for <span class="math inline">\(\hat{\beta}_1\)</span> and <span class="math inline">\(\hat{\beta}_0\)</span>. We end up with the same solutions as the method of least squares.</p>
<p>The code below finds the estiamted coeffficients for our study time example. The <code><a href="https://rdrr.io/r/stats/glm.html">glm()</a></code> function fits via method of maximum likelihood, whereas the <code><a href="https://rdrr.io/r/stats/lm.html">lm()</a></code> function from earlier fits via the method of least squares.</p>
<div class="sourceCode" id="cb192"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="co">##fit regression</span></span>
<span><span class="va">result.mle</span><span class="op">&lt;-</span><span class="fu"><a href="https://rdrr.io/r/stats/glm.html">glm</a></span><span class="op">(</span><span class="va">study</span><span class="op">~</span><span class="va">courses</span>, data<span class="op">=</span><span class="va">df</span><span class="op">)</span> <span class="co">##we use glm() function for ML instead</span></span>
<span><span class="co">##print out the estimated coefficients. Notice they are the same. </span></span>
<span><span class="va">result.mle</span></span></code></pre></div>
<pre><code>## 
## Call:  glm(formula = study ~ courses, data = df)
## 
## Coefficients:
## (Intercept)      courses  
##       58.45       120.39  
## 
## Degrees of Freedom: 5999 Total (i.e. Null);  5998 Residual
## Null Deviance:       63300000 
## Residual Deviance: 5317000   AIC: 57750</code></pre>
</div>
</div>
<div id="SLRinf" class="section level2" number="10.4">
<h2>
<span class="header-section-number">10.4</span> Inference with Simple Linear Regression<a class="anchor" aria-label="anchor" href="#SLRinf"><i class="fas fa-link"></i></a>
</h2>
<p>The process of using data from a sample to draw a conclusion about the population is called (statistical) <strong>inference</strong>. Two methods associated with inference are confidence intervals and hypothesis testing.</p>
<p>The most common inference deals with the slope, <span class="math inline">\(\beta_1\)</span>. We are usually assessing whether the slope is 0 or not, as a slope of 0 implies that there is no linear relationship between the variables (if the slope is 0, the value of the response is not affected by the value of the predictor).</p>
<p>We will cover the confidence interval and hypothesis test for the slope.</p>
<div id="properties-of-least-squares-estimators" class="section level3" number="10.4.1">
<h3>
<span class="header-section-number">10.4.1</span> Properties of Least Squares Estimators<a class="anchor" aria-label="anchor" href="#properties-of-least-squares-estimators"><i class="fas fa-link"></i></a>
</h3>
<p>Before proceeding, we will state a few properties associated with the least squares estimators <span class="math inline">\(\hat{\beta}_1, \hat{\beta}_0\)</span>. The key here is to realize that the sampling distribution of the least squares estimators are known. Do not be too torn up about these formulas as a lot of these are computed using R. The proof of these are beyond the scope of this class, and there are countless books that provide these proofs if you are interested.</p>
<ol style="list-style-type: decimal">
<li><p><span class="math inline">\(\mbox{E}(\hat{\beta}_1) = \beta_1\)</span>, <span class="math inline">\(\mbox{E}(\hat{\beta}_0) = \beta_0\)</span>. In other words, the least squares estimators are unbiased.</p></li>
<li><p>The variance of <span class="math inline">\(\hat{\beta}_1\)</span> is</p></li>
</ol>
<p><span class="math display" id="eq:10-varb1">\[\begin{equation}
\mbox{Var}(\hat{\beta}_1) = \frac{\sigma^{2}}{\sum_{i=1}^n{(x_{i}-\bar{x})^{2}}}
\tag{10.13}
\end{equation}\]</span></p>
<ol start="3" style="list-style-type: decimal">
<li>The variance of <span class="math inline">\(\hat{\beta}_0\)</span> is</li>
</ol>
<p><span class="math display" id="eq:10-varb0">\[\begin{equation}
\mbox{Var}(\hat{\beta}_0) = \sigma^2 \left[\frac{1}{n} + \frac{\bar{x}^2}{\sum_{i=1}^n (x_i -\bar{x})^2}\right]
\tag{10.14}
\end{equation}\]</span></p>
<ol start="4" style="list-style-type: decimal">
<li>
<span class="math inline">\(\hat{\beta}_1\)</span> and <span class="math inline">\(\hat{\beta}_0\)</span> both follow a normal distribution.</li>
</ol>
<p>Note that in equations <a href="linear-regression.html#eq:10-varb1">(10.13)</a> and <a href="linear-regression.html#eq:10-varb0">(10.14)</a>, we use <span class="math inline">\(s^2 = \frac{SS_{res}}{n-2} = MS_{res}\)</span> to estimate <span class="math inline">\(\sigma^2\)</span>, the variance of the errors, since <span class="math inline">\(\sigma^2\)</span> is a unknown value.</p>
<p>What these imply is that if we standardize <span class="math inline">\(\hat{\beta}_1\)</span> and <span class="math inline">\(\hat{\beta}_0\)</span>, these standardized quantities will follow a <span class="math inline">\(t_{n-2}\)</span> distribution, i.e.</p>
<p><span class="math display" id="eq:10-distb1">\[\begin{equation}
\frac{\hat{\beta}_1 - \beta_1}{se(\hat{\beta}_1)}\sim t_{n-2}
\tag{10.15}
\end{equation}\]</span></p>
<p>and</p>
<p><span class="math display" id="eq:10-distb0">\[\begin{equation}
\frac{\hat{\beta}_0 - \beta_0}{se(\hat{\beta}_0)}\sim t_{n-2},
\tag{10.16}
\end{equation}\]</span></p>
<p>where</p>
<p><span class="math display" id="eq:10-seb1">\[\begin{equation}
se(\hat{\beta}_1) = \sqrt{\frac{MS_{res}}{\sum_{i=1}^n{(x_{i}-\bar{x})^{2}}}} = \frac{s}{\sqrt{\sum_{i=1}^n{(x_{i}-\bar{x})^{2}}}}
\tag{10.17}
\end{equation}\]</span></p>
<p>and</p>
<p><span class="math display" id="eq:10-seb0">\[\begin{equation}
se(\hat{\beta}_0) = \sqrt{MS_{res}\left[\frac{1}{n} + \frac{\bar{x}^2}{\sum_{i=1}^n (x_i -\bar{x})^2}\right]} = s \sqrt{\frac{1}{n} + \frac{\bar{x}^2}{\sum_{i=1}^n (x_i -\bar{x})^2}}
\tag{10.18}
\end{equation}\]</span></p>
<p>Note: The standardized quantities in equations <a href="linear-regression.html#eq:10-distb1">(10.15)</a> and <a href="linear-regression.html#eq:10-distb0">(10.16)</a> follow a <span class="math inline">\(t_{n-2}\)</span> distribution. As mentioned in Section <a href="confidence-intervals.html#df">8.2.4.3</a>, we lose 1 degree of freedom for every equation that must be satisfied. In SLR, we have 2 equations, equations <a href="linear-regression.html#eq:10-b1">(10.9)</a> and <a href="linear-regression.html#eq:10-b0">(10.10)</a>, that must be satisfied when calculating the least squares estimators, therefore we have <span class="math inline">\(n-2\)</span> degrees of freedom.</p>
</div>
<div id="confidence-interval" class="section level3" number="10.4.2">
<h3>
<span class="header-section-number">10.4.2</span> Confidence Interval<a class="anchor" aria-label="anchor" href="#confidence-interval"><i class="fas fa-link"></i></a>
</h3>
<p>From equation <a href="linear-regression.html#eq:10-distb1">(10.15)</a>, we note that the standardized <span class="math inline">\(\hat{\beta}_1\)</span> follows a <span class="math inline">\(t_{n-2}\)</span> distribution, which is symmetric. This implies that the confidence interval for the slope takes on the form expressed in equation <a href="confidence-intervals.html#eq:8-CIbasic2">(8.2)</a>, which is <span class="math inline">\(\text{point estimate } \pm \text{ margin of error}\)</span>. We also know that the margin of error is the critical value multiplied by the standard error of the estimator.</p>
<p><em>Thought question</em>: Before scrolling any further, can you write out the formula for a confidence interval for the slope?</p>
<p>The <span class="math inline">\(100(1-\alpha)\%\)</span> CI for <span class="math inline">\(\beta_1\)</span> is</p>
<p><span class="math display" id="eq:10-CIb1">\[\begin{equation}
\hat{\beta}_1 \pm t_{n-2}^*  se(\hat{\beta}_1) = \hat{\beta}_1 \pm t_{n-2}^* {\sqrt\frac{MS_{res}}{\sum_{i=1}^n(x_i - \bar{x})^{2}}}.
\tag{10.19}
\end{equation}\]</span></p>
<p>The critical value <span class="math inline">\(t_{n-2}^*\)</span> is found in the usual manner; it corresponds to the <span class="math inline">\((1 - \alpha/2) \times 100\)</span>th percentile from a <span class="math inline">\(t_{n-2}\)</span> distribution.</p>
<p>Going back to our study time example, the 95% CI for the slope is (119.470237, 121.3159601).</p>
<div class="sourceCode" id="cb194"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="co">##CI for coefficients</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/stats/confint.html">confint</a></span><span class="op">(</span><span class="va">result</span>,level <span class="op">=</span> <span class="fl">0.95</span><span class="op">)</span><span class="op">[</span><span class="fl">2</span>,<span class="op">]</span></span></code></pre></div>
<pre><code>##    2.5 %   97.5 % 
## 119.4702 121.3160</code></pre>
<p>An interpretation of this CI is that there is 95% probability that the random interval (119.470237, 121.3159601) contains the true value of the slop.</p>
<p>The interval excludes 0, so there is a linear relationship between number of courses and study time.</p>
</div>
<div id="hypothesis-testing-1" class="section level3" number="10.4.3">
<h3>
<span class="header-section-number">10.4.3</span> Hypothesis Testing<a class="anchor" aria-label="anchor" href="#hypothesis-testing-1"><i class="fas fa-link"></i></a>
</h3>
<p>In the context of SLR, we usually want to test if the slope <span class="math inline">\(\beta_1\)</span> is 0 or not. If the slope is 0, there is no linear relationship between the variables. So the hypotheses are typically <span class="math inline">\(H_0: \beta_1 = 0, H_a: \beta_1 \neq 0.\)</span></p>
<p>In equation <a href="linear-regression.html#eq:10-distb1">(10.15)</a>, we noted that the standardized <span class="math inline">\(\hat{\beta}_1\)</span> follows a <span class="math inline">\(t_{n-2}\)</span> distribution, so the test statistic will be a t-statistic which takes on the general form in equation <a href="hypothesis-testing.html#eq:9-tstatGen">(9.2)</a>.</p>
<p><em>Thought question</em>: Before scrolling down any further, can you make an educated guess as to the formula of the t-statistic when testing the slope?</p>
<p>The test statistic is</p>
<p><span class="math display" id="eq:10-testslope">\[\begin{equation}
t = \frac{\hat{\beta}_1 - \text{ value in } H_0}{se(\hat{\beta}_1)}.
\tag{10.20}
\end{equation}\]</span></p>
<p>The p-value and critical value are found in the usual manner, based on a <span class="math inline">\(t_{n-2}\)</span> distribution, and are used in the usual manner.</p>
<p>We go back to the study time example:</p>
<div class="sourceCode" id="cb196"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="fu"><a href="https://rdrr.io/r/base/summary.html">summary</a></span><span class="op">(</span><span class="va">result</span><span class="op">)</span><span class="op">$</span><span class="va">coefficients</span> <span class="co">##print out est coeffs, SEs, t statistics, and p-vals</span></span></code></pre></div>
<pre><code>##              Estimate Std. Error   t value      Pr(&gt;|t|)
## (Intercept)  58.44829  1.9218752  30.41211 4.652442e-189
## courses     120.39310  0.4707614 255.74125  0.000000e+00</code></pre>
<p>From R, the reported t-statistic and p-value for the hypothesis test of the slope are 255.74125 and approximately 0 respectively. So we reject the null hypothesis: our data support the claim that there is a linear relationship between study time and number of courses taken. Note that R also reports these quantities for the hypothesis test for the intercept, although that is a test that is rarely used as it has less practical implications.</p>
<p>Note: The reported values are based on a two-sided alternative hypothesis and a null hypothesis of <span class="math inline">\(H_0: \beta_1 = 0\)</span>. If we have different hypotheses we will have to perform these calculations on our own.</p>
</div>
<div id="correlation" class="section level3" number="10.4.4">
<h3>
<span class="header-section-number">10.4.4</span> Correlation<a class="anchor" aria-label="anchor" href="#correlation"><i class="fas fa-link"></i></a>
</h3>
<p>Another commonly used measure in linear regression is the correlation, which we explored earlier in Section <a href="joint-distributions.html#corr">5.4.2</a>. Visually, variables with a higher correlation (in magnitude) means their observations will fall closer to the fitted line. A correlation of 1 or -1 will mean the observations fall perfectly on the fitted line.</p>
</div>
<div id="coefficient-of-determination" class="section level3" number="10.4.5">
<h3>
<span class="header-section-number">10.4.5</span> Coefficient of Determination<a class="anchor" aria-label="anchor" href="#coefficient-of-determination"><i class="fas fa-link"></i></a>
</h3>
<p>Another measure that is commonly used in linear regression is the <strong>coefficient of determination</strong>, usually denoted by <span class="math inline">\(R^2\)</span>. The value represents the proportion of variation in the response variable that can be explained by our linear regression. In SLR, it is numerically equal to the squared of the sample correlation.</p>
<p>A few notes about <span class="math inline">\(R^2\)</span>:</p>
<ul>
<li>
<span class="math inline">\(0 \leq R^2 \leq 1\)</span>.</li>
<li>Values closer to 1 indicate a better fit; values closer to 0 indicate a poorer fit.</li>
<li>Often reported as a percentage.</li>
</ul>
</div>
<div id="linear-regression-in-r" class="section level3" number="10.4.6">
<h3>
<span class="header-section-number">10.4.6</span> Linear Regression in R<a class="anchor" aria-label="anchor" href="#linear-regression-in-r"><i class="fas fa-link"></i></a>
</h3>
<p>Next, we will perform a simple linear regression on a real data set using R. We will work with the data set <code>elmhurst</code> from the <code>openintro</code> package in R.</p>
<div class="sourceCode" id="cb198"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="kw"><a href="https://rdrr.io/r/base/library.html">library</a></span><span class="op">(</span><span class="va"><a href="https://tidyverse.tidyverse.org">tidyverse</a></span><span class="op">)</span></span>
<span><span class="kw"><a href="https://rdrr.io/r/base/library.html">library</a></span><span class="op">(</span><span class="va"><a href="http://openintrostat.github.io/openintro/">openintro</a></span><span class="op">)</span></span>
<span><span class="va">Data</span><span class="op">&lt;-</span><span class="fu">openintro</span><span class="fu">::</span><span class="va"><a href="https://openintrostat.github.io/openintro/reference/elmhurst.html">elmhurst</a></span></span></code></pre></div>
<p>Type <code><a href="https://openintrostat.github.io/openintro/reference/elmhurst.html">?openintro::elmhurst</a></code> to read the documentation for data sets in R. Always seek to understand the background of your data! The key pieces of information are:</p>
<ul>
<li>A random sample of 50 students (all freshman from the 2011 class at Elmhurst College).</li>
<li>Family income of the student (units are missing).</li>
<li>Gift aid, in $1000s.</li>
</ul>
<p>We want to explore how family income may be related to gift aid, in a simple linear regression framework.</p>
<div id="visualization" class="section level4 unnumbered">
<h4>Visualization<a class="anchor" aria-label="anchor" href="#visualization"><i class="fas fa-link"></i></a>
</h4>
<p>We should always verify with scatter plot that the relationship is (approximately) linear before proceeding with correlation and simple linear regression!</p>
<div class="sourceCode" id="cb199"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="co">##scatterplot of gift aid against family income</span></span>
<span><span class="fu">ggplot2</span><span class="fu">::</span><span class="fu"><a href="https://ggplot2.tidyverse.org/reference/ggplot.html">ggplot</a></span><span class="op">(</span><span class="va">Data</span>, <span class="fu"><a href="https://ggplot2.tidyverse.org/reference/aes.html">aes</a></span><span class="op">(</span>x<span class="op">=</span><span class="va">family_income</span>,y<span class="op">=</span><span class="va">gift_aid</span><span class="op">)</span><span class="op">)</span><span class="op">+</span></span>
<span>  <span class="fu"><a href="https://ggplot2.tidyverse.org/reference/geom_point.html">geom_point</a></span><span class="op">(</span><span class="op">)</span><span class="op">+</span></span>
<span>  <span class="fu"><a href="https://ggplot2.tidyverse.org/reference/geom_smooth.html">geom_smooth</a></span><span class="op">(</span>method <span class="op">=</span> <span class="st">"lm"</span>, se<span class="op">=</span><span class="cn">FALSE</span><span class="op">)</span><span class="op">+</span></span>
<span>  <span class="fu"><a href="https://ggplot2.tidyverse.org/reference/labs.html">labs</a></span><span class="op">(</span>x<span class="op">=</span><span class="st">"Family Income"</span>, y<span class="op">=</span><span class="st">"Gift Aid"</span>, title<span class="op">=</span><span class="st">"Scatterplot of Gift Aid against Family"</span><span class="op">)</span></span></code></pre></div>
<pre><code>## `geom_smooth()` using formula = 'y ~ x'</code></pre>
<div class="figure">
<span style="display:block;" id="fig:10-scatter-eg"></span>
<img src="bookdown-demo_files/figure-html/10-scatter-eg-1.png" alt="Scatter Plot for Worked Example" width="672"><p class="caption">
Figure 10.8: Scatter Plot for Worked Example
</p>
</div>
<p>We note that the observations are fairly evenly scattered on both sides of the regression line, so a linear association exists. We see a negative linear association. As family income increases, the gift aid, on average, decreases.</p>
<p>We also do not see any observation with weird values that may warrant further investigation.</p>
</div>
<div id="regression" class="section level4 unnumbered">
<h4>Regression<a class="anchor" aria-label="anchor" href="#regression"><i class="fas fa-link"></i></a>
</h4>
<p>We use the <code><a href="https://rdrr.io/r/stats/lm.html">lm()</a></code> function to fit a regression model. We supply the response variable, then the predictor, with a <code>~</code> in between, then specify the dataframe via the <code>data</code> argument.</p>
<div class="sourceCode" id="cb201"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="co">##regress gift aid against family income</span></span>
<span><span class="va">result</span><span class="op">&lt;-</span><span class="fu"><a href="https://rdrr.io/r/stats/lm.html">lm</a></span><span class="op">(</span><span class="va">gift_aid</span><span class="op">~</span><span class="va">family_income</span>, data<span class="op">=</span><span class="va">Data</span><span class="op">)</span></span></code></pre></div>
<p>Use the <code><a href="https://rdrr.io/r/base/summary.html">summary()</a></code> function to display relevant information from this regression:</p>
<div class="sourceCode" id="cb202"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="co">##look at information regarding regression</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/base/summary.html">summary</a></span><span class="op">(</span><span class="va">result</span><span class="op">)</span></span></code></pre></div>
<pre><code>## 
## Call:
## lm(formula = gift_aid ~ family_income, data = Data)
## 
## Residuals:
##      Min       1Q   Median       3Q      Max 
## -10.1128  -3.6234  -0.2161   3.1587  11.5707 
## 
## Coefficients:
##               Estimate Std. Error t value Pr(&gt;|t|)    
## (Intercept)   24.31933    1.29145  18.831  &lt; 2e-16 ***
## family_income -0.04307    0.01081  -3.985 0.000229 ***
## ---
## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1
## 
## Residual standard error: 4.783 on 48 degrees of freedom
## Multiple R-squared:  0.2486, Adjusted R-squared:  0.2329 
## F-statistic: 15.88 on 1 and 48 DF,  p-value: 0.0002289</code></pre>
<p>We see the following values:</p>
<ul>
<li>
<span class="math inline">\(\hat{\beta}_1 =\)</span> -0.0430717. The estimated slope informs us the the predicted gift aid decreases by 0.0430717 thousands of dollars (or $43.07) per unit increase in family income.</li>
<li>
<span class="math inline">\(\hat{\beta}_0 =\)</span> 24.319329. For students with no family income, their predicted gift aid is $24 319.33. Note: from the scatter plot, we have an observation with 0 family income. We must be careful in not extrapolating when making predictions with our regression. We should only make predictions for family incomes between the minimum and maximum values of family incomes in our data.</li>
<li>
<span class="math inline">\(s\)</span> = 4.7825989, is the estimate of the standard deviation of the error terms, <span class="math inline">\(\sigma\)</span>. This is reported as residual standard error in R. Squaring this gives the estimated variance of the error terms.</li>
<li>
<span class="math inline">\(R^2 =\)</span> 0.2485582. The coefficient of determination informs us that about 24.86% of the variation in gift aid can be explained by family income. This is reported as multiple R-squared in R.</li>
</ul>
</div>
<div id="hypothesis-testing-2" class="section level4 unnumbered">
<h4>Hypothesis Testing<a class="anchor" aria-label="anchor" href="#hypothesis-testing-2"><i class="fas fa-link"></i></a>
</h4>
<p>Under coefficients, we can see the results of the hypothesis tests for <span class="math inline">\(\beta_1\)</span> and <span class="math inline">\(\beta_0\)</span>. Specifically, for <span class="math inline">\(\beta_1\)</span>:</p>
<ul>
<li>
<span class="math inline">\(\hat{\beta}_1\)</span> = -0.0430717</li>
<li>
<span class="math inline">\(se(\hat{\beta}_1)\)</span> = 0.0108095</li>
<li>the test statistic is <span class="math inline">\(t\)</span> = -3.984621</li>
<li>the corresponding p-value is 2.2887345^{-4}</li>
</ul>
<p>You can work out the p-value using R (slight difference due to rounding):</p>
<div class="sourceCode" id="cb204"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="co">##pvalue</span></span>
<span><span class="fl">2</span><span class="op">*</span><span class="fu"><a href="https://rdrr.io/r/stats/TDist.html">pt</a></span><span class="op">(</span><span class="op">-</span><span class="fu"><a href="https://rdrr.io/r/base/MathFun.html">abs</a></span><span class="op">(</span><span class="op">-</span><span class="fl">3.985</span><span class="op">)</span>, df <span class="op">=</span> <span class="fl">50</span><span class="op">-</span><span class="fl">2</span><span class="op">)</span></span></code></pre></div>
<pre><code>## [1] 0.0002285996</code></pre>
<p>Or find the critical value using R:</p>
<div class="sourceCode" id="cb206"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="co">##critical value</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/stats/TDist.html">qt</a></span><span class="op">(</span><span class="fl">1</span><span class="op">-</span><span class="fl">0.05</span><span class="op">/</span><span class="fl">2</span>, df <span class="op">=</span> <span class="fl">50</span><span class="op">-</span><span class="fl">2</span><span class="op">)</span></span></code></pre></div>
<pre><code>## [1] 2.010635</code></pre>
<p>Either way, we end up rejecting the null hypothesis. The data support the claim that there is a linear association between gift aid and family income.</p>
<p>Remember the <span class="math inline">\(t\)</span> tests for the regression coefficients are based on <span class="math inline">\(H_0: \beta_j = 0, H_a: \beta_j \neq 0\)</span>. The reported p-value is based on this set of null and alternative hypotheses. If your null and alternative hypotheses are different, you will need to compute your own test statistic and p-value.</p>
</div>
<div id="confidence-intervals-1" class="section level4 unnumbered">
<h4>Confidence Intervals<a class="anchor" aria-label="anchor" href="#confidence-intervals-1"><i class="fas fa-link"></i></a>
</h4>
<p>To find the 95% confidence intervals for the coefficients, we use the <code><a href="https://rdrr.io/r/stats/confint.html">confint()</a></code> function:</p>
<div class="sourceCode" id="cb208"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="co">##to produce 95% CIs for all regression coefficients</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/stats/confint.html">confint</a></span><span class="op">(</span><span class="va">result</span>,level <span class="op">=</span> <span class="fl">0.95</span><span class="op">)</span></span></code></pre></div>
<pre><code>##                     2.5 %      97.5 %
## (Intercept)   21.72269421 26.91596380
## family_income -0.06480555 -0.02133775</code></pre>
<p>The 95% CI for <span class="math inline">\(\beta_1\)</span> is (-0.0648056, -0.0213378). We have 95% confidence that for each additional thousand dollars in family income, the predicted gift aid decreases between $21.3378 and $64.8056.</p>
</div>
<div id="extract-values-from-r-objects" class="section level4 unnumbered">
<h4>Extract Values from R Objects<a class="anchor" aria-label="anchor" href="#extract-values-from-r-objects"><i class="fas fa-link"></i></a>
</h4>
<p>We can actually extract these values that are being reported from <code>summary(result)</code>. To see what can be extracted from an R object, use the <code><a href="https://rdrr.io/r/base/names.html">names()</a></code> function:</p>
<div class="sourceCode" id="cb210"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="co">##see what can be extracted from summary(result)</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/base/names.html">names</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/base/summary.html">summary</a></span><span class="op">(</span><span class="va">result</span><span class="op">)</span><span class="op">)</span></span></code></pre></div>
<pre><code>##  [1] "call"          "terms"         "residuals"     "coefficients" 
##  [5] "aliased"       "sigma"         "df"            "r.squared"    
##  [9] "adj.r.squared" "fstatistic"    "cov.unscaled"</code></pre>
<p>To extract the estimated coefficients:</p>
<div class="sourceCode" id="cb212"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="co">##extract coefficients</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/base/summary.html">summary</a></span><span class="op">(</span><span class="va">result</span><span class="op">)</span><span class="op">$</span><span class="va">coefficients</span></span></code></pre></div>
<pre><code>##                  Estimate Std. Error   t value     Pr(&gt;|t|)
## (Intercept)   24.31932901 1.29145027 18.831022 8.281020e-24
## family_income -0.04307165 0.01080947 -3.984621 2.288734e-04</code></pre>
<p>Notice the information is presented in a table. To extract a specific value, we can specify the row and column indices:</p>
<div class="sourceCode" id="cb214"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="co">##extract slope</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/base/summary.html">summary</a></span><span class="op">(</span><span class="va">result</span><span class="op">)</span><span class="op">$</span><span class="va">coefficients</span><span class="op">[</span><span class="fl">2</span>,<span class="fl">1</span><span class="op">]</span></span></code></pre></div>
<pre><code>## [1] -0.04307165</code></pre>
<div class="sourceCode" id="cb216"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="co">##extract intercept</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/base/summary.html">summary</a></span><span class="op">(</span><span class="va">result</span><span class="op">)</span><span class="op">$</span><span class="va">coefficients</span><span class="op">[</span><span class="fl">1</span>,<span class="fl">1</span><span class="op">]</span></span></code></pre></div>
<pre><code>## [1] 24.31933</code></pre>
<p>On your own, extract the values of the residual standard error and <span class="math inline">\(R^2\)</span>.</p>
</div>
<div id="prediction" class="section level4 unnumbered">
<h4>Prediction<a class="anchor" aria-label="anchor" href="#prediction"><i class="fas fa-link"></i></a>
</h4>
<p>A use of regression models is prediction. Suppose I want to predict the gift aid of a student with family income of 50 thousand dollars (assuming the unit is in thousands of dollars). We use the <code><a href="https://rdrr.io/r/stats/predict.html">predict()</a></code> function:</p>
<div class="sourceCode" id="cb218"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="co">##create data point for prediction</span></span>
<span><span class="va">newdata</span><span class="op">&lt;-</span><span class="fu"><a href="https://rdrr.io/r/base/data.frame.html">data.frame</a></span><span class="op">(</span>family_income<span class="op">=</span><span class="fl">50</span><span class="op">)</span></span>
<span><span class="co">##predicted gift aid when x=50</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/stats/predict.html">predict</a></span><span class="op">(</span><span class="va">result</span>,<span class="va">newdata</span><span class="op">)</span></span></code></pre></div>
<pre><code>##        1 
## 22.16575</code></pre>
<p>This student’s predicted gift aid is $22 165.75. Alternatively, you could have calculated this by plugging <span class="math inline">\(x=50\)</span> into the estimated SLR equation:</p>
<div class="sourceCode" id="cb220"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="fu"><a href="https://rdrr.io/r/base/summary.html">summary</a></span><span class="op">(</span><span class="va">result</span><span class="op">)</span><span class="op">$</span><span class="va">coefficients</span><span class="op">[</span><span class="fl">1</span>,<span class="fl">1</span><span class="op">]</span> <span class="op">+</span> <span class="fu"><a href="https://rdrr.io/r/base/summary.html">summary</a></span><span class="op">(</span><span class="va">result</span><span class="op">)</span><span class="op">$</span><span class="va">coefficients</span><span class="op">[</span><span class="fl">2</span>,<span class="fl">1</span><span class="op">]</span><span class="op">*</span><span class="fl">50</span></span></code></pre></div>
<pre><code>## [1] 22.16575</code></pre>
</div>
<div id="correlation-1" class="section level4 unnumbered">
<h4>Correlation<a class="anchor" aria-label="anchor" href="#correlation-1"><i class="fas fa-link"></i></a>
</h4>
<p>We use the <code><a href="https://rdrr.io/r/stats/cor.html">cor()</a></code> function to find the correlation between two quantitative variables:</p>
<div class="sourceCode" id="cb222"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="co">##correlation</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/stats/cor.html">cor</a></span><span class="op">(</span><span class="va">Data</span><span class="op">$</span><span class="va">family_income</span>,<span class="va">Data</span><span class="op">$</span><span class="va">gift_aid</span><span class="op">)</span></span></code></pre></div>
<pre><code>## [1] -0.4985561</code></pre>
<p>The correlation is -0.4985561. We have a moderate, negative linear association between family income and gift aid.</p>
<p>View the video below if you need a little more explanation on using R for linear regression:</p>
<iframe width="560px" height="320px" allowfullscreen="true" allow="autoplay *" title="Module 10: Linear Regression in R" src="https://virginiauniversity.instructuremedia.com/embed/e6ab985e-7b0f-4fe3-8eaa-90f8611b7f70" frameborder="0">
</iframe>
</div>
</div>
</div>
<div id="SLRacc" class="section level2" number="10.5">
<h2>
<span class="header-section-number">10.5</span> Assessing Model Accuracy<a class="anchor" aria-label="anchor" href="#SLRacc"><i class="fas fa-link"></i></a>
</h2>
<p>In the introduction of this module, we wrote that supervised methods such as linear regression have two primary uses:</p>
<ol style="list-style-type: decimal">
<li>Association: Quantify the relationship between variables. How does a change in the predictor variable change the value of the response variable?</li>
<li>Prediction: Predict a future value of a response variable, using information from predictor variables.</li>
</ol>
<p>In Section <a href="linear-regression.html#SLRinf">10.4</a>, we focused on the sampling distribution of the least squares estimators which was then used to construct confidence intervals and perform hypothesis tests. These tools are useful in trying to explain the relationship between the response variable and predictor, which is the first primary use of supervised methods. This has been the focus of a lot of traditional research in a wide variety of applications, for example:</p>
<ul>
<li>Medicine: Does increasing the dose of a medication reduce severity of symptoms?</li>
<li>Advertising: How does advertising in various media affect sales?</li>
<li>Environmental science: How does increased greenhouse emissions affect air quality?</li>
<li>Education: Does the use of AI tools aid in student learning outcomes?</li>
</ul>
<p>In this section, we will focus on the second primary use of supervised methods: prediction. When we focus on prediction, we mainly care about how close are the predicted (or fitted) values with their true values. A lot of what was mentioned in Section <a href="linear-regression.html#SLRinf">10.4</a> can actually be ignored (other than assumption 1 of the regression assumptions, since predictions will definitely suffer if this is not met) when our focus is on prediction. Prediction is in the domain of predictive analytics. While predictive analytics can be used in a wide variety of applications, the kind of questions they answer differ from the earlier questions, for example:</p>
<ul>
<li>Hospitality: Determining staffing needs for a casino. We want to be able to predict the right number of staff to hire: over-staffing costs money and under-staffing results in unhappy customers.</li>
<li>Online ads: Based on a user’s history on Amazon, Amazon would like to be able to recommend products that the user is highly likely to purchase.</li>
<li>Personalized medicine: Based on a patient’s history, what is the most effective treatment plan?</li>
<li>Dynamic pricing: airlines use dynamic pricing that adjusts price to maximize profits, based on real-time market conditions.</li>
</ul>
<p>In these questions, we want to know if our model can predict the value of the response variable well; we do not necessarily want to know how the predictors may explain the response variable.</p>
<div id="metrics-for-model-accuracy" class="section level3" number="10.5.1">
<h3>
<span class="header-section-number">10.5.1</span> Metrics for Model Accuracy<a class="anchor" aria-label="anchor" href="#metrics-for-model-accuracy"><i class="fas fa-link"></i></a>
</h3>
<p>We will introduce some common metrics that measure model accuracy. As we are talking about these in the framework of linear regression where the response variable is quantitative, these metrics only apply to supervised methods when the response variable is quantitative. There are separate metrics when the response variable is categorical which you will learn about next semester.</p>
<div id="mean-squared-error" class="section level4" number="10.5.1.1">
<h4>
<span class="header-section-number">10.5.1.1</span> Mean-Squared Error<a class="anchor" aria-label="anchor" href="#mean-squared-error"><i class="fas fa-link"></i></a>
</h4>
<p>The <strong>mean-squared error (MSE)</strong> is a commonly used metric to assess model accuracy. The MSE of predictions is</p>
<p><span class="math display" id="eq:10-MSE">\[\begin{equation}
MSE(\hat{y}) = \frac{\sum_{i=1}^n (y_i - \hat{y}_i)^2}{n}.
\tag{10.21}
\end{equation}\]</span></p>
<p>The MSE can be interpreted as an average squared prediction error. If you compare equation <a href="linear-regression.html#eq:10-MSE">(10.21)</a> with equation <a href="linear-regression.html#eq:10-SSres">(10.8)</a>, you may notice the numerator of the MSE is the <span class="math inline">\(SS_{res}\)</span>.</p>
<p>You may also recall that the MSE was introduced in Section <a href="est.html#estMSE">7.4.5</a>. In that section, we were measuring the average squared difference between an estimator and its parameter, rather than the average squared difference between a predicted value and its true value. From a terminology standpoint, they are both MSEs: one being the MSE of an estimator, the other being the MSE of predictions, so you may need to clarify which MSE is being discussed if someone just uses the term MSE.</p>
<p>It should then not come as a surprise that the MSE in equation <a href="linear-regression.html#eq:10-MSE">(10.21)</a> can be decomposed like the MSE of an estimator in equation <a href="est.html#eq:7-MSEdecomp">(7.5)</a>:</p>
<p><span class="math display" id="eq:10-MSEdecomp">\[\begin{equation}
MSE(\hat{y}) = Var(\hat{y}) + Bias(\hat{y})^2.
\tag{10.22}
\end{equation}\]</span></p>
<p>So the MSE can be decomposed into the variance of the predicted values and the squared bias of the predicted values. In a linear regression setting, the bias will be theoretically 0 if we find the correct form of <span class="math inline">\(f(x)\)</span> to relate the response variable with the predictors.</p>
</div>
<div id="root-mean-squared-error" class="section level4" number="10.5.1.2">
<h4>
<span class="header-section-number">10.5.1.2</span> Root Mean-Squared Error<a class="anchor" aria-label="anchor" href="#root-mean-squared-error"><i class="fas fa-link"></i></a>
</h4>
<p>The MSE is a popular metric for model accuracy because of its decomposition per equation <a href="linear-regression.html#eq:10-MSEdecomp">(10.22)</a>. However, interpreting its numerical value can be challenging, as the unit associated with the MSE is the squared of the unit of the response variable, which poses some difficulty in making a judgement as to whether a reported value for the MSE is actually large or small.</p>
<p>The <strong>root mean-squared error (RMSE)</strong> has been suggested, since it has the same unit as the response variable, making it easy to make a judgement as to whether a reported value is large or small:</p>
<p><span class="math display" id="eq:10-RMSE">\[\begin{equation}
RMSE(\hat{y}) = \sqrt{\frac{\sum_{i=1}^n (y_i - \hat{y}_i)^2}{n}} = \sqrt{MSE(\hat{y})}.
\tag{10.23}
\end{equation}\]</span></p>
<p>The RMSE is simply the square root of the MSE. Interpreting the RMSE gets tricky; it is tempting to interpret as the average prediction error, but it is not quite correct to do so, since <span class="math inline">\(\sqrt{\frac{\sum_{i=1}^n (y_i - \hat{y}_i)^2}{n}}\)</span> is not the same as <span class="math inline">\(\frac{\sum_{i=1}^n \sqrt{(y_i - \hat{y}_i)^2}}{n}\)</span>.</p>
</div>
<div id="mean-absolute-deviation" class="section level4" number="10.5.1.3">
<h4>
<span class="header-section-number">10.5.1.3</span> Mean Absolute Deviation<a class="anchor" aria-label="anchor" href="#mean-absolute-deviation"><i class="fas fa-link"></i></a>
</h4>
<p>In response to the difficulty in interpreting RMSE, another metric, called the <strong>mean absolute deviation (MAD)</strong> has been proposed:</p>
<p><span class="math display" id="eq:10-MAD">\[\begin{equation}
MAD(\hat{y}) = \frac{\sum_{i=1}^n |y_i - \hat{y}_i|}{n}.
\tag{10.24}
\end{equation}\]</span></p>
<p>The unit of MAD is the same as the response variable, and it can be interpreted as the average prediction error.</p>
</div>
<div id="additional-comments-about-mse-rmse-mad" class="section level4" number="10.5.1.4">
<h4>
<span class="header-section-number">10.5.1.4</span> Additional Comments about MSE, RMSE, MAD<a class="anchor" aria-label="anchor" href="#additional-comments-about-mse-rmse-mad"><i class="fas fa-link"></i></a>
</h4>
<p>In the rest of this module, we will be focusing on the MSE and RMSE, as certain properties for them have been proven mathematically. We will discuss these properties next.</p>
</div>
</div>
<div id="model-complexity" class="section level3" number="10.5.2">
<h3>
<span class="header-section-number">10.5.2</span> Model Complexity<a class="anchor" aria-label="anchor" href="#model-complexity"><i class="fas fa-link"></i></a>
</h3>
<p>Thus far, we have only considered simple linear regression, i.e. only one predictor in our model. This approach is useful if we want to focus on how a single predictor is related to the response variable. However, it should not be surprising to learn that in many settings, we can improve predictions of the response variable by either</p>
<ul>
<li>incorporating additional predictors (that are useful), or</li>
<li>considering polynomials for the predictor (if the relationship between the response variable and predictor is not linear), or</li>
<li>adding interactions between predictors, or</li>
<li>using any combination of these steps.</li>
</ul>
<p>With additional predictors, the linear regression model is written as</p>
<p><span class="math display" id="eq:10-MLRmod">\[\begin{equation}
y=\beta_0+\beta_{1}x_1 + \beta_2 x_2 + \cdots + \beta_k x_k + \epsilon,
\tag{10.25}
\end{equation}\]</span></p>
<p>if we incorporate <span class="math inline">\(k\)</span> quantitative predictors <span class="math inline">\(x_1, \cdots, x_k\)</span>.</p>
<p>If the relationship between the response variable and predictor is not linear, the linear regression model can be written as</p>
<p><span class="math display" id="eq:10-polymod">\[\begin{equation}
y=\beta_0+\beta_{1}x + \beta_2 x^2 + \cdots + \beta_k x^k + \epsilon,
\tag{10.26}
\end{equation}\]</span></p>
<p>if we consider a <span class="math inline">\(k\)</span>-degree polynomial for <span class="math inline">\(f(x)\)</span> to approximate the relationship between the response variable and the predictor.</p>
<p><strong>Interactions between predictors</strong> occur when the effect of one predictor on the response variable depends on the value of another predictor. Consider predicting recovery rates of patients, based on amount of medication administered and age of the patient. Perhaps higher doses of the medication speeds recovery, but only for young patients. However, the medication may not work at all for old patients. The effect of medication on recovery depends on the age of a patient, so there is an interaction between amount of medicine administered and age of patient. In this scenario, the linear regression model can be written as</p>
<p><span class="math display" id="eq:10-intmod">\[\begin{equation}
y=\beta_0+\beta_{1}x_1 + \beta_2 x_2 + \beta_3 x_1 x_2 + \epsilon,
\tag{10.27}
\end{equation}\]</span></p>
<p>where the term <span class="math inline">\(x_1 x_2\)</span> represents that we are considering an interaction between the predictors.</p>
<p>It is also possible for a linear regression model to have multiple predictors, higher order polynomials, and interactions. For example, <span class="math inline">\(y=\beta_0+\beta_{1}x_1 + \beta_2 x_2 + \beta_3 x_3 + \beta_4 x_3^2 + \beta_5 z_1 x_2 + \epsilon\)</span>.</p>
<p>For this class, we will only consider comparing the complexity of models that are <strong>nested</strong>. Two models are nested if one model can be derived from the other by setting some parameters of the more complex model to be equal to 0. As an example, the SLR model in equation <a href="linear-regression.html#eq:10-SLRmod">(10.1)</a> is nested within the model in equation <a href="linear-regression.html#eq:10-MLRmod">(10.25)</a>, as we can get the SLR model by setting <span class="math inline">\(\beta_2 = \beta_3 = \cdots = \beta_k = 0\)</span> in equation <a href="linear-regression.html#eq:10-MLRmod">(10.25)</a>.</p>
<p><em>Thought question</em>: Can you explain why the SLR model in equation <a href="linear-regression.html#eq:10-SLRmod">(10.1)</a> is nested within the model in equation <a href="linear-regression.html#eq:10-polymod">(10.26)</a>? Can you explain why the SLR model in equation <a href="linear-regression.html#eq:10-SLRmod">(10.1)</a> is nested within the model in equation <a href="linear-regression.html#eq:10-intmod">(10.27)</a>?</p>
<p>The <strong>complexity</strong> of linear regression models can be measured by the number of parameters in the model, which we denote by <span class="math inline">\(p\)</span>. So for the SLR model in equation <a href="linear-regression.html#eq:10-SLRmod">(10.1)</a>, <span class="math inline">\(p=2\)</span>, since we have <span class="math inline">\(\beta_0\)</span> and <span class="math inline">\(\beta_1\)</span>.</p>
<p>Note: Some consider models that are more complex to be more <strong>flexible</strong>, so fairly often what I call model complexity is sometimes called model flexibility.</p>
<p>So there are many potential models to consider. Metrics involving model accuracy can help us decide which model is the best in terms of prediction accuracy, by selecting the model that has the smallest MSE (or RMSE).</p>
</div>
<div id="training-test-split" class="section level3" number="10.5.3">
<h3>
<span class="header-section-number">10.5.3</span> Training-Test Split<a class="anchor" aria-label="anchor" href="#training-test-split"><i class="fas fa-link"></i></a>
</h3>
<p>After reading the previous subsections, one may consider the following strategy when assessing model accuracy and choosing between several nested models: fit all models under consideration using all available data, then choose the model that the smallest MSE (or RMSE).</p>
<p>While seemingly intuitive, this stragedy will not work. A property of the MSE and RMSE of nested models is that these values only decrease as the model gets more complex! We will not go through the whole proof of this, but intuitively, one can see why the more complex model will never do worse than the less complex model: this happens when the additional parameters are all 0. This property suggests that using this strategy will always end up selecting the most complex model, as the most complex model will always have the smallest MSE and RMSE.</p>
<p>Take a step back and remind ourselves what we actually want to do with prediction: after fitting the model, we want to be able to accurately predict the values of the response variable in the future. When we fit a model well based on the current data, the model may do poorly on new data. This is called <strong>overfitting</strong>.</p>
<p>So what we will do is to split a given data set into two portions. One portion, called the <strong>training data</strong>, is used to fit (or train) the model. The other portion, called the <strong>test data</strong>, is used to assess the prediction accuracy of the model. The test data are never used to fit the model, so the test data play the role of new data.</p>
<p>The MSE (or RMSE) is then calculated on both portions, and are called the training MSE and the test MSE. The relationship between the training and test MSEs with the complexity of nested models is shown below in Figure <a href="linear-regression.html#fig:10-MSE">10.9</a>:</p>
<div class="figure">
<span style="display:block;" id="fig:10-MSE"></span>
<img src="images/10-MSE.png" alt="Training &amp; Test MSE vs Complexity"><p class="caption">
Figure 10.9: Training &amp; Test MSE vs Complexity
</p>
</div>
<p>We make the following comments about Figure <a href="linear-regression.html#fig:10-MSE">10.9</a>:</p>
<ul>
<li>The blue curve represents how the training MSE behaves as models get more complex. As models get more complex, the training MSE never increases.</li>
<li>The black curve represents how the test MSE behaves as models get more complex. The test MSE is minimum when we find an appropriate level of complexity, and this is denoted by the red line. We call this model the optimal model. The test MSE increases as models get more complex, or less complex, than the optimal model.</li>
<li>Overfitted models are models that are more complex than the optimal model.
<ul>
<li>Overfitted models generally have low training MSE, and high test MSE. A large difference between these two is also indicative of overfitting.</li>
</ul>
</li>
<li>Underfitted models are models that are less complex than the optimal model.
<ul>
<li>Underfitted models generally have high training MSE and high test MSE.</li>
</ul>
</li>
<li>On average, the training MSE is lower than the test MSE for any model. This is because the model is usually estimated by minimizing some quantity that is related to the training MSE. For example, we minimize <span class="math inline">\(SS_{res}\)</span>, which is the numerator of the training MSE, for linear regression.</li>
</ul>
</div>
<div id="bias-variance-trade-off-model-complexity" class="section level3" number="10.5.4">
<h3>
<span class="header-section-number">10.5.4</span> Bias-Variance Trade-Off &amp; Model Complexity<a class="anchor" aria-label="anchor" href="#bias-variance-trade-off-model-complexity"><i class="fas fa-link"></i></a>
</h3>
<p>In equation <a href="linear-regression.html#eq:10-MSEdecomp">(10.22)</a>, we see that the MSE can be decomposed into the variance of the predicted values and the squared bias of the predicted values. In Figure <a href="linear-regression.html#fig:10-MSE">10.9</a>, we see that the MSE is related to the complexity of the model. So it should not be a surprise to see that the complexity of models is related to the bias and variance of the predictions from those models. This relationship is called the <strong>bias-variance tradeoff</strong>. This tradeoff tells us that if a model produces predictions that are more biased, the predictions will have smaller variance, and vice versa. We summarize the relationship between model complexity and bias and variance of their predictions:</p>
<ul>
<li><p>Bias increase as the model gets less complex. Models are too simple (or not complex enough) if we do not include important predictors, or misspecify the relationship between the response and the predictors, or fail to include relevant interactions.</p></li>
<li><p>Variance increase as the model gets more complex. Models are too complex if we include unnecessary predictors, include higher order polynomials that not needed, or include interactions when they are not needed.</p></li>
</ul>
<p>So we can see that to minimize the test MSE, we need to find the right balance between bias and variance of the model, by finding the right level of complexity.</p>
</div>
<div id="monte-carlo-simulations" class="section level3" number="10.5.5">
<h3>
<span class="header-section-number">10.5.5</span> Monte Carlo Simulations<a class="anchor" aria-label="anchor" href="#monte-carlo-simulations"><i class="fas fa-link"></i></a>
</h3>
<p>Next, we use Monte Carlo simulations to illustrate how MSE, bias, variance, and model complexity are related. In our simulation, we are going to simulate data from this model <span class="math inline">\(y = x^2 + \epsilon\)</span>, where <span class="math inline">\(\epsilon\)</span> is i.i.d. <span class="math inline">\(N(0,1)\)</span>. The true relationship between the response variable and the predictor is <span class="math inline">\(f(x) = x^2\)</span>. We write a function that simulates data from this model:</p>
<div class="sourceCode" id="cb224"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="co">##function to simulate y = x^2 + eps</span></span>
<span><span class="va">get.sim.data</span> <span class="op">&lt;-</span> <span class="kw">function</span><span class="op">(</span><span class="va">samp.size</span>, <span class="va">sigma</span><span class="op">)</span></span>
<span>  </span>
<span><span class="op">{</span></span>
<span>  </span>
<span>  <span class="va">x</span><span class="op">&lt;-</span><span class="fu"><a href="https://rdrr.io/r/stats/Uniform.html">runif</a></span><span class="op">(</span>n<span class="op">=</span><span class="va">samp.size</span><span class="op">)</span> <span class="co">##note that there are no assumptions made about x so can simulate from any distribution</span></span>
<span>  <span class="va">eps</span><span class="op">&lt;-</span><span class="fu"><a href="https://rdrr.io/r/stats/Normal.html">rnorm</a></span><span class="op">(</span>n<span class="op">=</span><span class="va">samp.size</span>, sd<span class="op">=</span><span class="va">sigma</span><span class="op">)</span></span>
<span>  <span class="va">y</span><span class="op">&lt;-</span><span class="va">x</span><span class="op">^</span><span class="fl">2</span> <span class="op">+</span> <span class="va">eps</span> <span class="co">##since f(x) = 2x^2</span></span>
<span>  </span>
<span>  <span class="fu"><a href="https://rdrr.io/r/base/data.frame.html">data.frame</a></span><span class="op">(</span><span class="va">x</span>,<span class="va">y</span><span class="op">)</span> <span class="co">##return a dataframe with x and y</span></span>
<span><span class="op">}</span></span></code></pre></div>
<p>Note that in a linear regression setting, there are no assumptions made about the predictor, so we are free to simulate the predictor from any distribution. We need to be sure to specify <span class="math inline">\(y = f(x) + \epsilon\)</span> where the errors are i.i.d. Normal with mean 0 and some fixed value for its variance, which we set to be 1.</p>
<p>We consider fitting a few different fitted lines for our simulated data:</p>
<ol style="list-style-type: decimal">
<li>Fitted line 1: <span class="math inline">\(\hat{y} = \hat{\beta}_0,\)</span> so <span class="math inline">\(\hat{f}(x) = \hat{\beta}_0\)</span>.</li>
<li>Fitted line 2: <span class="math inline">\(\hat{y} = \hat{\beta}_0 + \hat{\beta}_1 x,\)</span> so <span class="math inline">\(\hat{f}(x) = \hat{\beta}_0 + \hat{\beta}_1 x\)</span>.</li>
<li>Fitted line 3: <span class="math inline">\(\hat{y} = \hat{\beta}_0 + \hat{\beta}_1 x + \hat{\beta}_2 x^2,\)</span> so <span class="math inline">\(\hat{f}(x) = \hat{\beta}_0 + \hat{\beta}_1 x + \hat{\beta}_2 x^2\)</span>.</li>
<li>Fitted line 4: <span class="math inline">\(\hat{y} = \hat{\beta}_0 + \hat{\beta}_1 x + \cdots + \hat{\beta}_9 x^9,\)</span> so <span class="math inline">\(\hat{f}(x) = \hat{\beta}_0 + \hat{\beta}_1 x + \cdots + \hat{\beta}_9 x^9\)</span>.</li>
</ol>
<p>Because the true relationship is <span class="math inline">\(y = f(x) = x^2\)</span>, fitted lines 1 and 2 are not complex enough, fitted line 3 should be optimal, and fitted line 4 is too complex.</p>
<p>We simulate 1 replicate of 1000 observations, and then fit the four models.</p>
<div class="sourceCode" id="cb225"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="fu"><a href="https://rdrr.io/r/base/Random.html">set.seed</a></span><span class="op">(</span><span class="fl">11</span><span class="op">)</span></span>
<span></span>
<span><span class="va">n</span> <span class="op">&lt;-</span> <span class="fl">1000</span></span>
<span></span>
<span><span class="va">Data.sim</span><span class="op">&lt;-</span><span class="fu">get.sim.data</span><span class="op">(</span><span class="va">n</span>,<span class="fl">1</span><span class="op">)</span></span>
<span></span>
<span><span class="va">fit1</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/stats/lm.html">lm</a></span><span class="op">(</span><span class="va">y</span> <span class="op">~</span> <span class="fl">1</span>,                   data <span class="op">=</span> <span class="va">Data.sim</span><span class="op">)</span></span>
<span><span class="va">fit2</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/stats/lm.html">lm</a></span><span class="op">(</span><span class="va">y</span> <span class="op">~</span> <span class="va">x</span>,                   data <span class="op">=</span> <span class="va">Data.sim</span><span class="op">)</span></span>
<span><span class="va">fit3</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/stats/lm.html">lm</a></span><span class="op">(</span><span class="va">y</span> <span class="op">~</span> <span class="fu"><a href="https://rdrr.io/r/stats/poly.html">poly</a></span><span class="op">(</span><span class="va">x</span>, degree <span class="op">=</span> <span class="fl">2</span><span class="op">)</span>, data <span class="op">=</span> <span class="va">Data.sim</span><span class="op">)</span></span>
<span><span class="va">fit4</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/stats/lm.html">lm</a></span><span class="op">(</span><span class="va">y</span> <span class="op">~</span> <span class="fu"><a href="https://rdrr.io/r/stats/poly.html">poly</a></span><span class="op">(</span><span class="va">x</span>, degree <span class="op">=</span> <span class="fl">9</span><span class="op">)</span>, data <span class="op">=</span> <span class="va">Data.sim</span><span class="op">)</span></span></code></pre></div>
<p>In Figure <a href="linear-regression.html#fig:10-sims1">10.10</a>, we create a scatter plot of this simulated data, and overlay lines to represent the four fitted lines (in blue, red, green, orange). We also overlay a curve to represent the true relationship between the variables, in black.</p>
<div class="figure">
<span style="display:block;" id="fig:10-sims1"></span>
<img src="bookdown-demo_files/figure-html/10-sims1-1.png" alt="Four Polynomial Models Fitted to a Simulated Dataset" width="672"><p class="caption">
Figure 10.10: Four Polynomial Models Fitted to a Simulated Dataset
</p>
</div>
<p>From Figure <a href="linear-regression.html#fig:10-sims1">10.10</a>, we make the following comments:</p>
<ul>
<li>Fitted line 1, in blue, generally deviates far from the truth (in black).</li>
<li>Fitted line 2, in red, is closer to the truth, but there are portions where it does not match the truth.</li>
<li>Fitted line 3, in green, appears to be best in that it matches the closest to the truth, generally.</li>
<li>Fitted line 4, in orange, does not quite match the truth, especially at the extreme ends of the x-axis.</li>
</ul>
<p>Of course, these comments are based on 1 simulated data set, but if we were to create many replicates of the data set and create a similar scatter plot, we would expect to see similar characteristics.</p>
<p>To illustrate the idea behind the bias and variance of predictions, we simulate 3 replicates data sets, each of size 1000, and show fitted lines 1 and 4 on each replicate in Figure <a href="linear-regression.html#fig:10-sims3">10.11</a>. Let us remind ourselves of what was written earlier about the bias variance tradeoff and the complexity of models:</p>
<ul>
<li>Fitted line 1 is too simple, so we expect a high degree of bias and a low degree of variance.</li>
<li>Fitted line 4 is too complex, so we expect its bias to be theoretically 0, but it will have the highest degree of variance.</li>
</ul>
<div class="figure">
<span style="display:block;" id="fig:10-sims3"></span>
<img src="bookdown-demo_files/figure-html/10-sims3-1.png" alt="Fitted Lines 1 &amp; 4 on Three Different Simulated Datasets" width="672"><p class="caption">
Figure 10.11: Fitted Lines 1 &amp; 4 on Three Different Simulated Datasets
</p>
</div>
<p>For fitted line 1:</p>
<ul>
<li>We see that the the blue line always does a poor job capturing the truth in the data, therefore, it will have high bias as it will systematically over- or under-predict.</li>
<li>However, the blue lines are all pretty similar across all three simulated data sets. Therefore, it has low variance as the line is pretty much the same.</li>
</ul>
<p>For fitted line 4:</p>
<ul>
<li>We see that the orange lines are very different across all three simulated data sets. Therefore, it has high variance.</li>
<li>Fitted line 4 will be, on average, theoretically unbiased. However, being an unbiased but highly variable fitted line is can be problematic, since we only observe one data set in real life, so we cannot average the predictions over many data sets. This is why we also want fitted lines with low variance.</li>
</ul>
<p>We now carry out the Monte Carlo simulations, over 1000 replicates. Ideally, the number of replicates should be more, but this simulation takes some time to run (fitting high degree polynomials is computationally expensive). We will use these 1000 replicates to compute the MSE, bias, and variance of the four fitted lines, evaluated at the point <span class="math inline">\(x=0.1\)</span>. The simulation would do the following:</p>
<ul>
<li>Simulate 1 data set of 1000 observations.</li>
<li>Fitted fitted lines 1 to 4 on the data set.</li>
<li>Compute the predicted value <span class="math inline">\(\hat{y}\)</span> when <span class="math inline">\(x=0.9\)</span> based on each of the four fitted lines, and record these values.</li>
<li>Repeat these steps over 1000 replicates.</li>
</ul>
<p>Let’s take a look at the distributions of <span class="math inline">\(\hat{y}\)</span>s for each fitted line, as shown in Figure <a href="linear-regression.html#fig:10-simsboxplot">10.12</a>.</p>
<div class="figure">
<span style="display:block;" id="fig:10-simsboxplot"></span>
<img src="bookdown-demo_files/figure-html/10-simsboxplot-1.png" alt="Box Plots of Predictions for Four Fitted Lines" width="672"><p class="caption">
Figure 10.12: Box Plots of Predictions for Four Fitted Lines
</p>
</div>
<p>The blue horizontal line represents the true value of <span class="math inline">\(y\)</span> when <span class="math inline">\(x=0.1\)</span>. Let <span class="math inline">\(y_0\)</span> denote this value, i.e. <span class="math inline">\(y_0 = f(0.1) = 0.1^2\)</span>. Figure <a href="linear-regression.html#fig:10-simsboxplot">10.12</a> pretty much matches with what we expected:</p>
<ul>
<li>The variance, or spread, of <span class="math inline">\(\hat{y}\)</span>s is higher as the model gets more complex.</li>
<li>Fitted lines 1 and 2 are biased, since the middle of their boxplots are far away from the blue line, but fitted lines 3 and 4 are unbiased, since the middle of their boxplots matches with the blue line.</li>
</ul>
<p>We then evaluate the test MSE, bias, and variance of <span class="math inline">\(\hat{y}\)</span> from each fitted line:</p>
<ul>
<li>MSE: the average squared difference between <span class="math inline">\(\hat{y}\)</span> and <span class="math inline">\(y_0\)</span>.</li>
<li>Bias: the average difference between <span class="math inline">\(\hat{y}\)</span> and <span class="math inline">\(y_0\)</span>.</li>
<li>Variance: the sample variance of <span class="math inline">\(\hat{y}\)</span>.</li>
</ul>
<div class="inline-table"><table class="table table-sm">
<caption>
<span id="tab:10-table">Table 10.1: </span>Metrics for Four Fitted Lines</caption>
<thead><tr class="header">
<th align="left"></th>
<th align="right">degree</th>
<th align="right">MSE</th>
<th align="right">Squared.Bias</th>
<th align="right">Variance</th>
</tr></thead>
<tbody>
<tr class="odd">
<td align="left">line1</td>
<td align="right">0</td>
<td align="right">0.1063342</td>
<td align="right">0.1052441</td>
<td align="right">0.0010913</td>
</tr>
<tr class="even">
<td align="left">line2</td>
<td align="right">1</td>
<td align="right">0.0085085</td>
<td align="right">0.0055666</td>
<td align="right">0.0029448</td>
</tr>
<tr class="odd">
<td align="left">line3</td>
<td align="right">2</td>
<td align="right">0.0039253</td>
<td align="right">0.0000035</td>
<td align="right">0.0039257</td>
</tr>
<tr class="even">
<td align="left">line4</td>
<td align="right">9</td>
<td align="right">0.0095384</td>
<td align="right">0.0000074</td>
<td align="right">0.0095406</td>
</tr>
</tbody>
</table></div>
<p>The results in Table <a href="linear-regression.html#tab:10-table">10.1</a> pretty much match with our expectations:</p>
<ul>
<li>The variance of the predictions increase as the model gets more complicated (higher degree).</li>
<li>The (squared) bias of the predictions pretty much works in the opposite direction: it decreases as the model gets more complicated. We used the squared bias since the bias could be positive or negative so we get rid of the sign this way. Also, we know that the MSE is equal to the variance plus squared bias. The bias for fitted lines 3 and 4 are theoretically 0.</li>
<li>The test MSE is smallest for fitted line 3, as its form of <span class="math inline">\(\hat{f}(x)\)</span> matches the true <span class="math inline">\(f(x)\)</span>. So we see the optimal model balances the bias-variance tradeoff.</li>
</ul>
</div>
</div>
</div>
  <div class="chapter-nav">
<div class="prev"><a href="hypothesis-testing.html"><span class="header-section-number">9</span> Hypothesis Testing</a></div>
<div class="empty"></div>
</div></main><div class="col-md-3 col-lg-2 d-none d-md-block sidebar sidebar-chapter">
    <nav id="toc" data-toggle="toc" aria-label="On this page"><h2>On this page</h2>
      <ul class="nav navbar-nav">
<li><a class="nav-link" href="#linear-regression"><span class="header-section-number">10</span> Linear Regression</a></li>
<li>
<a class="nav-link" href="#introduction-6"><span class="header-section-number">10.1</span> Introduction</a><ul class="nav navbar-nav">
<li><a class="nav-link" href="#motivation"><span class="header-section-number">10.1.1</span> Motivation</a></li>
<li><a class="nav-link" href="#toy-example"><span class="header-section-number">10.1.2</span> Toy Example</a></li>
<li><a class="nav-link" href="#module-roadmap-8"><span class="header-section-number">10.1.3</span> Module Roadmap</a></li>
</ul>
</li>
<li>
<a class="nav-link" href="#SLR"><span class="header-section-number">10.2</span> Simple Linear Regression</a><ul class="nav navbar-nav">
<li><a class="nav-link" href="#model-setup"><span class="header-section-number">10.2.1</span> Model Setup</a></li>
<li><a class="nav-link" href="#assessing-assumptions"><span class="header-section-number">10.2.2</span> Assessing Assumptions</a></li>
</ul>
</li>
<li>
<a class="nav-link" href="#estSLR"><span class="header-section-number">10.3</span> Estimating Regression Coefficients</a><ul class="nav navbar-nav">
<li><a class="nav-link" href="#method-of-least-squares"><span class="header-section-number">10.3.1</span> Method of Least Squares</a></li>
<li><a class="nav-link" href="#method-of-maximum-likelihood"><span class="header-section-number">10.3.2</span> Method of Maximum Likelihood</a></li>
</ul>
</li>
<li>
<a class="nav-link" href="#SLRinf"><span class="header-section-number">10.4</span> Inference with Simple Linear Regression</a><ul class="nav navbar-nav">
<li><a class="nav-link" href="#properties-of-least-squares-estimators"><span class="header-section-number">10.4.1</span> Properties of Least Squares Estimators</a></li>
<li><a class="nav-link" href="#confidence-interval"><span class="header-section-number">10.4.2</span> Confidence Interval</a></li>
<li><a class="nav-link" href="#hypothesis-testing-1"><span class="header-section-number">10.4.3</span> Hypothesis Testing</a></li>
<li><a class="nav-link" href="#correlation"><span class="header-section-number">10.4.4</span> Correlation</a></li>
<li><a class="nav-link" href="#coefficient-of-determination"><span class="header-section-number">10.4.5</span> Coefficient of Determination</a></li>
<li><a class="nav-link" href="#linear-regression-in-r"><span class="header-section-number">10.4.6</span> Linear Regression in R</a></li>
</ul>
</li>
<li>
<a class="nav-link" href="#SLRacc"><span class="header-section-number">10.5</span> Assessing Model Accuracy</a><ul class="nav navbar-nav">
<li><a class="nav-link" href="#metrics-for-model-accuracy"><span class="header-section-number">10.5.1</span> Metrics for Model Accuracy</a></li>
<li><a class="nav-link" href="#model-complexity"><span class="header-section-number">10.5.2</span> Model Complexity</a></li>
<li><a class="nav-link" href="#training-test-split"><span class="header-section-number">10.5.3</span> Training-Test Split</a></li>
<li><a class="nav-link" href="#bias-variance-trade-off-model-complexity"><span class="header-section-number">10.5.4</span> Bias-Variance Trade-Off &amp; Model Complexity</a></li>
<li><a class="nav-link" href="#monte-carlo-simulations"><span class="header-section-number">10.5.5</span> Monte Carlo Simulations</a></li>
</ul>
</li>
</ul>

      <div class="book-extra">
        <ul class="list-unstyled">
          
        </ul>
</div>
    </nav>
</div>

</div>
</div> <!-- .container -->

<footer class="bg-primary text-light mt-5"><div class="container"><div class="row">

  <div class="col-12 col-md-6 mt-3">
    <p>"<strong>Understanding Uncertainty Course Notes</strong>" was written by Jeffrey Woo. It was last built on 2025-08-12.</p>
  </div>

  <div class="col-12 col-md-6 mt-3">
    <p>This book was built by the <a class="text-light" href="https://bookdown.org">bookdown</a> R package.</p>
  </div>

</div></div>
</footer><!-- dynamically load mathjax for compatibility with self-contained --><script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.9/latest.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script><script type="text/x-mathjax-config">const popovers = document.querySelectorAll('a.footnote-ref[data-toggle="popover"]');
for (let popover of popovers) {
  const div = document.createElement('div');
  div.setAttribute('style', 'position: absolute; top: 0, left:0; width:0, height:0, overflow: hidden; visibility: hidden;');
  div.innerHTML = popover.getAttribute('data-content');

  var has_math = div.querySelector("span.math");
  if (has_math) {
    document.body.appendChild(div);
    MathJax.Hub.Queue(["Typeset", MathJax.Hub, div]);
    MathJax.Hub.Queue(function() {
      popover.setAttribute('data-content', div.innerHTML);
      document.body.removeChild(div);
    })
  }
}
</script>
</body>
</html>
