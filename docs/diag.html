<!DOCTYPE html>
<html lang="en">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<title>Chapter 5 Model Diagnostics and Remedial Measures in SLR | Linear Models for Data Science</title>
<meta name="author" content="Jeffrey Woo">
<meta name="description" content="5.1 Introduction The regression model is based on a number of assumptions. Those assumptions are made so that we can apply commonly used probability distributions to we quantify the variability...">
<meta name="generator" content="bookdown 0.34 with bs4_book()">
<meta property="og:title" content="Chapter 5 Model Diagnostics and Remedial Measures in SLR | Linear Models for Data Science">
<meta property="og:type" content="book">
<meta property="og:description" content="5.1 Introduction The regression model is based on a number of assumptions. Those assumptions are made so that we can apply commonly used probability distributions to we quantify the variability...">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="Chapter 5 Model Diagnostics and Remedial Measures in SLR | Linear Models for Data Science">
<meta name="twitter:description" content="5.1 Introduction The regression model is based on a number of assumptions. Those assumptions are made so that we can apply commonly used probability distributions to we quantify the variability...">
<!-- JS --><script src="https://cdnjs.cloudflare.com/ajax/libs/clipboard.js/2.0.6/clipboard.min.js" integrity="sha256-inc5kl9MA1hkeYUt+EC3BhlIgyp/2jDIyBLS6k3UxPI=" crossorigin="anonymous"></script><script src="https://cdnjs.cloudflare.com/ajax/libs/fuse.js/6.4.6/fuse.js" integrity="sha512-zv6Ywkjyktsohkbp9bb45V6tEMoWhzFzXis+LrMehmJZZSys19Yxf1dopHx7WzIKxr5tK2dVcYmaCk2uqdjF4A==" crossorigin="anonymous"></script><script src="https://kit.fontawesome.com/6ecbd6c532.js" crossorigin="anonymous"></script><script src="libs/jquery-3.6.0/jquery-3.6.0.min.js"></script><meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<link href="libs/bootstrap-4.6.0/bootstrap.min.css" rel="stylesheet">
<script src="libs/bootstrap-4.6.0/bootstrap.bundle.min.js"></script><script src="libs/bs3compat-0.4.2/transition.js"></script><script src="libs/bs3compat-0.4.2/tabs.js"></script><script src="libs/bs3compat-0.4.2/bs3compat.js"></script><link href="libs/bs4_book-1.0.0/bs4_book.css" rel="stylesheet">
<script src="libs/bs4_book-1.0.0/bs4_book.js"></script><script src="https://cdnjs.cloudflare.com/ajax/libs/autocomplete.js/0.38.0/autocomplete.jquery.min.js" integrity="sha512-GU9ayf+66Xx2TmpxqJpliWbT5PiGYxpaG8rfnBEk1LL8l1KGkRShhngwdXK1UgqhAzWpZHSiYPc09/NwDQIGyg==" crossorigin="anonymous"></script><script src="https://cdnjs.cloudflare.com/ajax/libs/mark.js/8.11.1/mark.min.js" integrity="sha512-5CYOlHXGh6QpOFA/TeTylKLWfB3ftPsde7AnmhuitiTX4K5SqCLBeKro6sPS8ilsz1Q4NRx3v8Ko2IBiszzdww==" crossorigin="anonymous"></script><!-- CSS --><style type="text/css">
    
    div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
  </style>
<style type="text/css">
    /* Used with Pandoc 2.11+ new --citeproc when CSL is used */
    div.csl-bib-body { }
    div.csl-entry {
      clear: both;
        }
    .hanging div.csl-entry {
      margin-left:2em;
      text-indent:-2em;
    }
    div.csl-left-margin {
      min-width:2em;
      float:left;
    }
    div.csl-right-inline {
      margin-left:2em;
      padding-left:1em;
    }
    div.csl-indent {
      margin-left: 2em;
    }
  </style>
</head>
<body data-spy="scroll" data-target="#toc">

<div class="container-fluid">
<div class="row">
  <header class="col-sm-12 col-lg-3 sidebar sidebar-book"><a class="sr-only sr-only-focusable" href="#content">Skip to main content</a>

    <div class="d-flex align-items-start justify-content-between">
      <h1>
        <a href="index.html" title="">Linear Models for Data Science</a>
      </h1>
      <button class="btn btn-outline-primary d-lg-none ml-2 mt-1" type="button" data-toggle="collapse" data-target="#main-nav" aria-expanded="true" aria-controls="main-nav"><i class="fas fa-bars"></i><span class="sr-only">Show table of contents</span></button>
    </div>

    <div id="main-nav" class="collapse-lg">
      <form role="search">
        <input id="search" class="form-control" type="search" placeholder="Search" aria-label="Search">
</form>

      <nav aria-label="Table of contents"><h2>Table of contents</h2>
        <ul class="book-toc list-unstyled">
<li><a class="" href="index.html">Preface</a></li>
<li><a class="" href="wrangling.html"><span class="header-section-number">1</span> Data Wrangling with R</a></li>
<li><a class="" href="viz.html"><span class="header-section-number">2</span> Data Visualization with R Using ggplot2</a></li>
<li><a class="" href="slr.html"><span class="header-section-number">3</span> Basics with Simple Linear Regression (SLR)</a></li>
<li><a class="" href="inf.html"><span class="header-section-number">4</span> Inference with Simple Linear Regression (SLR)</a></li>
<li><a class="active" href="diag.html"><span class="header-section-number">5</span> Model Diagnostics and Remedial Measures in SLR</a></li>
<li><a class="" href="try.html"><span class="header-section-number">6</span> TABLE</a></li>
<li><a class="" href="intro.html"><span class="header-section-number">7</span> Introduction</a></li>
<li><a class="" href="literature.html"><span class="header-section-number">8</span> Literature</a></li>
<li><a class="" href="methods.html"><span class="header-section-number">9</span> Methods</a></li>
<li><a class="" href="applications.html"><span class="header-section-number">10</span> Applications</a></li>
<li><a class="" href="final-words.html"><span class="header-section-number">11</span> Final Words</a></li>
<li><a class="" href="references.html">References</a></li>
</ul>

        <div class="book-extra">
          
        </div>
      </nav>
</div>
  </header><main class="col-sm-12 col-md-9 col-lg-7" id="content"><div id="diag" class="section level1" number="5">
<h1>
<span class="header-section-number">5</span> Model Diagnostics and Remedial Measures in SLR<a class="anchor" aria-label="anchor" href="#diag"><i class="fas fa-link"></i></a>
</h1>
<div id="introduction-4" class="section level2" number="5.1">
<h2>
<span class="header-section-number">5.1</span> Introduction<a class="anchor" aria-label="anchor" href="#introduction-4"><i class="fas fa-link"></i></a>
</h2>
<p>The regression model is based on a number of assumptions. Those assumptions are made so that we can apply commonly used probability distributions to we quantify the variability associated with our estimated regression model. This means that if the assumptions are not met for our regression model, then how we quantify the variability associated with our model is no longer reliable. All our analysis with statistical inference becomes questionable.</p>
<p>In this module, you will learn how to assess whether the regression assumptions are met. We will explore ways in which we can transform our variables after diagnosing which assumptions are not met so that we can still proceed to build our regression model.</p>
</div>
<div id="assumptions-in-linear-regression" class="section level2" number="5.2">
<h2>
<span class="header-section-number">5.2</span> Assumptions in Linear Regression<a class="anchor" aria-label="anchor" href="#assumptions-in-linear-regression"><i class="fas fa-link"></i></a>
</h2>
<p>In module <a href="slr.html#slr">3</a>, we stated the SLR model as</p>
<p><span class="math display" id="eq:5SLRmod">\[\begin{equation}
y=\beta_0+\beta_{1} x + \epsilon.
\tag{5.1}
\end{equation}\]</span></p>
<p>where <span class="math inline">\(f(x) = \beta_0 + \beta_1 x\)</span>. We need to make some assumptions for the error term <span class="math inline">\(\epsilon\)</span>. Mathematically, the assumptions are expressed as</p>
<p><span class="math display" id="eq:5assumptions">\[\begin{equation}
\epsilon_1,\ldots,\epsilon_n \ i.i.d. \sim N(0,\sigma^2)
\tag{5.2}
\end{equation}\]</span></p>
<p>Breaking down <a href="diag.html#eq:5assumptions">(5.2)</a> the assumptions can be expressed as the following:</p>
<ol style="list-style-type: decimal">
<li>The errors have <strong>mean 0</strong>.</li>
<li>The errors have <strong>constant variance denoted by <span class="math inline">\(\sigma^2\)</span></strong>.</li>
<li>The errors are <strong>independent</strong>.</li>
<li>The errors are <strong>normally distributed</strong>.</li>
</ol>
<p>Let’s dig a little deeper into the meaning and implications of these 4 assumptions.</p>
<div id="assumption-1-errors-have-mean-0." class="section level3" number="5.2.1">
<h3>
<span class="header-section-number">5.2.1</span> Assumption 1: Errors have mean 0.<a class="anchor" aria-label="anchor" href="#assumption-1-errors-have-mean-0."><i class="fas fa-link"></i></a>
</h3>
<p>For each value of the predictor, the errors have <strong>mean 0</strong>. A by-product of this statement is that the relationship between <span class="math inline">\(y\)</span> and <span class="math inline">\(x\)</span>, as expressed via <span class="math inline">\(y \approx f(x)\)</span>, is correct. So, if <span class="math inline">\(f(x) = \beta_0 + \beta_1 x\)</span>, then the relationship is approximately linear.</p>
<p>The plots in Figure <!-- overall --> are based on simulated data. The scatterplot shown in Figure <!-- linear --> is an example of when this assumption is met. As we move from left to right on the plot, the data points are generally evenly scattered on both sides of the regression line that is overlaid.</p>
<!-- Add figure here. linear_plot.jpeg & quadratic_plot.jpg -->
<p>The scatterplot shown in Figure <!-- quadratic --> is an example of when this assumption is <strong>not</strong> met. As we move from left to right on the plot in Figure <!-- quadratic -->, the data points are generally not evenly scattered on both sides of the regression line that is overlaid.</p>
<ul>
<li>When <span class="math inline">\(-2 \leq x \leq -1.2\)</span>, the data points are generally above the regression line;</li>
<li>then when <span class="math inline">\(-1.2 &lt; x &lt; 1\)</span>, the data points are generally below the regression line;</li>
<li>and then when <span class="math inline">\(x \geq 1\)</span>, the data points are generally above the regression line.</li>
</ul>
<p><em>Please see the associated video for more explanation on how to use Figure <!-- overall --> to assess assumption 1.</em> <!-- overall --></p>
<div id="consequences-of-violating-this-assumption" class="section level4" number="5.2.1.1">
<h4>
<span class="header-section-number">5.2.1.1</span> Consequences of violating this assumption<a class="anchor" aria-label="anchor" href="#consequences-of-violating-this-assumption"><i class="fas fa-link"></i></a>
</h4>
<p><strong>Predictions will be biased</strong>. This means that predicted values will systematically over- or under- estimate the true values of the response variable. Of the 4 assumptions listed, this is <strong>most crucial assumption</strong>.</p>
<p>Using Figure <!-- quadratic --> as an example, this implies that</p>
<ul>
<li>when <span class="math inline">\(-2 \leq x \leq -1.2\)</span>, the regression line will systematically under-predict the response variable;</li>
<li>then when <span class="math inline">\(-1.2 &lt; x &lt; 1\)</span>, the regression line will systematically over-predict the response variable;</li>
<li>and then when <span class="math inline">\(x \geq 1\)</span>, the regression line will systematically under-predict the response variable.</li>
</ul>
</div>
</div>
<div id="assumption-2-errors-have-constant-variance" class="section level3" number="5.2.2">
<h3>
<span class="header-section-number">5.2.2</span> Assumption 2: Errors have constant variance<a class="anchor" aria-label="anchor" href="#assumption-2-errors-have-constant-variance"><i class="fas fa-link"></i></a>
</h3>
<p>For each value of the predictor, the error terms have <strong>constant variance</strong>, denoted by <span class="math inline">\(\sigma^2\)</span>. This implies that when looking at a scatterplot, the vertical variation of data points around the regression equation has the same magnitude everywhere.</p>
<p>The plots in Figure <!-- overall2 --> are based on simulated data. The scatterplot shown in Figure <!-- linear2 --> is an example of when this assumption is met (this figure is actually the same as Figure <!-- linear -->, so the data that produced these plots satisfy both assumptions). As we move from left to right on the plot, the vertical variation of the data points about the regression line is approximately constant.</p>
<!-- Add figure here. linear_plot.jpeg & increasing_var.jpg -->
<p>The scatterplot shown in Figure <!-- increasing --> is an example of when this assumption is <strong>not</strong> met. As we move from left to right on the plot in Figure <!-- increasing -->, the vertical variation of the data points about the regression line becomes larger as the value of the response variable gets larger, so the variance is not constant.</p>
<p><em>Please see the associated video for more explanation on how to use Figure <!-- overall2 --> to assess assumption 2.</em> <!-- overall2 --></p>
<div id="consequences-of-violating-this-assumption-1" class="section level4" number="5.2.2.1">
<h4>
<span class="header-section-number">5.2.2.1</span> Consequences of violating this assumption<a class="anchor" aria-label="anchor" href="#consequences-of-violating-this-assumption-1"><i class="fas fa-link"></i></a>
</h4>
<p><strong>Statistical inference will no longer be reliable.</strong> This means that the results from any hypothesis test, confidence interval, or prediction interval are no longer reliable.</p>
<p>Interestingly, for the scatterplot in Figure <!-- increasing -->, we can say that assumption 1 is met, since the the data points are generally evenly scattered on both sides of the regression line. Predictions will still be unbiased; the predicted response, <span class="math inline">\(\hat{y}\)</span>, do not systematically over- or under-predict the response variable. So if our goal is to assess if the relationship is approximately linear, this scatterplot is fine. We do lose the utility from hypothesis tests, CIs, and PIs.</p>
</div>
</div>
<div id="assumption-3-errors-are-independent" class="section level3" number="5.2.3">
<h3>
<span class="header-section-number">5.2.3</span> Assumption 3: Errors are independent<a class="anchor" aria-label="anchor" href="#assumption-3-errors-are-independent"><i class="fas fa-link"></i></a>
</h3>
<p>A by-product of this assumption is that the values of the response variable, <span class="math inline">\(y_i\)</span>, are independent from each other. Any <span class="math inline">\(y_i\)</span> does not depend on other values of the response variable.</p>
<div id="consequences-of-violating-this-assumption-2" class="section level4" number="5.2.3.1">
<h4>
<span class="header-section-number">5.2.3.1</span> Consequences of violating this assumption<a class="anchor" aria-label="anchor" href="#consequences-of-violating-this-assumption-2"><i class="fas fa-link"></i></a>
</h4>
<p><strong>Statistical inference will no longer be reliable.</strong> This means that the results from any hypothesis test, confidence interval, or prediction interval are no longer reliable.</p>
</div>
</div>
<div id="assumption-4-errors-are-normally-distributed" class="section level3" number="5.2.4">
<h3>
<span class="header-section-number">5.2.4</span> Assumption 4: Errors are normally distributed<a class="anchor" aria-label="anchor" href="#assumption-4-errors-are-normally-distributed"><i class="fas fa-link"></i></a>
</h3>
<p>If we were to create a density plot of the errors, the errors should follow a normal distribution.</p>
<div id="consequences-of-violating-this-assumption-3" class="section level4" number="5.2.4.1">
<h4>
<span class="header-section-number">5.2.4.1</span> Consequences of violating this assumption<a class="anchor" aria-label="anchor" href="#consequences-of-violating-this-assumption-3"><i class="fas fa-link"></i></a>
</h4>
<p>The regression model is fairly robust to the assumption that the errors are normally distributed. In other words, violation of this particular assumption is not very consequential. <strong>Of the 4 assumptions, this is the least crucial to satisfy.</strong></p>
</div>
</div>
</div>
<div id="assessing-regression-assumptions" class="section level2" number="5.3">
<h2>
<span class="header-section-number">5.3</span> Assessing Regression Assumptions<a class="anchor" aria-label="anchor" href="#assessing-regression-assumptions"><i class="fas fa-link"></i></a>
</h2>
<p>There are a few visualizations that help in detecting violations of the regression assumptions. These visualizations are:</p>
<ul>
<li>Scatterplot of <span class="math inline">\(y\)</span> against <span class="math inline">\(x\)</span> (assumptions 1 and 2).</li>
<li>Residual plot (assumptions 1 and 2).</li>
<li>Autocorrelation function (ACF) plot of residuals (assumption 3).</li>
<li>Normal probability plot of residuals (often called QQ plot) (assumption 4).</li>
</ul>
<div id="scatterplot" class="section level3" number="5.3.1">
<h3>
<span class="header-section-number">5.3.1</span> Scatterplot<a class="anchor" aria-label="anchor" href="#scatterplot"><i class="fas fa-link"></i></a>
</h3>
<p>We can examine the scatterplot of <span class="math inline">\(y\)</span> against <span class="math inline">\(x\)</span> to check for assumptions 1 and 2. We want to see the following in the scatterplot:</p>
<ul>
<li>
<strong>No nonlinear pattern</strong> (assumption 1).</li>
<li>Data points <strong>evenly scattered</strong> (for each value on the x-axis) around fitted line (assumption 1).</li>
<li>Vertical variation of data points constant (assumption 2).</li>
</ul>
<p>We have used Figure <!-- linear2 --> as an example of a scatterplot that meets these assumptions. Let us take a look at another example that we have worked with. This scatterplot is from the <code>elmhurst</code> dataset from the <code>openintro</code> package that we have been seeing in tutorials. We are regressing the amount of gift aid a student receives based on the student’s family income. The corresponding scatterplot is shown in in Figure <a href="diag.html#fig:elmhurst">5.1</a>.</p>
<div class="figure">
<span style="display:block;" id="fig:elmhurst"></span>
<img src="bookdown-demo_files/figure-html/elmhurst-1.png" alt="Scatterplot of Gift Aid Against Family Income" width="672"><p class="caption">
Figure 5.1: Scatterplot of Gift Aid Against Family Income
</p>
</div>
<p>In Figure <a href="diag.html#fig:elmhurst">5.1</a>, we see that the data points are evenly scattered around the fitted line. We also see the vertical variation of the data points is fairly constant. So assumptions that the errors have 0 mean and constant variance appear to be met.</p>
<div id="practice-question" class="section level4" number="5.3.1.1">
<h4>
<span class="header-section-number">5.3.1.1</span> Practice question<a class="anchor" aria-label="anchor" href="#practice-question"><i class="fas fa-link"></i></a>
</h4>
<p>The data are about the prices of used cars. We are regressing the sale price of the car against the age of the car. The corresponding scatterplot is shown in Figure <a href="diag.html#fig:mazda">5.2</a>. Based on Figure <a href="diag.html#fig:mazda">5.2</a>, which of assumptions 1 or 2 (or both, or neither), is met? We will go over this in the tutorial.</p>
<div class="figure">
<span style="display:block;" id="fig:mazda"></span>
<img src="bookdown-demo_files/figure-html/mazda-1.png" alt="Scatterplot of Sale Price Against Age" width="672"><p class="caption">
Figure 5.2: Scatterplot of Sale Price Against Age
</p>
</div>
</div>
</div>
<div id="residual-plot" class="section level3" number="5.3.2">
<h3>
<span class="header-section-number">5.3.2</span> Residual plot<a class="anchor" aria-label="anchor" href="#residual-plot"><i class="fas fa-link"></i></a>
</h3>
<p>While using the scatterplot is an intuitive way of assessing regression assumptions, it has a limitation. It cannot be used if we have multiple predictors in our regression, which we will encounter (and happens more often than just having one predictor). Another visualization that we can use to assess assumptions 1 and 2 is a <strong>residual plot</strong>. This is a scatterplot of residuals, <span class="math inline">\(e\)</span>, against fitted values, <span class="math inline">\(\hat{y}\)</span>. We want to observe the following in a residual plot.</p>
<ul>
<li>Residuals should be <strong>evenly scattered</strong> across the horizontal axis (assumption 1).</li>
<li>The residuals should have <strong>similar vertical variation</strong> across the plot (assumption 2).</li>
<li>Some writers combine these two points into the following statement: the residuals should fall in a <strong>horizontal band around 0</strong> with no apparent pattern (assumption 1, 2).</li>
</ul>
<p>The residual plots in Figure <!-- resplots --> are based on simulated data from Figures <!-- linear -->, <!-- quadratic -->, and <!-- increasing -->.</p>
<!-- Add figure here. resplot1.jpeg, resplot2.jpeg, resplot3.jpeg -->
<p>We make the following observations:
- From Figure <!-- res1 -->, we see that the residuals are evenly scattered across the horizontal axis, and their vertical variation is fairly constant across the plot. So both assumptions are met.
- From Figure <!-- res2 -->, we see that the residuals are <strong>not</strong> evenly scattered across the horizontal axis, although their vertical variation is fairly constant across the plot. So only assumption 1 is not met.
- From Figure <!-- res3 -->, we see that the residuals are evenly scattered across the horizontal axis, but their vertical variation is <strong>not constant</strong> across the plot. In fact, the vertical variation is increasing as we move from left to right. So only assumption 2 is not met.</p>
<p>If you compare the conclusions from the residuals plots and scatterplots, they are the same. In SLR, the takeaways should be consistent.</p>
<p><em>Please see the associated video for more explanation on how to use Figure <!-- resplots --> to assess assumptions 1 and 2.</em> <!-- resplots --></p>
<div id="practice-questions-2" class="section level4" number="5.3.2.1">
<h4>
<span class="header-section-number">5.3.2.1</span> Practice questions<a class="anchor" aria-label="anchor" href="#practice-questions-2"><i class="fas fa-link"></i></a>
</h4>
<ol style="list-style-type: decimal">
<li>The residual plot in Figure <a href="diag.html#fig:elmhurst-res">5.3</a> comes from regressing gift aid against family income for the <code>elmhurst</code> dataset. Based on this residual plot, which assumptions are met?</li>
</ol>
<div class="figure">
<span style="display:block;" id="fig:elmhurst-res"></span>
<img src="bookdown-demo_files/figure-html/elmhurst-res-1.png" alt="Residual Plot Regressing Gift Aid Against Family Income" width="672"><p class="caption">
Figure 5.3: Residual Plot Regressing Gift Aid Against Family Income
</p>
</div>
<ol start="2" style="list-style-type: decimal">
<li>The residual plot in Figure <a href="diag.html#fig:mazda-res">5.4</a> comes from regressing price of cars against age for the used cars dataset. Based on this residual plot, which assumptions are met?</li>
</ol>
<div class="figure">
<span style="display:block;" id="fig:mazda-res"></span>
<img src="bookdown-demo_files/figure-html/mazda-res-1.png" alt="Residual Plot Regressing Price Against Age for Used Cars" width="672"><p class="caption">
Figure 5.4: Residual Plot Regressing Price Against Age for Used Cars
</p>
</div>
<p><em>Please see the associated video as I go over these practice questions.</em></p>
</div>
</div>
<div id="acf-plot" class="section level3" number="5.3.3">
<h3>
<span class="header-section-number">5.3.3</span> ACF plot<a class="anchor" aria-label="anchor" href="#acf-plot"><i class="fas fa-link"></i></a>
</h3>
<p>Assumption 3 states that the errors are <strong>independent</strong>. This assumption implies that the values of the response variable are independent from each other. This assumption is typically assessed via knowing the nature of the data.</p>
<ul>
<li><p>If the observations were obtained from a random sample, it is likely that the observations will be independent from each other. This is the very nature of a random sample and why random samples are preferred over convenience samples.</p></li>
<li><p>If the data has some inherent sequence, it is likely the observations will not be independent, and are dependent. For example, if I record the value of a stock at the end of each day, the value at day 2 is likely to be related to its value at day 1. So the values of stock prices at the end of each day are not independent.</p></li>
</ul>
<p>An autocorrelation function (ACF) plot of the residuals may be a used to help assess if the assumption that the errors are independent is met. However, the plot is not a substitute for using your understanding about the nature of the data and should only be used as a confirmation.</p>
<p>The ACF plot measures the correlation between a vector of observations and the lagged versions of the observations. If the observations are uncorrelated, the correlations between the vector of observations and lagged versions of these observations are theoretically 0. We may create an ACF plot for the residuals from our regression.</p>
<p>The ACF plot in Figure <!-- acf --> and is based on simulated data that were independently generated.</p>
<!-- Add figure here. acf.jpeg, acf_ordered.jpeg -->
<p>A few notes about the ACF plot:</p>
<ul>
<li>The ACF at lag 0 is always 1. The correlation of any vector with itself is always 1.</li>
<li>The dashed horizontal lines represent critical values. An ACF at any lag beyond the critical value indicates an ACF that is significant. We have evidence of correlation (and hence dependence) in our residuals.</li>
<li>If the observed values for the response variable are independent, then we would expect the ACFs at lags greater than 0 to be insignificant. Do note that because we are conducting multiple hypothesis tests, do not be too alarmed if the ACFs are slightly beyond the critical values at an isolated lag or 2.</li>
</ul>
<p>Based on Figure <!-- acf -->, we see that the ACFs at all lags greater than 0 are insignificant. We do not have evidence the residuals are correlated with each other, so we do not have evidence that assumption 3 is not met.</p>
<p>Sometimes, the dataframe can be sorted in some manner (e.g. increasing order for response variable), and if so, we would actually expect to see significant correlations in the ACF plot. The ACF plot in Figure <!-- acf2 --> is such an example. The residuals are from the same simulated dataset, only with the data sorted by the response variable. If we had just looked at the ACF plot in Figure <!-- acf2 --> without understanding the data were simulated independently and then sorted, we would have erroneously concluded that the residuals are not independent and the regression assumption is not met.</p>
</div>
<div id="qq-plot" class="section level3" number="5.3.4">
<h3>
<span class="header-section-number">5.3.4</span> QQ plot<a class="anchor" aria-label="anchor" href="#qq-plot"><i class="fas fa-link"></i></a>
</h3>
<p>A normal probability plot (also called a QQ plot) is used to assess if the distribution of a variable is normal. It typically plots the residuals against their theoretical residual if they followed a normal distribution. A QQ line is typically overlaid. If the plots fall closely to the QQ line, we have evidence that the observations follow a normal distribution. Figure <a href="diag.html#fig:qq">5.5</a> shows a QQ plot that comes from a normally distributed variable.</p>
<div class="figure">
<span style="display:block;" id="fig:qq"></span>
<img src="images/qqplot.jpeg" alt="QQ Plot"><p class="caption">
Figure 5.5: QQ Plot
</p>
</div>
</div>
<div id="remedial-measures" class="section level3" number="5.3.5">
<h3>
<span class="header-section-number">5.3.5</span> Remedial measures<a class="anchor" aria-label="anchor" href="#remedial-measures"><i class="fas fa-link"></i></a>
</h3>
<p>We now know how to assess if specific regression assumptions are not met. The remedial measures involve transforming either the predictor variable and / or the response variable. These transformations are chosen to handle violations to assumptions 1 and / or 2 respectively. The general strategy on selecting which variable to transform:</p>
<ul>
<li>Transforming the response variable, <span class="math inline">\(y\)</span>, affects both assumptions 1 and 2.
<ul>
<li>Visually, we can think of transforming <span class="math inline">\(y\)</span> in terms of stretching or squeezing the scatterplot of <span class="math inline">\(y\)</span> against <span class="math inline">\(x\)</span> vertically. Thus, transforming <span class="math inline">\(y\)</span> affects the shape of the relationship and the vertical spread of the data points.</li>
<li>However, the <strong>choice on how we transform <span class="math inline">\(y\)</span> is based on handling assumption 2.</strong>
</li>
</ul>
</li>
<li>Transforming the predictor variable, <span class="math inline">\(x\)</span> affects assumption 1 and does not theoretically affect assumption 2.
<ul>
<li>Visually, we can think of transforming <span class="math inline">\(x\)</span> in terms of stretching or squeezing the scatterplot of <span class="math inline">\(y\)</span> against <span class="math inline">\(x\)</span> horizontally. Thus, transforming <span class="math inline">\(x\)</span> affects the shape of the relationship but not the vertical spread of the data points.</li>
<li>Therefore, <strong>transforming <span class="math inline">\(x\)</span> is based on handling assumption 1.</strong>
</li>
</ul>
</li>
<li>If assumption 2 is not met, we transform <span class="math inline">\(y\)</span> to stabilize the variance and make it constant.</li>
<li>If assumption 1 is not met, we transform <span class="math inline">\(x\)</span> to find the appropriate shape to relate the variables.</li>
<li>If both assumptions are not met, we transform <span class="math inline">\(y\)</span> first to stabilize the variance. Once assumption 2 is solved, check if assumption 1 is not met. If not met, transform <span class="math inline">\(x\)</span>.</li>
</ul>
<p>Assumption 1 deals with whether the way we have expressed how <span class="math inline">\(y\)</span> and <span class="math inline">\(x\)</span> are related, through <span class="math inline">\(f(x)\)</span>, is appropriate. Assumption 2 deals with the vertical variation of the data points in the scatterplot.</p>
</div>
</div>
<div id="remedial-measures-variance-stabilizing-transformations" class="section level2" number="5.4">
<h2>
<span class="header-section-number">5.4</span> Remedial Measures: Variance Stabilizing Transformations<a class="anchor" aria-label="anchor" href="#remedial-measures-variance-stabilizing-transformations"><i class="fas fa-link"></i></a>
</h2>
<p>We transform the response variable to stabilize the variance (assumption 2). There are a couple of ways to decide the appropriate transformation:</p>
<ol style="list-style-type: decimal">
<li>Pattern seen in residual plot can guide choice in how to transform the response variable.</li>
<li>Box-Cox plot.</li>
</ol>
<div id="use-pattern-in-residual-plot" class="section level3" number="5.4.1">
<h3>
<span class="header-section-number">5.4.1</span> Use Pattern in Residual Plot<a class="anchor" aria-label="anchor" href="#use-pattern-in-residual-plot"><i class="fas fa-link"></i></a>
</h3>
<p>We can stabilize the variance of the errors based on the residual plot, if we see either of the following scenarios:</p>
<ul>
<li>vertical variation of residuals <strong>increasing</strong> as fitted response increases, or as we move from left to right, as in Figure <!-- inc -->, or</li>
<li>vertical variation of residuals <strong>decreasing</strong> as fitted response increases, or as we move from left to right, as in Figure <!-- dec -->.</li>
</ul>
<!-- Add figure here. resplot3.jpeg, resplot4.jpeg --><p>Note that increasing variance as fitted response increases is much more common with real data. Generally, larger values of a variable are associated with larger spread.</p>
<p>We transform <span class="math inline">\(y\)</span> using <span class="math inline">\(y^{*} = y^{\lambda}\)</span>, with <span class="math inline">\(\lambda\)</span> chosen based on whether the variance of the residuals is increasing or decreasing with fitted response:</p>
<ul>
<li>For Figure <!-- inc -->, choose <span class="math inline">\(\lambda &lt; 1\)</span>.
<ul>
<li>If <span class="math inline">\(\lambda = 0\)</span>, it means we use a logarithmic transformation with base e, i.e. <span class="math inline">\(y^* = \log(y)\)</span>.</li>
<li>Note that a logarithm with no base means a natural log, or ln.</li>
</ul>
</li>
<li>For Figure <!-- dec -->, choose <span class="math inline">\(\lambda &gt; 1\)</span>.</li>
</ul>
<p>So based on the residual plot, we have a range of values for <span class="math inline">\(\lambda\)</span>.</p>
</div>
<div id="box-cox-plot" class="section level3" number="5.4.2">
<h3>
<span class="header-section-number">5.4.2</span> Box-Cox Plot<a class="anchor" aria-label="anchor" href="#box-cox-plot"><i class="fas fa-link"></i></a>
</h3>
<p>We can use a Box-Cox plot to help us narrow the range of <span class="math inline">\(\lambda\)</span> to use. It is a plot of the log-likelihood function against <span class="math inline">\(\lambda\)</span>, and we choose <span class="math inline">\(\lambda\)</span> that maximizes this log-likelihood function. For example, Figure <!-- bcplot --> shows the Box Cox plot generated for the regression associated with the residual plot in Figure <!-- dec -->.</p>
<!-- Add figure here. boxcox.jpeg -->
<p>Notice an approximate 95% CI is provided for <span class="math inline">\(\lambda\)</span>. A few comments on how to use the Box-Cox plot:</p>
<ul>
<li>Three vertical dashed lines are displayed: the middle line corresponds to the optimal value of <span class="math inline">\(\lambda\)</span>; the other two lines are the lower and upper bounds of a 95% CI for <span class="math inline">\(\lambda\)</span>.</li>
<li>We choose <span class="math inline">\(\lambda\)</span> within the CI (or even close to it) that is easy to understand. We do not have to choose the optimal value, especially if its value is difficult to interpret. In this example, I will choose <span class="math inline">\(\lambda = 2\)</span>, so a square transformation for <span class="math inline">\(y\)</span>. Transform response with <span class="math inline">\(y^* = y^2\)</span>. Regress <span class="math inline">\(y^*\)</span> against <span class="math inline">\(x\)</span>.</li>
<li>If 1 lies in the CI, <strong>no transformation</strong> on <span class="math inline">\(y\)</span> may be needed.</li>
<li>If a transformation is needed, a <strong>log transformation</strong> is preferred, since we can still interpret the estimated coefficients. It is difficult to interpret with any other type of transformation.</li>
<li>View the Box-Cox procedure as a guide for selecting a transformation, rather than being definitive.</li>
<li>Need to recheck the residuals after every transformation to assess if the transformation worked.</li>
</ul>
</div>
<div id="interpretation-with-log-transformed-response" class="section level3" number="5.4.3">
<h3>
<span class="header-section-number">5.4.3</span> Interpretation with Log Transformed Response<a class="anchor" aria-label="anchor" href="#interpretation-with-log-transformed-response"><i class="fas fa-link"></i></a>
</h3>
<p>A log transformation on the response is preferred over any other transformation, as we can still interpret regression coefficients. A couple of ways to interpret the estimated slope <span class="math inline">\(\hat{\beta}_1\)</span>:</p>
<ul>
<li>The predicted response variable is <strong>multiplied by a factor</strong> of <span class="math inline">\(\exp(\hat{\beta_1})\)</span> for a one-unit increase in the predictor.</li>
<li>We can also subtract 1 from <span class="math inline">\(\exp(\hat{\beta_1})\)</span> to express the change as a percentage.
<ul>
<li>If <span class="math inline">\(\hat{\beta}_1\)</span> is positive, we have a percent <strong>increase</strong>. The predicted response variable increases by <span class="math inline">\((\exp(\hat{\beta_1}) - 1) \times 100\)</span> percent for a one-unit increase in the predictor.</li>
<li>If <span class="math inline">\(\hat{\beta}_1\)</span> is negative, we have a percent <strong>decrease</strong>. The predicted response variable decreases by <span class="math inline">\((1 - \exp(\hat{\beta_1})) \times 100\)</span> percent for a one-unit increase in the predictor.</li>
</ul>
</li>
</ul>
<p><em>Please see the associated video as I go over the math explaining how we interpret regression coefficients when the response variable is log transformed.</em></p>
</div>
</div>
<div id="remedial-measures-linearization-transformations" class="section level2" number="5.5">
<h2>
<span class="header-section-number">5.5</span> Remedial Measures: Linearization Transformations<a class="anchor" aria-label="anchor" href="#remedial-measures-linearization-transformations"><i class="fas fa-link"></i></a>
</h2>
<p>We first ensure the variance has been stabilized and assumption 2 is met. If <span class="math inline">\(f(x)\)</span> does not accurately capture the relationship between the variables, we transform the predictor variable to meet assumption 1. Some writers call this a linearization transformation, as we seek to make the transformed version of the predictor variable, <span class="math inline">\(x^*\)</span>, to be approximately linear with the response variable (or transformed <span class="math inline">\(y\)</span>), i.e. <span class="math inline">\(y = \beta_0 + \beta_1 x^* + \epsilon\)</span>. We do not consider transforming the response variable to deal with assumption 1, as transforming the response variable is likely to reintroduce violation of assumption 2.</p>
<p>The general strategy on how to transform the predictor is via a scatterplot of <span class="math inline">\(y\)</span> (or <span class="math inline">\(y^*\)</span>) against <span class="math inline">\(x\)</span>. We use the pattern seen in the plot to decide how to transform the predictor. Some examples are shown in Figure <!-- xstar --> below.</p>
<!-- Add figure here. ylog.jpeg, yinv.jpeg, ysqrt.jpeg, y2.jpeg -->
<div id="hierarchical-principle" class="section level3" number="5.5.1">
<h3>
<span class="header-section-number">5.5.1</span> Hierarchical Principle<a class="anchor" aria-label="anchor" href="#hierarchical-principle"><i class="fas fa-link"></i></a>
</h3>
<p>One thing to be aware of is the <strong>hierarchical principle</strong>: if the relationship between the response and predictor is of a higher order polynomial (e.g. quadratic, cubic), the hierarchical principle states that the lower order terms should remain in the model. For example, if the relationship is of order <span class="math inline">\(h\)</span>, fit <span class="math inline">\(y = \beta_0 + \beta_1 x + \beta_2 x^2 + \cdots + \beta_h x^h + \epsilon\)</span> via a multiple linear regression framework. We will see how to do this in the next module.</p>
</div>
<div id="interpretation-with-log-transformed-predictor" class="section level3" number="5.5.2">
<h3>
<span class="header-section-number">5.5.2</span> Interpretation with Log Transformed Predictor<a class="anchor" aria-label="anchor" href="#interpretation-with-log-transformed-predictor"><i class="fas fa-link"></i></a>
</h3>
<p>A log transformation on the predictor is preferred over any other transformation, as we can still interpret the regression coefficient, <span class="math inline">\(\hat{\beta}_1\)</span>, in a couple of ways:</p>
<ul>
<li>For an <span class="math inline">\(a\%\)</span> increase in the predictor, the predicted response <strong>increases by <span class="math inline">\(\hat{\beta}_1 \log(1+ \frac{a}{100})\)</span>.</strong>
</li>
<li>
<span class="math inline">\(\log(1 + \frac{1}{100}) \approx \frac{1}{100}\)</span> (Taylor series). So an alternative interpretation is: for a 1% increase in the predictor, the predicted response increases by approximately <span class="math inline">\(\frac{\hat{\beta}_1}{100}\)</span>.</li>
</ul>
<p><em>Please see the associated video as I go over the math explaining how we interpret regression coefficients when the predictor variable is log transformed.</em></p>
</div>
<div id="interpretation-with-log-transformed-response-and-predictor" class="section level3" number="5.5.3">
<h3>
<span class="header-section-number">5.5.3</span> Interpretation with Log Transformed Response and Predictor<a class="anchor" aria-label="anchor" href="#interpretation-with-log-transformed-response-and-predictor"><i class="fas fa-link"></i></a>
</h3>
<p>If both response and predictor variables are log transformed, the regression coefficient, <span class="math inline">\(\hat{\beta}_1\)</span>, can be interpreted in a couple of ways:</p>
<ul>
<li><p>For an <span class="math inline">\(a\%\)</span> increase in the predictor, the predicted response is <strong>multiplied by <span class="math inline">\((1 + \frac{a}{100})^{\hat{\beta}_1}\)</span>.</strong></p></li>
<li><p><span class="math inline">\((1 + \frac{1}{100})^{\hat{\beta}_1} \approx 1 + \frac{\hat{\beta}_1}{100}\)</span> (Taylor series). So an alternative interpretation is: for a 1% increase in the predictor, the predicted response <strong>increases by approximately <span class="math inline">\(\hat{\beta}_1\)</span> percent.</strong> Note that this approximation works better when <span class="math inline">\({\hat{\beta}_1}\)</span> is small in magnitude.</p></li>
</ul>
<p><em>Please see the associated video as I go over the math explaining how we interpret regression coefficients when the both response and predictor variables are log transformed.</em></p>
</div>
<div id="some-general-comments-about-assessing-assumptions-and-transformations" class="section level3" number="5.5.4">
<h3>
<span class="header-section-number">5.5.4</span> Some General Comments about Assessing Assumptions and Transformations<a class="anchor" aria-label="anchor" href="#some-general-comments-about-assessing-assumptions-and-transformations"><i class="fas fa-link"></i></a>
</h3>
<ul>
<li><p>When assessing the assumptions with a residual plot, we are assessing if the assumptions are reasonably / approximately met.</p></li>
<li><p>With real data, assumptions are rarely met 100%.</p></li>
<li><p>If unsure, proceed with model building, and test how model performs on new data. If poor performance, go back to residual plot to assess what transformation will be appropriate.</p></li>
<li><p>Assess the plots to decide which variables need to be transformed, and how. The choice of transformation should be guided by what you see in the plots, and not by trial and error.</p></li>
<li><p>A residual plot should always be produced after each transformation. A Box Cox plot could also be produced. The plots should be assessed if the transformation helped in the way you intended.</p></li>
<li><p>Solve assumption 2 first, then assumption 1.</p></li>
</ul>
</div>
</div>
</div>
  <div class="chapter-nav">
<div class="prev"><a href="inf.html"><span class="header-section-number">4</span> Inference with Simple Linear Regression (SLR)</a></div>
<div class="next"><a href="try.html"><span class="header-section-number">6</span> TABLE</a></div>
</div></main><div class="col-md-3 col-lg-2 d-none d-md-block sidebar sidebar-chapter">
    <nav id="toc" data-toggle="toc" aria-label="On this page"><h2>On this page</h2>
      <ul class="nav navbar-nav">
<li><a class="nav-link" href="#diag"><span class="header-section-number">5</span> Model Diagnostics and Remedial Measures in SLR</a></li>
<li><a class="nav-link" href="#introduction-4"><span class="header-section-number">5.1</span> Introduction</a></li>
<li>
<a class="nav-link" href="#assumptions-in-linear-regression"><span class="header-section-number">5.2</span> Assumptions in Linear Regression</a><ul class="nav navbar-nav">
<li><a class="nav-link" href="#assumption-1-errors-have-mean-0."><span class="header-section-number">5.2.1</span> Assumption 1: Errors have mean 0.</a></li>
<li><a class="nav-link" href="#assumption-2-errors-have-constant-variance"><span class="header-section-number">5.2.2</span> Assumption 2: Errors have constant variance</a></li>
<li><a class="nav-link" href="#assumption-3-errors-are-independent"><span class="header-section-number">5.2.3</span> Assumption 3: Errors are independent</a></li>
<li><a class="nav-link" href="#assumption-4-errors-are-normally-distributed"><span class="header-section-number">5.2.4</span> Assumption 4: Errors are normally distributed</a></li>
</ul>
</li>
<li>
<a class="nav-link" href="#assessing-regression-assumptions"><span class="header-section-number">5.3</span> Assessing Regression Assumptions</a><ul class="nav navbar-nav">
<li><a class="nav-link" href="#scatterplot"><span class="header-section-number">5.3.1</span> Scatterplot</a></li>
<li><a class="nav-link" href="#residual-plot"><span class="header-section-number">5.3.2</span> Residual plot</a></li>
<li><a class="nav-link" href="#acf-plot"><span class="header-section-number">5.3.3</span> ACF plot</a></li>
<li><a class="nav-link" href="#qq-plot"><span class="header-section-number">5.3.4</span> QQ plot</a></li>
<li><a class="nav-link" href="#remedial-measures"><span class="header-section-number">5.3.5</span> Remedial measures</a></li>
</ul>
</li>
<li>
<a class="nav-link" href="#remedial-measures-variance-stabilizing-transformations"><span class="header-section-number">5.4</span> Remedial Measures: Variance Stabilizing Transformations</a><ul class="nav navbar-nav">
<li><a class="nav-link" href="#use-pattern-in-residual-plot"><span class="header-section-number">5.4.1</span> Use Pattern in Residual Plot</a></li>
<li><a class="nav-link" href="#box-cox-plot"><span class="header-section-number">5.4.2</span> Box-Cox Plot</a></li>
<li><a class="nav-link" href="#interpretation-with-log-transformed-response"><span class="header-section-number">5.4.3</span> Interpretation with Log Transformed Response</a></li>
</ul>
</li>
<li>
<a class="nav-link" href="#remedial-measures-linearization-transformations"><span class="header-section-number">5.5</span> Remedial Measures: Linearization Transformations</a><ul class="nav navbar-nav">
<li><a class="nav-link" href="#hierarchical-principle"><span class="header-section-number">5.5.1</span> Hierarchical Principle</a></li>
<li><a class="nav-link" href="#interpretation-with-log-transformed-predictor"><span class="header-section-number">5.5.2</span> Interpretation with Log Transformed Predictor</a></li>
<li><a class="nav-link" href="#interpretation-with-log-transformed-response-and-predictor"><span class="header-section-number">5.5.3</span> Interpretation with Log Transformed Response and Predictor</a></li>
<li><a class="nav-link" href="#some-general-comments-about-assessing-assumptions-and-transformations"><span class="header-section-number">5.5.4</span> Some General Comments about Assessing Assumptions and Transformations</a></li>
</ul>
</li>
</ul>

      <div class="book-extra">
        <ul class="list-unstyled">
          
        </ul>
</div>
    </nav>
</div>

</div>
</div> <!-- .container -->

<footer class="bg-primary text-light mt-5"><div class="container"><div class="row">

  <div class="col-12 col-md-6 mt-3">
    <p>"<strong>Linear Models for Data Science</strong>" was written by Jeffrey Woo. It was last built on 2024-07-09.</p>
  </div>

  <div class="col-12 col-md-6 mt-3">
    <p>This book was built by the <a class="text-light" href="https://bookdown.org">bookdown</a> R package.</p>
  </div>

</div></div>
</footer><!-- dynamically load mathjax for compatibility with self-contained --><script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.9/latest.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script><script type="text/x-mathjax-config">const popovers = document.querySelectorAll('a.footnote-ref[data-toggle="popover"]');
for (let popover of popovers) {
  const div = document.createElement('div');
  div.setAttribute('style', 'position: absolute; top: 0, left:0; width:0, height:0, overflow: hidden; visibility: hidden;');
  div.innerHTML = popover.getAttribute('data-content');

  var has_math = div.querySelector("span.math");
  if (has_math) {
    document.body.appendChild(div);
    MathJax.Hub.Queue(["Typeset", MathJax.Hub, div]);
    MathJax.Hub.Queue(function() {
      popover.setAttribute('data-content', div.innerHTML);
      document.body.removeChild(div);
    })
  }
}
</script>
</body>
</html>
