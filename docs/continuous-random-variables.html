<!DOCTYPE html>
<html lang="en">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<title>Chapter 4 Continuous Random Variables | Understanding Uncertainty Course Notes</title>
<meta name="author" content="Jeffrey Woo">
<meta name="description" content="This module is based on Introduction to Probability (Blitzstein, Hwang), Chapters 5 and 6. You can access the book for free at https://stat110.hsites.harvard.edu/ (and then click on Book). Please...">
<meta name="generator" content="bookdown 0.43 with bs4_book()">
<meta property="og:title" content="Chapter 4 Continuous Random Variables | Understanding Uncertainty Course Notes">
<meta property="og:type" content="book">
<meta property="og:description" content="This module is based on Introduction to Probability (Blitzstein, Hwang), Chapters 5 and 6. You can access the book for free at https://stat110.hsites.harvard.edu/ (and then click on Book). Please...">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="Chapter 4 Continuous Random Variables | Understanding Uncertainty Course Notes">
<meta name="twitter:description" content="This module is based on Introduction to Probability (Blitzstein, Hwang), Chapters 5 and 6. You can access the book for free at https://stat110.hsites.harvard.edu/ (and then click on Book). Please...">
<!-- JS --><script src="https://cdnjs.cloudflare.com/ajax/libs/clipboard.js/2.0.6/clipboard.min.js" integrity="sha256-inc5kl9MA1hkeYUt+EC3BhlIgyp/2jDIyBLS6k3UxPI=" crossorigin="anonymous"></script><script src="https://cdnjs.cloudflare.com/ajax/libs/fuse.js/6.4.6/fuse.js" integrity="sha512-zv6Ywkjyktsohkbp9bb45V6tEMoWhzFzXis+LrMehmJZZSys19Yxf1dopHx7WzIKxr5tK2dVcYmaCk2uqdjF4A==" crossorigin="anonymous"></script><script src="https://kit.fontawesome.com/6ecbd6c532.js" crossorigin="anonymous"></script><script src="libs/jquery-3.6.0/jquery-3.6.0.min.js"></script><meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<link href="libs/bootstrap-4.6.0/bootstrap.min.css" rel="stylesheet">
<script src="libs/bootstrap-4.6.0/bootstrap.bundle.min.js"></script><script src="libs/bs3compat-0.9.0/transition.js"></script><script src="libs/bs3compat-0.9.0/tabs.js"></script><script src="libs/bs3compat-0.9.0/bs3compat.js"></script><link href="libs/bs4_book-1.0.0/bs4_book.css" rel="stylesheet">
<script src="libs/bs4_book-1.0.0/bs4_book.js"></script><script src="https://cdnjs.cloudflare.com/ajax/libs/autocomplete.js/0.38.0/autocomplete.jquery.min.js" integrity="sha512-GU9ayf+66Xx2TmpxqJpliWbT5PiGYxpaG8rfnBEk1LL8l1KGkRShhngwdXK1UgqhAzWpZHSiYPc09/NwDQIGyg==" crossorigin="anonymous"></script><script src="https://cdnjs.cloudflare.com/ajax/libs/mark.js/8.11.1/mark.min.js" integrity="sha512-5CYOlHXGh6QpOFA/TeTylKLWfB3ftPsde7AnmhuitiTX4K5SqCLBeKro6sPS8ilsz1Q4NRx3v8Ko2IBiszzdww==" crossorigin="anonymous"></script><!-- CSS --><style type="text/css">
    
    div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
  </style>
</head>
<body data-spy="scroll" data-target="#toc">

<div class="container-fluid">
<div class="row">
  <header class="col-sm-12 col-lg-3 sidebar sidebar-book"><a class="sr-only sr-only-focusable" href="#content">Skip to main content</a>

    <div class="d-flex align-items-start justify-content-between">
      <h1>
        <a href="index.html" title="">Understanding Uncertainty Course Notes</a>
      </h1>
      <button class="btn btn-outline-primary d-lg-none ml-2 mt-1" type="button" data-toggle="collapse" data-target="#main-nav" aria-expanded="true" aria-controls="main-nav"><i class="fas fa-bars"></i><span class="sr-only">Show table of contents</span></button>
    </div>

    <div id="main-nav" class="collapse-lg">
      <form role="search">
        <input id="search" class="form-control" type="search" placeholder="Search" aria-label="Search">
</form>

      <nav aria-label="Table of contents"><h2>Table of contents</h2>
        <ul class="book-toc list-unstyled">
<li><a class="" href="index.html">Preface</a></li>
<li><a class="" href="descriptive.html"><span class="header-section-number">1</span> Descriptive Statistics</a></li>
<li><a class="" href="probability.html"><span class="header-section-number">2</span> Probability</a></li>
<li><a class="" href="discrete-random-variables.html"><span class="header-section-number">3</span> Discrete Random Variables</a></li>
<li><a class="active" href="continuous-random-variables.html"><span class="header-section-number">4</span> Continuous Random Variables</a></li>
</ul>

        <div class="book-extra">
          
        </div>
      </nav>
</div>
  </header><main class="col-sm-12 col-md-9 col-lg-7" id="content"><div id="continuous-random-variables" class="section level1" number="4">
<h1>
<span class="header-section-number">4</span> Continuous Random Variables<a class="anchor" aria-label="anchor" href="#continuous-random-variables"><i class="fas fa-link"></i></a>
</h1>
<p>This module is based on Introduction to Probability (Blitzstein, Hwang), Chapters 5 and 6. You can access the book for free at <a href="https://stat110.hsites.harvard.edu/" class="uri">https://stat110.hsites.harvard.edu/</a> (and then click on Book). Please note that I cover additional topics, and skip certain topics from the book. You may skip Sections ??? from the book.</p>
<div id="introduction" class="section level2" number="4.1">
<h2>
<span class="header-section-number">4.1</span> Introduction<a class="anchor" aria-label="anchor" href="#introduction"><i class="fas fa-link"></i></a>
</h2>
<p>In the previous module, we learned about discrete random variables. We learned how their distributions can be described by the PMFs and CDFs, how to find their expected values and variances, as well as common distributions for discrete random variables. We will learn about their counterparts when dealing with continuous random variables. The concepts are similar, but how they are computed can be quite different.</p>
<p>As a reminder:</p>
<ul>
<li>A <strong>discrete random variable</strong> can only take on a countable (finite or infinite) number of values.</li>
<li>A <strong>continuous random variable</strong> can take on an uncountable number of values in an interval of real numbers.</li>
</ul>
<p>For example, height of an American adult is a continuous random variable, as height can take on any value in interval between 40 and 100 inches. All values between 40 and 100 are possible. We cannot list all possible real numbers in this range as the list is never ending.</p>
<p>The sample space associated with a continuous random variable will be difficult to list, since it takes on an uncountable number of values. Using the example of heights of American adults, any real number between 40 and 100 inches is possible.</p>
<p>This is different from a discrete random variable where we would list the sample space, or support, and then find the probability associated with each value in the support.</p>
<p>Similar to discrete random variables, we want to describe the shape of the distribution, the centrality, and the spread of a continuous random distribution so we have an idea of probabilities associated with different ranges of values of the random variable.</p>
</div>
<div id="cumulative-distribution-functions-cdfs-1" class="section level2" number="4.2">
<h2>
<span class="header-section-number">4.2</span> Cumulative Distribution Functions (CDFs)<a class="anchor" aria-label="anchor" href="#cumulative-distribution-functions-cdfs-1"><i class="fas fa-link"></i></a>
</h2>
<p>We start by talking about the cumulative distribution function, as its definition applies to both discrete and continuous random variables. The CDF of a random variable <span class="math inline">\(X\)</span> is <span class="math inline">\(F_X(x) = P(X \leq x)\)</span>. The difference lies in how a CDF looks visually and how it is calculated.</p>
<p>Take a look at the CDF of a discrete random variable and the CDF of a continuous random variable below in Figure <a href="continuous-random-variables.html#fig:4-compare">4.1</a>:</p>
<div class="figure">
<span style="display:block;" id="fig:4-compare"></span>
<img src="bookdown-demo_files/figure-html/4-compare-1.png" alt="CDF for Discrete RV vs CDF for Continuous RV" width="672"><p class="caption">
Figure 4.1: CDF for Discrete RV vs CDF for Continuous RV
</p>
</div>
<p>As mentioned in the previous module, the CDF for a discrete random variable is what is called a step function, as it jumps at each value of the support. On the other hand, the CDF for a continuous random variable increases smoothly as its sample space is infinite.</p>
<p>The height of the CDF informs us the percentile associated with the value of the random variable. Looking at the CDF for the continuous random variable in Figure <a href="continuous-random-variables.html#fig:4-compare">4.1</a>, the height is 0.5 when the random variable is 0, so a value of 0 corresponds to the 50th percentile of this distribution.</p>
<p>The technical definition of a continuous random variable is: A random variable has a continuous distribution if its CDF is differentiable.</p>
<p>A discrete random variable fails in this definition since its derivative is undefined at the jumps.</p>
<div id="valid-cdfs-1" class="section level3" number="4.2.1">
<h3>
<span class="header-section-number">4.2.1</span> Valid CDFs<a class="anchor" aria-label="anchor" href="#valid-cdfs-1"><i class="fas fa-link"></i></a>
</h3>
<p>The criteria for a valid CDF is the same, it does not matter if the random variable is discrete or continuous:</p>
<ul>
<li>non decreasing. This means that as <span class="math inline">\(x\)</span> gets larger, the CDF either stays the same or increases. Visually, a graph of the CDF should never decreases as <span class="math inline">\(x\)</span> increases.</li>
<li>approach 1 as <span class="math inline">\(x\)</span> approaches infinity and approach 0 as <span class="math inline">\(x\)</span> approaches negative infinity. Visually, a graph of the CDF should be equal to or close to 1 for large values of x, and it should be equal to or close to 0 for small values of x.</li>
</ul>
<p><em>Thought question</em>: Look at the CDFs for our example in Figure <a href="continuous-random-variables.html#fig:4-compare">4.1</a>, and see how they satisfy the criteria listed above for a valid CDF.</p>
</div>
</div>
<div id="probability-density-functions-pdfs" class="section level2" number="4.3">
<h2>
<span class="header-section-number">4.3</span> Probability Density Functions (PDFs)<a class="anchor" aria-label="anchor" href="#probability-density-functions-pdfs"><i class="fas fa-link"></i></a>
</h2>
<p>The <strong>probability density function (PDF)</strong> of a continuous random variable is analogous to the PMF of a discrete random variable.</p>
<p>The definition of the PDF for continuous random variables is the following: for a continuous random variable <span class="math inline">\(X\)</span> with CDF <span class="math inline">\(F_X(x)\)</span>, the PDF of <span class="math inline">\(X\)</span>, <span class="math inline">\(f_X(x)\)</span>, is the derivative of its CDF, in other words, <span class="math inline">\(f_X(x) = F_X^{\prime}(x)\)</span>. The support of <span class="math inline">\(X\)</span> is the set of <span class="math inline">\(x\)</span> where <span class="math inline">\(f_X(x) &gt;0\)</span>.</p>
<p>The relationship between the PDF and CDF of a continuous random variable <span class="math inline">\(X\)</span> can be expressed as</p>
<p><span class="math display" id="eq:4-PDFCDF">\[\begin{equation}
F_X(x) = P(X \leq x) = \int_{-\infty}^{x} f_X(x) dx.
\tag{4.1}
\end{equation}\]</span></p>
<p>We take a look at an example below. Suppose we have a continuous random variable <span class="math inline">\(X\)</span> with its CDF and PDF displayed below, and we want to find <span class="math inline">\(P(X \leq 1)\)</span>:</p>
<div class="figure">
<span style="display:block;" id="fig:4-prob"></span>
<img src="bookdown-demo_files/figure-html/4-prob-1.png" alt="Probabilities from CDF and PDF" width="672"><p class="caption">
Figure 4.2: Probabilities from CDF and PDF
</p>
</div>
<p>We can find <span class="math inline">\(P(X \leq 1)\)</span> in two different ways:</p>
<ul>
<li>from the CDF, find the value of 1 on the horizontal axis, and read off the corresponding value on the vertical axis (blue lines). This tells us that <span class="math inline">\(P(X \leq 1) = 0.84\)</span>.</li>
<li>from the PDF, find the area under the PDF where <span class="math inline">\(X \leq 1\)</span>. This area corresponds to the shaded region in blue, and will be equal to 0.84 if we performed the integration per equation <a href="continuous-random-variables.html#eq:4-PDFCDF">(4.1)</a>.</li>
</ul>
<p>Compare equation <a href="continuous-random-variables.html#eq:4-PDFCDF">(4.1)</a> with equation <a href="discrete-random-variables.html#eq:3-CDF">(3.1)</a> and note the similarities and differences. For discrete CDFs, we sum the PMF over all values less than or equal to <span class="math inline">\(x\)</span>, whereas for continuous CDFs, we integrate, or accumulate the area, under the PDF over all values less than or equal to <span class="math inline">\(x\)</span>. Some people view the integral as a continuous version of a summation.</p>
<p>From equation <a href="continuous-random-variables.html#eq:4-PDFCDF">(4.1)</a>, we can generalize a way to find the probability <span class="math inline">\(P(a&lt;X&lt;b)\)</span> for a continuous random variable <span class="math inline">\(X\)</span>:</p>
<p><span class="math display" id="eq:4-integrate">\[\begin{equation}
P(a&lt;X&lt;b) = F_X(b) - F_X(a) = \int_{a}^{b} f_X(x) dx.
\tag{4.2}
\end{equation}\]</span></p>
<p>In other words, to find the probability for a range of values for <span class="math inline">\(X\)</span>, we just find the area under its PDF for that range of values. Going back to our example, if we want to find <span class="math inline">\(P(0&lt;X&lt;1)\)</span>, we will find the area under its PDF for <span class="math inline">\(0&lt;X&lt;1\)</span>, like in Figure <a href="continuous-random-variables.html#fig:4-prob2">4.3</a> below:</p>
<div class="figure">
<span style="display:block;" id="fig:4-prob2"></span>
<img src="bookdown-demo_files/figure-html/4-prob2-1.png" alt="Probabilities from PDF" width="672"><p class="caption">
Figure 4.3: Probabilities from PDF
</p>
</div>
<p>As mentioned, the PDF of a continuous random variable is analogous, but not exactly the same as, to the PMF of a discrete random variable. One common misconception is that the PDF tells us a probability, for example, that the value of <span class="math inline">\(f_X(2) = P(X=2)\)</span>, if <span class="math inline">\(X\)</span> is continuous. This is only correct if <span class="math inline">\(X\)</span> is discrete. In fact, if we look at equation <a href="continuous-random-variables.html#eq:4-integrate">(4.2)</a> a little more closely, <span class="math inline">\(P(X=c) = 0\)</span> if <span class="math inline">\(X\)</span> is continuous and <span class="math inline">\(c\)</span> is a constant, since the area under its PDF will be 0.</p>
<div id="valid-pdfs" class="section level3" number="4.3.1">
<h3>
<span class="header-section-number">4.3.1</span> Valid PDFs<a class="anchor" aria-label="anchor" href="#valid-pdfs"><i class="fas fa-link"></i></a>
</h3>
<p>The PDF of a continuous random variable must satisfy the following criteria:</p>
<ul>
<li>Non negative: <span class="math inline">\(f_X(x) \geq 0\)</span>,</li>
<li>Integrates to 1: <span class="math inline">\(\int_{-\infty}^{\infty}f_X(x) dx = 1\)</span>.</li>
</ul>
</div>
<div id="estimating-pdfs-with-data" class="section level3" number="4.3.2">
<h3>
<span class="header-section-number">4.3.2</span> Estimating PDFs with Data<a class="anchor" aria-label="anchor" href="#estimating-pdfs-with-data"><i class="fas fa-link"></i></a>
</h3>
</div>
</div>
<div id="summaries-of-a-distribution" class="section level2" number="4.4">
<h2>
<span class="header-section-number">4.4</span> Summaries of a Distribution<a class="anchor" aria-label="anchor" href="#summaries-of-a-distribution"><i class="fas fa-link"></i></a>
</h2>
<p>Next, we will talk about some common summaries associated with a distribution. These involve measures of centrality and variance, which we have covered before. We will also talk about a couple of other measures: skewness and kurtosis.</p>
<div id="expectations-1" class="section level3" number="4.4.1">
<h3>
<span class="header-section-number">4.4.1</span> Expectations<a class="anchor" aria-label="anchor" href="#expectations-1"><i class="fas fa-link"></i></a>
</h3>
<p>The <strong>expected value</strong> of a continuous random variable <span class="math inline">\(X\)</span> is</p>
<p><span class="math display" id="eq:4-EX">\[\begin{equation}
E(X) = \int_{-\infty}^{\infty} x f_X(x) dx.
\tag{4.3}
\end{equation}\]</span></p>
<p>Another common notation for <span class="math inline">\(E(X)\)</span> is <span class="math inline">\(\mu\)</span>, or sometimes <span class="math inline">\(\mu_X\)</span> show that we are writing the mean of the random variable <span class="math inline">\(X\)</span>.</p>
<p>If we compare equation <a href="continuous-random-variables.html#eq:4-EX">(4.3)</a> with equation <a href="discrete-random-variables.html#eq:3-EX">(3.2)</a>, we notice that we use an integral instead of a summation now that we are working with continuous random variables.</p>
<p>The interpretation of expected values is still the same: the expectation of a random variable can be interpreted as the long-run mean of the random variable, i.e. if we were able to repeat the experiment an infinite number of times, the expectation of the random variable will be the average result among all the experiments. It is still a measure of centrality of the random variable.</p>
<p>The <strong>linearity of expectations</strong> still hold in the same way, per equation <a href="discrete-random-variables.html#eq:3-linEX">(3.3)</a>. It does not matter if the random variable is discrete or continuous.</p>
<p>The <strong>Law of the Unconscious Statistician (LOTUS)</strong> also still applies. For a continuous random variable <span class="math inline">\(X\)</span>, it is (unsurprisingly):</p>
<p><span class="math display" id="eq:4-lotus">\[\begin{equation}
E(g(X)) = \int_{-\infty}^{\infty} g(x) f_X(x).
\tag{4.4}
\end{equation}\]</span></p>
<p>Notice again when we compare equation <a href="continuous-random-variables.html#eq:4-lotus">(4.4)</a> with its discrete counterpart in equation <a href="discrete-random-variables.html#eq:3-lotus">(3.4)</a>: we have just replaced the summation with an integral.</p>
<p><em>Thought question</em>: Can you guess how to write the equation for the variance of a continuous random variable? Hint: Equation <a href="discrete-random-variables.html#eq:3-var2">(3.5)</a> is how to find the variance for a discrete random variable.</p>
<div id="median-1" class="section level4" number="4.4.1.1">
<h4>
<span class="header-section-number">4.4.1.1</span> Median<a class="anchor" aria-label="anchor" href="#median-1"><i class="fas fa-link"></i></a>
</h4>
<p>The value <span class="math inline">\(m\)</span> is the <strong>median</strong> of a random variable <span class="math inline">\(X\)</span> if <span class="math inline">\(P(X \leq c) \geq \frac{1}{2}\)</span> and <span class="math inline">\(P(X \geq c) \geq \frac{1}{2}\)</span>.</p>
<p>Intuitively, the median is the value <span class="math inline">\(m\)</span> which splits the area under the PDF into half (or as close to half as possible if the random variable is discrete). Half the area will be to the left of <span class="math inline">\(m\)</span>, the other half of the area will be to the right of <span class="math inline">\(m\)</span>.</p>
</div>
<div id="mode-1" class="section level4" number="4.4.1.2">
<h4>
<span class="header-section-number">4.4.1.2</span> Mode<a class="anchor" aria-label="anchor" href="#mode-1"><i class="fas fa-link"></i></a>
</h4>
<p>For a continuous random variable <span class="math inline">\(X\)</span>, the mode is the value <span class="math inline">\(c\)</span> that maximizes the PDF: <span class="math inline">\(f_X(c) \geq f_X(x)\)</span> for all <span class="math inline">\(x\)</span>.</p>
<p>For a discrete random variable <span class="math inline">\(X\)</span>, the mode is the value <span class="math inline">\(c\)</span> that maximizes the PMF: <span class="math inline">\(P(X=c) \geq P(X=x)\)</span> for all <span class="math inline">\(x\)</span>. Intuitively, the mode is the most commonly occurring value of a discrete random variable</p>
</div>
<div id="loss-functions" class="section level4" number="4.4.1.3">
<h4>
<span class="header-section-number">4.4.1.3</span> Loss Functions<a class="anchor" aria-label="anchor" href="#loss-functions"><i class="fas fa-link"></i></a>
</h4>
<p>A goal of statistical modeling is to use the model to make predictions. We want to be able to quantify the quality of our prediction, or the prediction error. Consider we have an experiment that can be described by a random variable <span class="math inline">\(X\)</span>, and we want to predict the value of the next experiment. The mean and median are natural guesses for the value of the next experiment.</p>
<p>It turns out there a several ways to quantify our prediction error. These are usually called loss functions. Suppose our predicted value is denoted by <span class="math inline">\(x_{pred}\)</span>. A couple of common loss functions:</p>
<ul>
<li>
<strong>Mean squared error (MSE)</strong>: <span class="math inline">\(E(X-x_{pred})^2\)</span>,</li>
<li>
<strong>Mean absolute error (MAE)</strong>: <span class="math inline">\(E|X-x_{pred}|\)</span>.</li>
</ul>
<p>It turns out that the expected value <span class="math inline">\(E(X)\)</span> minimizes the MSE, and the median minimizes the MAE. So depending on what loss function suits our analysis, we could use either the mean or median for our predictions. We will cover these ideas in more detail in a later module (and indeed in later courses in this program).</p>
</div>
</div>
<div id="variance-1" class="section level3" number="4.4.2">
<h3>
<span class="header-section-number">4.4.2</span> Variance<a class="anchor" aria-label="anchor" href="#variance-1"><i class="fas fa-link"></i></a>
</h3>
<p>The <strong>variance</strong> of a continuous random variable <span class="math inline">\(X\)</span> is</p>
<p><span class="math display" id="eq:4-var">\[\begin{equation}
Var(X) = \int_{-\infty}^{\infty} (x-\mu)^2 f_X(x) dx.
\tag{4.5}
\end{equation}\]</span></p>
<p>The properties of variance is still the same as in Section <a href="discrete-random-variables.html#var-prop">3.4.3.1</a>. It does not matter if the random variable is discrete or continuous. A common notation used for variance is <span class="math inline">\(\sigma^2\)</span>, or sometimes <span class="math inline">\(\sigma_X^2\)</span> to show its the variance of the random variable <span class="math inline">\(X\)</span>.</p>
</div>
<div id="moments" class="section level3" number="4.4.3">
<h3>
<span class="header-section-number">4.4.3</span> Moments<a class="anchor" aria-label="anchor" href="#moments"><i class="fas fa-link"></i></a>
</h3>
<p>Before talking about measures that is used to describe distributions, we will cover some terminology that is used for these measures. Suppose we have a random variable <span class="math inline">\(X\)</span>.</p>
<ul>
<li>The <strong><span class="math inline">\(n\)</span>th moment</strong> of <span class="math inline">\(X\)</span> is <span class="math inline">\(E(X^n)\)</span>. So the expected value, or the mean, is sometimes called the first moment.</li>
<li>The <strong><span class="math inline">\(n\)</span>th central moment</strong> of <span class="math inline">\(X\)</span> is <span class="math inline">\(E((X-\mu)^n)\)</span>. So the variance is sometimes called the second central moment.</li>
<li>The <strong><span class="math inline">\(n\)</span> standardized moment</strong> of <span class="math inline">\(X\)</span> is <span class="math inline">\(E(\frac{(X-\mu)^n}{\sigma})\)</span>.</li>
</ul>
</div>
<div id="skewness" class="section level3" number="4.4.4">
<h3>
<span class="header-section-number">4.4.4</span> Skewness<a class="anchor" aria-label="anchor" href="#skewness"><i class="fas fa-link"></i></a>
</h3>
<p>One measure that is used to describe the shape of a distribution is skewness, which is a measure of symmetry (or measure of skewness). The <strong>skew</strong> of a random variable <span class="math inline">\(X\)</span> is the third standardized moment:</p>
<p><span class="math display" id="eq:4-skew">\[\begin{equation}
Skew(X) = E \left(\frac{(X-\mu)^3}{\sigma} \right)
\tag{4.6}
\end{equation}\]</span></p>
<p>A random variable <span class="math inline">\(X\)</span> has a **symmetric distribution about * if <span class="math inline">\(X - \mu\)</span> has the same distribution as <span class="math inline">\(\mu - X\)</span>. Fairly often, people will just say that <span class="math inline">\(X\)</span> is symmetric; it is almost always assumed that the symmetry is about its mean.</p>
<p>Intuitively, symmetry means that the PDF of <span class="math inline">\(X\)</span> to the left of its mean is the mirror image of the PDF of <span class="math inline">\(X\)</span> to the right of its mean. We look at a couple of examples below in Figure <a href="continuous-random-variables.html#fig:4-symm">4.4</a>:</p>
<div class="figure">
<span style="display:block;" id="fig:4-symm"></span>
<img src="bookdown-demo_files/figure-html/4-symm-1.png" alt="PDFs for Symmetric RV vs Skewed RV" width="672"><p class="caption">
Figure 4.4: PDFs for Symmetric RV vs Skewed RV
</p>
</div>
<p>The blue vertical lines indicate the mean of these distributions. Notice the mirror image in the first plot, but not in the second plot.</p>
<p>If a distribution is not symmetric, we can say its distribution is asymmetric, or is skewed. The values of <span class="math inline">\(Skew(X)\)</span> that are associated with different shapes are:</p>
<ul>
<li>
<span class="math inline">\(Skew(X) = 0\)</span>: <span class="math inline">\(X\)</span> is symmetric.</li>
<li>
<span class="math inline">\(Skew(X) &gt; 0\)</span>: <span class="math inline">\(X\)</span> is right skewed.</li>
<li>
<span class="math inline">\(Skew(X) &lt; 0\)</span>: <span class="math inline">\(X\)</span> is left skewed.</li>
</ul>
</div>
<div id="kurtosis" class="section level3" number="4.4.5">
<h3>
<span class="header-section-number">4.4.5</span> Kurtosis<a class="anchor" aria-label="anchor" href="#kurtosis"><i class="fas fa-link"></i></a>
</h3>
<p>One more measure deals with the <strong>tail</strong> behavior of a distribution. Visually, the tails of a PDF are associated with probabilities of extreme values for a random variable. A distribution that is heavy tailed means that extreme values (on both ends) are more likely to occur. Tail behavior is an important consideration in risk management in finance: e.g. a heavy left tail in the PDF could mean a financial crisis. Figure <a href="continuous-random-variables.html#fig:4-kurt">4.5</a> shows an example of a heavy tailed distribution (in blue), compared to a Gaussian distribution (in black). We will talk more about the Gaussian distribution in the next subsection.</p>
<div class="figure">
<span style="display:block;" id="fig:4-kurt"></span>
<img src="bookdown-demo_files/figure-html/4-kurt-1.png" alt="PDF for Heavy Tailed Distribution" width="672"><p class="caption">
Figure 4.5: PDF for Heavy Tailed Distribution
</p>
</div>
<p>A common measure of tail behavior is the <strong>Kurtosis</strong>. The kurtosis of a random variable <span class="math inline">\(X\)</span> is the shifted fourth standardized moment:</p>
<p><span class="math display" id="eq:4-kurt">\[\begin{equation}
Kurt(X) = E \left(\frac{(X-\mu)^4}{\sigma} \right) - 3.
\tag{4.7}
\end{equation}\]</span></p>
<p>The reason for subtracting (or shifting by) 3 is so that the Gaussian distribution (a commonly used distribution for continuous random variables) is 0. Note: Some people call equation <a href="continuous-random-variables.html#eq:4-kurt">(4.7)</a> the <strong>excess kurtosis</strong> and the kurtosis does not subtract the 3.</p>
<p>The values of <span class="math inline">\(Kurt(X)\)</span> that are associated with tail behaviors are:</p>
<ul>
<li>
<span class="math inline">\(Kurt(X) = 0\)</span>: <span class="math inline">\(X\)</span> is similar tails to Gaussian distribution.</li>
<li>
<span class="math inline">\(Kurt(X) &gt; 0\)</span>: <span class="math inline">\(X\)</span> has heavier tails compared to Gaussian distribution (extreme values more likely).</li>
<li>
<span class="math inline">\(SKurt(X) &lt; 0\)</span>: <span class="math inline">\(X\)</span> has smaller tails compared to Gaussian distribution (extreme values less likely).</li>
</ul>
</div>
</div>
<div id="common-continuous-random-variables" class="section level2" number="4.5">
<h2>
<span class="header-section-number">4.5</span> Common Continuous Random Variables<a class="anchor" aria-label="anchor" href="#common-continuous-random-variables"><i class="fas fa-link"></i></a>
</h2>
<div id="uniform" class="section level3" number="4.5.1">
<h3>
<span class="header-section-number">4.5.1</span> Uniform<a class="anchor" aria-label="anchor" href="#uniform"><i class="fas fa-link"></i></a>
</h3>
<div id="universality-of-uniform" class="section level4" number="4.5.1.1">
<h4>
<span class="header-section-number">4.5.1.1</span> Universality of Uniform<a class="anchor" aria-label="anchor" href="#universality-of-uniform"><i class="fas fa-link"></i></a>
</h4>
</div>
</div>
<div id="normal" class="section level3" number="4.5.2">
<h3>
<span class="header-section-number">4.5.2</span> Normal<a class="anchor" aria-label="anchor" href="#normal"><i class="fas fa-link"></i></a>
</h3>

</div>
</div>
</div>
  <div class="chapter-nav">
<div class="prev"><a href="discrete-random-variables.html"><span class="header-section-number">3</span> Discrete Random Variables</a></div>
<div class="empty"></div>
</div></main><div class="col-md-3 col-lg-2 d-none d-md-block sidebar sidebar-chapter">
    <nav id="toc" data-toggle="toc" aria-label="On this page"><h2>On this page</h2>
      <ul class="nav navbar-nav">
<li><a class="nav-link" href="#continuous-random-variables"><span class="header-section-number">4</span> Continuous Random Variables</a></li>
<li><a class="nav-link" href="#introduction"><span class="header-section-number">4.1</span> Introduction</a></li>
<li>
<a class="nav-link" href="#cumulative-distribution-functions-cdfs-1"><span class="header-section-number">4.2</span> Cumulative Distribution Functions (CDFs)</a><ul class="nav navbar-nav"><li><a class="nav-link" href="#valid-cdfs-1"><span class="header-section-number">4.2.1</span> Valid CDFs</a></li></ul>
</li>
<li>
<a class="nav-link" href="#probability-density-functions-pdfs"><span class="header-section-number">4.3</span> Probability Density Functions (PDFs)</a><ul class="nav navbar-nav">
<li><a class="nav-link" href="#valid-pdfs"><span class="header-section-number">4.3.1</span> Valid PDFs</a></li>
<li><a class="nav-link" href="#estimating-pdfs-with-data"><span class="header-section-number">4.3.2</span> Estimating PDFs with Data</a></li>
</ul>
</li>
<li>
<a class="nav-link" href="#summaries-of-a-distribution"><span class="header-section-number">4.4</span> Summaries of a Distribution</a><ul class="nav navbar-nav">
<li><a class="nav-link" href="#expectations-1"><span class="header-section-number">4.4.1</span> Expectations</a></li>
<li><a class="nav-link" href="#variance-1"><span class="header-section-number">4.4.2</span> Variance</a></li>
<li><a class="nav-link" href="#moments"><span class="header-section-number">4.4.3</span> Moments</a></li>
<li><a class="nav-link" href="#skewness"><span class="header-section-number">4.4.4</span> Skewness</a></li>
<li><a class="nav-link" href="#kurtosis"><span class="header-section-number">4.4.5</span> Kurtosis</a></li>
</ul>
</li>
<li>
<a class="nav-link" href="#common-continuous-random-variables"><span class="header-section-number">4.5</span> Common Continuous Random Variables</a><ul class="nav navbar-nav">
<li><a class="nav-link" href="#uniform"><span class="header-section-number">4.5.1</span> Uniform</a></li>
<li><a class="nav-link" href="#normal"><span class="header-section-number">4.5.2</span> Normal</a></li>
</ul>
</li>
</ul>

      <div class="book-extra">
        <ul class="list-unstyled">
          
        </ul>
</div>
    </nav>
</div>

</div>
</div> <!-- .container -->

<footer class="bg-primary text-light mt-5"><div class="container"><div class="row">

  <div class="col-12 col-md-6 mt-3">
    <p>"<strong>Understanding Uncertainty Course Notes</strong>" was written by Jeffrey Woo. It was last built on 2025-06-13.</p>
  </div>

  <div class="col-12 col-md-6 mt-3">
    <p>This book was built by the <a class="text-light" href="https://bookdown.org">bookdown</a> R package.</p>
  </div>

</div></div>
</footer><!-- dynamically load mathjax for compatibility with self-contained --><script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.9/latest.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script><script type="text/x-mathjax-config">const popovers = document.querySelectorAll('a.footnote-ref[data-toggle="popover"]');
for (let popover of popovers) {
  const div = document.createElement('div');
  div.setAttribute('style', 'position: absolute; top: 0, left:0; width:0, height:0, overflow: hidden; visibility: hidden;');
  div.innerHTML = popover.getAttribute('data-content');

  var has_math = div.querySelector("span.math");
  if (has_math) {
    document.body.appendChild(div);
    MathJax.Hub.Queue(["Typeset", MathJax.Hub, div]);
    MathJax.Hub.Queue(function() {
      popover.setAttribute('data-content', div.innerHTML);
      document.body.removeChild(div);
    })
  }
}
</script>
</body>
</html>
