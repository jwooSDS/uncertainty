<!DOCTYPE html>
<html lang="en">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<title>Chapter 4 Continuous Random Variables | Understanding Uncertainty Course Notes</title>
<meta name="author" content="Jeffrey Woo">
<meta name="description" content="This module is based on Introduction to Probability (Blitzstein, Hwang), Chapters 5 and 6. You can access the book for free at https://stat110.hsites.harvard.edu/ (and then click on Book). Please...">
<meta name="generator" content="bookdown 0.43 with bs4_book()">
<meta property="og:title" content="Chapter 4 Continuous Random Variables | Understanding Uncertainty Course Notes">
<meta property="og:type" content="book">
<meta property="og:description" content="This module is based on Introduction to Probability (Blitzstein, Hwang), Chapters 5 and 6. You can access the book for free at https://stat110.hsites.harvard.edu/ (and then click on Book). Please...">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="Chapter 4 Continuous Random Variables | Understanding Uncertainty Course Notes">
<meta name="twitter:description" content="This module is based on Introduction to Probability (Blitzstein, Hwang), Chapters 5 and 6. You can access the book for free at https://stat110.hsites.harvard.edu/ (and then click on Book). Please...">
<!-- JS --><script src="https://cdnjs.cloudflare.com/ajax/libs/clipboard.js/2.0.6/clipboard.min.js" integrity="sha256-inc5kl9MA1hkeYUt+EC3BhlIgyp/2jDIyBLS6k3UxPI=" crossorigin="anonymous"></script><script src="https://cdnjs.cloudflare.com/ajax/libs/fuse.js/6.4.6/fuse.js" integrity="sha512-zv6Ywkjyktsohkbp9bb45V6tEMoWhzFzXis+LrMehmJZZSys19Yxf1dopHx7WzIKxr5tK2dVcYmaCk2uqdjF4A==" crossorigin="anonymous"></script><script src="https://kit.fontawesome.com/6ecbd6c532.js" crossorigin="anonymous"></script><script src="libs/jquery-3.6.0/jquery-3.6.0.min.js"></script><meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<link href="libs/bootstrap-4.6.0/bootstrap.min.css" rel="stylesheet">
<script src="libs/bootstrap-4.6.0/bootstrap.bundle.min.js"></script><script src="libs/bs3compat-0.9.0/transition.js"></script><script src="libs/bs3compat-0.9.0/tabs.js"></script><script src="libs/bs3compat-0.9.0/bs3compat.js"></script><link href="libs/bs4_book-1.0.0/bs4_book.css" rel="stylesheet">
<script src="libs/bs4_book-1.0.0/bs4_book.js"></script><script src="https://cdnjs.cloudflare.com/ajax/libs/autocomplete.js/0.38.0/autocomplete.jquery.min.js" integrity="sha512-GU9ayf+66Xx2TmpxqJpliWbT5PiGYxpaG8rfnBEk1LL8l1KGkRShhngwdXK1UgqhAzWpZHSiYPc09/NwDQIGyg==" crossorigin="anonymous"></script><script src="https://cdnjs.cloudflare.com/ajax/libs/mark.js/8.11.1/mark.min.js" integrity="sha512-5CYOlHXGh6QpOFA/TeTylKLWfB3ftPsde7AnmhuitiTX4K5SqCLBeKro6sPS8ilsz1Q4NRx3v8Ko2IBiszzdww==" crossorigin="anonymous"></script><!-- CSS --><style type="text/css">
    
    div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
  </style>
</head>
<body data-spy="scroll" data-target="#toc">

<div class="container-fluid">
<div class="row">
  <header class="col-sm-12 col-lg-3 sidebar sidebar-book"><a class="sr-only sr-only-focusable" href="#content">Skip to main content</a>

    <div class="d-flex align-items-start justify-content-between">
      <h1>
        <a href="index.html" title="">Understanding Uncertainty Course Notes</a>
      </h1>
      <button class="btn btn-outline-primary d-lg-none ml-2 mt-1" type="button" data-toggle="collapse" data-target="#main-nav" aria-expanded="true" aria-controls="main-nav"><i class="fas fa-bars"></i><span class="sr-only">Show table of contents</span></button>
    </div>

    <div id="main-nav" class="collapse-lg">
      <form role="search">
        <input id="search" class="form-control" type="search" placeholder="Search" aria-label="Search">
</form>

      <nav aria-label="Table of contents"><h2>Table of contents</h2>
        <ul class="book-toc list-unstyled">
<li><a class="" href="index.html">Preface</a></li>
<li><a class="" href="descriptive.html"><span class="header-section-number">1</span> Descriptive Statistics</a></li>
<li><a class="" href="probability.html"><span class="header-section-number">2</span> Probability</a></li>
<li><a class="" href="discrete-random-variables.html"><span class="header-section-number">3</span> Discrete Random Variables</a></li>
<li><a class="active" href="continuous-random-variables.html"><span class="header-section-number">4</span> Continuous Random Variables</a></li>
<li><a class="" href="joint-distributions.html"><span class="header-section-number">5</span> Joint Distributions</a></li>
<li><a class="" href="inequalities-limit-theorems-and-simulations.html"><span class="header-section-number">6</span> Inequalities, Limit Theorems, and Simulations</a></li>
</ul>

        <div class="book-extra">
          
        </div>
      </nav>
</div>
  </header><main class="col-sm-12 col-md-9 col-lg-7" id="content"><div id="continuous-random-variables" class="section level1" number="4">
<h1>
<span class="header-section-number">4</span> Continuous Random Variables<a class="anchor" aria-label="anchor" href="#continuous-random-variables"><i class="fas fa-link"></i></a>
</h1>
<p>This module is based on Introduction to Probability (Blitzstein, Hwang), Chapters 5 and 6. You can access the book for free at <a href="https://stat110.hsites.harvard.edu/" class="uri">https://stat110.hsites.harvard.edu/</a> (and then click on Book). Please note that I cover additional topics, and skip certain topics from the book. You may skip Examples 5.1.6, 5.1.7, Proposition 5.2.3, Example 5.2.4, Sections 5.2.6, 5.2.7, Definition 5.3.7, Theorem 5.3.8, Example 5.4.7, Sections 5.5, 5.6, 5.7, Proposition 6.2.5, 6.2.6, Theorem 6.3.4, Sections 6.4 to 6.7 from the book.</p>
<div id="introduction" class="section level2" number="4.1">
<h2>
<span class="header-section-number">4.1</span> Introduction<a class="anchor" aria-label="anchor" href="#introduction"><i class="fas fa-link"></i></a>
</h2>
<p>In the previous module, we learned about discrete random variables. We learned how their distributions can be described by the PMFs and CDFs, how to find their expected values and variances, as well as common distributions for discrete random variables. We will learn about their counterparts when dealing with continuous random variables. The concepts are similar, but how they are computed can be quite different.</p>
<p>As a reminder:</p>
<ul>
<li>A <strong>discrete random variable</strong> can only take on a countable (finite or infinite) number of values.</li>
<li>A <strong>continuous random variable</strong> can take on an uncountable number of values in an interval of real numbers.</li>
</ul>
<p>For example, height of an American adult is a continuous random variable, as height can take on any value in interval between 40 and 100 inches. All values between 40 and 100 are possible. We cannot list all possible real numbers in this range as the list is never ending.</p>
<p>The sample space associated with a continuous random variable will be difficult to list, since it takes on an uncountable number of values. Using the example of heights of American adults, any real number between 40 and 100 inches is possible.</p>
<p>This is different from a discrete random variable where we would list the sample space, or support, and then find the probability associated with each value in the support.</p>
<p>Similar to discrete random variables, we want to describe the shape of the distribution, the centrality, and the spread of a continuous random distribution so we have an idea of probabilities associated with different ranges of values of the random variable.</p>
</div>
<div id="cumulative-distribution-functions-cdfs-1" class="section level2" number="4.2">
<h2>
<span class="header-section-number">4.2</span> Cumulative Distribution Functions (CDFs)<a class="anchor" aria-label="anchor" href="#cumulative-distribution-functions-cdfs-1"><i class="fas fa-link"></i></a>
</h2>
<p>We start by talking about the cumulative distribution function, as its definition applies to both discrete and continuous random variables. The CDF of a random variable <span class="math inline">\(X\)</span> is <span class="math inline">\(F_X(x) = P(X \leq x)\)</span>. The difference lies in how a CDF looks visually.</p>
<p>Take a look at the CDF of a discrete random variable and the CDF of a continuous random variable below in Figure <a href="continuous-random-variables.html#fig:4-compare">4.1</a>:</p>
<div class="figure">
<span style="display:block;" id="fig:4-compare"></span>
<img src="bookdown-demo_files/figure-html/4-compare-1.png" alt="CDF for Discrete RV vs CDF for Continuous RV" width="672"><p class="caption">
Figure 4.1: CDF for Discrete RV vs CDF for Continuous RV
</p>
</div>
<p>As mentioned in the previous module, the CDF for a discrete random variable is what is called a step function, as it jumps at each value of the support. On the other hand, the CDF for a continuous random variable increases smoothly as its sample space is infinite.</p>
<p>The height of the CDF informs us the percentile associated with the value of the random variable. Looking at the CDF for the continuous random variable in Figure <a href="continuous-random-variables.html#fig:4-compare">4.1</a>, the height is 0.5 when the random variable is 0, so a value of 0 corresponds to the 50th percentile of this distribution.</p>
<p>The technical definition of a continuous random variable is: A random variable has a continuous distribution if its CDF is differentiable.</p>
<p>A discrete random variable fails in this definition since its derivative is undefined at the jumps.</p>
<div id="valid-cdfs-1" class="section level3" number="4.2.1">
<h3>
<span class="header-section-number">4.2.1</span> Valid CDFs<a class="anchor" aria-label="anchor" href="#valid-cdfs-1"><i class="fas fa-link"></i></a>
</h3>
<p>The criteria for a valid CDF is the same, it does not matter if the random variable is discrete or continuous:</p>
<ul>
<li>non decreasing. This means that as <span class="math inline">\(x\)</span> gets larger, the CDF either stays the same or increases. Visually, a graph of the CDF never decreases as <span class="math inline">\(x\)</span> increases.</li>
<li>approach 1 as <span class="math inline">\(x\)</span> approaches infinity and approach 0 as <span class="math inline">\(x\)</span> approaches negative infinity. Visually, a graph of the CDF should be equal to or close to 1 for large values of x, and it should be equal to or close to 0 for small values of x.</li>
</ul>
<p><em>Thought question</em>: Look at the CDFs for our example in Figure <a href="continuous-random-variables.html#fig:4-compare">4.1</a>, and see how they satisfy the criteria listed above for a valid CDF.</p>
</div>
</div>
<div id="probability-density-functions-pdfs" class="section level2" number="4.3">
<h2>
<span class="header-section-number">4.3</span> Probability Density Functions (PDFs)<a class="anchor" aria-label="anchor" href="#probability-density-functions-pdfs"><i class="fas fa-link"></i></a>
</h2>
<p>The <strong>probability density function (PDF)</strong> of a continuous random variable is analogous to the PMF of a discrete random variable.</p>
<p>The definition of the PDF for continuous random variables is the following: for a continuous random variable <span class="math inline">\(X\)</span> with CDF <span class="math inline">\(F_X(x)\)</span>, the PDF of <span class="math inline">\(X\)</span>, <span class="math inline">\(f_X(x)\)</span>, is the derivative of its CDF, in other words, <span class="math inline">\(f_X(x) = F_X^{\prime}(x)\)</span>. The support of <span class="math inline">\(X\)</span> is the set of <span class="math inline">\(x\)</span> where <span class="math inline">\(f_X(x) &gt;0\)</span>.</p>
<p>The relationship between the PDF and CDF of a continuous random variable <span class="math inline">\(X\)</span> can be expressed as</p>
<p><span class="math display" id="eq:4-PDFCDF">\[\begin{equation}
F_X(x) = P(X \leq x) = \int_{-\infty}^{x} f_X(x) dx.
\tag{4.1}
\end{equation}\]</span></p>
<p>We take a look at an example below. Suppose we have a continuous random variable <span class="math inline">\(X\)</span> with its CDF and PDF displayed below, and we want to find <span class="math inline">\(P(X \leq 1)\)</span>:</p>
<div class="figure">
<span style="display:block;" id="fig:4-prob"></span>
<img src="bookdown-demo_files/figure-html/4-prob-1.png" alt="Probabilities from CDF and PDF" width="672"><p class="caption">
Figure 4.2: Probabilities from CDF and PDF
</p>
</div>
<p>We can find <span class="math inline">\(P(X \leq 1)\)</span> in two different ways:</p>
<ul>
<li>from the CDF, find the value of 1 on the horizontal axis, and read off the corresponding value on the vertical axis (blue lines). This tells us that <span class="math inline">\(P(X \leq 1) = 0.84\)</span>.</li>
<li>from the PDF, find the area under the PDF where <span class="math inline">\(X \leq 1\)</span>. This area corresponds to the shaded region in blue, and will be equal to 0.84 if we performed the integration per equation <a href="continuous-random-variables.html#eq:4-PDFCDF">(4.1)</a>.</li>
</ul>
<p>Compare equation <a href="continuous-random-variables.html#eq:4-PDFCDF">(4.1)</a> with equation <a href="discrete-random-variables.html#eq:3-CDF">(3.1)</a> and note the similarities and differences. For discrete CDFs, we sum the PMF over all values less than or equal to <span class="math inline">\(x\)</span>, whereas for continuous CDFs, we integrate, or accumulate the area, under the PDF over all values less than or equal to <span class="math inline">\(x\)</span>. Some people view the integral as a continuous version of a summation.</p>
<p>From equation <a href="continuous-random-variables.html#eq:4-PDFCDF">(4.1)</a>, we can generalize a way to find the probability <span class="math inline">\(P(a&lt;X&lt;b)\)</span> for a continuous random variable <span class="math inline">\(X\)</span>:</p>
<p><span class="math display" id="eq:4-integrate">\[\begin{equation}
P(a&lt;X&lt;b) = F_X(b) - F_X(a) = \int_{a}^{b} f_X(x) dx.
\tag{4.2}
\end{equation}\]</span></p>
<p>In other words, to find the probability for a range of values for <span class="math inline">\(X\)</span>, we just find the area under its PDF for that range of values. Going back to our example, if we want to find <span class="math inline">\(P(0&lt;X&lt;1)\)</span>, we will find the area under its PDF for <span class="math inline">\(0&lt;X&lt;1\)</span>, like in Figure <a href="continuous-random-variables.html#fig:4-prob2">4.3</a> below:</p>
<div class="figure">
<span style="display:block;" id="fig:4-prob2"></span>
<img src="bookdown-demo_files/figure-html/4-prob2-1.png" alt="Probabilities from PDF" width="672"><p class="caption">
Figure 4.3: Probabilities from PDF
</p>
</div>
<p>As mentioned, the PDF of a continuous random variable is analogous, but not exactly the same as, to the PMF of a discrete random variable. One common misconception is that the PDF tells us a probability, for example, that the value of <span class="math inline">\(f_X(2) = P(X=2)\)</span>, if <span class="math inline">\(X\)</span> is continuous. This is only correct if <span class="math inline">\(X\)</span> is discrete. In fact, if we look at equation <a href="continuous-random-variables.html#eq:4-integrate">(4.2)</a> a little more closely, <span class="math inline">\(P(X=c) = 0\)</span> if <span class="math inline">\(X\)</span> is continuous and <span class="math inline">\(c\)</span> is a constant, since the area under its PDF will be 0.</p>
<div id="valid-pdfs" class="section level3" number="4.3.1">
<h3>
<span class="header-section-number">4.3.1</span> Valid PDFs<a class="anchor" aria-label="anchor" href="#valid-pdfs"><i class="fas fa-link"></i></a>
</h3>
<p>The PDF of a continuous random variable must satisfy the following criteria:</p>
<ul>
<li>Non negative: <span class="math inline">\(f_X(x) \geq 0\)</span>,</li>
<li>Integrates to 1: <span class="math inline">\(\int_{-\infty}^{\infty}f_X(x) dx = 1\)</span>.</li>
</ul>
</div>
<div id="pdfs-and-density-plots" class="section level3" number="4.3.2">
<h3>
<span class="header-section-number">4.3.2</span> PDFs and Density Plots<a class="anchor" aria-label="anchor" href="#pdfs-and-density-plots"><i class="fas fa-link"></i></a>
</h3>
<p>Recall in Section <a href="discrete-random-variables.html#pmfhist">3.2.2</a>, we learned that for discrete random variables, the PMF and histogram are related. The PMF represents the long-run proportion, while the histogram represents the relative frequency based on our data. As the sample size gets larger, the PMF should match the histogram.</p>
<p>Similarly for continuous random variables, the PDF and the density plot are related. The PDF is associated with the distribution of a known random variable, while the density plot is estimated from our data, and if our data follows a known random variable, the PDF should match the density plot as the sample size gets larger.</p>
<p>We will go over some of the details on how density plots are created at the end of this module, in Section <a href="continuous-random-variables.html#KDE">4.6.1</a>, as we still need to cover a bit more concepts.</p>
</div>
</div>
<div id="summaries-of-a-distribution" class="section level2" number="4.4">
<h2>
<span class="header-section-number">4.4</span> Summaries of a Distribution<a class="anchor" aria-label="anchor" href="#summaries-of-a-distribution"><i class="fas fa-link"></i></a>
</h2>
<p>Next, we will talk about some common summaries associated with a distribution. These involve measures of centrality and variance, which we have covered before. We will also talk about a couple of other measures: skewness and kurtosis.</p>
<div id="expectations-1" class="section level3" number="4.4.1">
<h3>
<span class="header-section-number">4.4.1</span> Expectations<a class="anchor" aria-label="anchor" href="#expectations-1"><i class="fas fa-link"></i></a>
</h3>
<p>The <strong>expected value</strong> of a continuous random variable <span class="math inline">\(X\)</span> is</p>
<p><span class="math display" id="eq:4-EX">\[\begin{equation}
E(X) = \int_{-\infty}^{\infty} x f_X(x) dx.
\tag{4.3}
\end{equation}\]</span></p>
<p>Another common notation for <span class="math inline">\(E(X)\)</span> is <span class="math inline">\(\mu\)</span>, or sometimes <span class="math inline">\(\mu_X\)</span> show that we are writing the mean of the random variable <span class="math inline">\(X\)</span>.</p>
<p>If we compare equation <a href="continuous-random-variables.html#eq:4-EX">(4.3)</a> with equation <a href="discrete-random-variables.html#eq:3-EX">(3.2)</a>, we notice that we use an integral instead of a summation now that we are working with continuous random variables.</p>
<p>The interpretation of expected values is still the same: the expectation of a random variable can be interpreted as the long-run mean of the random variable, i.e. if we were able to repeat the experiment an infinite number of times, the expectation of the random variable will be the average result among all the experiments. It is still a measure of centrality of the random variable.</p>
<p>The <strong>linearity of expectations</strong> still hold in the same way, per equation <a href="discrete-random-variables.html#eq:3-linEX">(3.3)</a>. It does not matter if the random variable is discrete or continuous.</p>
<p>The <strong>Law of the Unconscious Statistician (LOTUS)</strong> also still applies. For a continuous random variable <span class="math inline">\(X\)</span>, it is (unsurprisingly):</p>
<p><span class="math display" id="eq:4-lotus">\[\begin{equation}
E(g(X)) = \int_{-\infty}^{\infty} g(x) f_X(x).
\tag{4.4}
\end{equation}\]</span></p>
<p>Notice again when we compare equation <a href="continuous-random-variables.html#eq:4-lotus">(4.4)</a> with its discrete counterpart in equation <a href="discrete-random-variables.html#eq:3-lotus">(3.4)</a>: we have just replaced the summation with an integral.</p>
<p><em>Thought question</em>: Can you guess how to write the equation for the variance of a continuous random variable? Hint: the variance for a discrete random variable is given in equation <a href="discrete-random-variables.html#eq:3-var2">(3.5)</a>.</p>
<div id="median-1" class="section level4" number="4.4.1.1">
<h4>
<span class="header-section-number">4.4.1.1</span> Median<a class="anchor" aria-label="anchor" href="#median-1"><i class="fas fa-link"></i></a>
</h4>
<p>The value <span class="math inline">\(m\)</span> is the <strong>median</strong> of a random variable <span class="math inline">\(X\)</span> if <span class="math inline">\(P(X \leq c) \geq \frac{1}{2}\)</span> and <span class="math inline">\(P(X \geq c) \geq \frac{1}{2}\)</span>.</p>
<p>Intuitively, the median is the value <span class="math inline">\(m\)</span> which splits the area under the PDF into half (or as close to half as possible if the random variable is discrete). Half the area will be to the left of <span class="math inline">\(m\)</span>, the other half of the area will be to the right of <span class="math inline">\(m\)</span>.</p>
</div>
<div id="mode-1" class="section level4" number="4.4.1.2">
<h4>
<span class="header-section-number">4.4.1.2</span> Mode<a class="anchor" aria-label="anchor" href="#mode-1"><i class="fas fa-link"></i></a>
</h4>
<p>For a continuous random variable <span class="math inline">\(X\)</span>, the mode is the value <span class="math inline">\(c\)</span> that maximizes the PDF: <span class="math inline">\(f_X(c) \geq f_X(x)\)</span> for all <span class="math inline">\(x\)</span>.</p>
<p>For a discrete random variable <span class="math inline">\(X\)</span>, the mode is the value <span class="math inline">\(c\)</span> that maximizes the PMF: <span class="math inline">\(P(X=c) \geq P(X=x)\)</span> for all <span class="math inline">\(x\)</span>. Intuitively, the mode is the most commonly occurring value of a discrete random variable</p>
</div>
<div id="loss-functions" class="section level4" number="4.4.1.3">
<h4>
<span class="header-section-number">4.4.1.3</span> Loss Functions<a class="anchor" aria-label="anchor" href="#loss-functions"><i class="fas fa-link"></i></a>
</h4>
<p>A goal of statistical modeling is to use the model to make predictions. We want to be able to quantify the quality of our prediction, or the prediction error. Suppose we have an experiment that can be described by a random variable <span class="math inline">\(X\)</span>, and we want to predict the value of the next experiment. The mean and median are natural guesses for the value of the next experiment.</p>
<p>It turns out there a several ways to quantify our prediction error. These are usually called loss functions. Suppose our predicted value is denoted by <span class="math inline">\(x_{pred}\)</span>. A couple of common loss functions are:</p>
<ul>
<li>
<strong>Mean squared error (MSE)</strong>: <span class="math inline">\(E(X-x_{pred})^2\)</span>,</li>
<li>
<strong>Mean absolute error (MAE)</strong>: <span class="math inline">\(E|X-x_{pred}|\)</span>.</li>
</ul>
<p>It turns out that the expected value <span class="math inline">\(E(X)\)</span> minimizes the MSE, and the median minimizes the MAE. So depending on what loss function suits our analysis, we could use either the mean or median for our predictions. We will cover these ideas in more detail in a later module (and indeed in later courses in this program).</p>
</div>
</div>
<div id="variance-1" class="section level3" number="4.4.2">
<h3>
<span class="header-section-number">4.4.2</span> Variance<a class="anchor" aria-label="anchor" href="#variance-1"><i class="fas fa-link"></i></a>
</h3>
<p>The <strong>variance</strong> of a continuous random variable <span class="math inline">\(X\)</span> is</p>
<p><span class="math display" id="eq:4-var">\[\begin{equation}
Var(X) = \int_{-\infty}^{\infty} (x-\mu)^2 f_X(x) dx.
\tag{4.5}
\end{equation}\]</span></p>
<p>The properties of variance is still the same as in Section <a href="discrete-random-variables.html#var-prop">3.4.3.1</a>. It does not matter if the random variable is discrete or continuous. A common notation used for variance is <span class="math inline">\(\sigma^2\)</span>, or sometimes <span class="math inline">\(\sigma_X^2\)</span> to show it is the variance of the random variable <span class="math inline">\(X\)</span>.</p>
</div>
<div id="moments" class="section level3" number="4.4.3">
<h3>
<span class="header-section-number">4.4.3</span> Moments<a class="anchor" aria-label="anchor" href="#moments"><i class="fas fa-link"></i></a>
</h3>
<p>Before talking about other measures that are used to describe distributions, we will cover some terminology that is used for these measures. Suppose we have a random variable <span class="math inline">\(X\)</span>.</p>
<ul>
<li>The <strong><span class="math inline">\(n\)</span>th moment</strong> of <span class="math inline">\(X\)</span> is <span class="math inline">\(E(X^n)\)</span>. So the expected value, or the mean, is sometimes called the first moment.</li>
<li>The <strong><span class="math inline">\(n\)</span>th central moment</strong> of <span class="math inline">\(X\)</span> is <span class="math inline">\(E((X-\mu)^n)\)</span>. So the variance is sometimes called the second central moment.</li>
<li>The <strong><span class="math inline">\(n\)</span> standardized moment</strong> of <span class="math inline">\(X\)</span> is <span class="math inline">\(E(\frac{(X-\mu)^n}{\sigma})\)</span>.</li>
</ul>
</div>
<div id="skewness" class="section level3" number="4.4.4">
<h3>
<span class="header-section-number">4.4.4</span> Skewness<a class="anchor" aria-label="anchor" href="#skewness"><i class="fas fa-link"></i></a>
</h3>
<p>One measure that is used to describe the shape of a distribution is skewness, which is a measure of symmetry (or measure of skewness). The <strong>skew</strong> of a random variable <span class="math inline">\(X\)</span> is the third standardized moment:</p>
<p><span class="math display" id="eq:4-skew">\[\begin{equation}
Skew(X) = E \left(\frac{(X-\mu)^3}{\sigma} \right)
\tag{4.6}
\end{equation}\]</span></p>
<p>A random variable <span class="math inline">\(X\)</span> has a <strong>symmetric distribution about its mean</strong> if <span class="math inline">\(X - \mu\)</span> has the same distribution as <span class="math inline">\(\mu - X\)</span>. Fairly often, people will just say that <span class="math inline">\(X\)</span> is symmetric; it is almost always assumed that the symmetry is about its mean.</p>
<p>Intuitively, symmetry means that the PDF of <span class="math inline">\(X\)</span> to the left of its mean is the mirror image of the PDF of <span class="math inline">\(X\)</span> to the right of its mean. We look at a couple of examples below in Figure <a href="continuous-random-variables.html#fig:4-symm">4.4</a>:</p>
<div class="figure">
<span style="display:block;" id="fig:4-symm"></span>
<img src="bookdown-demo_files/figure-html/4-symm-1.png" alt="PDFs for Symmetric RV vs Skewed RV" width="672"><p class="caption">
Figure 4.4: PDFs for Symmetric RV vs Skewed RV
</p>
</div>
<p>The blue vertical lines indicate the mean of these distributions. Notice the mirror image in the first plot, but not in the second plot.</p>
<p>If a distribution is not symmetric, we can say its distribution is asymmetric, or is skewed. The values of <span class="math inline">\(Skew(X)\)</span> that are associated with different shapes are:</p>
<ul>
<li>
<span class="math inline">\(Skew(X) = 0\)</span>: <span class="math inline">\(X\)</span> is symmetric.</li>
<li>
<span class="math inline">\(Skew(X) &gt; 0\)</span>: <span class="math inline">\(X\)</span> is right skewed.</li>
<li>
<span class="math inline">\(Skew(X) &lt; 0\)</span>: <span class="math inline">\(X\)</span> is left skewed.</li>
</ul>
</div>
<div id="kurtosis" class="section level3" number="4.4.5">
<h3>
<span class="header-section-number">4.4.5</span> Kurtosis<a class="anchor" aria-label="anchor" href="#kurtosis"><i class="fas fa-link"></i></a>
</h3>
<p>One more measure deals with the <strong>tail</strong> behavior of a distribution. Visually, the tails of a PDF are associated with probabilities of extreme values for a random variable. A distribution that is heavy tailed means that extreme values (on both ends) are more likely to occur. Tail behavior is an important consideration in risk management in finance: e.g. a heavy left tail in the PDF could mean a financial crisis. Figure <a href="continuous-random-variables.html#fig:4-kurt">4.5</a> shows an example of a heavy tailed distribution (in blue), compared to a Gaussian distribution (in black). We will talk more about the Gaussian distribution in the next subsection.</p>
<div class="figure">
<span style="display:block;" id="fig:4-kurt"></span>
<img src="bookdown-demo_files/figure-html/4-kurt-1.png" alt="PDF for Heavy Tailed Distribution" width="672"><p class="caption">
Figure 4.5: PDF for Heavy Tailed Distribution
</p>
</div>
<p>A common measure of tail behavior is the <strong>Kurtosis</strong>. The kurtosis of a random variable <span class="math inline">\(X\)</span> is the shifted fourth standardized moment:</p>
<p><span class="math display" id="eq:4-kurt">\[\begin{equation}
Kurt(X) = E \left(\frac{(X-\mu)^4}{\sigma} \right) - 3.
\tag{4.7}
\end{equation}\]</span></p>
<p>The reason for subtracting (or shifting by) 3 is so that the Gaussian distribution (a commonly used distribution for continuous random variables) has a kurtosis of 0. Note: Some authors call equation <a href="continuous-random-variables.html#eq:4-kurt">(4.7)</a> the <strong>excess kurtosis</strong> and the kurtosis does not subtract the 3.</p>
<p>The values of <span class="math inline">\(Kurt(X)\)</span> that are associated with tail behaviors are:</p>
<ul>
<li>
<span class="math inline">\(Kurt(X) = 0\)</span>: <span class="math inline">\(X\)</span> is similar tails to Gaussian distribution.</li>
<li>
<span class="math inline">\(Kurt(X) &gt; 0\)</span>: <span class="math inline">\(X\)</span> has heavier tails compared to Gaussian distribution (extreme values more likely).</li>
<li>
<span class="math inline">\(Kurt(X) &lt; 0\)</span>: <span class="math inline">\(X\)</span> has smaller tails compared to Gaussian distribution (extreme values less likely).</li>
</ul>
</div>
</div>
<div id="common-continuous-random-variables" class="section level2" number="4.5">
<h2>
<span class="header-section-number">4.5</span> Common Continuous Random Variables<a class="anchor" aria-label="anchor" href="#common-continuous-random-variables"><i class="fas fa-link"></i></a>
</h2>
<p>Next, we will introduce some commonly used distributions that may be used for continuous random variables. A number of common statistical models (for example, linear regression) are based on these distributions.</p>
<div id="uniform" class="section level3" number="4.5.1">
<h3>
<span class="header-section-number">4.5.1</span> Uniform<a class="anchor" aria-label="anchor" href="#uniform"><i class="fas fa-link"></i></a>
</h3>
<p>A random variable that follows a uniform distribution on the interval <span class="math inline">\((a,b)\)</span> is a completely random number between <span class="math inline">\(a\)</span> and <span class="math inline">\(b\)</span>. Notionally, an upper case <span class="math inline">\(U\)</span> is usually used to denote a uniform random variable. <span class="math inline">\(U\)</span> is said to have a <strong>uniform distribution</strong> on the interval <span class="math inline">\((a,b)\)</span>, denoted as <span class="math inline">\(U \sim(a,b)\)</span>, if its PDF is</p>
<p><span class="math display" id="eq:4-U">\[\begin{equation}
f_X(x) = \begin{cases}
  \frac{1}{b-a} &amp; \text{if } a&lt;x&lt;b \\
  0 &amp; \text{otherwise }.
\end{cases}
\tag{4.8}
\end{equation}\]</span></p>
<p>Note that the parameters <span class="math inline">\(a,b\)</span> also help define the support of a uniform distribution. Figure <a href="continuous-random-variables.html#fig:4-U">4.6</a> below displays a plot of the PDF of a <span class="math inline">\(U(a,b)\)</span>:</p>
<div class="figure">
<span style="display:block;" id="fig:4-U"></span>
<img src="images/04-U.png" alt="PDF of U(a,b). Picture from https://en.wikipedia.org/wiki/Continuous_uniform_distribution"><p class="caption">
Figure 4.6: PDF of U(a,b). Picture from <a href="https://en.wikipedia.org/wiki/Continuous_uniform_distribution" class="uri">https://en.wikipedia.org/wiki/Continuous_uniform_distribution</a>
</p>
</div>
<p><em>Thought question</em>: Can you verify that this is a valid PDF?</p>
<p>Figure <a href="continuous-random-variables.html#fig:4-Ucdf">4.7</a> below displays a plot of the CDF of a <span class="math inline">\(U(a,b)\)</span>:</p>
<div class="figure">
<span style="display:block;" id="fig:4-Ucdf"></span>
<img src="images/04-Ucdf.png" alt="CDF of U(a,b). Picture from https://en.wikipedia.org/wiki/Continuous_uniform_distribution"><p class="caption">
Figure 4.7: CDF of U(a,b). Picture from <a href="https://en.wikipedia.org/wiki/Continuous_uniform_distribution" class="uri">https://en.wikipedia.org/wiki/Continuous_uniform_distribution</a>
</p>
</div>
<p>Some properties of the uniform distribution:</p>
<ul>
<li>Its mean is <span class="math inline">\(E(U) = \frac{a+b}{2}\)</span>.</li>
<li>Its variance is <span class="math inline">\(Var(U) = \frac{(b-a)^2}{12}\)</span>.</li>
<li>Its skewness is 0, so it is symmetric.</li>
<li>Its kurtosis is -<span class="math inline">\(\frac{6}{5}\)</span>, so its tails are not as heavy compared to a Gaussian distribution.</li>
</ul>
<p><em>Thought question</em>: Can you see why a uniform distribution is symmetric? Can you see why its tails are not heavy?</p>
<p>If the support of a uniform distribution is between 0 and 1, we have a <strong>standard uniform distribution</strong>. We will talk about the importance of the standard uniform distribution in the next subsection.</p>
<div id="universality-of-uniform" class="section level4" number="4.5.1.1">
<h4>
<span class="header-section-number">4.5.1.1</span> Universality of Uniform<a class="anchor" aria-label="anchor" href="#universality-of-uniform"><i class="fas fa-link"></i></a>
</h4>
<p>It turns out that we can construct a random variable with any continuous distribution based on a standard uniform distribution. This fact is used to simulate random numbers from continuous distributions. This fact is called the <strong>Universality of the Uniform</strong>: Let <span class="math inline">\(F_X(x)\)</span> denote the CDF of a continuous random variable <span class="math inline">\(X\)</span>, then:</p>
<ol style="list-style-type: decimal">
<li>Let <span class="math inline">\(U \sim U(0,1)\)</span> and <span class="math inline">\(X = F^{-1}(U)\)</span>. Then <span class="math inline">\(X\)</span> is a random variable with CDF <span class="math inline">\(F_X(x)\)</span>.</li>
<li>
<span class="math inline">\(F_X(X) \sim U(0,1)\)</span>.</li>
</ol>
<p>To give some insight into what this means, we look at an example. Another continuous distribution is called the standard logistic distribution, which we will denote with <span class="math inline">\(X\)</span>. Its CDF is</p>
<p><span class="math display">\[
F_X(x) = \frac{e^x}{1+e^x}.
\]</span>
Let <span class="math inline">\(U \sim U(0,1)\)</span>. The first part of the universality of the uniform informs us that the inverse of the CDF for the standard logistic is <span class="math inline">\(F_X^{-1}(U) \sim X\)</span>, so we invert <span class="math inline">\(F_X(x)\)</span> to get its inverse <span class="math inline">\(F_X^{-1}(x)\)</span>. This is done by setting the CDF of <span class="math inline">\(X\)</span> to be equal to <span class="math inline">\(u\)</span>, i.e. let <span class="math inline">\(u = \frac{e^x}{1+e^x}\)</span>, and solving for <span class="math inline">\(x\)</span>:</p>
<p><span class="math display">\[
\begin{split}
u + u e^x &amp;= e^x\\
\implies u &amp;= e^x (1-u) \\
\implies e^x &amp;= \frac{u}{1-u} \\
\implies x &amp;= \log (\frac{u}{1-u}).
\end{split}
\]</span></p>
<p>Therefore <span class="math inline">\(F^{-1}(u) = \log (\frac{u}{1-u})\)</span> and <span class="math inline">\(F^{-1}(U) = \log (\frac{U}{1-U})\)</span>. Therefore <span class="math inline">\(\log (\frac{U}{1-U})\)</span> follows a standard logistic distribution.</p>
<p>Let us use simulations to show what is going on. First, we simulate 10,000 reps from a standard uniform distribution, then invert these values using <span class="math inline">\(\log (\frac{u}{1-u})\)</span>, and create the density plot of <span class="math inline">\(\log (\frac{u}{1-u})\)</span>. These steps are shown in Figure <a href="continuous-random-variables.html#fig:4-universe">4.8</a> below:</p>
<div class="sourceCode" id="cb48"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="fu"><a href="https://rdrr.io/r/base/Random.html">set.seed</a></span><span class="op">(</span><span class="fl">4</span><span class="op">)</span></span>
<span></span>
<span><span class="va">reps</span><span class="op">&lt;-</span><span class="fl">10000</span> <span class="co">##number of reps</span></span>
<span><span class="va">u</span><span class="op">&lt;-</span><span class="fu"><a href="https://rdrr.io/r/stats/Uniform.html">runif</a></span><span class="op">(</span><span class="va">reps</span><span class="op">)</span> <span class="co">##simulate standard uniform</span></span>
<span><span class="va">invert</span><span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/Log.html">log</a></span><span class="op">(</span><span class="va">u</span><span class="op">/</span><span class="op">(</span><span class="fl">1</span><span class="op">-</span><span class="va">u</span><span class="op">)</span><span class="op">)</span> <span class="co">##invert based on F inverse. These should now follow standard logistic</span></span>
<span></span>
<span><span class="fu"><a href="https://rdrr.io/r/graphics/par.html">par</a></span><span class="op">(</span>mfrow<span class="op">=</span><span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span><span class="fl">1</span>,<span class="fl">3</span><span class="op">)</span><span class="op">)</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/graphics/plot.default.html">plot</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/stats/density.html">density</a></span><span class="op">(</span><span class="va">u</span><span class="op">)</span>, main<span class="op">=</span><span class="st">"Density Plot from 10,000 U's"</span><span class="op">)</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/graphics/plot.default.html">plot</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/stats/density.html">density</a></span><span class="op">(</span><span class="va">invert</span><span class="op">)</span>, main<span class="op">=</span><span class="st">"Density Plot after Inverting"</span>, xlim<span class="op">=</span><span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span><span class="op">-</span><span class="fl">6</span>,<span class="fl">6</span><span class="op">)</span><span class="op">)</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/graphics/curve.html">curve</a></span><span class="op">(</span><span class="va">dlogis</span>, from <span class="op">=</span> <span class="op">-</span><span class="fl">7</span>, to <span class="op">=</span> <span class="fl">7</span>, main <span class="op">=</span> <span class="st">"PDF for Logistic"</span>, ylab<span class="op">=</span><span class="st">"Density"</span>, xlab<span class="op">=</span><span class="st">""</span><span class="op">)</span></span></code></pre></div>
<div class="figure">
<span style="display:block;" id="fig:4-universe"></span>
<img src="bookdown-demo_files/figure-html/4-universe-1.png" alt="Uniform to Logistic" width="672"><p class="caption">
Figure 4.8: Uniform to Logistic
</p>
</div>
<p>From Figure <a href="continuous-random-variables.html#fig:4-universe">4.8</a>:</p>
<ul>
<li>The first plot shows the density plot from our 10,000 reps from a standard normal. This is close to the PDF of a standard uniform.</li>
<li>The second plot shows the density plot after inverting our 10,000 reps from a standard normal, i.e. <span class="math inline">\(F^{-1}(u) = \log (\frac{u}{1-u})\)</span>.</li>
<li>The third plot shows the PDF of a standard logistic. Notice how similar this looks to the second plot.</li>
</ul>
<p>So we see that <span class="math inline">\(\log (\frac{U}{1-U})\)</span> follows a standard logistic distribution.</p>
<p>The second part of the universality of the uniform informs us that if <span class="math inline">\(X\)</span> follows a standard logistic distribution, then <span class="math inline">\(F(X) = \frac{e^X}{1 + e^X} \sim U(0,1)\)</span>.</p>
<p>So, we can see the purpose of the universality of the uniform:</p>
<ul>
<li>For part 1, we can simulate reps from any distribution, as long as we know its CDF. The software you use may not be able to simulate reps from a particular distribution, but you can write code to simulate reps from this distribution based on the standard uniform.</li>
<li>For part 2, we can convert a random variable with an unknown distribution to one that is known: the standard uniform.</li>
</ul>
</div>
</div>
<div id="normdist" class="section level3" number="4.5.2">
<h3>
<span class="header-section-number">4.5.2</span> Normal<a class="anchor" aria-label="anchor" href="#normdist"><i class="fas fa-link"></i></a>
</h3>
<p>Another widely used distribution for continuous random variables is the normal, or Gaussian distribution. This is a distribution that is symmetric and bell-shaped. This is probably the most important distribution in statistics and data science due to the central limit theorem. We will define this theorem in a later module, but loosely speaking, it says that if we take the average of a bunch of random variables, the average will approximate a normal distribution, even if the random variables are individually not normal.</p>
<p>A lot of questions that we wish to answer are based on averages. For example</p>
<ul>
<li>Does the implementation of certain technologies in a class improve test scores for students, on average?</li>
<li>Are male Gentoo penguins heavier than their female counterparts, on average?</li>
<li>Does replacing traffic lights with a roundabout reduce the number of traffic accidents, on average?</li>
</ul>
<p>What the central limit theorem implies is that even if test scores, weights of Gentoo penguins, and number of traffic accidents do not follow a normal distribution, their average values will approximate a normal distribution.</p>
<div id="standard-normal" class="section level4" number="4.5.2.1">
<h4>
<span class="header-section-number">4.5.2.1</span> Standard Normal<a class="anchor" aria-label="anchor" href="#standard-normal"><i class="fas fa-link"></i></a>
</h4>
<p>First, we will talk about the <strong>standard normal distribution</strong>, as other normal distributions can be viewed as variations of the standard normal. A standard normal distribution has mean 0 and variance 1. It is usually denoted by <span class="math inline">\(Z\)</span>. We can also write <span class="math inline">\(Z \sim N(0,1)\)</span> to say that <span class="math inline">\(Z\)</span> is normally distributed with mean 0 and variance 1. The PDF of a standard normal distribution is:</p>
<p><span class="math display" id="eq:4-Z">\[\begin{equation}
\phi(z) = \frac{1}{\sqrt{2 \pi}} e^{-z^2/2}.
\tag{4.9}
\end{equation}\]</span></p>
<p>Notice the constant <span class="math inline">\(\frac{1}{\sqrt{2 \pi}}\)</span> in equation <a href="continuous-random-variables.html#eq:4-Z">(4.9)</a>. Its presence is needed to make the PDF valid, since the PDF must integrate to 1. Such constants are called <strong>normalizing constants</strong>.</p>
<p>Figure <a href="continuous-random-variables.html#fig:4-Z">4.9</a> below displays its PDF:</p>
<div class="sourceCode" id="cb49"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="fu"><a href="https://rdrr.io/r/graphics/curve.html">curve</a></span><span class="op">(</span><span class="va">dnorm</span>, from <span class="op">=</span> <span class="op">-</span><span class="fl">4</span>, to <span class="op">=</span> <span class="fl">4</span>, main <span class="op">=</span> <span class="st">"PDF for Z"</span>, ylab<span class="op">=</span><span class="st">"Density"</span>, xlab<span class="op">=</span><span class="st">""</span><span class="op">)</span></span></code></pre></div>
<div class="figure">
<span style="display:block;" id="fig:4-Z"></span>
<img src="bookdown-demo_files/figure-html/4-Z-1.png" alt="PDF of Standard Normal" width="672"><p class="caption">
Figure 4.9: PDF of Standard Normal
</p>
</div>
<p>From Figure <a href="continuous-random-variables.html#fig:4-Z">4.9</a>, we can see the following properties of a standard normal distribution (these will apply for any normal distribution):</p>
<ul>
<li>Its PDF is symmetric about its mean. In Figure <a href="continuous-random-variables.html#fig:4-Z">4.9</a>, the PDF is symmetric about 0, i.e. <span class="math inline">\(\phi(-z) = \phi(z)\)</span>.</li>
<li>This implies that the tail areas are also symmetric. For example, <span class="math inline">\(P(Z \leq -2) = P(Z \geq 2)\)</span>.</li>
<li>Its skew is 0, since it is symmetric.</li>
</ul>
<p>There is actually no closed-formed equation for the CDF of a standard normal (or any normal distribution). We write <span class="math inline">\(\Phi(z) = P(Z \leq z) = \int_{\infty}^z \phi(z) dz\)</span> to express the CDF of a standard normal.</p>
<p>Notice that we have special letters <span class="math inline">\(Z, \phi, \Phi\)</span> to denote the standard normal distribution. This is an indication of how often it is used to warrant its own notation.</p>
</div>
<div id="norm" class="section level4" number="4.5.2.2">
<h4>
<span class="header-section-number">4.5.2.2</span> From Standard Normal to Other Normals<a class="anchor" aria-label="anchor" href="#norm"><i class="fas fa-link"></i></a>
</h4>
<p>If <span class="math inline">\(Z \sim N(0,1)\)</span>, then <span class="math inline">\(X = \mu + \sigma Z \sim N(\mu, \sigma^2)\)</span>. In other words, if <span class="math inline">\(Z\)</span> is standard normal, then <span class="math inline">\(X = \mu + \sigma Z\)</span> follows a normal distribution with mean <span class="math inline">\(\mu\)</span> and variance <span class="math inline">\(\sigma^2\)</span>. The parameters of a normal distribution are the mean <span class="math inline">\(\mu\)</span> and variance <span class="math inline">\(\sigma^2\)</span>.</p>
<p>Note that some authors say the parameters are the mean <span class="math inline">\(\mu\)</span> and standard deviation <span class="math inline">\(\sigma\)</span> instead, so be careful when reading notation associated with normal distributions from various sources. For example, <span class="math inline">\(N(0,2)\)</span> in our class and our book means normal distribution with mean 0 and variance 2, but for some other authors, <span class="math inline">\(N(0,2)\)</span> means normal distribution with mean 0 and standard deviation 2. Indeed, the functions in R use this alternate parameterization, so you need to be careful.</p>
<p><em>Thought question</em>: Can you use the linearity of expectations to explain why <span class="math inline">\(X\)</span> has mean <span class="math inline">\(\mu\)</span>? Can you use properties of variance from Section <a href="discrete-random-variables.html#var-prop">3.4.3.1</a> to explain why <span class="math inline">\(X\)</span> has variance <span class="math inline">\(\sigma^2\)</span>?</p>
<p>Notice how we started from a standard normal <span class="math inline">\(Z\)</span>, and transformed <span class="math inline">\(Z\)</span> by multiplying it by <span class="math inline">\(\sigma\)</span> and then adding <span class="math inline">\(\mu\)</span> to get any normal distribution. This transformation is called a <strong>location-scale</strong> transformation, or sometimes shifting and scaling. The scale changes since we multiply by a constant <span class="math inline">\(\sigma\)</span>; the location is transformed since its mean changes from 0 to <span class="math inline">\(\mu\)</span>.</p>
<p>We can also reverse this transformation and state the following: If <span class="math inline">\(X \sim N(\mu, \sigma^2)\)</span>, then <span class="math inline">\(Z = \frac{X-\mu}{\sigma} \sim N(0,1)\)</span>. If we start with <span class="math inline">\(X \sim N(\mu, \sigma^2)\)</span>, then we can transform <span class="math inline">\(X\)</span> by subtracting , and then dividing by <span class="math inline">\(\sigma\)</span>, to obtain <span class="math inline">\(Z\)</span>. This particular transformation is called <strong>standardization</strong>:</p>
<p><span class="math display" id="eq:4-standardize">\[\begin{equation}
Z = \frac{X-\mu}{\sigma}.
\tag{4.10}
\end{equation}\]</span></p>
<p>The PDF of any normal distribution <span class="math inline">\(X \sim N(\mu, \sigma^2)\)</span> is</p>
<p><span class="math display" id="eq:4-pdfNormal">\[\begin{equation}
f_X(x) = \frac{1}{\sigma \sqrt{2 \pi}} \exp \left(-\frac{(x - \mu)^2}{2 \sigma^2} \right).
\tag{4.11}
\end{equation}\]</span></p>
<p><em>Thought question</em>: Compare equations <a href="continuous-random-variables.html#eq:4-pdfNormal">(4.11)</a> and <a href="continuous-random-variables.html#eq:4-Z">(4.9)</a>. Can you see how equation <a href="continuous-random-variables.html#eq:4-Z">(4.9)</a> can be derived from equation <a href="continuous-random-variables.html#eq:4-pdfNormal">(4.11)</a>?</p>
</div>
<div id="rulenorm" class="section level4" number="4.5.2.3">
<h4>
<span class="header-section-number">4.5.2.3</span> 68-95-99.7% Rule<a class="anchor" aria-label="anchor" href="#rulenorm"><i class="fas fa-link"></i></a>
</h4>
<p>The following property holds for any normal distribution, and is often called the <strong>68-99-99.7%</strong> rule. For any normal distribution <span class="math inline">\(X \sim N(\mu, \sigma^2)\)</span>:</p>
<ul>
<li>
<span class="math inline">\(P(\mu - \sigma &lt; X &lt; \mu + \sigma) \approx 0.68\)</span>,</li>
<li>
<span class="math inline">\(P(\mu - 2\sigma &lt; X &lt; \mu + 2\sigma) \approx 0.95\)</span>,</li>
<li>
<span class="math inline">\(P(\mu - 3\sigma &lt; X &lt; \mu + 3\sigma) \approx 0.997\)</span>.</li>
</ul>
<p>What these mean is that for any normal distribution:</p>
<ul>
<li>About 68% of observed values will fall within 1 standard deviation of the mean,</li>
<li>About 95% of observed values will fall within 2 standard deviations of the mean, and</li>
<li>About 99.7% of observed values will fall within 3 standard deviations of the mean.</li>
</ul>
<p>The last statement is the basis for the term <a href="https://en.wikipedia.org/wiki/Six_Sigma">six sigma</a> used in manufacturing, since the range virtually all data points should fall within a range that is six sigma wide (assuming they follow a normal distribution). Visually, this rule is shown in Figure <a href="continuous-random-variables.html#fig:4-rule">4.10</a> when applied to the standard normal:</p>
<div class="figure">
<span style="display:block;" id="fig:4-rule"></span>
<img src="bookdown-demo_files/figure-html/4-rule-1.png" alt="68-95-99.7 Rule" width="672"><p class="caption">
Figure 4.10: 68-95-99.7 Rule
</p>
</div>
<p>We will work out the first statement, that about 68% of the observed values will fall within 1 standard deviation of the mean for any normal distribution. We will use R to help us verify this rule for a standard normal:</p>
<div class="sourceCode" id="cb50"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">upper1</span><span class="op">&lt;-</span><span class="fu"><a href="https://rdrr.io/r/stats/Normal.html">pnorm</a></span><span class="op">(</span><span class="fl">1</span><span class="op">)</span> <span class="co">## what is percentile associated with Z=1 (i.e. 1 standard deviation above mean)</span></span>
<span><span class="va">lower1</span><span class="op">&lt;-</span><span class="fu"><a href="https://rdrr.io/r/stats/Normal.html">pnorm</a></span><span class="op">(</span><span class="op">-</span><span class="fl">1</span><span class="op">)</span> <span class="co">## what is percentile associated with Z=-1 (i.e. 1 standard deviation below mean)</span></span>
<span><span class="va">upper1</span><span class="op">-</span><span class="va">lower1</span> <span class="co">## find proportion in between 1 SD above and below mean.</span></span></code></pre></div>
<pre><code>## [1] 0.6826895</code></pre>
<p><em>Thought question</em>: how would you tweak this code to verify the other two statements associated with the 68-95-99.7% rule?</p>
</div>
</div>
</div>
<div id="using-r" class="section level2" number="4.6">
<h2>
<span class="header-section-number">4.6</span> Using R<a class="anchor" aria-label="anchor" href="#using-r"><i class="fas fa-link"></i></a>
</h2>
<p>R has built in functions to compute the density, CDF, percentiles, as well as simulate data of common distributions. We will start with a random variable <span class="math inline">\(Y \sim N(1, 9)\)</span> as an example.</p>
<ol style="list-style-type: decimal">
<li>To find <span class="math inline">\(f_Y(2)\)</span>, use:</li>
</ol>
<div class="sourceCode" id="cb52"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="fu"><a href="https://rdrr.io/r/stats/Normal.html">dnorm</a></span><span class="op">(</span><span class="fl">2</span>, <span class="fl">1</span>, <span class="fl">3</span><span class="op">)</span> <span class="co">##supply the value of Y you want, then the parameters mu and sigma</span></span></code></pre></div>
<pre><code>## [1] 0.1257944</code></pre>
<p>The density <span class="math inline">\(f_Y(2)\)</span> is 0.1257944. Note: In R, the normal distribution is parameterized by the mean and standard deviation, which is different from these set of notes and our book, which uses the mean and variance.</p>
<ol start="2" style="list-style-type: decimal">
<li>To find <span class="math inline">\(P(Y \leq 2)\)</span>, use:</li>
</ol>
<div class="sourceCode" id="cb54"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="fu"><a href="https://rdrr.io/r/stats/Normal.html">pnorm</a></span><span class="op">(</span><span class="fl">2</span>, <span class="fl">1</span>, <span class="fl">3</span><span class="op">)</span> <span class="co">##supply the value of Y you want, then the parameters mu and sigma</span></span></code></pre></div>
<pre><code>## [1] 0.6305587</code></pre>
<p>The probability that <span class="math inline">\(Y\)</span> is less than or equal to 2 is 0.6305587.</p>
<p>Alternatively, we can standardize this normal distribution, and use the standard normal. The standardization, per equation <a href="continuous-random-variables.html#eq:4-standardize">(4.10)</a>, gives us</p>
<p><span class="math display">\[
z = \frac{2-1}{3} = \frac{1}{3},
\]</span></p>
<p>so</p>
<p><span class="math display">\[
\begin{split}
P(Y \leq 2) &amp;= P(\frac{Y-\mu}{\sigma} \leq \frac{2-1}{3}) \\
            &amp;= P(Z \leq \frac{1}{3}) \\
            &amp;= \Phi(\frac{1}{3})
\end{split}
\]</span></p>
<p>which can be found using</p>
<div class="sourceCode" id="cb56"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="fu"><a href="https://rdrr.io/r/stats/Normal.html">pnorm</a></span><span class="op">(</span><span class="fl">1</span><span class="op">/</span><span class="fl">3</span><span class="op">)</span> <span class="co">##don't supply mu and sigma means you want to use standard normal</span></span></code></pre></div>
<pre><code>## [1] 0.6305587</code></pre>
<p>which gives the same answer as <code>pnorm(2,1,3)</code>.</p>
<ol start="3" style="list-style-type: decimal">
<li>To find the value on the support that corresponds to the 90th percentile, use:</li>
</ol>
<div class="sourceCode" id="cb58"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="fu"><a href="https://rdrr.io/r/stats/Normal.html">qnorm</a></span><span class="op">(</span><span class="fl">0.9</span>, <span class="fl">1</span>, <span class="fl">3</span><span class="op">)</span> <span class="co">##supply the value of the percentile you need, then the parameters mu and sigma</span></span></code></pre></div>
<pre><code>## [1] 4.844655</code></pre>
<p>The 90th percentile of <span class="math inline">\(Y \sim N(1,3)\)</span> is 4.844655.</p>
<p>If we want to use the standard normal, we could find its 90th percentile:</p>
<div class="sourceCode" id="cb60"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="fu"><a href="https://rdrr.io/r/stats/Normal.html">qnorm</a></span><span class="op">(</span><span class="fl">0.9</span><span class="op">)</span></span></code></pre></div>
<pre><code>## [1] 1.281552</code></pre>
<p>and then apply the location scale transformation</p>
<div class="sourceCode" id="cb62"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="fu"><a href="https://rdrr.io/r/stats/Normal.html">qnorm</a></span><span class="op">(</span><span class="fl">0.9</span><span class="op">)</span><span class="op">*</span><span class="fl">3</span> <span class="op">+</span> <span class="fl">1</span> <span class="co">##multiply by sigma, then add mu</span></span></code></pre></div>
<pre><code>## [1] 4.844655</code></pre>
<p>which is the same answer as <code>qnorm(0.9,1,3)</code>.</p>
<ol start="4" style="list-style-type: decimal">
<li>To simulate 10 draws (repetitions) of <span class="math inline">\(Y\)</span>, use:</li>
</ol>
<div class="sourceCode" id="cb64"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="fu"><a href="https://rdrr.io/r/base/Random.html">set.seed</a></span><span class="op">(</span><span class="fl">2</span><span class="op">)</span> <span class="co">##use set.seed() so we get the same random numbers each time the code is run</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/stats/Normal.html">rnorm</a></span><span class="op">(</span><span class="fl">10</span>, <span class="fl">1</span>, <span class="fl">3</span><span class="op">)</span> <span class="co">##supply the number of simulated data you need, then the parameters mu and sigma</span></span></code></pre></div>
<pre><code>##  [1] -1.6907436  1.5545476  5.7635360 -2.3911270  0.7592447  1.3972609
##  [7]  3.1238642  0.2809059  6.9534218  0.5836390</code></pre>
<p>This outputs a vector of length 10. Each value represents the result of each rep. So the first value drawn from <span class="math inline">\(Y \sim N(1,3)\)</span> -1.6907436, the second value drawn is 1.5545476 and so on.</p>
<p>Just like in Section <a href="discrete-random-variables.html#Rdis">3.6</a>, notice these functions all ended with <code>norm</code>. We just added a different letter first, depending on whether we want the density (analogous to PDF), CDF, percentile, or random draw. The letters are <code>d</code>, <code>p</code>, <code>q</code>, and <code>r</code> respectively.</p>
<p>One thing to note: if we do not supply the mean and standard deviation, for example we type <code>rnorm(10)</code>, R will assume you want to use a standard normal distribution, so <code>rnorm(10)</code> will draw 10 random numbers from a standard normal.</p>
<div id="KDE" class="section level3" number="4.6.1">
<h3>
<span class="header-section-number">4.6.1</span> Density Plots and Kernel Density Estimation<a class="anchor" aria-label="anchor" href="#KDE"><i class="fas fa-link"></i></a>
</h3>
<p>We are now ready to talk about how density plots, like the ones in Figure <a href="continuous-random-variables.html#fig:4-universe">4.8</a> are created. Recall the difference between density plots and PDFs:</p>
<ul>
<li>A plot of the PDF describes the distribution of a known random variable.</li>
<li>A density plot is based on our data, and is used to describe the distribution of our data. Our data may or may not follow a commonly known random variable. If it does, then a plot of the PDF and the density plot should match up as we gather more and more data.</li>
</ul>
<p>Proportions are found in the same way, by finding the area under the PDF or density plot for the appropriate range on the support.</p>
<p>Suppose we have <span class="math inline">\(n\)</span> observed values of an unknown random variable <span class="math inline">\(X\)</span>: <span class="math inline">\(x_1, x_2, \cdots, x_n\)</span>.The density <span class="math inline">\(f\)</span> of <span class="math inline">\(X\)</span> is unknown and we want to estimate it with our data. To estimate the density <span class="math inline">\(f\)</span>, we use the <strong>kernel density estimator</strong>:</p>
<p><span class="math display" id="eq:4-KDE">\[\begin{equation}
\hat{f}_h(x) = \frac{1}{nh} \sum_{i=1}^n K \left( \frac{x-x_i}{h}\right ),
\tag{4.12}
\end{equation}\]</span></p>
<p>where <span class="math inline">\(K\)</span> is the <strong>kernel</strong> and <span class="math inline">\(h\)</span> is a smoothing parameter, often called the <strong>bandwidth</strong>. Looking at equation <a href="continuous-random-variables.html#eq:4-KDE">(4.12)</a>, the KDE can be viewed as a weighted average of the relative likelihood of observing a particular value, with values nearer the specific value on the support receiving a higher weight.</p>
<p>The kernel can be viewed as a weighting function, with the weights following the shape of a distribution that the user specifies (usually symmetric). Common kernel functions and their shapes are displayed in Figure <a href="continuous-random-variables.html#fig:4-KDE">4.11</a>:</p>
<div class="figure">
<span style="display:block;" id="fig:4-KDE"></span>
<img src="images/04-KDE.png" alt="Common Kernals. Picture adapted from https://tgstewart.cloud/compprob/kde.html"><p class="caption">
Figure 4.11: Common Kernals. Picture adapted from <a href="https://tgstewart.cloud/compprob/kde.html" class="uri">https://tgstewart.cloud/compprob/kde.html</a>
</p>
</div>
<p>The horizontal axis on the kernel can be viewed as the distance of the value of a data point from a specific value on the support, and the mid point on the horizontal axis represents a distance of 0.</p>
<ul>
<li>Looking at the normal kernel, nearest values receive the highest weight, and values further away receive less weight.</li>
<li>For the uniform kernel, values within a certain distance receive a weight, and values beyond a certain distance receive no weight.</li>
<li>The Epanechnikov (parabolic) kernel is a mix of both: values beyond a certain distance receive no weight, and values within a certain distance receive a weight that is roughly inversely proportional to the distance.</li>
</ul>
<p><span class="math inline">\(h\)</span> is the smoothing parameter and is analogous to bin width in histograms. Larger values result in smoother looking density plots.</p>
<p>Let us go back to an old example. We will use the <code>loan50</code> dataset from the <code>openintro</code> package. The data originally consist of thousands of loans made through the Lending Club platform, but we will randomly select 50 of these loans. Let us study the interest rate the loans the 50 applicants received.</p>
<div class="sourceCode" id="cb66"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="kw"><a href="https://rdrr.io/r/base/library.html">library</a></span><span class="op">(</span><span class="va"><a href="http://openintrostat.github.io/openintro/">openintro</a></span><span class="op">)</span></span>
<span></span>
<span><span class="co">##create object for data</span></span>
<span><span class="va">Data</span><span class="op">&lt;-</span><span class="va">loan50</span></span>
<span></span>
<span><span class="co">##create density plot using default</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/graphics/plot.default.html">plot</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/stats/density.html">density</a></span><span class="op">(</span><span class="va">Data</span><span class="op">$</span><span class="va">interest_rate</span><span class="op">)</span>, main<span class="op">=</span><span class="st">"Density Plot of Interest Rates"</span><span class="op">)</span></span></code></pre></div>
<div class="figure">
<span style="display:block;" id="fig:4-R"></span>
<img src="bookdown-demo_files/figure-html/4-R-1.png" alt="Density Plot for 50 Interest Rates" width="672"><p class="caption">
Figure 4.12: Density Plot for 50 Interest Rates
</p>
</div>
<p>This actually uses KDE with the default settings: kernel is normal, and the bandwidth is based on <a href="https://en.wikipedia.org/wiki/Kernel_density_estimation#A_rule-of-thumb_bandwidth_estimator">Silverman’s rule of thumb</a>.</p>
<p>To change these, we add the <code>kernel</code> and <code>adjust</code> argument when using the <code><a href="https://rdrr.io/r/stats/density.html">density()</a></code> function, for example, to use the Epanechnikov kernal with twice the default bandwidth:</p>
<div class="sourceCode" id="cb67"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="co">##create density plot using different settings</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/graphics/plot.default.html">plot</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/stats/density.html">density</a></span><span class="op">(</span><span class="va">Data</span><span class="op">$</span><span class="va">interest_rate</span>, kernel <span class="op">=</span> <span class="st">"epanechnikov"</span>, adjust <span class="op">=</span> <span class="fl">2</span><span class="op">)</span>, </span>
<span>     main<span class="op">=</span><span class="st">"Density Plot of Interest Rates"</span><span class="op">)</span></span></code></pre></div>
<div class="figure">
<span style="display:block;" id="fig:4-R2"></span>
<img src="bookdown-demo_files/figure-html/4-R2-1.png" alt="Density Plot for 50 Interest Rates, Epanechnikov Kernel, Twice the Bandwidth" width="672"><p class="caption">
Figure 4.13: Density Plot for 50 Interest Rates, Epanechnikov Kernel, Twice the Bandwidth
</p>
</div>
</div>
<div id="density-plots-and-histograms" class="section level3" number="4.6.2">
<h3>
<span class="header-section-number">4.6.2</span> Density Plots and Histograms<a class="anchor" aria-label="anchor" href="#density-plots-and-histograms"><i class="fas fa-link"></i></a>
</h3>
<p>In Section <a href="descriptive.html#densplots">1.2.3</a>, we mentioned that density plots can be viewed as smoothed versions of a histogram. We create a histogram of interest rates, and overlay a density plot in blue, per Figure <a href="continuous-random-variables.html#fig:4-R3">4.14</a> below:</p>
<div class="sourceCode" id="cb68"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="fu"><a href="https://rdrr.io/r/graphics/hist.html">hist</a></span><span class="op">(</span><span class="va">Data</span><span class="op">$</span><span class="va">interest_rate</span>, prob <span class="op">=</span> <span class="cn">TRUE</span>, main <span class="op">=</span> <span class="st">"Histogram with Density Plot"</span>, xlab<span class="op">=</span><span class="st">"Interest Rates"</span><span class="op">)</span></span>
<span></span>
<span><span class="co">##create density plot using default</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/graphics/lines.html">lines</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/stats/density.html">density</a></span><span class="op">(</span><span class="va">Data</span><span class="op">$</span><span class="va">interest_rate</span><span class="op">)</span>, col<span class="op">=</span><span class="st">"blue"</span><span class="op">)</span></span></code></pre></div>
<div class="figure">
<span style="display:block;" id="fig:4-R3"></span>
<img src="bookdown-demo_files/figure-html/4-R3-1.png" alt="Histogram and Density Plot for 50 Interest Rates" width="672"><p class="caption">
Figure 4.14: Histogram and Density Plot for 50 Interest Rates
</p>
</div>
</div>
<div id="numerical-summaries" class="section level3" number="4.6.3">
<h3>
<span class="header-section-number">4.6.3</span> Numerical Summaries<a class="anchor" aria-label="anchor" href="#numerical-summaries"><i class="fas fa-link"></i></a>
</h3>
<p>Equations <a href="continuous-random-variables.html#eq:4-EX">(4.3)</a>, <a href="continuous-random-variables.html#eq:4-var">(4.5)</a>, <a href="continuous-random-variables.html#eq:4-skew">(4.6)</a>, and <a href="continuous-random-variables.html#eq:4-kurt">(4.7)</a> are used to obtain the mean, variance, skewness, and kurtosis of a known distribution from a random variable. To calculate these quantities based on a sample of observed data, <span class="math inline">\(x_1, x_2, \cdots, x_n\)</span>, we use:</p>
<p><span class="math display" id="eq:4-xbar">\[\begin{equation}
\bar{x} =  \frac{1}{n} \sum_{i=1}^n x_i,
\tag{4.13}
\end{equation}\]</span></p>
<p><span class="math display" id="eq:4-sampvar">\[\begin{equation}
s_X^2 =  \frac{1}{n-1} \sum_{i=1}^n (x_i - \bar{x})^2,
\tag{4.14}
\end{equation}\]</span></p>
<p>where <span class="math inline">\(\bar{x}\)</span> and <span class="math inline">\(s_x^2\)</span> denote the sample mean and variance respectively. The sample skewness and sample kurtosis are</p>
<p><span class="math display" id="eq:4-sampskew">\[\begin{equation}
\text{sample skewness } =  \frac{\frac{1}{n} \sum_{i=1}^n (x_i - \bar{x})^3}{s_X^3},
\tag{4.15}
\end{equation}\]</span></p>
<p>and</p>
<p><span class="math display" id="eq:4-sampkurt">\[\begin{equation}
\text{sample kurtosis } =  \frac{\frac{1}{n} \sum_{i=1}^n (x_i - \bar{x})^4}{s_X^4} - 3.
\tag{4.16}
\end{equation}\]</span></p>
<p>The functions <code><a href="https://rdrr.io/r/base/mean.html">mean()</a></code>, <code><a href="https://rdrr.io/r/stats/cor.html">var()</a></code>, <code><a href="https://rdrr.io/pkg/moments/man/skewness.html">skewness()</a></code>, and <code><a href="https://rdrr.io/pkg/moments/man/kurtosis.html">kurtosis()</a></code> compute these quantities in R. The latter two functions come from the <code>moments</code> package so be sure to install and load it prior to using them.</p>
<div class="sourceCode" id="cb69"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="fu"><a href="https://rdrr.io/r/base/mean.html">mean</a></span><span class="op">(</span><span class="va">Data</span><span class="op">$</span><span class="va">interest_rate</span><span class="op">)</span> <span class="co">##mean</span></span></code></pre></div>
<pre><code>## [1] 11.5672</code></pre>
<div class="sourceCode" id="cb71"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="fu"><a href="https://rdrr.io/r/stats/cor.html">var</a></span><span class="op">(</span><span class="va">Data</span><span class="op">$</span><span class="va">interest_rate</span><span class="op">)</span> <span class="co">##variance</span></span></code></pre></div>
<pre><code>## [1] 25.52387</code></pre>
<div class="sourceCode" id="cb73"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="kw"><a href="https://rdrr.io/r/base/library.html">library</a></span><span class="op">(</span><span class="va"><a href="https://www.r-project.org">moments</a></span><span class="op">)</span></span>
<span><span class="fu">moments</span><span class="fu">::</span><span class="fu"><a href="https://rdrr.io/pkg/moments/man/skewness.html">skewness</a></span><span class="op">(</span><span class="va">Data</span><span class="op">$</span><span class="va">interest_rate</span><span class="op">)</span> <span class="co">##greater than 0</span></span></code></pre></div>
<pre><code>## [1] 1.102193</code></pre>
<div class="sourceCode" id="cb75"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="fu">moments</span><span class="fu">::</span><span class="fu"><a href="https://rdrr.io/pkg/moments/man/kurtosis.html">kurtosis</a></span><span class="op">(</span><span class="va">Data</span><span class="op">$</span><span class="va">interest_rate</span><span class="op">)</span> <span class="co">##greater than 0</span></span></code></pre></div>
<pre><code>## [1] 3.651631</code></pre>
<p>So our data is also right skewed and heavy tailed.</p>

</div>
</div>
</div>
  <div class="chapter-nav">
<div class="prev"><a href="discrete-random-variables.html"><span class="header-section-number">3</span> Discrete Random Variables</a></div>
<div class="next"><a href="joint-distributions.html"><span class="header-section-number">5</span> Joint Distributions</a></div>
</div></main><div class="col-md-3 col-lg-2 d-none d-md-block sidebar sidebar-chapter">
    <nav id="toc" data-toggle="toc" aria-label="On this page"><h2>On this page</h2>
      <ul class="nav navbar-nav">
<li><a class="nav-link" href="#continuous-random-variables"><span class="header-section-number">4</span> Continuous Random Variables</a></li>
<li><a class="nav-link" href="#introduction"><span class="header-section-number">4.1</span> Introduction</a></li>
<li>
<a class="nav-link" href="#cumulative-distribution-functions-cdfs-1"><span class="header-section-number">4.2</span> Cumulative Distribution Functions (CDFs)</a><ul class="nav navbar-nav"><li><a class="nav-link" href="#valid-cdfs-1"><span class="header-section-number">4.2.1</span> Valid CDFs</a></li></ul>
</li>
<li>
<a class="nav-link" href="#probability-density-functions-pdfs"><span class="header-section-number">4.3</span> Probability Density Functions (PDFs)</a><ul class="nav navbar-nav">
<li><a class="nav-link" href="#valid-pdfs"><span class="header-section-number">4.3.1</span> Valid PDFs</a></li>
<li><a class="nav-link" href="#pdfs-and-density-plots"><span class="header-section-number">4.3.2</span> PDFs and Density Plots</a></li>
</ul>
</li>
<li>
<a class="nav-link" href="#summaries-of-a-distribution"><span class="header-section-number">4.4</span> Summaries of a Distribution</a><ul class="nav navbar-nav">
<li><a class="nav-link" href="#expectations-1"><span class="header-section-number">4.4.1</span> Expectations</a></li>
<li><a class="nav-link" href="#variance-1"><span class="header-section-number">4.4.2</span> Variance</a></li>
<li><a class="nav-link" href="#moments"><span class="header-section-number">4.4.3</span> Moments</a></li>
<li><a class="nav-link" href="#skewness"><span class="header-section-number">4.4.4</span> Skewness</a></li>
<li><a class="nav-link" href="#kurtosis"><span class="header-section-number">4.4.5</span> Kurtosis</a></li>
</ul>
</li>
<li>
<a class="nav-link" href="#common-continuous-random-variables"><span class="header-section-number">4.5</span> Common Continuous Random Variables</a><ul class="nav navbar-nav">
<li><a class="nav-link" href="#uniform"><span class="header-section-number">4.5.1</span> Uniform</a></li>
<li><a class="nav-link" href="#normdist"><span class="header-section-number">4.5.2</span> Normal</a></li>
</ul>
</li>
<li>
<a class="nav-link" href="#using-r"><span class="header-section-number">4.6</span> Using R</a><ul class="nav navbar-nav">
<li><a class="nav-link" href="#KDE"><span class="header-section-number">4.6.1</span> Density Plots and Kernel Density Estimation</a></li>
<li><a class="nav-link" href="#density-plots-and-histograms"><span class="header-section-number">4.6.2</span> Density Plots and Histograms</a></li>
<li><a class="nav-link" href="#numerical-summaries"><span class="header-section-number">4.6.3</span> Numerical Summaries</a></li>
</ul>
</li>
</ul>

      <div class="book-extra">
        <ul class="list-unstyled">
          
        </ul>
</div>
    </nav>
</div>

</div>
</div> <!-- .container -->

<footer class="bg-primary text-light mt-5"><div class="container"><div class="row">

  <div class="col-12 col-md-6 mt-3">
    <p>"<strong>Understanding Uncertainty Course Notes</strong>" was written by Jeffrey Woo. It was last built on 2025-07-04.</p>
  </div>

  <div class="col-12 col-md-6 mt-3">
    <p>This book was built by the <a class="text-light" href="https://bookdown.org">bookdown</a> R package.</p>
  </div>

</div></div>
</footer><!-- dynamically load mathjax for compatibility with self-contained --><script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.9/latest.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script><script type="text/x-mathjax-config">const popovers = document.querySelectorAll('a.footnote-ref[data-toggle="popover"]');
for (let popover of popovers) {
  const div = document.createElement('div');
  div.setAttribute('style', 'position: absolute; top: 0, left:0; width:0, height:0, overflow: hidden; visibility: hidden;');
  div.innerHTML = popover.getAttribute('data-content');

  var has_math = div.querySelector("span.math");
  if (has_math) {
    document.body.appendChild(div);
    MathJax.Hub.Queue(["Typeset", MathJax.Hub, div]);
    MathJax.Hub.Queue(function() {
      popover.setAttribute('data-content', div.innerHTML);
      document.body.removeChild(div);
    })
  }
}
</script>
</body>
</html>
