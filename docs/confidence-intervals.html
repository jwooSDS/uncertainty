<!DOCTYPE html>
<html lang="en">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<title>Chapter 8 Confidence Intervals | Understanding Uncertainty Course Notes</title>
<meta name="author" content="Jeffrey Woo">
<meta name="description" content="This module is based on Introduction to Probability for Data Science (Chan), Chapter 9.1 and 9.2. You can access the book for free at https://probability4datascience.com. Please note that I cover...">
<meta name="generator" content="bookdown 0.43 with bs4_book()">
<meta property="og:title" content="Chapter 8 Confidence Intervals | Understanding Uncertainty Course Notes">
<meta property="og:type" content="book">
<meta property="og:description" content="This module is based on Introduction to Probability for Data Science (Chan), Chapter 9.1 and 9.2. You can access the book for free at https://probability4datascience.com. Please note that I cover...">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="Chapter 8 Confidence Intervals | Understanding Uncertainty Course Notes">
<meta name="twitter:description" content="This module is based on Introduction to Probability for Data Science (Chan), Chapter 9.1 and 9.2. You can access the book for free at https://probability4datascience.com. Please note that I cover...">
<!-- JS --><script src="https://cdnjs.cloudflare.com/ajax/libs/clipboard.js/2.0.6/clipboard.min.js" integrity="sha256-inc5kl9MA1hkeYUt+EC3BhlIgyp/2jDIyBLS6k3UxPI=" crossorigin="anonymous"></script><script src="https://cdnjs.cloudflare.com/ajax/libs/fuse.js/6.4.6/fuse.js" integrity="sha512-zv6Ywkjyktsohkbp9bb45V6tEMoWhzFzXis+LrMehmJZZSys19Yxf1dopHx7WzIKxr5tK2dVcYmaCk2uqdjF4A==" crossorigin="anonymous"></script><script src="https://kit.fontawesome.com/6ecbd6c532.js" crossorigin="anonymous"></script><script src="libs/jquery-3.6.0/jquery-3.6.0.min.js"></script><meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<link href="libs/bootstrap-4.6.0/bootstrap.min.css" rel="stylesheet">
<script src="libs/bootstrap-4.6.0/bootstrap.bundle.min.js"></script><script src="libs/bs3compat-0.9.0/transition.js"></script><script src="libs/bs3compat-0.9.0/tabs.js"></script><script src="libs/bs3compat-0.9.0/bs3compat.js"></script><link href="libs/bs4_book-1.0.0/bs4_book.css" rel="stylesheet">
<script src="libs/bs4_book-1.0.0/bs4_book.js"></script><script src="https://cdnjs.cloudflare.com/ajax/libs/autocomplete.js/0.38.0/autocomplete.jquery.min.js" integrity="sha512-GU9ayf+66Xx2TmpxqJpliWbT5PiGYxpaG8rfnBEk1LL8l1KGkRShhngwdXK1UgqhAzWpZHSiYPc09/NwDQIGyg==" crossorigin="anonymous"></script><script src="https://cdnjs.cloudflare.com/ajax/libs/mark.js/8.11.1/mark.min.js" integrity="sha512-5CYOlHXGh6QpOFA/TeTylKLWfB3ftPsde7AnmhuitiTX4K5SqCLBeKro6sPS8ilsz1Q4NRx3v8Ko2IBiszzdww==" crossorigin="anonymous"></script><!-- CSS --><style type="text/css">
    
    div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
  </style>
</head>
<body data-spy="scroll" data-target="#toc">

<div class="container-fluid">
<div class="row">
  <header class="col-sm-12 col-lg-3 sidebar sidebar-book"><a class="sr-only sr-only-focusable" href="#content">Skip to main content</a>

    <div class="d-flex align-items-start justify-content-between">
      <h1>
        <a href="index.html" title="">Understanding Uncertainty Course Notes</a>
      </h1>
      <button class="btn btn-outline-primary d-lg-none ml-2 mt-1" type="button" data-toggle="collapse" data-target="#main-nav" aria-expanded="true" aria-controls="main-nav"><i class="fas fa-bars"></i><span class="sr-only">Show table of contents</span></button>
    </div>

    <div id="main-nav" class="collapse-lg">
      <form role="search">
        <input id="search" class="form-control" type="search" placeholder="Search" aria-label="Search">
</form>

      <nav aria-label="Table of contents"><h2>Table of contents</h2>
        <ul class="book-toc list-unstyled">
<li><a class="" href="index.html">Preface</a></li>
<li><a class="" href="descriptive.html"><span class="header-section-number">1</span> Descriptive Statistics</a></li>
<li><a class="" href="probability.html"><span class="header-section-number">2</span> Probability</a></li>
<li><a class="" href="discrete-random-variables.html"><span class="header-section-number">3</span> Discrete Random Variables</a></li>
<li><a class="" href="continuous-random-variables.html"><span class="header-section-number">4</span> Continuous Random Variables</a></li>
<li><a class="" href="joint-distributions.html"><span class="header-section-number">5</span> Joint Distributions</a></li>
<li><a class="" href="inequalities-limit-theorems-and-simulations.html"><span class="header-section-number">6</span> Inequalities, Limit Theorems, and Simulations</a></li>
<li><a class="" href="est.html"><span class="header-section-number">7</span> Estimation</a></li>
<li><a class="active" href="confidence-intervals.html"><span class="header-section-number">8</span> Confidence Intervals</a></li>
<li><a class="" href="hypothesis-testing.html"><span class="header-section-number">9</span> Hypothesis Testing</a></li>
<li><a class="" href="linear-regression.html"><span class="header-section-number">10</span> Linear Regression</a></li>
<li><a class="" href="bayesian-modeling-part-1.html"><span class="header-section-number">11</span> Bayesian Modeling Part 1</a></li>
<li><a class="" href="bayesian-modeling-part-2.html"><span class="header-section-number">12</span> Bayesian Modeling Part 2</a></li>
</ul>

        <div class="book-extra">
          
        </div>
      </nav>
</div>
  </header><main class="col-sm-12 col-md-9 col-lg-7" id="content"><div id="confidence-intervals" class="section level1" number="8">
<h1>
<span class="header-section-number">8</span> Confidence Intervals<a class="anchor" aria-label="anchor" href="#confidence-intervals"><i class="fas fa-link"></i></a>
</h1>
<p>This module is based on Introduction to Probability for Data Science (Chan), Chapter 9.1 and 9.2. You can access the book for free at <a href="https://probability4datascience.com" class="uri">https://probability4datascience.com</a>. Please note that I cover additional topics, and skip certain topics from the book.</p>
<div id="introduction-4" class="section level2" number="8.1">
<h2>
<span class="header-section-number">8.1</span> Introduction<a class="anchor" aria-label="anchor" href="#introduction-4"><i class="fas fa-link"></i></a>
</h2>
<p>In Section <a href="est.html#est">7</a>, we use data from our sample to estimate parameters of a population. For example, we could use the sample mean systolic blood pressure of 750 randomly selected American adults to estimate the mean systolic blood pressure of all American adults. We also established that estimators such as the sample mean have randomness in them. If we were to obtain another random sample of 750 American adults, the sample mean blood pressure from this other sample is likely to be different from the original random sample. So is there uncertainty in our estimator due to random sampling. We also learned about ways to measure how “well” an estimator does in estimating the parameter, such as bias, variance, standard error, and mean-squared error of the estimator.</p>
<p>In this section, we will introduce confidence intervals. Confidence intervals build on the ideas from Section <a href="est.html#est">7</a>: that estimators are random and we can quantify their uncertainty. The purpose of a confidence interval is to provide a range of plausible values for an unknown population parameter, based on a sample. A confidence interval not only provides the estimated value of the parameter, but also a measure of uncertainty associated with the estimation.</p>
<p>We will first cover confidence intervals for the mean and confidence intervals for the proportion, two of the most basic confidence intervals. These intervals are based on the fact that the distribution of their corresponding estimators, the sample mean and sample proportion, are known as long as certain conditions are met. You will notice that the general ideas in finding confidence intervals are pretty similar; confidence intervals for other estimators that have known distributions will be constructed similarly. In the last subsection of this module, we will learn about the bootstrap, which is used when the distribution of an estimator is unknown.</p>
<div id="module-roadmap-6" class="section level3" number="8.1.1">
<h3>
<span class="header-section-number">8.1.1</span> Module Roadmap<a class="anchor" aria-label="anchor" href="#module-roadmap-6"><i class="fas fa-link"></i></a>
</h3>
<ul>
<li>Section <a href="confidence-intervals.html#CImean">8.2</a> covers the key concepts in confidence intervals and how they apply to the mean.</li>
<li>Section <a href="confidence-intervals.html#CIprop">8.3</a> shows how the concepts from the previous section carry over to the confidence interval for the proportion.</li>
<li>Section <a href="confidence-intervals.html#bootstrap">8.4</a> goes over the method to create confidence intervals with estimators with unknown sampling distributions.</li>
</ul>
</div>
</div>
<div id="CImean" class="section level2" number="8.2">
<h2>
<span class="header-section-number">8.2</span> Confidence Interval for the Mean<a class="anchor" aria-label="anchor" href="#CImean"><i class="fas fa-link"></i></a>
</h2>
<div id="randomness-of-estimators" class="section level3" number="8.2.1">
<h3>
<span class="header-section-number">8.2.1</span> Randomness of Estimators<a class="anchor" aria-label="anchor" href="#randomness-of-estimators"><i class="fas fa-link"></i></a>
</h3>
<p>Suppose we are trying to estimate the mean systolic blood pressure of all American adults, by using the sample mean of 750 randomly selected American adults. The sample mean is an estimator for the population mean. We want to be able to report the value of the estimator, as well as our uncertainty about the estimator. A way to measure the uncertainty of an estimator is through the variance or standard error of the estimator. Larger values indicate a higher degree of uncertainty, as an estimator with larger variance means that the value of the estimator is likely to be different among random samples.</p>
<p>The Monte Carlo simulations in Section <a href="est.html#estprops">7.4</a> show that there is a distribution associated with an estimator. We will start with the sample mean, since its distribution is known (see Section <a href="est.html#sampdistmean">7.4.4.1</a>). We will talk about the confidence interval for the mean first, before generalizing these ideas to other estimators with known distributions.</p>
</div>
<div id="randomness-of-confidence-intervals" class="section level3" number="8.2.2">
<h3>
<span class="header-section-number">8.2.2</span> Randomness of Confidence Intervals<a class="anchor" aria-label="anchor" href="#randomness-of-confidence-intervals"><i class="fas fa-link"></i></a>
</h3>
<p>The <strong>confidence interval</strong> is a probability applied to the estimator <span class="math inline">\(\bar{X}_n\)</span>. Instead of focusing on the estimated value of the sample mean and its variance, we construct a confidence interval for the mean that takes the form:</p>
<p><span class="math display" id="eq:8-CIbasic">\[\begin{equation}
I = \left(\bar{X}_n - \epsilon, \bar{X}_n + \epsilon \right).
\tag{8.1}
\end{equation}\]</span></p>
<p>Some terminology associated with intervals of the form in equation <a href="confidence-intervals.html#eq:8-CIbasic">(8.1)</a>:</p>
<ul>
<li>
<p><span class="math inline">\(\epsilon\)</span> is often called the <strong>margin of error</strong>. (Yes that margin of error that you often see reported in elections polls). This value is a function of the standard error of the estimator, so it gives a measure of uncertainty of the estimated value.</p>
<ul>
<li>Remember that the uncertainty being measured is the uncertainty due to random sampling, not due to other sources of uncertainty such as not getting a representative sample, people lying, etc. As mentioned in earlier modules, other methods are used to handle such issues and belong to the field of survey sampling, which is very interesting and an active area of research. We will not get into these issues in this class.</li>
</ul>
</li>
<li><p>The value of <span class="math inline">\(\bar{X}_n - \epsilon\)</span> is often called the <strong>lower bound</strong> of the confidence interval.</p></li>
<li><p>The value of <span class="math inline">\(\bar{X}_n + \epsilon\)</span> is often called the <strong>upper bound</strong> of the confidence interval.</p></li>
<li><p>The value of <span class="math inline">\(\bar{X}_n\)</span> is often called the <strong>point estimate</strong> of the the population mean.</p></li>
</ul>
<p>Equation <a href="confidence-intervals.html#eq:8-CIbasic">(8.1)</a> is sometimes expressed as</p>
<p><span class="math display" id="eq:8-CIbasic2">\[\begin{equation}
\text{point estimate } \pm \text{ margin of error}.
\tag{8.2}
\end{equation}\]</span></p>
<p>Indeed, a lot of confidence intervals take on the form expressed in equation <a href="confidence-intervals.html#eq:8-CIbasic2">(8.2)</a>, as long as the sampling distribution of the estimator is symmetric. Given the interval for the mean expressed in equation <a href="confidence-intervals.html#eq:8-CIbasic">(8.1)</a>, we ask what is the probability that the interval <span class="math inline">\(I\)</span> includes the true value of the parameter <span class="math inline">\(\mu\)</span>, i.e. we want to evaluate</p>
<p><span class="math display" id="eq:8-CIprob">\[\begin{equation}
P(\mu \in  I) = P(\bar{X}_n - \epsilon \leq \mu \leq \bar{X}_n + \epsilon).
\tag{8.3}
\end{equation}\]</span></p>
<p>It is important to bear in mind that since the estimator, the sample mean <span class="math inline">\(\bar{X}_n\)</span> is a random variable, there will also be randomness in the interval <span class="math inline">\(I\)</span>. The numerical values of the lower and upper bounds will change with a different random sample, since the value of <span class="math inline">\(\bar{X}_n\)</span> will change.</p>
<p>The idea of the interval <span class="math inline">\(I\)</span> being random is represented in Figure <a href="confidence-intervals.html#fig:8-CI">8.1</a> below:</p>
<div class="figure">
<span style="display:block;" id="fig:8-CI"></span>
<img src="images/08-CI.png" alt="Randomness of Confidence Interval. Picture from  https://en.wikipedia.org/wiki/Confidence_interval"><p class="caption">
Figure 8.1: Randomness of Confidence Interval. Picture from <a href="https://en.wikipedia.org/wiki/Confidence_interval" class="uri">https://en.wikipedia.org/wiki/Confidence_interval</a>
</p>
</div>
<ul>
<li>The density curve in the top of Figure <a href="confidence-intervals.html#fig:8-CI">8.1</a> represents the PDF of a random variable, that represents the distribution of some variable in the population that we wish to study.</li>
<li>Each row of dots represents the values of 10 randomly sampled data points from the PDF.</li>
<li>The colored lines in each row represent the lower and upper bounds of a 50% confidence interval calculated from the sampled data points in the row.</li>
<li>The colored dot in the center of the confidence interval represents <span class="math inline">\(\bar{x}\)</span> for the sampled data points in the row.</li>
<li>The intervals in blue represent confidence intervals that contain the value of <span class="math inline">\(\mu\)</span>, while the intervals in red represent confidence intervals that do not contain the value of <span class="math inline">\(\mu\)</span>.</li>
</ul>
<p>In Figure <a href="confidence-intervals.html#fig:8-CI">8.1</a>, we note that 50% of the confidence intervals capture the value of <span class="math inline">\(\mu\)</span>, so the probability per equation <a href="confidence-intervals.html#eq:8-CIprob">(8.3)</a> is 0.5. This matches with theory since each confidence interval in Figure <a href="confidence-intervals.html#fig:8-CI">8.1</a> was computed at 50% confidence.</p>
<p>If we were to create 95% confidence intervals for each row in Figure <a href="confidence-intervals.html#fig:8-CI">8.1</a>, the upper and lower bounds of the intervals will be adjusted so that we will expect 19 of these 20 intervals to contain the value of <span class="math inline">\(\mu\)</span>.</p>
<p>This illustration gives us an interpretation of the probability associated with a confidence interval per equation <a href="confidence-intervals.html#eq:8-CIprob">(8.3)</a>: When we construct a 95% confidence interval, there is a 95% chance the random interval <span class="math inline">\(I\)</span> will contain the true value of the parameter. In other words, if we have 100 random samples and we construct 95% confidence intervals based on each sample, we expect 95 of these intervals to contain the value of the parameter.</p>
<p>The idea of the probability that the random interval <span class="math inline">\(I\)</span> captures the true parameter gives rise to the <strong>confidence level</strong>. The confidence level of a confidence interval is denoted by <span class="math inline">\(1-\alpha\)</span>, i.e. if we construct an interval at 95% confidence, <span class="math inline">\(\alpha=0.05\)</span>. Equation <a href="confidence-intervals.html#eq:8-CIprob">(8.3)</a> can be written as</p>
<p><span class="math display" id="eq:8-CIalpha">\[\begin{equation}
P(\bar{X}_n - \epsilon \leq \mu \leq \bar{X}_n + \epsilon) = 1 - \alpha.
\tag{8.4}
\end{equation}\]</span></p>
<p>We will then say <span class="math inline">\(I\)</span> is a <span class="math inline">\((1-\alpha) \times 100\%\)</span> confidence interval, or <span class="math inline">\(I\)</span> is a confidence interval with confidence level of <span class="math inline">\((1-\alpha) \times 100\%\)</span>.</p>
<p>Now that we have established that confidence intervals are random and the definition of the confidence level, we are ready to go into the details of constructing the confidence interval for the mean.</p>
</div>
<div id="constructing-confidence-interval-for-the-mean" class="section level3" number="8.2.3">
<h3>
<span class="header-section-number">8.2.3</span> Constructing Confidence Interval for the Mean<a class="anchor" aria-label="anchor" href="#constructing-confidence-interval-for-the-mean"><i class="fas fa-link"></i></a>
</h3>
<p>We remind ourselves of the sampling distribution of the sample mean, <span class="math inline">\(\bar{X}_n\)</span>, from Section <a href="est.html#sampdistmean">7.4.4.1</a>. There are a couple of conditions to consider:</p>
<ol style="list-style-type: decimal">
<li><p><span class="math inline">\(X_1, \cdots, X_n\)</span> are i.i.d. from a normal distribution with finite mean <span class="math inline">\(\mu\)</span> and finite variance <span class="math inline">\(\sigma^2\)</span>. Then <span class="math inline">\(\bar{X}_n \sim N(\mu, \frac{\sigma^2}{n})\)</span>.</p></li>
<li><p><span class="math inline">\(X_1, \cdots, X_n\)</span> are i.i.d. from any distribution with finite mean <span class="math inline">\(\mu\)</span> and finite variance <span class="math inline">\(\sigma^2\)</span>, and if <span class="math inline">\(n\)</span> is large enough, then <span class="math inline">\(\bar{X}_n\)</span> is approximately <span class="math inline">\(N(\mu, \frac{\sigma^2}{n})\)</span>.</p></li>
</ol>
<p>If either of these conditions are met, then the distribution of <span class="math inline">\(\bar{X}_n\)</span> after standardization is either a standard normal or approaches a standard normal distribution, so <span class="math inline">\(\frac{\bar{X}_n - \mu}{\frac{\sigma}{\sqrt{n}}} = \frac{\bar{X}_n - \mu}{\sqrt{Var(\bar{X})}} = \frac{\bar{X}_n - \mu}{SE(\bar{X}_n)}\)</span> is either standard normal or approximately standard normal when <span class="math inline">\(n\)</span> is large enough.</p>
<p>To simplify the notation as it pertains to the confidence interval for the mean, we will let <span class="math inline">\(\hat{Z} = \frac{\bar{X}_n - \mu}{SE(\bar{X})}\)</span>, so <span class="math inline">\(\hat{Z}\)</span> is standard normal or approximately standard normal. <span class="math inline">\(\hat{Z}\)</span> can be called the <strong>standardized version of the sample mean</strong> or a <strong>standardized score</strong>.</p>
<div id="critical-value" class="section level4" number="8.2.3.1">
<h4>
<span class="header-section-number">8.2.3.1</span> Critical Value<a class="anchor" aria-label="anchor" href="#critical-value"><i class="fas fa-link"></i></a>
</h4>
<p>We perform some math operations on equation <a href="confidence-intervals.html#eq:8-CIalpha">(8.4)</a> to see how we can construct a confidence interval for the mean:</p>
<p><span class="math display" id="eq:8-CIcrit">\[\begin{equation}
\begin{split}
P(\bar{X}_n - \epsilon \leq \mu \leq \bar{X}_n + \epsilon) &amp;= 1 - \alpha \\
\implies P(|\bar{X}_n - \mu| \leq \epsilon) &amp;= 1 - \alpha \\
\implies P \left(|\hat{Z}| = |\frac{\bar{X}_n - \mu}{SE(\bar{X}_n)}|  \leq \frac{\epsilon}{SE(\bar{X}_n)} = z^{*} \right) &amp;= 1 - \alpha \\
\implies P(|\hat{Z}| \leq z^{*}) &amp;= 1 - \alpha \\
\implies P(-z^{*} \leq \hat{Z} \leq z^{*}) &amp;= 1 - \alpha.
\end{split}
\tag{8.5}
\end{equation}\]</span></p>
<p>In equation <a href="confidence-intervals.html#eq:8-CIcrit">(8.5)</a>, <span class="math inline">\(z^*\)</span> is called the <strong>critical value</strong>. So we can see how it is related to the margin of error, <span class="math inline">\(\epsilon\)</span>: the margin of error is the critical value multiplied by the standard error of the estimator (which in this case is the standard error of the sample mean since we are constructing the confidence interval for the mean).</p>
<p>In words, equation <a href="confidence-intervals.html#eq:8-CIcrit">(8.5)</a> says that we want to find the critical value <span class="math inline">\(z^*\)</span> so that the probability that a standardized score is between <span class="math inline">\(-z^*\)</span> and <span class="math inline">\(z^*\)</span> is <span class="math inline">\(1 - \alpha\)</span>. Visually, this probability is displayed in Figure <a href="confidence-intervals.html#fig:8-crit">8.2</a> below when <span class="math inline">\(\alpha=0.05\)</span>. We want to find the values on the horizontal axis so that the blue shaded area corresponds to a value of 0.95 (recall that area under a PDF represents probability).</p>
<div class="figure">
<span style="display:block;" id="fig:8-crit"></span>
<img src="bookdown-demo_files/figure-html/8-crit-1.png" alt="Finding Critical Value with 95% Confidence" width="672"><p class="caption">
Figure 8.2: Finding Critical Value with 95% Confidence
</p>
</div>
<p>We continue working with equation <a href="confidence-intervals.html#eq:8-CIcrit">(8.5)</a> to see how we obtain the value of <span class="math inline">\(z^*\)</span>, as long as either of the two conditions for the sampling distribution of <span class="math inline">\(\bar{X}_n\)</span> to be known are met:</p>
<p><span class="math display" id="eq:8-CIcrit2">\[\begin{equation}
\begin{split}
P(-z^{*} \leq \hat{Z} \leq z^{*}) &amp;= P(\hat{Z} \leq z^{*}) - P(\hat{Z} \leq -z^{*}) \\
                                  &amp;= \Phi(z^{*}) - \Phi(-z^{*}) = 1 - \alpha.
\end{split}
\tag{8.6}
\end{equation}\]</span></p>
<p>where <span class="math inline">\(\Phi(z) = P(\hat{Z} \leq z)\)</span> is the CDF of a standard normal. Due to the symmetry of the standard normal, <span class="math inline">\(\Phi(-z^{*}) = 1- \Phi(z^{*})\)</span>, and we sub this into equation <a href="confidence-intervals.html#eq:8-CIcrit2">(8.6)</a> and continue working with it to solve for <span class="math inline">\(z^*\)</span>:</p>
<p><span class="math display" id="eq:8-CIcrit3">\[\begin{equation}
\begin{split}
P(-z^{*} \leq \hat{Z} \leq z^{*}) &amp;= 2 \Phi(z^*) - 1 = 1 - \alpha \\
\implies \Phi(z^*) &amp;= 1 - \frac{\alpha}{2} \\
\implies z^* &amp;= \Phi^{-1} \left(1 - \frac{\alpha}{2} \right)
\end{split}
\tag{8.7}
\end{equation}\]</span></p>
<p>So <span class="math inline">\(z^*\)</span> is found by inverting the CDF of a standard normal evaluated at <span class="math inline">\(1 - \frac{\alpha}{2}\)</span>. This quantity can be easily be found using R. For example, for 95% confidence, <span class="math inline">\(\alpha = 0.05\)</span>, so we type:</p>
<div class="sourceCode" id="cb140"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">alpha</span><span class="op">&lt;-</span><span class="fl">0.05</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/stats/Normal.html">qnorm</a></span><span class="op">(</span><span class="fl">1</span><span class="op">-</span><span class="va">alpha</span><span class="op">/</span><span class="fl">2</span><span class="op">)</span></span></code></pre></div>
<pre><code>## [1] 1.959964</code></pre>
<p>which tells us the critical value is 1.96 for 95% confidence.</p>
<p>Note: the <code><a href="https://rdrr.io/r/stats/Normal.html">qnorm()</a></code> function was introduced in a bit more detail in Section <a href="continuous-random-variables.html#RCont">4.6</a>, so feel free to go back to review.</p>
<p>View the video below that explains these steps in a bit more detail:</p>
<iframe width="560px" height="320px" allowfullscreen="true" allow="autoplay *" title="Module 08: Critical Value" src="https://virginiauniversity.instructuremedia.com/embed/74aa9eff-5d3f-4c24-be5d-5798d93f7aa7" frameborder="0">
</iframe>
<p><em>Thought question</em>: Can you show that the critical value for 96% confidence is about 2.054? Can you show that the critical value for 98% confidence is about 2.326?</p>
</div>
<div id="confidence-interval-for-the-mean" class="section level4" number="8.2.3.2">
<h4>
<span class="header-section-number">8.2.3.2</span> Confidence Interval for the Mean<a class="anchor" aria-label="anchor" href="#confidence-interval-for-the-mean"><i class="fas fa-link"></i></a>
</h4>
<p>We are now ready to put the pieces together to work on the confidence interval for the mean:</p>
<p><span class="math display" id="eq:8-CImeanwork">\[\begin{equation}
\begin{split}
P(-z^{*} \leq \hat{Z} \leq z^{*}) &amp;= P(-z^{*} \leq \frac{\bar{X}_n - \mu}{SE(\bar{X}_n)} \leq z^{*}) \\
                                  &amp;= P\left(-z^{*}SE(\bar{X}_n) \leq \bar{X}_n - \mu \leq z^{*}SE(\bar{X}_n)\right) \\
                                  &amp;= P\left(\bar{X}_n - z^{*}SE(\bar{X}_n) \leq \mu \leq \bar{X}_n + z^{*}SE(\bar{X}_n)\right) \\
                                  &amp;= P\left(\bar{X}_n - z^{*} \frac{\sigma}{\sqrt{n}} \leq \mu \leq \bar{X}_n + z^{*} \frac{\sigma}{\sqrt{n}}\right).
\end{split}
\tag{8.8}
\end{equation}\]</span></p>
<p>Therefore, the <span class="math inline">\((1-\alpha) \times 100\%\)</span> confidence interval for the mean is</p>
<p><span class="math display" id="eq:8-CImean">\[\begin{equation}
\left( \bar{x}_n - z^{*} \frac{\sigma}{\sqrt{n}}, \bar{x}_n + z^{*} \frac{\sigma}{\sqrt{n}} \right).
\tag{8.9}
\end{equation}\]</span></p>
<p>View the video below that explains these steps in a bit more detail:</p>
<iframe width="560px" height="320px" allowfullscreen="true" allow="autoplay *" title="Module 08: CI for Mean" src="https://virginiauniversity.instructuremedia.com/embed/a1569325-fe99-49d4-80ff-dd7c3967b450" frameborder="0">
</iframe>
<p>Again, the formula in equation <a href="confidence-intervals.html#eq:8-CImean">(8.9)</a> is only valid if either of the two conditions in Section <a href="est.html#sampdistmean">7.4.4.1</a> is met, i.e. either the data are originally normal, or if the sample size is large enough.</p>
<p>There are several rules of thumb that exist to assess if the “sample size is large enough” (usually at least 25 or 30 is suggested). However, as we mentioned in Section (considerCLT), there is no fixed answer to this question. It depends on the distribution of the data. In general, the more skewed data is, <span class="math inline">\(n\)</span> needs to be larger for the approximation to work.</p>
</div>
<div id="CP" class="section level4" number="8.2.3.3">
<h4>
<span class="header-section-number">8.2.3.3</span> Coverage Probability<a class="anchor" aria-label="anchor" href="#CP"><i class="fas fa-link"></i></a>
</h4>
<p>Figure <a href="confidence-intervals.html#fig:8-CI">8.1</a> illustrates the concept of <strong>coverage probability</strong>. 20 random samples were drawn, and corresponding 50% confidence intervals were constructed, and we find that 10 out of 20 of these intervals contained the true value of the parameter. The coverage probability is 50%, since 10 out of 20 intervals contained the true value value of the parameter. This matches the confidence level of 50%. The coverage probability should match the confidence level, if not, the distribution that we used for the sampling distribution of the estimator is probably incorrect.</p>
<p>So in general, for confidence intervals constructed at <span class="math inline">\((1-\alpha) \times 100\%\)</span> confidence, the coverage probability of the confidence intervals should be <span class="math inline">\((1-\alpha) \times 100\%\)</span>.</p>
<p>We run a Monte Carlo simulation to show that the coverage probability of confidence intervals for the mean using equation <a href="confidence-intervals.html#eq:8-CImean">(8.9)</a>. The code below does the following:</p>
<ul>
<li>Simulate <span class="math inline">\(X_1, \cdots, X_{10}\)</span> i.i.d. from standard normal.</li>
<li>Calculate 95% confidence interval for the mean using <a href="confidence-intervals.html#eq:8-CImean">(8.9)</a>.</li>
<li>Assess if the calculated confidence interval contains 0, since the true mean is 0 (we simulated data from standard normal)</li>
<li>Repeat these steps for a total of 10 thousand replicates.</li>
<li>Count the number of confidence intervals that contain 0, and divide by the number of replicates. This value estimates the coverage probability.
<ul>
<li>If we did things correctly (correct use of formula, correct distribution for estimator), this estimated coverage probability should be close to the confidence level of 95%.</li>
</ul>
</li>
</ul>
<div class="sourceCode" id="cb142"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">n</span><span class="op">&lt;-</span><span class="fl">10</span> <span class="co">## sample size of each random sample</span></span>
<span><span class="va">alpha</span><span class="op">&lt;-</span><span class="fl">0.05</span></span>
<span><span class="va">reps</span><span class="op">&lt;-</span><span class="fl">10000</span> </span>
<span></span>
<span><span class="va">CIs</span><span class="op">&lt;-</span><span class="fu"><a href="https://rdrr.io/r/base/array.html">array</a></span><span class="op">(</span><span class="fl">0</span>, <span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span><span class="va">reps</span>,<span class="fl">2</span><span class="op">)</span><span class="op">)</span> <span class="co">##store lower and upper bounds of CI</span></span>
<span><span class="va">contain</span><span class="op">&lt;-</span><span class="fu"><a href="https://rdrr.io/r/base/array.html">array</a></span><span class="op">(</span><span class="fl">0</span>, <span class="va">reps</span><span class="op">)</span> <span class="co">##store assessment if the true mean is contained within bounds of CI</span></span>
<span></span>
<span><span class="fu"><a href="https://rdrr.io/r/base/Random.html">set.seed</a></span><span class="op">(</span><span class="fl">100</span><span class="op">)</span></span>
<span><span class="kw">for</span> <span class="op">(</span><span class="va">i</span> <span class="kw">in</span> <span class="fl">1</span><span class="op">:</span><span class="va">reps</span><span class="op">)</span></span>
<span>  </span>
<span><span class="op">{</span></span>
<span>  </span>
<span>  <span class="va">X</span><span class="op">&lt;-</span><span class="fu"><a href="https://rdrr.io/r/stats/Normal.html">rnorm</a></span><span class="op">(</span><span class="va">n</span><span class="op">)</span> <span class="co">##draw n data points from standard normal</span></span>
<span>  <span class="co">##calculate elements needed for CI of mean</span></span>
<span>  <span class="va">xbar</span><span class="op">&lt;-</span><span class="fu"><a href="https://rdrr.io/r/base/mean.html">mean</a></span><span class="op">(</span><span class="va">X</span><span class="op">)</span> <span class="co">##sample mean</span></span>
<span>  <span class="va">SE</span><span class="op">&lt;-</span><span class="fl">1</span><span class="op">/</span><span class="fu"><a href="https://rdrr.io/r/base/MathFun.html">sqrt</a></span><span class="op">(</span><span class="va">n</span><span class="op">)</span> <span class="co">##SE of sample mean</span></span>
<span>  <span class="va">crit</span><span class="op">&lt;-</span><span class="fu"><a href="https://rdrr.io/r/stats/Normal.html">qnorm</a></span><span class="op">(</span><span class="fl">1</span><span class="op">-</span><span class="va">alpha</span><span class="op">/</span><span class="fl">2</span><span class="op">)</span> <span class="co">##critical value</span></span>
<span>  <span class="va">ME</span><span class="op">&lt;-</span><span class="va">crit</span><span class="op">*</span><span class="va">SE</span> <span class="co">##margin of error</span></span>
<span>  <span class="va">CIs</span><span class="op">[</span><span class="va">i</span>,<span class="fl">1</span><span class="op">]</span><span class="op">&lt;-</span><span class="va">xbar</span><span class="op">-</span><span class="va">ME</span> <span class="co">##lower bound of CI</span></span>
<span>  <span class="va">CIs</span><span class="op">[</span><span class="va">i</span>,<span class="fl">2</span><span class="op">]</span><span class="op">&lt;-</span><span class="va">xbar</span><span class="op">+</span><span class="va">ME</span> <span class="co">##upper bound of CI</span></span>
<span>  <span class="va">contain</span><span class="op">[</span><span class="va">i</span><span class="op">]</span><span class="op">&lt;-</span><span class="va">CIs</span><span class="op">[</span><span class="va">i</span>,<span class="fl">1</span><span class="op">]</span><span class="op">&lt;</span><span class="fl">0</span> <span class="op">&amp;</span> <span class="va">CIs</span><span class="op">[</span><span class="va">i</span>,<span class="fl">2</span><span class="op">]</span><span class="op">&gt;</span><span class="fl">0</span> <span class="co">##assess if CI contains 0, the true mean</span></span>
<span>  </span>
<span><span class="op">}</span></span>
<span></span>
<span><span class="co">##find proportion of CIs from random samples that contain 0, should be close to 1 - alpha</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/base/sum.html">sum</a></span><span class="op">(</span><span class="va">contain</span><span class="op">)</span><span class="op">/</span><span class="va">reps</span> </span></code></pre></div>
<pre><code>## [1] 0.9493</code></pre>
<p>The estimated coverage probability based on 10 thousand replicates is 94.93%, very close to the confidence level of 95%. This informs us that the distribution assumed for the sample mean was correct.</p>
<p>View the video below which explains the Monte Carlo simulation in a bit more detail:</p>
<iframe width="560px" height="320px" allowfullscreen="true" allow="autoplay *" title="Module 08: Coverage Probability" src="https://virginiauniversity.instructuremedia.com/embed/5e91e2c8-f648-4f9a-9831-4ac51f666f90" frameborder="0">
</iframe>
</div>
<div id="worked-example-1" class="section level4" number="8.2.3.4">
<h4>
<span class="header-section-number">8.2.3.4</span> Worked Example<a class="anchor" aria-label="anchor" href="#worked-example-1"><i class="fas fa-link"></i></a>
</h4>
<p>On the basis of extensive tests, the yield point of a particular type of mild steel reinforcing bar is known to be normally distributed with <span class="math inline">\(\sigma=100\)</span> pounds. The composition of the bar has been slightly modified, but the modification is not believed to have affected either the normality or the value of <span class="math inline">\(\sigma\)</span>. If a random sample of 25 modified bars resulted in a sample average yield point of 8439 pounds, compute a 90% CI for the true average yield point of the modified bars.</p>
<p>From the question, we summarize the information as:</p>
<ul>
<li>
<span class="math inline">\(n = 25\)</span>,</li>
<li>
<span class="math inline">\(\bar{x} = 8439\)</span>,</li>
<li>
<span class="math inline">\(\sigma = 100\)</span>,</li>
<li>
<span class="math inline">\(\alpha = 0.1\)</span>, so <span class="math inline">\(z^*\)</span> is found using <code>qnorm(1-0.1/2)</code> which is 1.644854.</li>
</ul>
<p>Since we are assuming the distribution of the yield points to be normally distribution, the sample means will be normally distributed regardless of the sample size, so we can proceed computing the confidence interval for the true average yield point using equation <a href="confidence-intervals.html#eq:8-CImean">(8.9)</a>:</p>
<p><span class="math display">\[
\left( 8439 - 1.644854 \frac{100}{\sqrt{25}} , 8439 + 1.644854 \frac{100}{\sqrt{25}} \right).
\]</span>
Working everything out, we get (8406.103, 8471.891).</p>
<p>Interpreting the CI: There is 90% probability that the random interval (8406.103, 8471.891) will include the true average yield point of modified bars.</p>
<p>What else can we say from the confidence interval?</p>
<ul>
<li><p>Values outside the confidence interval are considered to be “ruled out” as plausible values of the parameter. So if we wanted to assess if the average yield point of modified bars is 8000 pounds, our interval does not support this claim, since the value of 8000 lies outside the interval. We can say our data do not support the claim that the average yield point of modified bars is 8000 pounds.</p></li>
<li>
<p>Values inside the confidence interval are considered to be plausible values of the parameter. Any value inside the interval is considered plausible. A common mistake will be to specify a certain value in the interval, and conclude that the parameter is equal to that specific value. For example, it will be a mistake to say that since the value 8410 lies inside the interval, the interval supports the claim that the average yield point of modified bars is 8410 pounds. This is because other values in the interval are still considered plausible.</p>
<ul>
<li>In such a situation, we will say that our data do not support the claim that the average yield point of modified bars is different from 8410 pounds, since 8410 lies inside the interval. We cannot rule out the value of 8410.</li>
</ul>
</li>
</ul>
</div>
</div>
<div id="CImeant" class="section level3" number="8.2.4">
<h3>
<span class="header-section-number">8.2.4</span> Confidence Interval for the Mean Using t Distribution<a class="anchor" aria-label="anchor" href="#CImeant"><i class="fas fa-link"></i></a>
</h3>
<p>You may have noticed that when we calculate a confidence interval for the mean using equation <a href="confidence-intervals.html#eq:8-CImean">(8.9)</a>, it involves knowing the value of <span class="math inline">\(\sigma^2\)</span>, the variance of the variable in the population, which is a parameter. However, we have mentioned that the whole purpose of estimation and confidence intervals is to estimate the value of unknown parameters and quantify the uncertainty associated with the estimate. The numerical value of parameters are very rarely known! So how could we actually use equation <a href="confidence-intervals.html#eq:8-CImean">(8.9)</a> in real life?</p>
<p>The solution to this is fairly intuitive, we use the sample variance <span class="math inline">\(s^2 = \frac{1}{n-1} \sum_{i=1}^n (x_i - \bar{x})^2\)</span> to estimate <span class="math inline">\(\sigma^2\)</span>. What is less intuitive is that the distribution of the standardized version of the sample mean changes.</p>
<p>We had earlier mentioned that if certain conditions are met, then <span class="math inline">\(\hat{Z} = \frac{\bar{X}_n - \mu}{\frac{\sigma}{\sqrt{n}}}\)</span> is either standard normal or approximately standard normal. If we estimate <span class="math inline">\(\sigma^2\)</span> with <span class="math inline">\(s^2\)</span> and replace <span class="math inline">\(\sigma\)</span> with <span class="math inline">\(s\)</span> in <span class="math inline">\(\hat{Z}\)</span>, we get a new random variable</p>
<p><span class="math display" id="eq:8-tstat">\[\begin{equation}
T =  \frac{\bar{X}_n - \mu}{\frac{s}{\sqrt{n}}} = \frac{\bar{X}_n - \mu}{SE(\bar{X}_n)},
\tag{8.10}
\end{equation}\]</span></p>
<p>where <span class="math inline">\(SE(\bar{X}_n)\)</span> is now <span class="math inline">\(\frac{s}{\sqrt{n}}\)</span>. It turns out that <span class="math inline">\(T\)</span> follows another well-known distribution, called the <strong><span class="math inline">\(t\)</span> distribution with <span class="math inline">\(n-1\)</span> degrees of freedom</strong>. The PDF of a <span class="math inline">\(t\)</span> distribution is pretty long and is not needed for this course (you can look it up on your own), but we take a look at the plot of its PDF and compare it with the plot of the PDF of a standard normal, in Figure <a href="confidence-intervals.html#fig:8-t">8.3</a> below.</p>
<div class="sourceCode" id="cb144"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="co">##plot PDF from -5 to 5</span></span>
<span><span class="va">x</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/seq.html">seq</a></span><span class="op">(</span><span class="op">-</span><span class="fl">5</span>, <span class="fl">5</span>, length.out <span class="op">=</span> <span class="fl">100</span><span class="op">)</span></span>
<span></span>
<span><span class="co">##plot the standard normal </span></span>
<span><span class="fu"><a href="https://rdrr.io/r/graphics/curve.html">curve</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/stats/Normal.html">dnorm</a></span><span class="op">(</span><span class="va">x</span><span class="op">)</span>, from <span class="op">=</span> <span class="op">-</span><span class="fl">5</span>, to <span class="op">=</span> <span class="fl">5</span>, lwd <span class="op">=</span> <span class="fl">2</span>,</span>
<span>      ylab <span class="op">=</span> <span class="st">"Density"</span>, xlab <span class="op">=</span> <span class="st">"x"</span>, </span>
<span>      main <span class="op">=</span> <span class="st">"Pdf of Standard Normal and t-Distribution"</span><span class="op">)</span></span>
<span></span>
<span><span class="co">##overlay the t-distribution with 1 and 10 degree of freedom</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/graphics/curve.html">curve</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/stats/TDist.html">dt</a></span><span class="op">(</span><span class="va">x</span>, df <span class="op">=</span> <span class="fl">1</span><span class="op">)</span>, from <span class="op">=</span> <span class="op">-</span><span class="fl">5</span>, to <span class="op">=</span> <span class="fl">5</span>, col <span class="op">=</span> <span class="st">"red"</span>, lwd <span class="op">=</span> <span class="fl">2</span>, add <span class="op">=</span> <span class="cn">TRUE</span><span class="op">)</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/graphics/curve.html">curve</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/stats/TDist.html">dt</a></span><span class="op">(</span><span class="va">x</span>, df <span class="op">=</span> <span class="fl">10</span><span class="op">)</span>, from <span class="op">=</span> <span class="op">-</span><span class="fl">5</span>, to <span class="op">=</span> <span class="fl">5</span>, col <span class="op">=</span> <span class="st">"blue"</span>, lwd <span class="op">=</span> <span class="fl">2</span>, add <span class="op">=</span> <span class="cn">TRUE</span><span class="op">)</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/graphics/legend.html">legend</a></span><span class="op">(</span><span class="st">"topright"</span>, legend <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span><span class="st">"Standard Normal"</span>, <span class="st">"t-Distribution (df=1)"</span>, <span class="st">"t-Distribution (df=10)"</span><span class="op">)</span>,</span>
<span>       col <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span><span class="st">"black"</span>, <span class="st">"red"</span>, <span class="st">"blue"</span><span class="op">)</span>, lty <span class="op">=</span> <span class="fl">1</span>, lwd <span class="op">=</span> <span class="fl">2</span><span class="op">)</span></span></code></pre></div>
<div class="figure">
<span style="display:block;" id="fig:8-t"></span>
<img src="bookdown-demo_files/figure-html/8-t-1.png" alt="Plot of PDF for Z and t Distributions" width="672"><p class="caption">
Figure 8.3: Plot of PDF for Z and t Distributions
</p>
</div>
<p>From Figure <a href="confidence-intervals.html#fig:8-t">8.3</a>, we note a few things about the <span class="math inline">\(t\)</span> distribution:</p>
<ul>
<li>It is centered at 0, just like a standard normal.</li>
<li>It is also symmetric and bell-shaped, just like a standard normal.</li>
<li>It has heavier tails than the standard normal. In other words, extreme values (large or small) have slightly higher probabilities of occurring for a <span class="math inline">\(t\)</span> distribution than for a standard normal.</li>
<li>As the degree of freedom increases, the <span class="math inline">\(t\)</span> distribution gets closer to a standard normal. Notice how the blue curve is closer to the standard normal curve in black, than the red curve is with the black curve. In fact, one can show mathematically that the PDF of a <span class="math inline">\(t\)</span> distribution converges to the PDF of a standard normal as the degree of freedom increases to infinity.</li>
</ul>
<p>How does the fact that we are working with a <span class="math inline">\(t\)</span> distribution instead of a standard normal affect how we calculate a confidence interval for the mean? How does equation <a href="confidence-intervals.html#eq:8-CImean">(8.9)</a> change? We replace <span class="math inline">\(\sigma^2\)</span> with <span class="math inline">\(s^2\)</span> and <span class="math inline">\(z^*\)</span> with <span class="math inline">\(t^*\)</span>. The critical value is now denoted by <span class="math inline">\(t^*\)</span> to emphasize that it is based on the <span class="math inline">\(t\)</span> distribution.</p>
<p>As an example to find the critical value of a <span class="math inline">\(t\)</span> distribution with 10 degrees of freedom and with 95% confidence:</p>
<div class="sourceCode" id="cb145"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">alpha</span><span class="op">&lt;-</span><span class="fl">0.05</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/stats/TDist.html">qt</a></span><span class="op">(</span><span class="fl">1</span><span class="op">-</span><span class="va">alpha</span><span class="op">/</span><span class="fl">2</span>, <span class="fl">10</span><span class="op">)</span> <span class="co">##supply percentile first, then value of df</span></span></code></pre></div>
<pre><code>## [1] 2.228139</code></pre>
<p>Therefore, the <span class="math inline">\((1-\alpha) \times 100\%\)</span> confidence interval for the mean , when <span class="math inline">\(\sigma^2\)</span> is unknown, is</p>
<p><span class="math display" id="eq:8-CImeant">\[\begin{equation}
\left( \bar{x}_n - t^{*} \frac{s}{\sqrt{n}}, \bar{x}_n + t^{*} \frac{s}{\sqrt{n}} \right).
\tag{8.11}
\end{equation}\]</span></p>
<p>Again, the formula in equation <a href="confidence-intervals.html#eq:8-CImeant">(8.11)</a> is only valid if either of the two conditions in Section <a href="est.html#sampdistmean">7.4.4.1</a> is met, i.e. either the data are originally normal, or if the sample size is large enough.</p>
<div id="worked-example-2" class="section level4" number="8.2.4.1">
<h4>
<span class="header-section-number">8.2.4.1</span> Worked Example<a class="anchor" aria-label="anchor" href="#worked-example-2"><i class="fas fa-link"></i></a>
</h4>
<p>A sample of 66 obese adults was put on a low-carbohydrate diet for a year. The average weight loss was 11 lb and the
standard deviation was 19 lb. Calculate a 99% confidence interval for the true average weight loss. Does the confidence interval provide support for the claim that the mean weight loss is positive?</p>
<p>From the question, we summarize the information as:</p>
<ul>
<li>
<span class="math inline">\(n = 66\)</span>,</li>
<li><span class="math inline">\(df = n-1 = 65\)</span></li>
<li>
<span class="math inline">\(\bar{x} = 11\)</span>,</li>
<li>
<span class="math inline">\(s = 19\)</span>,</li>
<li>
<span class="math inline">\(\alpha = 0.01\)</span>, so <span class="math inline">\(t^*\)</span> is found using <code>qt(1-0.01/2, 65)</code> which is 2.653604.</li>
</ul>
<p>We have no information about the distribution of the data. However, the sample size is 66, which is usually large enough to use the CLT, so we can use equation <a href="confidence-intervals.html#eq:8-CImeant">(8.11)</a>:</p>
<p><span class="math display">\[
\left( 11 - 2.653604 \frac{19}{\sqrt{66}} , 11 + 2.653604 \frac{19}{\sqrt{66}} \right)
\]</span>
which gives (4.793914, 17.20609). Since the entire confidence interval lies above 0, our data support the claim that the mean weight loss is positive.</p>
</div>
<div id="confidence-interval-for-mean-using-t-vs-using-z" class="section level4" number="8.2.4.2">
<h4>
<span class="header-section-number">8.2.4.2</span> Confidence Interval for Mean Using <span class="math inline">\(t\)</span> VS Using <span class="math inline">\(z\)</span><a class="anchor" aria-label="anchor" href="#confidence-interval-for-mean-using-t-vs-using-z"><i class="fas fa-link"></i></a>
</h4>
<p>A couple of interesting things to note with critical values associated with the <span class="math inline">\(t\)</span> distribution:</p>
<ul>
<li>For the same level of confidence, <span class="math inline">\(t^*\)</span> will never be smaller than <span class="math inline">\(z^*\)</span>.</li>
</ul>
<p><em>Thought question</em>: Can you give an intuitive explanation as to why? Using Figure <a href="confidence-intervals.html#fig:8-t">8.3</a> may be helpful.</p>
<ul>
<li>The value of <span class="math inline">\(t^*\)</span> gets closer to the value of <span class="math inline">\(z^*\)</span> as the degree of freedom gets larger. This implies that <span class="math inline">\(t^*\)</span> is approximately equal to <span class="math inline">\(z^*\)</span> for large sample sizes.</li>
</ul>
<p>Recall that the margin of error is the critical value multiplied by the standard error of the estimator. So margin of errors tend to be larger when <span class="math inline">\(\sigma^2\)</span> is unknown. It should make intuitive sense that we have a higher degree of uncertainty since we have an additional parameter to estimate.</p>
<p>Since the margin of error tends to be larger when <span class="math inline">\(\sigma^2\)</span> is unknown, it means the <strong>width</strong> of the confidence interval for the mean tends to be wider when <span class="math inline">\(\sigma^2\)</span> is unknown. The width of a confidence interval is the difference between its upper and lower bound, or twice the margin of error.</p>
<p><em>Thought question</em>: Try running the Monte Carlo simulation in Section <a href="confidence-intervals.html#CP">8.2.3.3</a>, but now use the sample variance instead of the population variance. Find the coverage probabilities of the confidence interval if we use the correct formula in equation <a href="confidence-intervals.html#eq:8-CImeant">(8.11)</a>, and if we use a <span class="math inline">\(z^*\)</span> as the critical value with the sample variance.</p>
</div>
<div id="df" class="section level4" number="8.2.4.3">
<h4>
<span class="header-section-number">8.2.4.3</span> Degrees of Freedom<a class="anchor" aria-label="anchor" href="#df"><i class="fas fa-link"></i></a>
</h4>
<p>An intuitive explanation of degrees of freedom is the number of independent pieces of information that can take on any numerical value, when estimating a parameter.</p>
<p>Generally speaking, we lose 1 degree of freedom for every equation that must be satisfied. In the context of estimating the population mean using the sample mean, we must always satisfy the equation <span class="math inline">\(\bar{x} = \frac{\sum_{i=1}^n x_i}{n}\)</span>, so we lose 1 degree of freedom from an original set of <span class="math inline">\(n\)</span> observations, so the degree of freedom is <span class="math inline">\(n-1\)</span> when calculating the CI for the mean.</p>
<p>View the video below that explains the idea behind degrees of freedom with a toy example:</p>
<iframe width="560px" height="320px" allowfullscreen="true" allow="autoplay *" title="Module 08: Degrees of Freedom" src="https://virginiauniversity.instructuremedia.com/embed/d231fbbc-16a4-4a0d-85c0-e5ae3db0dee2" frameborder="0">
</iframe>
</div>
</div>
<div id="factors-affecting-precision-of-confidence-intervals" class="section level3" number="8.2.5">
<h3>
<span class="header-section-number">8.2.5</span> Factors Affecting Precision of Confidence Intervals<a class="anchor" aria-label="anchor" href="#factors-affecting-precision-of-confidence-intervals"><i class="fas fa-link"></i></a>
</h3>
<p>The width of a confidence interval is used as a measure of <strong>precision</strong>. A wider width indicates less precision and a higher degree of uncertainty with our estimate. Narrower widths are preferred, as we are able to narrow the range of plausible values for the unknown parameter by ruling out more values. The factors (that are within the researcher’s control) affecting the width of a confidence interval are the items used in calculating the margin of error:</p>
<ul>
<li>The level of confidence, <span class="math inline">\(1 - \alpha\)</span>: The width increases as the level of confidence increases.</li>
</ul>
<p><em>Though question</em>: Can you use Figure <a href="confidence-intervals.html#fig:8-crit">8.2</a> to help you explain the width increases as level of confidence increases?</p>
<ul>
<li>The sample size, <span class="math inline">\(n\)</span>: As <span class="math inline">\(n\)</span> increases, the width decreases. This implies that we have more precision, and a lower degree of uncertainty, with larger sample sizes.</li>
</ul>
<p>Other factors such as whether <span class="math inline">\(\sigma^2\)</span> is unknown or not, and the value of the variance, are usually not controllable by the researcher.</p>
</div>
</div>
<div id="CIprop" class="section level2" number="8.3">
<h2>
<span class="header-section-number">8.3</span> Confidence Interval for the Proportion<a class="anchor" aria-label="anchor" href="#CIprop"><i class="fas fa-link"></i></a>
</h2>
<p>Next, we will go over the confidence interval for the proportion. This is another common confidence interval. The sample proportion is an estimator for the population proportion.</p>
<p>Proportions are used to summarize categorical variables, whereas means are used to summarize quantitative variables. One way to decide if the variable is categorical or quantitative is to ask whether arithmetic operations make sense when performed on the variable. If such operations make sense, the variable is quantitative, if not, the variable is categorical. Consider the following two research questions:</p>
<ul>
<li><p>We measure the systolic blood pressure of a sample of American adults. The variable is quantitative, since the answer is a numeric value where arithmetic operations can be applied. We then calculate the average systolic blood pressure of American adults. So we can work on a confidence interval for the mean, using equation <a href="confidence-intervals.html#eq:8-CImeant">(8.11)</a>.</p></li>
<li><p>We ask a sample of voters whether they support a particular candidate. The variable is categorical, since the answer is yes or no, and we cannot apply arithmetic operations to the answer. We then calculate the proportion of voters who support the candidate. So we need a confidence interval for the proportion.</p></li>
</ul>
<div id="sampdistprops" class="section level3" number="8.3.1">
<h3>
<span class="header-section-number">8.3.1</span> Sampling Distribution of Sample Proportions<a class="anchor" aria-label="anchor" href="#sampdistprops"><i class="fas fa-link"></i></a>
</h3>
<p>We can use the Central Limit Theorem (CLT) to approximate the sampling distribution of sample proportions. A sketch of how to derive this sampling distribution is as follows:</p>
<ul>
<li><p>Let <span class="math inline">\(X_1, \cdots, X_n\)</span> be i.i.d. Bernoulli with success probability p. Using equations <a href="discrete-random-variables.html#eq:3-bern-EX">(3.9)</a> and <a href="discrete-random-variables.html#eq:3-bern-var">(3.10)</a>, we know that <span class="math inline">\(E(X_i) = p\)</span> and <span class="math inline">\(Var(X_i) = p(1-p)\)</span>.</p></li>
<li><p>Let <span class="math inline">\(S = X_1 + \cdots + X_n = \sum_{i=1}^n X_i\)</span> denote the number of successes in a sample of size <span class="math inline">\(n\)</span>. Using properties of expectations and variances, we know that <span class="math inline">\(E(S) = np\)</span> and <span class="math inline">\(Var(S) = np(1-p)\)</span>.</p></li>
<li><p>The sample proportion, <span class="math inline">\(\hat{p}\)</span>, is just the number of successes divided by the sample size, so <span class="math inline">\(\hat{p} = \frac{S}{n} = \frac{\sum_{i=1}^n X_i}{n} = \bar{X}_n\)</span>. Therefore, using property of expectations and variances, we know that <span class="math inline">\(E(\hat{p}) = p\)</span> and <span class="math inline">\(Var(\hat{p}) = \frac{p(1-p)}{n}\)</span>.</p></li>
<li><p>The CLT informs us if <span class="math inline">\(n\)</span> is large enough, then <span class="math inline">\(\hat{p}\)</span> is approximately <span class="math inline">\(N\left(p, \frac{p(1-p)}{n}\right)\)</span>. Therefore, the distribution of <span class="math inline">\(\hat{p}\)</span> after standardization is approximately standard normal, i.e. the standardized score <span class="math inline">\(\hat{Z} = \frac{\hat{p}-p}{\sqrt{\frac{p(1-p)}{n}}}\)</span> is approximately standard normal.</p></li>
</ul>
<p>View the video below for a more detailed explanation of the sampling distribution of sample proportions:</p>
<iframe width="560px" height="320px" allowfullscreen="true" allow="autoplay *" title="Module 08: Sampling Distribution of Sample Proportion" src="https://virginiauniversity.instructuremedia.com/embed/84d115c1-bacb-429f-b777-560eda3f4e6b" frameborder="0">
</iframe>
</div>
<div id="constructing-confidence-interval-for-the-proportion" class="section level3" number="8.3.2">
<h3>
<span class="header-section-number">8.3.2</span> Constructing Confidence Interval for the Proportion<a class="anchor" aria-label="anchor" href="#constructing-confidence-interval-for-the-proportion"><i class="fas fa-link"></i></a>
</h3>
<p>A lot of the concepts and math from the confidence interval for the mean carry over to the confidence interval for the proportion. We will skip the math for the confidence interval for the proportion (although you should be able to derive this result by following the steps and adjusting).</p>
<p>The <span class="math inline">\((1-\alpha) \times 100\%\)</span> confidence interval for the proportion is</p>
<p><span class="math display" id="eq:8-CIprop">\[\begin{equation}
\left( \hat{p} - z^{*} \sqrt{\frac{\hat{p}(1-\hat{p})}{n}}, \hat{p} + z^{*} \sqrt{\frac{\hat{p}(1-\hat{p})}{n}} \right).
\tag{8.12}
\end{equation}\]</span></p>
<p>From equation <a href="confidence-intervals.html#eq:8-CIprop">(8.12)</a>, we can see that</p>
<ul>
<li>The point estimate is <span class="math inline">\(\hat{p}\)</span>.</li>
<li>The standard error of <span class="math inline">\(\hat{p}\)</span> is <span class="math inline">\(SE(\hat{p}) = \sqrt{\frac{\hat{p}(1-\hat{p})}{n}}\)</span>.</li>
<li>The margin of error is <span class="math inline">\(z^{*} \sqrt{\frac{\hat{p}(1-\hat{p})}{n}}\)</span>.</li>
</ul>
<p>The framework of confidence intervals usually takes on the form given in equation <a href="confidence-intervals.html#eq:8-CIbasic2">(8.2)</a>: we add and subtract the margin of error to and from the point estimate. The margin of error is the critical value multiplied by the standard error of the estimator. We see the confidence intervals for the mean and proportion take on this framework, and so do most confidence intervals of other estimators, as long as the sampling distribution of the estimator is symmetric.</p>
<p>Note: equation <a href="confidence-intervals.html#eq:8-CIprop">(8.12)</a> is based on proportions, and not percentages. After calculating the confidence interval for the proportion, reporters often convert these values to percentages by multiplying them by a hundred.</p>
<p>From equation <a href="confidence-intervals.html#eq:8-CIprop">(8.12)</a>, we can also see that we have the factors that affect the width of the confidence interval for the proportion is the same as the confidence interval for the mean.</p>
<div id="conditions-for-confidence-interval-for-the-proportion" class="section level4" number="8.3.2.1">
<h4>
<span class="header-section-number">8.3.2.1</span> Conditions for Confidence Interval for the Proportion<a class="anchor" aria-label="anchor" href="#conditions-for-confidence-interval-for-the-proportion"><i class="fas fa-link"></i></a>
</h4>
<p>We wrote that if <span class="math inline">\(n\)</span> is large enough, the CLT informs us that the sample proportion <span class="math inline">\(\hat{p}\)</span> can be approximated by a normal distribution. How large is large enough? Again, various rules of thumb are recommended, and they usually follow along the lines of needing at least a certain number of successes, <span class="math inline">\(n\hat{p}\)</span>, and failures, <span class="math inline">\(n(1-\hat{p})\)</span> in our sample. Values of at least 5 or 10 are usually recommended. Just bear in mind that the approximation works better as the number of successes and failures, <span class="math inline">\(n\hat{p}\)</span> and <span class="math inline">\(n(1-\hat{p})\)</span>, increases.</p>
</div>
<div id="worked-example-3" class="section level4" number="8.3.2.2">
<h4>
<span class="header-section-number">8.3.2.2</span> Worked Example<a class="anchor" aria-label="anchor" href="#worked-example-3"><i class="fas fa-link"></i></a>
</h4>
<p>The Texas College Tobacco Project survey administered in 2016 found that in a sample of 5767 undergraduates ages 18–25, 525 said they had used electronic cigarettes at least once during the previous 30 days. Find a 95% confidence interval for the proportion of all students in the population sampled who used e-cigarettes during the previous 30 days. Also report the margin of error. Do the data support the claim that more than 5% of students in the population used e-cigarettes during the previous 30 days?</p>
<p>From the question, we summarize the information as:</p>
<ul>
<li>
<span class="math inline">\(n = 5767\)</span>,</li>
<li>
<span class="math inline">\(\hat{p} = \frac{525}{5767}\)</span>,</li>
<li>
<span class="math inline">\(\alpha = 0.05\)</span>, so <span class="math inline">\(z^*\)</span> is found using <code>qnorm(1-0.05/2)</code> which is 1.959964.</li>
</ul>
<p>The number of “successes” is 525 and the number of “failures” is <span class="math inline">\(5767-525 = 5242\)</span>. These are both a lot larger than 10, so we can work with sampling distribution of <span class="math inline">\(\hat{p}\)</span> and calculate a confidence interval using equation <a href="confidence-intervals.html#eq:8-CIprop">(8.12)</a>:</p>
<p><span class="math display">\[
\left( \frac{525}{5767} - 1.959964 \sqrt{\frac{\frac{525}{5767}(1-\frac{525}{5767})}{5767}} , \frac{525}{5767} + 1.959964 \sqrt{\frac{\frac{525}{5767}(1-\frac{525}{5767})}{5767}} \right)
\]</span></p>
<p>which gives (0.08361097, 0.09845943), with margin of error 0.007424228. The data support the claim that more than 5% of students in the population used e-cigarettes in the previous 30 days, since the entire interval lies above 0.05.</p>
<p>There is a 95% probability that the random interval (0.08361097, 0.09845943) contains the true proportion of students in the population used e-cigarettes in the previous 30 days.</p>
</div>
</div>
<div id="minimum-sample-size" class="section level3" number="8.3.3">
<h3>
<span class="header-section-number">8.3.3</span> Minimum Sample Size<a class="anchor" aria-label="anchor" href="#minimum-sample-size"><i class="fas fa-link"></i></a>
</h3>
<p>Fairly often, researchers want to know what sample size needed in order to guarantee a margin of error that is no more than some specific value, as they want to guarantee a certain level of precision in their report. Mathematically, we want to set <span class="math inline">\(z^{*} \sqrt{\frac{\hat{p}(1-\hat{p})}{n}} \leq M\)</span> where <span class="math inline">\(M\)</span> denotes some value of the margin of error that needs to be guaranteed.</p>
<p>While we will not know the value of <span class="math inline">\(\hat{p}\)</span> before collecting data, we mentioned in Section <a href="discrete-random-variables.html#bernprop">3.5.1.1</a> that the value of <span class="math inline">\(p(1-p)\)</span> is maximized at <span class="math inline">\(p=\frac{1}{2}\)</span>. This implies that the <span class="math inline">\(SE(\hat{p})\)</span> is maximized at <span class="math inline">\(\hat{p} = \frac{1}{2}\)</span>. As long as the <span class="math inline">\(M\)</span> is satisfied when <span class="math inline">\(\hat{p} = \frac{1}{2}\)</span>, <span class="math inline">\(M\)</span> will be satisfied for any value of <span class="math inline">\(\hat{p}\)</span>. So subbing in <span class="math inline">\(\hat{p} = \frac{1}{2}\)</span> into <span class="math inline">\(z^{*} \sqrt{\frac{\hat{p}(1-\hat{p})}{n}} \leq M\)</span>, we have</p>
<p><span class="math display" id="eq:8-minsamp">\[\begin{equation}
\begin{split}
z^{*} \sqrt{\frac{\hat{p}(1-\hat{p})}{n}} &amp; \leq M \\
\implies z^* \sqrt{\frac{\frac{1}{2}\frac{1}{2}}{n}}  &amp; \leq M \\
\implies \left(\frac{z^*}{2M}\right)^2 &amp; \leq n.
\end{split}
\tag{8.13}
\end{equation}\]</span></p>
<p>Note: always be sure to round up to the nearest whole number, after working out the LHS of the inequality <a href="confidence-intervals.html#eq:8-minsamp">(8.13)</a>, to guarantee that your margin of error is no more than <span class="math inline">\(M\)</span>.</p>
<div id="worked-example-4" class="section level4" number="8.3.3.1">
<h4>
<span class="header-section-number">8.3.3.1</span> Worked Example<a class="anchor" aria-label="anchor" href="#worked-example-4"><i class="fas fa-link"></i></a>
</h4>
<p>A state legislator wishes to survey residents of her district to see what proportion of the electorate is aware of her position on using state funds to pay for abortions. What sample size is necessary if the 95% CI for p is to have a margin of error that is no more than 3 percentage points?</p>
<p>Using the inequality <a href="confidence-intervals.html#eq:8-minsamp">(8.13)</a>,</p>
<p><span class="math display">\[
\begin{split}
\left(\frac{1.959964}{2 \times 0.03}\right)^2 \leq n \\
1067.072 \leq n.
\end{split}
\]</span>
So she will need to sample at least 1068 residents to guarantee a margin of error that is no more than 3 percentage points.</p>
</div>
</div>
</div>
<div id="bootstrap" class="section level2" number="8.4">
<h2>
<span class="header-section-number">8.4</span> The Bootstrap<a class="anchor" aria-label="anchor" href="#bootstrap"><i class="fas fa-link"></i></a>
</h2>
<p>In the previous subsections, we work out the confidence interval of an estimator by using the framework in equation <a href="confidence-intervals.html#eq:8-CIbasic2">(8.2)</a>: we add and subtract the margin of error to and from the point estimate. The margin of error is the critical value multiplied by the standard error of the estimator. The critical value is based on the sampling distribution of the estimator. This framework requires us to know the sampling distribution and the standard error of the estimator.</p>
<p>However, there are estimators whose sampling distributions and standard errors are unknown, for example, the sample median. How can we construct confidence intervals for the median? Or how can we quantify our uncertainty of the sample median as an estimator of the population median?</p>
<p>One idea could be to collect many random samples, compute the sample median from each sample, and create a density plot like we did in Monte Carlo simulations in Figure <a href="est.html#fig:7-bias">7.4</a>. However, this is not feasible in real life as we rarely have the time and money to go around obtaining many random samples. We are only able to work with the one random sample that we have.</p>
<p>A method to work around these issues is the <strong>bootstrap</strong>. The bootstrap will enable us to:</p>
<ul>
<li><p>Estimate the standard error and variance of estimators if there are no known formulas for them.</p></li>
<li><p>Construct confidence intervals for estimators with unknown sampling distributions and unknown formulas for their standard errors.</p></li>
</ul>
<p>We can do these without having to collect data from many random samples. Before going into how the bootstrap works, we remind ourselves of how Monte Carlo simulations work in terms of finding the sampling distribution and standard errors of estimators.</p>
<div id="estimating-sampling-distributions-using-many-random-samples" class="section level3" number="8.4.1">
<h3>
<span class="header-section-number">8.4.1</span> Estimating Sampling Distributions Using Many Random Samples<a class="anchor" aria-label="anchor" href="#estimating-sampling-distributions-using-many-random-samples"><i class="fas fa-link"></i></a>
</h3>
<p>When using Monte Carlo simulations to estimate the sampling distribution of estimators (see the code that generated Figures <a href="est.html#fig:7-bias">7.4</a> and <a href="est.html#fig:7-consistent">7.5</a> as examples), we typically did the following:</p>
<ol style="list-style-type: decimal">
<li><p>Assume the data follow some known distribution with PDF <span class="math inline">\(f_X\)</span> and CDF <span class="math inline">\(F_X\)</span>.</p></li>
<li><p>Simulate <span class="math inline">\(n\)</span> data points from this assumed distribution <span class="math inline">\(K\)</span> times, with <span class="math inline">\(K\)</span> being large, to obtain <span class="math inline">\(K\)</span> replicate datasets of size <span class="math inline">\(n\)</span> denoted by <span class="math inline">\(\mathcal{X}^{1}, \cdots, \mathcal{X}^{K}\)</span>. (In a lot of my code <span class="math inline">\(K\)</span> is called <code>reps</code>).</p></li>
<li><p>For each dataset, calculate the value of the estimator <span class="math inline">\(\hat{\Theta}\)</span>. Since we have <span class="math inline">\(K\)</span> datasets, we will have <span class="math inline">\(K\)</span> values for the estimators, denoted by <span class="math inline">\(\hat{\Theta}^{1}, \cdots, \hat{\Theta}^{K}\)</span>.</p></li>
<li><p>We estimate the true mean of <span class="math inline">\(\hat{\Theta}\)</span> by taking the sample mean of <span class="math inline">\(\hat{\Theta}^{1}, \cdots, \hat{\Theta}^{K}\)</span>.</p></li>
<li><p>We estimate the true variance of <span class="math inline">\(\hat{\Theta}\)</span> by taking the sample variance of <span class="math inline">\(\hat{\Theta}^{1}, \cdots, \hat{\Theta}^{K}\)</span>. Square-rooting this value gives an estimate for the standard error of <span class="math inline">\(\hat{\Theta}\)</span>.</p></li>
</ol>
<p>Note: this step is a bit different than what is described in the book. In the book, the author uses the population variance, instead of the sample variance. The population variance divides the sum of the squared deviations from the mean by <span class="math inline">\(K\)</span>, while the sample variance divides by <span class="math inline">\(K-1\)</span>. I have seen some authors divide by <span class="math inline">\(K\)</span>, and others divide by <span class="math inline">\(K-1\)</span>. The <code><a href="https://rdrr.io/r/stats/cor.html">var()</a></code> function in R calculates the sample variance, so I will admit that there is a level of convenience to use the sample variance so I can use an R function. <span class="math inline">\(K\)</span> needs to be large for the bootstrap to work well, so the end result is the numerical values are very close to each other.</p>
<ol start="6" style="list-style-type: decimal">
<li>We create a density plot of <span class="math inline">\(\hat{\Theta}^{1}, \cdots, \hat{\Theta}^{K}\)</span> to estimate the sampling distribution of <span class="math inline">\(\hat{\Theta}\)</span>.</li>
</ol>
<p>As written earlier, the problem with applying this idea to an estimator that has an unknown sampling distribution is that we do not know its associated PDF <span class="math inline">\(f_X\)</span> and CDF <span class="math inline">\(F_X\)</span>. So we cannot carry out the steps listed. What the bootstrap does is to mimic these steps, when we only have access to data from one random sample. Next, we describe the steps that are carried out in the bootstrap. Pay attention to the similarities and differences in these steps with the steps above.</p>
</div>
<div id="the-bootstrap-algorithm" class="section level3" number="8.4.2">
<h3>
<span class="header-section-number">8.4.2</span> The Bootstrap Algorithm<a class="anchor" aria-label="anchor" href="#the-bootstrap-algorithm"><i class="fas fa-link"></i></a>
</h3>
<p>Instead of simulating <span class="math inline">\(K\)</span> replicate datasets from an assumed distribution, we simulate <span class="math inline">\(K\)</span> replicate datasets by sampling, <strong>with replacement</strong>, <span class="math inline">\(n\)</span> data points from the original dataset. The “with replacement” term is key: it means that when a data point is sampled, it is returned to the original dataset and it can be sampled again.</p>
<p>See the video below for a visual representation of sampling with replacement:</p>
<iframe width="560px" height="320px" allowfullscreen="true" allow="autoplay *" title="Module 08: Sampling with Replacement" src="https://virginiauniversity.instructuremedia.com/embed/7e81e93d-dd67-4034-9823-fb6a42135dcb" frameborder="0">
</iframe>
<p>Let <span class="math inline">\(\mathcal{X}\)</span> denote the original dataset with <span class="math inline">\(n\)</span> observations. Let <span class="math inline">\(\hat{\theta}\)</span> denote the value of the estimator from the original dataset.</p>
<ol style="list-style-type: decimal">
<li><p>Synthesize <span class="math inline">\(K\)</span> bootstrapped datasets <span class="math inline">\(\mathcal{Y}^{(1)}, \cdots, \mathcal{Y}^{(K)}\)</span> where each bootstrapped dataset is derived by sampling <span class="math inline">\(n\)</span> data points from the original dataset, with replacement.</p></li>
<li><p>For each dataset, calculate the value of the estimator <span class="math inline">\(\hat{\Theta}\)</span>. Since we have <span class="math inline">\(K\)</span> bootstrapped datasets, we will have <span class="math inline">\(K\)</span> values for the estimators, denoted by <span class="math inline">\(\hat{\Theta}^{(1)}, \cdots, \hat{\Theta}^{(K)}\)</span>.</p></li>
<li><p>We estimate the true mean of <span class="math inline">\(\hat{\Theta}\)</span> by taking the sample mean of <span class="math inline">\(\hat{\Theta}^{(1)}, \cdots, \hat{\Theta}^{(K)}\)</span>. This value is denoted by <span class="math inline">\(M_b(\hat{\Theta})\)</span>.</p></li>
<li><p>We estimate the true variance of <span class="math inline">\(\hat{\Theta}\)</span> by taking the sample variance of <span class="math inline">\(\hat{\Theta}^{(1)}, \cdots, \hat{\Theta}^{(K)}\)</span>. This value is denoted by <span class="math inline">\(V_b(\hat{\Theta})\)</span>, which we call the bootstrap variance of the estimator. Square-rooting <span class="math inline">\(V_b(\hat{\Theta})\)</span> gives an estimate for the standard error of <span class="math inline">\(\hat{\Theta}\)</span>, denoted by <span class="math inline">\(SE_b(\Theta)\)</span>. If <span class="math inline">\(K\)</span> is large enough, <span class="math inline">\(V_b(\hat{\Theta})\)</span> will approximate the true variance of the estimator.</p></li>
</ol>
<p>Note: see the note in step 5 in the previous subsection. The same note applies here.</p>
<ol start="5" style="list-style-type: decimal">
<li>We can create a density plot of <span class="math inline">\(\hat{\Theta}^{(1)}, \cdots, \hat{\Theta}^{(K)}\)</span> to estimate the sampling distribution of <span class="math inline">\(\hat{\Theta}\)</span>. If <span class="math inline">\(K\)</span> is large enough, this density plot will approximate the true PDF <span class="math inline">\(f_X\)</span> well.</li>
</ol>
<p>Comparing this algorithm with Monte Carlo simulations, the main differences are:</p>
<ul>
<li><p>With the bootstrap, we do not assume any distribution for the original data. With Monte Carlo simulations, we do make an assumption on the distribution for the original data.</p></li>
<li><p>The <span class="math inline">\(K\)</span> datasets are produced slightly differently. For the bootstrap, they are synthesized by sampling with replacement <span class="math inline">\(n\)</span> data points from the original dataset of size <span class="math inline">\(n\)</span>. With Monte Carlo simulations, we draw <span class="math inline">\(n\)</span> data points from its assumed distribution.</p></li>
</ul>
<p>The math that proves the bootstrap works is beyond the level of this class, although it is freely available online.</p>
<p>The bootstrap variance of an estimator, <span class="math inline">\(V_b(\hat{\Theta})\)</span>, tends to work better as <span class="math inline">\(K\)</span> increases and as <span class="math inline">\(n\)</span> increases.</p>
</div>
<div id="confidence-intervals-using-the-bootstrap" class="section level3" number="8.4.3">
<h3>
<span class="header-section-number">8.4.3</span> Confidence Intervals using the Bootstrap<a class="anchor" aria-label="anchor" href="#confidence-intervals-using-the-bootstrap"><i class="fas fa-link"></i></a>
</h3>
<p>The bootstrap <span class="math inline">\((1-\alpha) \times 100\%\)</span> confidence interval for <span class="math inline">\(\theta\)</span> is</p>
<p><span class="math display" id="eq:8-CIboot">\[\begin{equation}
\left( 2 \hat{\theta} - \hat{\theta}_{1-\alpha/2}^*, 2 \hat{\theta} - \hat{\theta}_{\alpha/2}^* \right),
\tag{8.14}
\end{equation}\]</span></p>
<p>where <span class="math inline">\(\hat{\theta}_{\beta}^*\)</span> is the <span class="math inline">\(\beta \times 100\)</span>th percentile of <span class="math inline">\(\hat{\Theta}^{(1)}, \cdots, \hat{\Theta}^{(K)}\)</span>, the <span class="math inline">\(K\)</span> numerical values of the estimator from each bootstrapped sample.</p>
<p>You may notice that the confidence interval in equation <a href="confidence-intervals.html#eq:8-CIboot">(8.14)</a> does not fit the general framework given in equation <a href="confidence-intervals.html#eq:8-CIbasic2">(8.2)</a>. This is because the sampling distribution of the estimator may not be symmetric.</p>
<p>The math that derives this confidence interval is beyond the level of this class, although it is freely available online.</p>
<p>The bootstrap confidence interval tends to work better as <span class="math inline">\(K\)</span> increases and as <span class="math inline">\(n\)</span> increases.</p>
</div>
<div id="worked-example-5" class="section level3" number="8.4.4">
<h3>
<span class="header-section-number">8.4.4</span> Worked Example<a class="anchor" aria-label="anchor" href="#worked-example-5"><i class="fas fa-link"></i></a>
</h3>
<p>We have data on 400 credit card customers. A density plot of their credit limits is shown below:</p>
<div class="sourceCode" id="cb147"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="kw"><a href="https://rdrr.io/r/base/library.html">library</a></span><span class="op">(</span><span class="va"><a href="https://www.statlearning.com">ISLR2</a></span><span class="op">)</span></span>
<span><span class="va">Data</span><span class="op">&lt;-</span><span class="fu">ISLR2</span><span class="fu">::</span><span class="va"><a href="https://rdrr.io/pkg/ISLR2/man/Credit.html">Credit</a></span><span class="op">$</span><span class="va">Limit</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/graphics/plot.default.html">plot</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/stats/density.html">density</a></span><span class="op">(</span><span class="va">Data</span><span class="op">)</span>, xlab<span class="op">=</span><span class="st">"Credit Limit"</span>, main<span class="op">=</span><span class="st">"Density Plot of Credit Limits"</span><span class="op">)</span></span></code></pre></div>
<div class="figure">
<span style="display:block;" id="fig:8-credit"></span>
<img src="bookdown-demo_files/figure-html/8-credit-1.png" alt="Density Plot of Credit Limits" width="672"><p class="caption">
Figure 8.4: Density Plot of Credit Limits
</p>
</div>
<p>The density plot in Figure <a href="confidence-intervals.html#fig:8-credit">8.4</a> informs us that the values of credit limits is skewed, so the median is a better measure of centrality for credit limits. We wish to estimate the population median credit limit, give a measure of uncertainty, as well as report a 95% confidence interval for the median credit limit. Since the sampling distribution of the sample median is unknown, we use the bootstrap:</p>
<div class="sourceCode" id="cb148"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="co">##get sample size</span></span>
<span><span class="va">n</span><span class="op">&lt;-</span><span class="fu"><a href="https://rdrr.io/r/base/length.html">length</a></span><span class="op">(</span><span class="va">Data</span><span class="op">)</span></span>
<span></span>
<span><span class="va">K</span><span class="op">&lt;-</span><span class="fl">1000</span> <span class="co">##Number of bootstrap samples</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/base/Random.html">set.seed</a></span><span class="op">(</span><span class="fl">77</span><span class="op">)</span></span>
<span></span>
<span><span class="co">##the sample function, with replace=TRUE, samples with replacement. </span></span>
<span><span class="co">##The replicate function repeats the sample function K times</span></span>
<span><span class="va">boot.samples</span><span class="op">&lt;-</span><span class="fu"><a href="https://rdrr.io/r/base/lapply.html">replicate</a></span><span class="op">(</span><span class="va">K</span>,<span class="fu"><a href="https://rdrr.io/r/base/sample.html">sample</a></span><span class="op">(</span><span class="va">Data</span>,<span class="va">n</span>,replace<span class="op">=</span><span class="cn">TRUE</span><span class="op">)</span><span class="op">)</span></span>
<span><span class="co">##Each column in boot.samples represents each bootstrap sample</span></span>
<span></span>
<span><span class="co">##find the sample median of each column, since each column represents a bootstrap sample</span></span>
<span><span class="va">boot.medians</span><span class="op">&lt;-</span><span class="fu"><a href="https://rdrr.io/r/base/apply.html">apply</a></span><span class="op">(</span><span class="va">boot.samples</span>,<span class="fl">2</span>,<span class="va">median</span><span class="op">)</span></span>
<span></span>
<span><span class="co">##bootstrap SE of the median is just the SD of the sample medians from each bootstrap sample</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/stats/sd.html">sd</a></span><span class="op">(</span><span class="va">boot.medians</span><span class="op">)</span></span></code></pre></div>
<pre><code>## [1] 152.0525</code></pre>
<div class="sourceCode" id="cb150"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="co">##bootstrap variance of the median is just the variance of the sample medians from each bootstrap sample</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/stats/cor.html">var</a></span><span class="op">(</span><span class="va">boot.medians</span><span class="op">)</span></span></code></pre></div>
<pre><code>## [1] 23119.95</code></pre>
<div class="sourceCode" id="cb152"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="co">##find bootstrap CI</span></span>
<span><span class="va">alpha</span><span class="op">&lt;-</span><span class="fl">0.05</span></span>
<span></span>
<span><span class="co">##median from original sample</span></span>
<span><span class="va">orig.median</span><span class="op">&lt;-</span><span class="fu"><a href="https://rdrr.io/r/stats/median.html">median</a></span><span class="op">(</span><span class="va">Data</span><span class="op">)</span></span>
<span><span class="va">orig.median</span></span></code></pre></div>
<pre><code>## [1] 4622.5</code></pre>
<div class="sourceCode" id="cb154"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="co">##lower and upper bound of the CI</span></span>
<span><span class="fl">2</span><span class="op">*</span><span class="va">orig.median</span><span class="op">-</span><span class="fu"><a href="https://rdrr.io/r/stats/quantile.html">quantile</a></span><span class="op">(</span><span class="va">boot.medians</span>,<span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span><span class="fl">1</span><span class="op">-</span><span class="va">alpha</span><span class="op">/</span><span class="fl">2</span>,<span class="va">alpha</span><span class="op">/</span><span class="fl">2</span><span class="op">)</span><span class="op">)</span></span></code></pre></div>
<pre><code>##    97.5%     2.5% 
## 4380.000 4948.525</code></pre>
<p>We report the following:</p>
<ul>
<li>The sample median is $4622.5.</li>
<li>The standard error of the sample median is $152.0525.</li>
<li>The 95% confidence interval for the median credit limit is (4380.000, 4948.525). There is 95% probability that the true population median credit limit lies in the random interval between $4380 and $4948.525.</li>
</ul>
<p>View the video below that explains the functions used in the code:</p>
<iframe width="560px" height="320px" allowfullscreen="true" allow="autoplay *" title="Module 08: Code for Bootstrap" src="https://virginiauniversity.instructuremedia.com/embed/14a288a6-29e6-4bc3-aa1b-39ea2959c85d" frameborder="0">
</iframe>

</div>
</div>
</div>
  <div class="chapter-nav">
<div class="prev"><a href="est.html"><span class="header-section-number">7</span> Estimation</a></div>
<div class="next"><a href="hypothesis-testing.html"><span class="header-section-number">9</span> Hypothesis Testing</a></div>
</div></main><div class="col-md-3 col-lg-2 d-none d-md-block sidebar sidebar-chapter">
    <nav id="toc" data-toggle="toc" aria-label="On this page"><h2>On this page</h2>
      <ul class="nav navbar-nav">
<li><a class="nav-link" href="#confidence-intervals"><span class="header-section-number">8</span> Confidence Intervals</a></li>
<li>
<a class="nav-link" href="#introduction-4"><span class="header-section-number">8.1</span> Introduction</a><ul class="nav navbar-nav"><li><a class="nav-link" href="#module-roadmap-6"><span class="header-section-number">8.1.1</span> Module Roadmap</a></li></ul>
</li>
<li>
<a class="nav-link" href="#CImean"><span class="header-section-number">8.2</span> Confidence Interval for the Mean</a><ul class="nav navbar-nav">
<li><a class="nav-link" href="#randomness-of-estimators"><span class="header-section-number">8.2.1</span> Randomness of Estimators</a></li>
<li><a class="nav-link" href="#randomness-of-confidence-intervals"><span class="header-section-number">8.2.2</span> Randomness of Confidence Intervals</a></li>
<li><a class="nav-link" href="#constructing-confidence-interval-for-the-mean"><span class="header-section-number">8.2.3</span> Constructing Confidence Interval for the Mean</a></li>
<li><a class="nav-link" href="#CImeant"><span class="header-section-number">8.2.4</span> Confidence Interval for the Mean Using t Distribution</a></li>
<li><a class="nav-link" href="#factors-affecting-precision-of-confidence-intervals"><span class="header-section-number">8.2.5</span> Factors Affecting Precision of Confidence Intervals</a></li>
</ul>
</li>
<li>
<a class="nav-link" href="#CIprop"><span class="header-section-number">8.3</span> Confidence Interval for the Proportion</a><ul class="nav navbar-nav">
<li><a class="nav-link" href="#sampdistprops"><span class="header-section-number">8.3.1</span> Sampling Distribution of Sample Proportions</a></li>
<li><a class="nav-link" href="#constructing-confidence-interval-for-the-proportion"><span class="header-section-number">8.3.2</span> Constructing Confidence Interval for the Proportion</a></li>
<li><a class="nav-link" href="#minimum-sample-size"><span class="header-section-number">8.3.3</span> Minimum Sample Size</a></li>
</ul>
</li>
<li>
<a class="nav-link" href="#bootstrap"><span class="header-section-number">8.4</span> The Bootstrap</a><ul class="nav navbar-nav">
<li><a class="nav-link" href="#estimating-sampling-distributions-using-many-random-samples"><span class="header-section-number">8.4.1</span> Estimating Sampling Distributions Using Many Random Samples</a></li>
<li><a class="nav-link" href="#the-bootstrap-algorithm"><span class="header-section-number">8.4.2</span> The Bootstrap Algorithm</a></li>
<li><a class="nav-link" href="#confidence-intervals-using-the-bootstrap"><span class="header-section-number">8.4.3</span> Confidence Intervals using the Bootstrap</a></li>
<li><a class="nav-link" href="#worked-example-5"><span class="header-section-number">8.4.4</span> Worked Example</a></li>
</ul>
</li>
</ul>

      <div class="book-extra">
        <ul class="list-unstyled">
          
        </ul>
</div>
    </nav>
</div>

</div>
</div> <!-- .container -->

<footer class="bg-primary text-light mt-5"><div class="container"><div class="row">

  <div class="col-12 col-md-6 mt-3">
    <p>"<strong>Understanding Uncertainty Course Notes</strong>" was written by Jeffrey Woo. It was last built on 2025-08-19.</p>
  </div>

  <div class="col-12 col-md-6 mt-3">
    <p>This book was built by the <a class="text-light" href="https://bookdown.org">bookdown</a> R package.</p>
  </div>

</div></div>
</footer><!-- dynamically load mathjax for compatibility with self-contained --><script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.9/latest.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script><script type="text/x-mathjax-config">const popovers = document.querySelectorAll('a.footnote-ref[data-toggle="popover"]');
for (let popover of popovers) {
  const div = document.createElement('div');
  div.setAttribute('style', 'position: absolute; top: 0, left:0; width:0, height:0, overflow: hidden; visibility: hidden;');
  div.innerHTML = popover.getAttribute('data-content');

  var has_math = div.querySelector("span.math");
  if (has_math) {
    document.body.appendChild(div);
    MathJax.Hub.Queue(["Typeset", MathJax.Hub, div]);
    MathJax.Hub.Queue(function() {
      popover.setAttribute('data-content', div.innerHTML);
      document.body.removeChild(div);
    })
  }
}
</script>
</body>
</html>
