<!DOCTYPE html>
<html lang="en">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<title>Chapter 5 Joint Distributions | Understanding Uncertainty Course Notes</title>
<meta name="author" content="Jeffrey Woo">
<meta name="description" content="This module is based on Introduction to Probability (Blitzstein, Hwang), Chapters 7 and 9. You can access the book for free at https://stat110.hsites.harvard.edu/ (and then click on Book). Please...">
<meta name="generator" content="bookdown 0.43 with bs4_book()">
<meta property="og:title" content="Chapter 5 Joint Distributions | Understanding Uncertainty Course Notes">
<meta property="og:type" content="book">
<meta property="og:description" content="This module is based on Introduction to Probability (Blitzstein, Hwang), Chapters 7 and 9. You can access the book for free at https://stat110.hsites.harvard.edu/ (and then click on Book). Please...">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="Chapter 5 Joint Distributions | Understanding Uncertainty Course Notes">
<meta name="twitter:description" content="This module is based on Introduction to Probability (Blitzstein, Hwang), Chapters 7 and 9. You can access the book for free at https://stat110.hsites.harvard.edu/ (and then click on Book). Please...">
<!-- JS --><script src="https://cdnjs.cloudflare.com/ajax/libs/clipboard.js/2.0.6/clipboard.min.js" integrity="sha256-inc5kl9MA1hkeYUt+EC3BhlIgyp/2jDIyBLS6k3UxPI=" crossorigin="anonymous"></script><script src="https://cdnjs.cloudflare.com/ajax/libs/fuse.js/6.4.6/fuse.js" integrity="sha512-zv6Ywkjyktsohkbp9bb45V6tEMoWhzFzXis+LrMehmJZZSys19Yxf1dopHx7WzIKxr5tK2dVcYmaCk2uqdjF4A==" crossorigin="anonymous"></script><script src="https://kit.fontawesome.com/6ecbd6c532.js" crossorigin="anonymous"></script><script src="libs/jquery-3.6.0/jquery-3.6.0.min.js"></script><meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<link href="libs/bootstrap-4.6.0/bootstrap.min.css" rel="stylesheet">
<script src="libs/bootstrap-4.6.0/bootstrap.bundle.min.js"></script><script src="libs/bs3compat-0.9.0/transition.js"></script><script src="libs/bs3compat-0.9.0/tabs.js"></script><script src="libs/bs3compat-0.9.0/bs3compat.js"></script><link href="libs/bs4_book-1.0.0/bs4_book.css" rel="stylesheet">
<script src="libs/bs4_book-1.0.0/bs4_book.js"></script><script src="https://cdnjs.cloudflare.com/ajax/libs/autocomplete.js/0.38.0/autocomplete.jquery.min.js" integrity="sha512-GU9ayf+66Xx2TmpxqJpliWbT5PiGYxpaG8rfnBEk1LL8l1KGkRShhngwdXK1UgqhAzWpZHSiYPc09/NwDQIGyg==" crossorigin="anonymous"></script><script src="https://cdnjs.cloudflare.com/ajax/libs/mark.js/8.11.1/mark.min.js" integrity="sha512-5CYOlHXGh6QpOFA/TeTylKLWfB3ftPsde7AnmhuitiTX4K5SqCLBeKro6sPS8ilsz1Q4NRx3v8Ko2IBiszzdww==" crossorigin="anonymous"></script><!-- CSS --><style type="text/css">
    
    div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
  </style>
</head>
<body data-spy="scroll" data-target="#toc">

<div class="container-fluid">
<div class="row">
  <header class="col-sm-12 col-lg-3 sidebar sidebar-book"><a class="sr-only sr-only-focusable" href="#content">Skip to main content</a>

    <div class="d-flex align-items-start justify-content-between">
      <h1>
        <a href="index.html" title="">Understanding Uncertainty Course Notes</a>
      </h1>
      <button class="btn btn-outline-primary d-lg-none ml-2 mt-1" type="button" data-toggle="collapse" data-target="#main-nav" aria-expanded="true" aria-controls="main-nav"><i class="fas fa-bars"></i><span class="sr-only">Show table of contents</span></button>
    </div>

    <div id="main-nav" class="collapse-lg">
      <form role="search">
        <input id="search" class="form-control" type="search" placeholder="Search" aria-label="Search">
</form>

      <nav aria-label="Table of contents"><h2>Table of contents</h2>
        <ul class="book-toc list-unstyled">
<li><a class="" href="index.html">Preface</a></li>
<li><a class="" href="descriptive.html"><span class="header-section-number">1</span> Descriptive Statistics</a></li>
<li><a class="" href="probability.html"><span class="header-section-number">2</span> Probability</a></li>
<li><a class="" href="discrete-random-variables.html"><span class="header-section-number">3</span> Discrete Random Variables</a></li>
<li><a class="" href="continuous-random-variables.html"><span class="header-section-number">4</span> Continuous Random Variables</a></li>
<li><a class="active" href="joint-distributions.html"><span class="header-section-number">5</span> Joint Distributions</a></li>
<li><a class="" href="inequalities-limit-theorems-and-simulations.html"><span class="header-section-number">6</span> Inequalities, Limit Theorems, and Simulations</a></li>
</ul>

        <div class="book-extra">
          
        </div>
      </nav>
</div>
  </header><main class="col-sm-12 col-md-9 col-lg-7" id="content"><div id="joint-distributions" class="section level1" number="5">
<h1>
<span class="header-section-number">5</span> Joint Distributions<a class="anchor" aria-label="anchor" href="#joint-distributions"><i class="fas fa-link"></i></a>
</h1>
<p>This module is based on Introduction to Probability (Blitzstein, Hwang), Chapters 7 and 9. You can access the book for free at <a href="https://stat110.hsites.harvard.edu/" class="uri">https://stat110.hsites.harvard.edu/</a> (and then click on Book). Please note that I cover additional topics, and skip certain topics from the book. You may skip Story 7.1.9, Theorems 7.1.10 to 7.1.12, Examples 7.1.23 to 7.1.26, Section 7.2, Examples 7.3.6 to 7.3.8, 7.4.8 (parts d and f only), Definition 7.5.6, Examples 9.1.8 to 9.1.10, Example 9.2.5, Theorem 9.3.2, Example 9.3.3, Theorems 9.3.7 to 9.3.9, and Sections 9.4 to 9.6 from the book.</p>
<div id="introduction-1" class="section level2" number="5.1">
<h2>
<span class="header-section-number">5.1</span> Introduction<a class="anchor" aria-label="anchor" href="#introduction-1"><i class="fas fa-link"></i></a>
</h2>
<p>In the previous two modules, we learned how to summarize the distribution of individual random variables. We are now ready to extend the concepts from these modules and apply them to a slightly different setting, where we are analyzing how multiple variables are related to each other. It is extremely common to want to analyze the relationship between at least two variables. The book lists a few examples, but here are a few more:</p>
<ul>
<li>Public policy: How does increasing expenditure on infrastructure impact economic development?</li>
<li>Education: How do smaller class sizes and higher teacher pay impact student learning outcomes?</li>
<li>Marketing: How does the design of a website influence the probability of a customer purchasing an item?</li>
</ul>
<p>This module will consider these variables jointly, in other words, how they relate to each other. A lot of the concepts such as CDF, PDF, PMF, expectations, variances, and so on will have analogous versions when considering variables jointly.</p>
</div>
<div id="joint-distributions-for-discrete-rvs" class="section level2" number="5.2">
<h2>
<span class="header-section-number">5.2</span> Joint Distributions for Discrete RVs<a class="anchor" aria-label="anchor" href="#joint-distributions-for-discrete-rvs"><i class="fas fa-link"></i></a>
</h2>
<p>We will start with discrete random variables, then move on to continuous random variables. To keep things simple, we will use two random variables to explain concepts. These concepts can then be generalized to any number of random variables.</p>
<p>Recall that for a single discrete random variable <span class="math inline">\(X\)</span>, we use the PMF to inform us the support of <span class="math inline">\(X\)</span> and the probability associated with each value of the support. We said that the PMF informs us about the distribution of the random variable <span class="math inline">\(X\)</span>.</p>
<p>We now have two discrete random variables, <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span>. The <strong>joint distribution</strong> of <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span> provides the probability associated with each possible combination of <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span>. The <strong>joint PMF</strong> of <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span> is</p>
<p><span class="math display" id="eq:5-jointPMF">\[\begin{equation}
p_{X,Y}(x,y) = P(X=x, Y=y).
\tag{5.1}
\end{equation}\]</span></p>
<p>Equation <a href="joint-distributions.html#eq:5-jointPMF">(5.1)</a> can be read as the probability that the random variables <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span> are equal to <span class="math inline">\(x\)</span> and <span class="math inline">\(y\)</span> respectively. Recall that upper case letters are usually used to denote random variables, and lower case letters are usually used as a placeholder for an actual numerical value that the random variable could take.</p>
<p>Joint distributions are sometimes called <strong>multivariate distributions</strong>. If we are looking at the distribution of one random variable, its distribution can be called a <strong>univariate distribution</strong>.</p>
<p>Joint PMFs can be displayed via a table, like in Table <a href="joint-distributions.html#tab:simple-table">5.1</a> below. In this example, we consider how study time, <span class="math inline">\(X\)</span>, is related with grades, <span class="math inline">\(Y\)</span>, with</p>
<ul>
<li>
<span class="math inline">\(X=1\)</span> for studying 0 to 5 hours a week,</li>
<li>
<span class="math inline">\(X=2\)</span> for studying 6 to 10 hours a week, and</li>
<li>
<span class="math inline">\(X=3\)</span> for studying more than 10 hours a week.</li>
<li>
<span class="math inline">\(Y=1\)</span> denotes getting an A,</li>
<li>
<span class="math inline">\(Y=2\)</span> denotes getting a B, and</li>
<li>
<span class="math inline">\(Y=3\)</span> denotes getting a C or lower.</li>
</ul>
<div class="inline-table"><table class="table table-sm">
<caption>
<span id="tab:simple-table">Table 5.1: </span> Example Joint PMF of Study Time (<span class="math inline">\(X\)</span>) and Grades (<span class="math inline">\(Y\)</span>)</caption>
<thead><tr class="header">
<th align="left"></th>
<th align="left">X=1</th>
<th align="left">X=2</th>
<th align="left">X=3</th>
</tr></thead>
<tbody>
<tr class="odd">
<td align="left"><strong>Y=1</strong></td>
<td align="left">0.05</td>
<td align="left">0.15</td>
<td align="left">0.30</td>
</tr>
<tr class="even">
<td align="left"><strong>Y=2</strong></td>
<td align="left">0.05</td>
<td align="left">0.20</td>
<td align="left">0.10</td>
</tr>
<tr class="odd">
<td align="left"><strong>Y=3</strong></td>
<td align="left">0.10</td>
<td align="left">0.05</td>
<td align="left">0</td>
</tr>
</tbody>
</table></div>
<p>We could also write the joint PMF as:</p>
<ul>
<li><span class="math inline">\(P(X=1, Y=1) = 0.05\)</span></li>
<li><span class="math inline">\(P(X=1, Y=2) = 0.05\)</span></li>
<li><span class="math inline">\(P(X=1, Y=3) = 0.10\)</span></li>
<li><span class="math inline">\(P(X=2, Y=1) = 0.15\)</span></li>
<li><span class="math inline">\(P(X=2, Y=2) = 0.20\)</span></li>
<li><span class="math inline">\(P(X=2, Y=3) = 0.05\)</span></li>
<li><span class="math inline">\(P(X=3, Y=1) = 0.30\)</span></li>
<li><span class="math inline">\(P(X=3, Y=2) = 0.10\)</span></li>
<li><span class="math inline">\(P(X=3, Y=3) = 0\)</span></li>
</ul>
<p>Just like the PMFs of a single discrete random variable must sum to 1 and each PMF must be non negative, the joint PMFs of discrete random variables must sum to 1 and each individual PMF must be non negative to be valid.</p>
<p><em>Thought question</em>: Can you verify that the joint PMF in Table <a href="joint-distributions.html#tab:simple-table">5.1</a> is valid?</p>
<p>The <strong>joint CDF</strong> of any discrete random variables <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span> is</p>
<p><span class="math display" id="eq:5-jointCDF">\[\begin{equation}
F_{X,Y}(x,y) = P(X \leq x, Y \leq y).
\tag{5.2}
\end{equation}\]</span></p>
<p><em>Thought question</em>: Compare equation <a href="joint-distributions.html#eq:5-jointCDF">(5.2)</a> with its univariate counterpart in equation <a href="discrete-random-variables.html#eq:3-CDF">(3.1)</a>. Can you see the similarities and differences?</p>
<div id="marginal-distributions-for-discrete-rvs" class="section level3" number="5.2.1">
<h3>
<span class="header-section-number">5.2.1</span> Marginal Distributions for Discrete RVs<a class="anchor" aria-label="anchor" href="#marginal-distributions-for-discrete-rvs"><i class="fas fa-link"></i></a>
</h3>
<p>From the joint distribution of <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span>, we can get the distribution of each individual random variable. We call this the <strong>marginal distribution</strong>, or unconditional distribution, of <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span>. The marginal distribution of <span class="math inline">\(X\)</span> gives us information about the distribution of <span class="math inline">\(X\)</span>, without taking <span class="math inline">\(Y\)</span> into consideration. To get the marginal PMF of <span class="math inline">\(X\)</span> from the joint PMF of <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span>:</p>
<p><span class="math display" id="eq:5-margPMF">\[\begin{equation}
P(X=x) = \sum_y P(X=x, Y=y).
\tag{5.3}
\end{equation}\]</span></p>
<p>Note that the summation is performed over the support of <span class="math inline">\(Y\)</span>. We go back to Table <a href="joint-distributions.html#tab:simple-table">5.1</a> as an example. Suppose we want to find the marginal distribution of study times, <span class="math inline">\(X\)</span>. Applying equation <a href="joint-distributions.html#eq:5-margPMF">(5.3)</a>:</p>
<p><span class="math display">\[
\begin{split}
P(X=1) &amp;= \sum_y P(X=1, Y=y)\\
&amp;= P(X=1, Y=1) + P(X=1, Y=2) + P(X=1, Y=3) \\
&amp;= 0.05 + 0.05 + 0.10\\
&amp;= 0.2,
\end{split}
\]</span></p>
<p><span class="math display">\[
\begin{split}
P(X=2) &amp;= \sum_y P(X=2, Y=y)\\
&amp;= P(X=2, Y=1) + P(X=2, Y=2) + P(X=2, Y=3) \\
&amp;= 0.15 + 0.20 + 0.05\\
&amp;= 0.4,
\end{split}
\]</span></p>
<p>and</p>
<p><span class="math display">\[
\begin{split}
P(X=3) &amp;= \sum_y P(X=3, Y=y)\\
&amp;= P(X=3, Y=1) + P(X=3, Y=2) + P(X=3, Y=3) \\
&amp;= 0.30 + 0.10 + 0\\
&amp;= 0.4.
\end{split}
\]</span></p>
<p>We can add this information to Table <a href="joint-distributions.html#tab:simple-table">5.1</a>, to create Table <a href="joint-distributions.html#tab:5-marg-table">5.2</a></p>
<div class="inline-table"><table class="table table-sm">
<caption>
<span id="tab:5-marg-table">Table 5.2: </span> Example Joint PMF of Study Time (<span class="math inline">\(X\)</span>) and Grades (<span class="math inline">\(Y\)</span>), with Marginal PMF of Study Time</caption>
<thead><tr class="header">
<th align="left"></th>
<th align="left">X=1</th>
<th align="left">X=2</th>
<th align="left">X=3</th>
</tr></thead>
<tbody>
<tr class="odd">
<td align="left"><strong>Y=1</strong></td>
<td align="left">0.05</td>
<td align="left">0.15</td>
<td align="left">0.30</td>
</tr>
<tr class="even">
<td align="left"><strong>Y=2</strong></td>
<td align="left">0.05</td>
<td align="left">0.20</td>
<td align="left">0.10</td>
</tr>
<tr class="odd">
<td align="left"><strong>Y=3</strong></td>
<td align="left">0.10</td>
<td align="left">0.05</td>
<td align="left">0</td>
</tr>
<tr class="even">
<td align="left"><strong>Total</strong></td>
<td align="left">0.2</td>
<td align="left">0.4</td>
<td align="left">0.4</td>
</tr>
</tbody>
</table></div>
<p>Notice we just added the probabilities in each column to get the marginal PMF of <span class="math inline">\(X\)</span>, and write these probabilities to the margin of the table (hence the term marginal PMF).</p>
<p>You may notice that the marginal PMF of <span class="math inline">\(X\)</span> ends up being just the PMF of <span class="math inline">\(X\)</span>. The term marginal is used to imply that the PMF was derived from a joint PMF, even if the information is the same.</p>
<p><em>Thought question</em>: Can you see how equation <a href="joint-distributions.html#eq:5-margPMF">(5.3)</a> is based on the Law of Total Probability in equation <a href="probability.html#eq:total">(2.10)</a>?</p>
<p>Likewise, to obtain the marginal PMF of <span class="math inline">\(Y\)</span> from the joint PMF of <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span>:</p>
<p><span class="math display" id="eq:5-margPMF2">\[\begin{equation}
P(X=x) = \sum_x P(X=x, Y=y).
\tag{5.4}
\end{equation}\]</span></p>
<p>The summation is now performed over the support of <span class="math inline">\(X\)</span>.</p>
<p><em>Thought question</em>: Can you verify the marginal PMF for grades displayed Table <a href="joint-distributions.html#tab:5-marg-table2">5.3</a> below?</p>
<div class="inline-table"><table class="table table-sm">
<caption>
<span id="tab:5-marg-table2">Table 5.3: </span> Example Joint PMF of Study Time (<span class="math inline">\(X\)</span>) and Grades (<span class="math inline">\(Y\)</span>), with Marginal PMF of Study Time and Study Time</caption>
<thead><tr class="header">
<th align="left"></th>
<th align="left">X=1</th>
<th align="left">X=2</th>
<th align="left">X=3</th>
<th align="left">Total</th>
</tr></thead>
<tbody>
<tr class="odd">
<td align="left"><strong>Y=1</strong></td>
<td align="left">0.05</td>
<td align="left">0.15</td>
<td align="left">0.30</td>
<td align="left">0.50</td>
</tr>
<tr class="even">
<td align="left"><strong>Y=2</strong></td>
<td align="left">0.05</td>
<td align="left">0.20</td>
<td align="left">0.10</td>
<td align="left">0.35</td>
</tr>
<tr class="odd">
<td align="left"><strong>Y=3</strong></td>
<td align="left">0.10</td>
<td align="left">0.05</td>
<td align="left">0</td>
<td align="left">0.15</td>
</tr>
<tr class="even">
<td align="left"><strong>Total</strong></td>
<td align="left">0.2</td>
<td align="left">0.4</td>
<td align="left">0.4</td>
<td align="left">1</td>
</tr>
</tbody>
</table></div>
</div>
<div id="conddist" class="section level3" number="5.2.2">
<h3>
<span class="header-section-number">5.2.2</span> Conditional Distributions for Discrete RVs<a class="anchor" aria-label="anchor" href="#conddist"><i class="fas fa-link"></i></a>
</h3>
<p>We may need to update the distribution of one of the variables based on the observed value of the other variable, or we need the distribution of one of the variables based on a specific value of the other variable. This leads to the <strong>conditional PMF</strong>.</p>
<p>Suppose we want to update the distribution of <span class="math inline">\(Y\)</span> based on the observed value <span class="math inline">\(X=x\)</span>, or we want the distribution of <span class="math inline">\(Y\)</span> only for observations where <span class="math inline">\(X=x\)</span> (or in other words, <span class="math inline">\(X\)</span> is equal to a specific value <span class="math inline">\(x\)</span>). If <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span> are both discrete, the conditional PMF of <span class="math inline">\(Y\)</span> given <span class="math inline">\(X=x\)</span> is:</p>
<p><span class="math display" id="eq:5-condPMFY">\[\begin{equation}
P(Y=y|X=x) = \frac{P(X=x, Y=y)}{P(X=x)}.
\tag{5.5}
\end{equation}\]</span></p>
<p>The conditional PMF of <span class="math inline">\(Y\)</span> given <span class="math inline">\(X=x\)</span> is essentially the joint PMF of <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span> divided by the marginal PMF of <span class="math inline">\(X\)</span>. Note that the conditional PMF of <span class="math inline">\(Y\)</span> given <span class="math inline">\(X=x\)</span> is viewed as a function with the value of <span class="math inline">\(x\)</span> being fixed.</p>
<p>We go back to Table <a href="joint-distributions.html#tab:simple-table">5.1</a> as an example on how to find conditional PMFs. Suppose we want to find the distribution of grades for students who study little (0 to 5 hours per week). We want the conditional PMF of <span class="math inline">\(Y\)</span> given that <span class="math inline">\(X=1\)</span>. Applying equation <a href="joint-distributions.html#eq:5-condPMFY">(5.5)</a> to Table <a href="joint-distributions.html#tab:5-marg-table2">5.3</a>, we have</p>
<p><span class="math display">\[
\begin{split}
P(Y=1|X=1) &amp;= \frac{P(X=1, Y=1)}{P(X=1)}\\
&amp;= \frac{0.05}{0.2} \\
&amp;= 0.25,
\end{split}
\]</span></p>
<p><span class="math display">\[
\begin{split}
P(Y=2|X=1) &amp;= \frac{P(X=1, Y=2)}{P(X=1)}\\
&amp;= \frac{0.05}{0.2} \\
&amp;= 0.25,
\end{split}
\]</span></p>
<p>and</p>
<p><span class="math display">\[
\begin{split}
P(Y=3|X=1) &amp;= \frac{P(X=1, Y=3)}{P(X=1)}\\
&amp;= \frac{0.10}{0.2} \\
&amp;= 0.5.
\end{split}
\]</span>
The frequentist interpretation of these values is that among the students who studied little, they have a 50% chance of getting a C or lower, a 25% chance of getting a B, and a 25% chance of getting an A.</p>
<p>The Bayesian interpretation of these values is that if I know the student studied little, the student has a 50% chance of getting a C or lower, a 25% chance of getting a B, and a 25% chance of getting an A.</p>
<p><em>Thought question</em>: Can you show the conditional PMF of <span class="math inline">\(Y\)</span> given <span class="math inline">\(X=3\)</span> based on Table <a href="joint-distributions.html#tab:5-marg-table2">5.3</a> is <span class="math inline">\(P(Y=1|X=3) = 0.75, P(Y=2|X=3) = 0.25, P(Y=3|X=3) = 0\)</span>?</p>
<p>To find the conditional PMF of <span class="math inline">\(X\)</span> given <span class="math inline">\(Y=y\)</span>:</p>
<p><span class="math display" id="eq:5-condPMFX">\[\begin{equation}
P(X=x|Y=y) = \frac{P(X=x, Y=y)}{P(Y=y)}.
\tag{5.6}
\end{equation}\]</span></p>
<p><em>Thought question</em>: Can you show the conditional PMF of <span class="math inline">\(X\)</span> given <span class="math inline">\(Y=1\)</span> based on Table <a href="joint-distributions.html#tab:5-marg-table2">5.3</a> is <span class="math inline">\(P(X=1|Y=1) = 0.1, P(X=2|Y=1) = 0.3, P(X=3|Y=1) = 0.6\)</span>?</p>
</div>
<div id="bayes-rule-1" class="section level3" number="5.2.3">
<h3>
<span class="header-section-number">5.2.3</span> Bayes’ Rule<a class="anchor" aria-label="anchor" href="#bayes-rule-1"><i class="fas fa-link"></i></a>
</h3>
<p>We can apply Bayes’ Rule for an alternative way of finding the conditional PMF of <span class="math inline">\(Y\)</span> given <span class="math inline">\(X=x\)</span>. Equation <a href="joint-distributions.html#eq:5-condPMFY">(5.5)</a> can be written as:</p>
<p><span class="math display" id="eq:5-condPMFbayes">\[\begin{equation}
P(Y=y|X=x) = \frac{P(X=x|Y=y) P(Y=y)}{P(X=x)}.
\tag{5.7}
\end{equation}\]</span></p>
</div>
<div id="law-of-total-probability-1" class="section level3" number="5.2.4">
<h3>
<span class="header-section-number">5.2.4</span> Law of Total Probability<a class="anchor" aria-label="anchor" href="#law-of-total-probability-1"><i class="fas fa-link"></i></a>
</h3>
<p>We can apply the law of total probability to the denominator of equations <a href="joint-distributions.html#eq:5-condPMFY">(5.5)</a> and <a href="joint-distributions.html#eq:5-condPMFbayes">(5.7)</a>, i.e. <span class="math inline">\(P(X=x) = \sum_y P(X=x|Y=y) P(Y=y)\)</span>, so the conditional PMF of <span class="math inline">\(Y\)</span> given <span class="math inline">\(X=x\)</span> can also be written as</p>
<p><span class="math display" id="eq:5-condPMFtotal">\[\begin{equation}
P(Y=y|X=x) = \frac{P(X=x|Y=y) P(Y=y)}{\sum_y P(X=x|Y=y) P(Y=y)}.
\tag{5.8}
\end{equation}\]</span></p>
</div>
<div id="indepdence-of-discrete-rvs" class="section level3" number="5.2.5">
<h3>
<span class="header-section-number">5.2.5</span> Indepdence of Discrete RVs<a class="anchor" aria-label="anchor" href="#indepdence-of-discrete-rvs"><i class="fas fa-link"></i></a>
</h3>
<p>The notion of whether two random variables are <strong>independent</strong> or not (also called dependent) is whether the random variables have an association, or in other words, does changing the value of one random variable affect the distribution of the other?</p>
<p>If <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span> are discrete random variables, they are independent only if, for all values in the support of <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span>:</p>
<p><span class="math display" id="eq:5-ind">\[\begin{equation}
P(X=x, Y=y) = P(X=x) P(Y=y).
\tag{5.9}
\end{equation}\]</span></p>
<p>An equivalent condition is that for all values in the support of <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span>:</p>
<p><span class="math display" id="eq:5-ind2">\[\begin{equation}
P(Y=y | X=x) = P(Y=y),
\tag{5.10}
\end{equation}\]</span></p>
<p>or</p>
<p><span class="math display" id="eq:5-ind3">\[\begin{equation}
P(X=x | Y=y) = P(X=x),
\tag{5.11}
\end{equation}\]</span></p>
<p>To show that <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span> are independent, we need to show one of equations <a href="joint-distributions.html#eq:5-ind">(5.9)</a>, <a href="joint-distributions.html#eq:5-ind2">(5.10)</a>, or <a href="joint-distributions.html#eq:5-ind3">(5.11)</a> to be true for <strong>all values in the support</strong> of <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span>. To show that <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span> are dependent, we need to show one of equations <a href="joint-distributions.html#eq:5-ind">(5.9)</a>, <a href="joint-distributions.html#eq:5-ind2">(5.10)</a>, or <a href="joint-distributions.html#eq:5-ind3">(5.11)</a> to be false for <strong>just one value</strong> of <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span>.</p>
<p>Equations <a href="joint-distributions.html#eq:5-ind2">(5.10)</a> and <a href="joint-distributions.html#eq:5-ind3">(5.11)</a> are pretty intuitive. These equations say that the conditional distribution of one variable, given the other, is the same as the marginal distribution of the variable. This means the distribution of the variable is not influenced by knowlege about the other variable.</p>
<p>The first equation <a href="joint-distributions.html#eq:5-ind">(5.9)</a> informs us that if the discrete variables are independent, their joint PMF is equal to the product of their marginal PMFs.</p>
<p>We go back to the study time and grades example shown in Table <a href="joint-distributions.html#tab:5-marg-table2">5.3</a>. Study time and grades are dependent (or not independent) since <span class="math inline">\(P(Y=1|X=1) = 0.25\)</span> but <span class="math inline">\(P(Y=1) = 0.5\)</span>. They are not equal so study time and grades are not independent. It is usually easier to prove a condition is not met by providing a <strong>counterexample</strong>: find one specific example where the condition is false.</p>
<p>If study time and grades were independent, we needed to show <span class="math inline">\(P(Y=1|X=x) = P(Y=1)\)</span> when <span class="math inline">\(X=1,2,3\)</span>, <span class="math inline">\(P(Y=2|X=x) = P(Y=2)\)</span> when <span class="math inline">\(X=1,2,3\)</span>, and <span class="math inline">\(P(Y=3|X=x) = P(Y=3)\)</span> when <span class="math inline">\(X=1,2,3\)</span>. It is usually more tedious to prove a condition is met as we have to show the condition is met under all circumstances.</p>
<p>Very often, knowing the context of the random variables helps. Since we expect students who study more to get better grades, we expect these variables to be dependent, so we know we just need to provide a counterexample.</p>
</div>
</div>
<div id="joint-marginal-conditional-distributions-for-continuous-rvs" class="section level2" number="5.3">
<h2>
<span class="header-section-number">5.3</span> Joint, Marginal, Conditional Distributions for Continuous RVs<a class="anchor" aria-label="anchor" href="#joint-marginal-conditional-distributions-for-continuous-rvs"><i class="fas fa-link"></i></a>
</h2>
<p>Recall in the previous modules the CDFs and PDFs for a continuous random variable are similar to CDFs and PMFs for discrete random variables. The continuous versions are generally found by swapping summations with integrals. The same general idea applies with joint distributions when both random variables are continuous.</p>
<p>Now suppose that <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span> denote random variables that are continuous. It is required that the <strong>joint CDF</strong> <span class="math inline">\(F_{X,Y}(x,y) = P(X \leq x, Y \leq y)\)</span> is differentiable with respect to <span class="math inline">\(x\)</span> and <span class="math inline">\(y\)</span>. Their <strong>joint PDF</strong> is the partial derivative of their joint CDF with respect to <span class="math inline">\(x\)</span> and <span class="math inline">\(y\)</span>: <span class="math inline">\(f_{X,Y}(x,y) = \frac{\partial^2}{\partial x \partial y} F_{X,Y}(x,y)\)</span>.</p>
<p>Similar to univariate PDFs, for joint PDFs to be valid, we require that:</p>
<ul>
<li>
<span class="math inline">\(f_{X,Y}(x,y) \geq 0\)</span> and</li>
<li>
<span class="math inline">\(\int_{-\infty}^\infty \int_{-\infty}^\infty f_{X,Y}(x,y)  dx dy = 1\)</span>.</li>
</ul>
<p>To find probabilities, for example <span class="math inline">\(P(a&lt;X&lt;b, c&lt;Y&lt;d)\)</span>, we integrate the joint PDF over the two-dimensional region, i.e. <span class="math inline">\(\int_{c}^d \int_{a}^b f_{X,Y}(x,y) dx dy\)</span>.</p>
<p>The <strong>marginal PDF</strong> of <span class="math inline">\(X\)</span> can be found by integrating their joint PDF over the support of <span class="math inline">\(Y\)</span>:</p>
<p><span class="math display" id="eq:5-margPDF">\[\begin{equation}
f_X(x) = \int_{-\infty}^\infty f_{X,Y}(x,y) dy.
\tag{5.12}
\end{equation}\]</span></p>
<p>The conditional PDF of <span class="math inline">\(Y\)</span> given <span class="math inline">\(X=x\)</span> is</p>
<p><span class="math display" id="eq:5-condPDF">\[\begin{equation}
f_{Y|X}(y|x) = \frac{f_{X,Y}(x,y)}{f_X(x)}
\tag{5.13}
\end{equation}\]</span></p>
<p>Bayes’ rule for continuous random variables is</p>
<p><span class="math display" id="eq:5-condPDFbayes">\[\begin{equation}
f_{Y|X}(y|x) = \frac{f_{X|Y}(x|y) f_Y(y)}{f_X(x)}
\tag{5.14}
\end{equation}\]</span></p>
<p>And the law of total probability is</p>
<p><span class="math display" id="eq:5-condPDFtotal">\[\begin{equation}
f_X(x) = \int_{-\infty}^\infty f_{X|Y}(x|y) f_Y(y) dy.
\tag{5.15}
\end{equation}\]</span></p>
<p>Continuous random variables <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span> are independent if for all values of <span class="math inline">\(x\)</span> and <span class="math inline">\(y\)</span>:</p>
<p><span class="math display" id="eq:5-indcont1">\[\begin{equation}
F_{X,Y} (x,y) = F_X(x) F_Y(y)
\tag{5.16}
\end{equation}\]</span></p>
<p>or</p>
<p><span class="math display" id="eq:5-indcont2">\[\begin{equation}
f_{X,Y} (x,y) = f_X(x) f_Y(y)
\tag{5.17}
\end{equation}\]</span></p>
<p>or</p>
<p><span class="math display" id="eq:5-indcont3">\[\begin{equation}
f_{Y|X} (y|x) = f_Y(y)
\tag{5.18}
\end{equation}\]</span></p>
<p>or</p>
<p><span class="math display" id="eq:5-indcont4">\[\begin{equation}
f_{X|Y} (x|y) = f_X(x).
\tag{5.19}
\end{equation}\]</span></p>
</div>
<div id="covariance-and-correlation" class="section level2" number="5.4">
<h2>
<span class="header-section-number">5.4</span> Covariance and Correlation<a class="anchor" aria-label="anchor" href="#covariance-and-correlation"><i class="fas fa-link"></i></a>
</h2>
<p>In the previous modules, we used summaries such as the mean, variance, skewness, and kurtosis to provide some insight into the distribution of a single random variable. When we have multiple random variables, one question we have is how are the random variables related to each other. Summaries that are used to quantify the <strong>linear relationship</strong> between two quantitative random variables are the covariance and correlation.</p>
<p>Generally speaking, two random variables have a positive covariance and correlation if they increase or decrease together, i.e. if <span class="math inline">\(X\)</span> increases, <span class="math inline">\(Y\)</span> also generally increases; if <span class="math inline">\(X\)</span> decreases, <span class="math inline">\(Y\)</span> also generally decreases.</p>
<p>Two random variables have a negative covariance and correlation if they move in the opposite direction, i.e. if <span class="math inline">\(X\)</span> increases, <span class="math inline">\(Y\)</span> generally decreases; if <span class="math inline">\(X\)</span> decreases, <span class="math inline">\(Y\)</span> generally increases. Figure <a href="joint-distributions.html#fig:5-covs">5.1</a> below displays these ideas visually through scatter plots. The scatter plot on the left shows an example of a pair of random variables having positive covariance, and the scatter plot on the right shows an example of a pair of random variables having negative covariance.</p>
<div class="figure">
<span style="display:block;" id="fig:5-covs"></span>
<img src="bookdown-demo_files/figure-html/5-covs-1.png" alt="Positive Covariance (Left), Negative Covariance (Right)" width="672"><p class="caption">
Figure 5.1: Positive Covariance (Left), Negative Covariance (Right)
</p>
</div>
<p>One more thing to note: covariance and correlations can be calculated for random variables as long as they are quantitative, but not if at least one of them is categorical. The concept of increasing a random variable that is categorical does not make intuitive sense, for example, if we have a random variable that denotes the color of eyes, what does increasing color of eyes mean?</p>
<div id="id_5-cov" class="section level3" number="5.4.1">
<h3>
<span class="header-section-number">5.4.1</span> Covariance<a class="anchor" aria-label="anchor" href="#id_5-cov"><i class="fas fa-link"></i></a>
</h3>
<p>We now define covariance. The <strong>covariance</strong> between random variables <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span> is</p>
<p><span class="math display" id="eq:5-cov">\[\begin{equation}
Cov(X,Y) = E\left[(X- \mu_X)(Y - \mu_Y) \right].
\tag{5.20}
\end{equation}\]</span></p>
<p>Looking at equation <a href="joint-distributions.html#eq:5-cov">(5.20)</a>, we see that if both <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span> generally move in the same direction, then <span class="math inline">\(X - \mu_x\)</span> and <span class="math inline">\(Y - \mu_y\)</span> will either be both positive or both negative, therefore their product is positive, on average. If <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span> generally move in opposite directions, then <span class="math inline">\(X - \mu_x\)</span> and <span class="math inline">\(Y - \mu_y\)</span> have opposite signs, therefore their product is negative, on average.</p>
<p>Some key properties for covariance:</p>
<ul>
<li>
<span class="math inline">\(Cov(X,X) = Var(X)\)</span>.The covariance of any random variable with itself is its variance.</li>
<li>
<span class="math inline">\(Cov(X,Y) = Cov(Y,X)\)</span>. The covariance between <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span> is the same as the covariance between <span class="math inline">\(Y\)</span> and <span class="math inline">\(X\)</span>.</li>
<li>
<span class="math inline">\(Cov(X,c) = 0\)</span> for any constant <span class="math inline">\(c\)</span>. Since a constant does not move, it has no relationship with <span class="math inline">\(X\)</span>.</li>
<li>
<span class="math inline">\(Cov(aX,Y) = a Cov(X,Y)\)</span> for any constant <span class="math inline">\(a\)</span>. This implies that covariance is affected by the units of <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span>.</li>
<li>
<span class="math inline">\(Var(X + Y) = Var(X) + Var(Y) + 2 Cov(X,Y)\)</span>.</li>
<li>If <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span> are independent, then <span class="math inline">\(Cov(X,Y) = 0\)</span>.</li>
<li>However, <span class="math inline">\(Cov(X,Y) = 0\)</span> does not mean that <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span> are independent. This is a common misconception. Remember that covariance measures linear relationship. The relationship between <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span> could be non linear, and in such instances, the covariance should not be used. Figure <a href="joint-distributions.html#fig:5-cov0">5.2</a> below provides an example this. In this figure, <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span> have a quadratic relationship, so they are clearly not independent, yet their covariance is virtually 0.</li>
</ul>
<div class="sourceCode" id="cb77"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">x</span><span class="op">&lt;-</span><span class="fu"><a href="https://rdrr.io/r/base/seq.html">seq</a></span><span class="op">(</span><span class="op">-</span><span class="fl">1</span>,<span class="fl">1</span>, by<span class="op">=</span><span class="fl">0.01</span><span class="op">)</span></span>
<span><span class="va">y</span><span class="op">&lt;-</span><span class="va">x</span><span class="op">^</span><span class="fl">2</span></span>
<span></span>
<span><span class="co">##note from plot that X &amp; Y do not have a linear relationship</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/graphics/plot.default.html">plot</a></span><span class="op">(</span><span class="va">x</span>,<span class="va">y</span>, xlab<span class="op">=</span><span class="st">"X"</span>, ylab<span class="op">=</span><span class="st">"Y"</span><span class="op">)</span></span></code></pre></div>
<div class="figure">
<span style="display:block;" id="fig:5-cov0"></span>
<img src="bookdown-demo_files/figure-html/5-cov0-1.png" alt="Covariance with Non Linear Relationship" width="672"><p class="caption">
Figure 5.2: Covariance with Non Linear Relationship
</p>
</div>
<div class="sourceCode" id="cb78"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="fu"><a href="https://rdrr.io/r/stats/cor.html">cov</a></span><span class="op">(</span><span class="va">x</span>,<span class="va">y</span><span class="op">)</span> <span class="co">##covariance is virtually 0</span></span></code></pre></div>
<pre><code>## [1] 1.19967e-17</code></pre>
<p>Suppose we have two vectors of observed data, each of size <span class="math inline">\(n\)</span>: <span class="math inline">\(X = (x_1, x_2, \cdots, x_n)\)</span> and <span class="math inline">\(Y = (y_1, y_2, \cdots, y_n)\)</span>. Their <strong>sample covariance</strong> is</p>
<p><span class="math display" id="eq:5-sampcov">\[\begin{equation}
s_{x,y} = \frac{\sum_{i=1}^n (x_i - \bar{x})(y_i - \bar{y})}{n-1}
\tag{5.21}
\end{equation}\]</span></p>
<p>We noted earlier that covariance is affected by the units of the variables. Suppose one variable is measured in meters, and we convert it to become centimeters. The value of the covariance will get multiplied by 100. People find it easier to interpret a measure that does not depend on the units. This is where the correlation comes in: it is a unitless version of the covariance.</p>
</div>
<div id="correlation" class="section level3" number="5.4.2">
<h3>
<span class="header-section-number">5.4.2</span> Correlation<a class="anchor" aria-label="anchor" href="#correlation"><i class="fas fa-link"></i></a>
</h3>
<p>The <strong>correlation</strong> between random variables <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span> is</p>
<p><span class="math display" id="eq:5-corr">\[\begin{equation}
\rho = Corr(X,Y) = \frac{Cov(X,Y)}{\sqrt{Var(X) Var(Y)}}.
\tag{5.22}
\end{equation}\]</span></p>
<p>The <strong>sample correlation</strong> for two vectors of observed data, each of size <span class="math inline">\(n\)</span>: <span class="math inline">\(X = (x_1, x_2, \cdots, x_n)\)</span> and <span class="math inline">\(Y = (y_1, y_2, \cdots, y_n)\)</span>, is</p>
<p><span class="math display" id="eq:5-sampcorr">\[\begin{equation}
r = \frac{\sum_{i=1}^n (x_i - \bar{x})(y_i - \bar{y})}{\sqrt{\sum_{i=1}^n (x_i - \bar{x})^2 \sum_{i=1}^n (y_i - \bar{y})^2}}
\tag{5.23}
\end{equation}\]</span></p>
<p>Some key properties of correlation:</p>
<ul>
<li>It is bounded between -1 and 1.</li>
<li>Values closer to -1 or 1 indicate a stronger linear relationship.</li>
<li>Values closer to 0 indicate a weaker linear relationship.</li>
<li>Its numerical value is unchanged with location and / or scale changes.</li>
<li>If <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span> are independent, then <span class="math inline">\(Corr(X,Y) = 0\)</span>.</li>
<li>However, <span class="math inline">\(Corr(X,Y) = 0\)</span> does not mean that <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span> are independent.</li>
<li>Correlation should only be used if the relationship between <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span> is linear.</li>
</ul>
<p>Figure <a href="joint-distributions.html#fig:5-corrs">5.3</a> below shows examples of some scatterplots and their sample correlations. For the left plot, the data points fall close to a straight line, is negative, and so its correlation is close to -1. The middle plot has no linear relationship, so we do not see a trend with one variable increasing or decreasing as the other variable increases. The right plot shows the data points being fairly close to a straight line, but not as close as the left plot), so its correlation is not as close to 1 (or -1).</p>
<div class="figure">
<span style="display:block;" id="fig:5-corrs"></span>
<img src="bookdown-demo_files/figure-html/5-corrs-1.png" alt="Strong Negative Correlation (Left), No Correlation (Middle), Moderate Positive Correlation (Right)" width="672"><p class="caption">
Figure 5.3: Strong Negative Correlation (Left), No Correlation (Middle), Moderate Positive Correlation (Right)
</p>
</div>
</div>
</div>
<div id="conditional-expectation" class="section level2" number="5.5">
<h2>
<span class="header-section-number">5.5</span> Conditional Expectation<a class="anchor" aria-label="anchor" href="#conditional-expectation"><i class="fas fa-link"></i></a>
</h2>
<p>In Section <a href="probability.html#condprob">2.3</a> and <a href="joint-distributions.html#conddist">5.2.2</a>, we explored the notion of conditional probabilities and conditional distributions, which are used for:</p>
<ul>
<li>Updating the probability and distribution of a random variable <span class="math inline">\(Y\)</span>, after observing a certain outcome of another random variable <span class="math inline">\(X\)</span>, or</li>
<li>Restricting the probability and distribution of a random variable <span class="math inline">\(Y\)</span> to a certain value of another random variable.</li>
</ul>
<p>These represent the Bayesian and frequentist viewpoints of conditional probability and conditional distribution.</p>
<p>It turns out a similar idea applies to the expected value of a random variable. Recall that the expected value of a random variable is the long-run average, in other words, the average value after observing the random variable an infinite number of times.</p>
<p>The conditional expectation of a random variable is its long run-average:</p>
<ul>
<li>after observing a certain outcome of another variable or event, or</li>
<li>after restricting our attention to cases when another random variable is fixed or equal to a specific value.</li>
</ul>
<p>Fairly often, we use statistical models to predict a response variable <span class="math inline">\(Y\)</span> based on a predictor <span class="math inline">\(X\)</span>. Predictions for values of <span class="math inline">\(Y\)</span> based on observed values of <span class="math inline">\(X\)</span> usually use the conditional expectation of <span class="math inline">\(Y\)</span> given <span class="math inline">\(X\)</span>. Given what we see with the predictor, the long run average of <span class="math inline">\(Y\)</span> ends up being used as the predicted value of the response variable. This is the basis for most statistical models.</p>
<p>There are two slightly different notions of conditional expectations:</p>
<ul>
<li>Conditional expectation of random variable <span class="math inline">\(Y\)</span> given event <span class="math inline">\(A\)</span>. If <span class="math inline">\(A\)</span> has happened, what is the expected value of <span class="math inline">\(Y\)</span>?</li>
<li>Conditional expectation of random variable <span class="math inline">\(Y\)</span> given another random variable <span class="math inline">\(X\)</span>. If we fix the value of the random variable <span class="math inline">\(X\)</span> to any value on its support, what is the expected value of <span class="math inline">\(Y\)</span>?</li>
</ul>
<p>The second notion is the one that is usually used for statistical models, but we will cover both as the first notion is easier to understand, and should help us understand the second notion.</p>
<div id="conditional-expectation-given-event" class="section level3" number="5.5.1">
<h3>
<span class="header-section-number">5.5.1</span> Conditional Expectation Given Event<a class="anchor" aria-label="anchor" href="#conditional-expectation-given-event"><i class="fas fa-link"></i></a>
</h3>
<p>Recall that the expectation <span class="math inline">\(E(Y)\)</span> of a random variable <span class="math inline">\(Y\)</span> is its long-run average. If <span class="math inline">\(Y\)</span> is discrete, we take a weighted average involving the probabilities in its PMF <span class="math inline">\(P(Y=y)\)</span>. The calculation of the <strong>conditional expectation</strong> <span class="math inline">\(E(Y|A)\)</span> where <span class="math inline">\(A\)</span> is an event that has occurred simply replaces the probabilities <span class="math inline">\(P(Y=y)\)</span> with conditional probabilities <span class="math inline">\(P(Y=y|A)\)</span>. Therefore, for a discrete random variable <span class="math inline">\(Y\)</span>,</p>
<p><span class="math display" id="eq:5-condexpdis">\[\begin{equation}
E(Y|A) = \sum_y y P(Y=y|A)
\tag{5.24}
\end{equation}\]</span></p>
<p>where the sum is over the support of <span class="math inline">\(Y\)</span>. Notice how we are summing the product of the support with its corresponding conditional probability, whereas to find <span class="math inline">\(E(Y)\)</span>, we sum the product of the support with its corresponding unconditional probability.</p>
<p>If <span class="math inline">\(Y\)</span> is continuous, we use the conditional PDF instead:</p>
<p><span class="math display" id="eq:5-condexpcont">\[\begin{equation}
E(Y|A) = \int_{-\infty}^{\infty} y f(y|A) dy.
\tag{5.25}
\end{equation}\]</span></p>
<p>The key is to understand the intuition behind conditional expectations, and we will use simulation to approximate this (the approximation works better if we use more simulated data). Simulation represents the frequentist viewpoint of conditional expectation. The code below does the following:</p>
<ul>
<li>Generate 100 values of <span class="math inline">\(X\)</span> uniformly on the support <span class="math inline">\(\{1,2,3,4\}\)</span>.</li>
<li>Simulate <span class="math inline">\(Y\)</span> using <span class="math inline">\(Y = 10 + X + \epsilon\)</span> where <span class="math inline">\(\epsilon \sim N(0,1)\)</span>.</li>
<li>Represent these values on a scatter plot, and also overlay a line that represents the sample mean of <span class="math inline">\(Y\)</span>, which estimates <span class="math inline">\(E(Y)\)</span>. This is simply the average value on the y-axis for all 100 data points. This is the plot on the left in Figure <a href="joint-distributions.html#fig:5-condexp">5.4</a> below.</li>
<li>Represent these values on a scatter plot, but use blue to denote the event <span class="math inline">\(A\)</span> which is when <span class="math inline">\(X=1\)</span>. A line that represents the sample mean of <span class="math inline">\(Y\)</span>, only for these blue data points (i.e. only when <span class="math inline">\(X=1\)</span>), is overlaid. This value estimates <span class="math inline">\(E(Y|A)\)</span> or <span class="math inline">\(E(Y|X=1)\)</span>. This is the plot on the right in Figure <a href="joint-distributions.html#fig:5-condexp">5.4</a> below. In calculating this sample mean, we could have completed disregarded the black data points when <span class="math inline">\(X\)</span> was not 1.</li>
</ul>
<div class="sourceCode" id="cb80"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="fu"><a href="https://rdrr.io/r/base/Random.html">set.seed</a></span><span class="op">(</span><span class="fl">40</span><span class="op">)</span></span>
<span><span class="va">n</span><span class="op">&lt;-</span><span class="fl">100</span> <span class="co">##100 data points</span></span>
<span></span>
<span><span class="co">##generate X</span></span>
<span><span class="va">x</span><span class="op">&lt;-</span><span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/base/rep.html">rep</a></span><span class="op">(</span><span class="fl">1</span>,<span class="va">n</span><span class="op">/</span><span class="fl">4</span><span class="op">)</span>, <span class="fu"><a href="https://rdrr.io/r/base/rep.html">rep</a></span><span class="op">(</span><span class="fl">2</span>,<span class="va">n</span><span class="op">/</span><span class="fl">4</span><span class="op">)</span>, <span class="fu"><a href="https://rdrr.io/r/base/rep.html">rep</a></span><span class="op">(</span><span class="fl">3</span>,<span class="va">n</span><span class="op">/</span><span class="fl">4</span><span class="op">)</span>, <span class="fu"><a href="https://rdrr.io/r/base/rep.html">rep</a></span><span class="op">(</span><span class="fl">4</span>,<span class="va">n</span><span class="op">/</span><span class="fl">4</span><span class="op">)</span><span class="op">)</span> </span>
<span></span>
<span><span class="co">##simulate Y</span></span>
<span><span class="va">y</span><span class="op">&lt;-</span> <span class="fl">10</span> <span class="op">+</span> <span class="va">x</span> <span class="op">+</span> <span class="fu"><a href="https://rdrr.io/r/stats/Normal.html">rnorm</a></span><span class="op">(</span><span class="va">n</span><span class="op">)</span></span>
<span></span>
<span><span class="fu"><a href="https://rdrr.io/r/graphics/par.html">par</a></span><span class="op">(</span>mfrow<span class="op">=</span><span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span><span class="fl">1</span>,<span class="fl">2</span><span class="op">)</span><span class="op">)</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/graphics/plot.default.html">plot</a></span><span class="op">(</span><span class="va">x</span>,<span class="va">y</span>, main<span class="op">=</span><span class="st">"Estimated E(Y) Overlaid"</span><span class="op">)</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/graphics/abline.html">abline</a></span><span class="op">(</span>h<span class="op">=</span><span class="fu"><a href="https://rdrr.io/r/base/mean.html">mean</a></span><span class="op">(</span><span class="va">y</span><span class="op">)</span><span class="op">)</span> <span class="co">##add line to represent est E(Y)</span></span>
<span></span>
<span><span class="fu"><a href="https://rdrr.io/r/graphics/plot.default.html">plot</a></span><span class="op">(</span><span class="va">x</span>,<span class="va">y</span>, col <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/ifelse.html">ifelse</a></span><span class="op">(</span><span class="va">x</span> <span class="op">==</span> <span class="fl">1</span>,<span class="st">'blue'</span>, <span class="st">'black'</span><span class="op">)</span>, pch <span class="op">=</span> <span class="fl">19</span>, main<span class="op">=</span><span class="st">"Estimated E(Y|X=1) Overlaid"</span> <span class="op">)</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/graphics/abline.html">abline</a></span><span class="op">(</span>h<span class="op">=</span><span class="fu"><a href="https://rdrr.io/r/base/mean.html">mean</a></span><span class="op">(</span><span class="va">y</span><span class="op">[</span>x<span class="op">=</span><span class="fl">1</span><span class="op">]</span><span class="op">)</span>, col<span class="op">=</span><span class="st">"blue"</span><span class="op">)</span> <span class="co">##add line to represent est E(Y|X=1)</span></span></code></pre></div>
<div class="figure">
<span style="display:block;" id="fig:5-condexp"></span>
<img src="bookdown-demo_files/figure-html/5-condexp-1.png" alt="Comparison of E(Y) and E(Y|X=1)" width="672"><p class="caption">
Figure 5.4: Comparison of E(Y) and E(Y|X=1)
</p>
</div>
<p>So, we can interpret the conditional expectation <span class="math inline">\(E(Y|A)\)</span> as the long-run average of <span class="math inline">\(Y\)</span> (only) when <span class="math inline">\(A\)</span> has happened. It is the long-run average of <span class="math inline">\(Y\)</span> when a certain condition is met.</p>
</div>
<div id="conditional-expectation-given-random-variable" class="section level3" number="5.5.2">
<h3>
<span class="header-section-number">5.5.2</span> Conditional Expectation Given Random Variable<a class="anchor" aria-label="anchor" href="#conditional-expectation-given-random-variable"><i class="fas fa-link"></i></a>
</h3>
<p>The conditional expectation of <span class="math inline">\(Y\)</span> given a random variable <span class="math inline">\(X\)</span> is slightly different. In the simulated example in the previous subsection, we set <span class="math inline">\(X\)</span> to be a specific value. Now, we consider the long-run average of <span class="math inline">\(Y\)</span> for each value, instead of a specific value, in the support of <span class="math inline">\(X\)</span>.</p>
<p>One way to think about this is to consider <span class="math inline">\(E(Y|X=x)\)</span>, where <span class="math inline">\(x\)</span> is any value on the support for <span class="math inline">\(X\)</span>. If <span class="math inline">\(Y\)</span> is discrete, this conditional expectation is:</p>
<p><span class="math display" id="eq:5-condexpdis2">\[\begin{equation}
E(Y|A) = \sum_y y P(Y=y|X=x)
\tag{5.26}
\end{equation}\]</span></p>
<p>where the sum is over the support of <span class="math inline">\(Y\)</span>.</p>
<p>If <span class="math inline">\(Y\)</span> is continuous:</p>
<p><span class="math display" id="eq:5-condexpcont2">\[\begin{equation}
E(Y|A) = \int_{-\infty}^{\infty} y f(y|x) dy.
\tag{5.27}
\end{equation}\]</span></p>
<p>We go back to the simulated example in the previous subsection to explain what <span class="math inline">\(E(Y|X=x)\)</span> represents. Recall that the support for <span class="math inline">\(X\)</span> is <span class="math inline">\(\{1,2,3,4\}\)</span> and that <span class="math inline">\(Y = 10 + X + \epsilon\)</span> where <span class="math inline">\(\epsilon \sim N(0,1)\)</span>. So</p>
<p><span class="math display">\[
\begin{split}
E(Y|X=x) &amp;= E(10 + X + \epsilon | X=x)\\
&amp;= E(10 + x + \epsilon) \\
&amp;= E(10) + E(x) + E(\epsilon) \\
&amp;= 10 + x + 0 \\
&amp;= 10 + x.
\end{split}
\]</span></p>
<p>A brief explanation of each step:</p>
<ul>
<li>To go from line 1 to line 2, we subbed in <span class="math inline">\(x\)</span> for <span class="math inline">\(X\)</span>, since we are setting <span class="math inline">\(X=x\)</span>.</li>
<li>To go from line 2 to line 3, we apply the linearity of expectations.</li>
<li>To go from line 3 to 4, <span class="math inline">\(E(c)=c\)</span> for any constant. In this case, we are fixing <span class="math inline">\(x\)</span> to be a value in the support so it is a constant, and <span class="math inline">\(E(\epsilon) = 0\)</span> since <span class="math inline">\(\epsilon \sim N(0,1)\)</span>.</li>
</ul>
<p>So <span class="math inline">\(E(Y|X) = 10 + X\)</span>. What this means is that:</p>
<ul>
<li>When <span class="math inline">\(X=1\)</span>, <span class="math inline">\(E(Y|X=1) = 11\)</span>,</li>
<li>When <span class="math inline">\(X=2\)</span>, <span class="math inline">\(E(Y|X=1) = 12\)</span>,</li>
<li>When <span class="math inline">\(X=3\)</span>, <span class="math inline">\(E(Y|X=1) = 13\)</span>, and</li>
<li>When <span class="math inline">\(X=4\)</span>, <span class="math inline">\(E(Y|X=1) = 14\)</span>.</li>
</ul>
<p>Note: We set up <span class="math inline">\(Y = 10 + X + \epsilon\)</span> where <span class="math inline">\(\epsilon \sim N(0,1)\)</span> in the simulation. This follows the framework for linear regression which sets up <span class="math inline">\(Y = \beta_0 + \beta_1 X + \epsilon\)</span> where <span class="math inline">\(\epsilon \sim N(0,\sigma^2)\)</span>, i.e. <span class="math inline">\(\epsilon\)</span> is normal with mean 0 and a variance that is a fixed value. The conditional expectation given <span class="math inline">\(X\)</span> ends up being the prediction for <span class="math inline">\(Y\)</span> that minimizes the mean squared error in linear regression.</p>
</div>
</div>
<div id="common-multivariate-distributions" class="section level2" number="5.6">
<h2>
<span class="header-section-number">5.6</span> Common Multivariate Distributions<a class="anchor" aria-label="anchor" href="#common-multivariate-distributions"><i class="fas fa-link"></i></a>
</h2>
<p>We now cover two of the most common multivariate distributions: the multinomial distribution and multivariate normal distribution for discrete and continuous random variables respectively.</p>
<div id="multinomial" class="section level3" number="5.6.1">
<h3>
<span class="header-section-number">5.6.1</span> Multinomial<a class="anchor" aria-label="anchor" href="#multinomial"><i class="fas fa-link"></i></a>
</h3>
<p>The <strong>multinomial</strong> distribution can be viewed as a generalization of the binomial distribution into higher dimensions. Recall that for the binomial distribution, we carry out <span class="math inline">\(n\)</span> trials, and for each trial we record whether it as a success or failure, in other words, there are only two outcomes for each trial. The multinomial distribution differs in that there can be more than two outcomes for each trial. For example, we randomly select <span class="math inline">\(n\)</span> adults and ask them for their political affiliation. The affiliation could be Democrat, Republican, other party, or no affiliation, so there are four possible outcomes or categories for each person.</p>
<p>The set up of the multinomial distribution is as follows:</p>
<ul>
<li>We have <span class="math inline">\(n\)</span> independent trials, and each trial belongs to one of <span class="math inline">\(k\)</span> categories.</li>
<li>Each trial belongs to category <span class="math inline">\(j\)</span> with probability <span class="math inline">\(p_j\)</span>, where <span class="math inline">\(p_j\)</span> is non negative and <span class="math inline">\(\sum_{j=1}^k p_j = 1\)</span>, i.e. they sum to one.</li>
<li>Let <span class="math inline">\(X_1\)</span> denote the number of trials belonging to category 1, <span class="math inline">\(X_2\)</span> denote the number of trials belonging to category 2, and so on. Then <span class="math inline">\(X_1 + \cdots X_k = n\)</span>.</li>
</ul>
<p>We then say that <span class="math inline">\(\boldsymbol{X} = (X_1, \cdots, X_k)\)</span> is said to have a multinomial distribution with parameters <span class="math inline">\(n\)</span> and <span class="math inline">\(\boldsymbol{p} = (p_1, \cdots, p_k)\)</span>. This can be written as <span class="math inline">\(\boldsymbol{X} \sim Mult_k(n, \boldsymbol{p})\)</span>.</p>
<p>Note that the vectors <span class="math inline">\(\boldsymbol{X}\)</span> and <span class="math inline">\(\boldsymbol{p}\)</span> are written in bold. Vectors and matrices are commonly written using bold to distinguish them from scalars, which are not in bold. <span class="math inline">\(\boldsymbol{X}\)</span> is an example of what we call a random vector, as it is a vector of random variables <span class="math inline">\(X_1, \cdots, X_k\)</span>.</p>
<p>If <span class="math inline">\(\boldsymbol{X} \sim Mult_k(n, \boldsymbol{p})\)</span>, its PMF is</p>
<p><span class="math display" id="eq:5-multinomial">\[\begin{equation}
P(X_1 = n_1, \cdots, X_k = n_k) = \frac{n!}{n_{1}! \cdots n_{k}!} p_1^{n_1} \cdots p_k^{n_k},
\tag{5.28}
\end{equation}\]</span></p>
<p>where <span class="math inline">\(n_1 + \cdots + n_k = n\)</span>.</p>
<p>Let us use a toy example. Going back to political affiliations. Suppose among American voters, 28% identify as Democrats, 29% identify as Republicans, and 10% identify have other affiliations, and 33% are independents. Let <span class="math inline">\(X_1, X_2, X_3, X_4\)</span> denote the number of Democrats, Republicans, others, and independents. The joint distribution of <span class="math inline">\(X_1, X_2, X_3, X_4\)</span> is <span class="math inline">\(\boldsymbol{X} = (X_1, X_2, X_3, X_4) \sim Mult_4(0.28, 0.29, 0.1, 0.33)\)</span>.</p>
<p>Suppose we want to find the probability that in a sample of 10 voters, 2 are Democrats, 3 are Republicans, and 1 has another affiliation, and 4 are Independents:</p>
<p><span class="math display">\[
\begin{split}
P(X_1 = 2, X_2 = 3, X_3 = 1, X_4 = 4) &amp;= \frac{10!}{2! 3! 1!4!} 0.28^{2} 0.29^{3} 0.1^1 0.33^4\\
&amp;= 0.02857172.
\end{split}
\]</span></p>
<p>Or use</p>
<div class="sourceCode" id="cb81"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="fu"><a href="https://rdrr.io/r/stats/Multinom.html">dmultinom</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span><span class="fl">2</span>,<span class="fl">3</span>,<span class="fl">1</span>,<span class="fl">4</span><span class="op">)</span>, prob<span class="op">=</span><span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span><span class="fl">0.28</span>,<span class="fl">0.29</span>,<span class="fl">0.1</span>,<span class="fl">0.33</span><span class="op">)</span><span class="op">)</span> <span class="co">##specify X1, X2, X3, X4, then p1,p2,p3, p4</span></span></code></pre></div>
<pre><code>## [1] 0.02857172</code></pre>
<div id="multinomial-marginals" class="section level4" number="5.6.1.1">
<h4>
<span class="header-section-number">5.6.1.1</span> Multinomial Marginals<a class="anchor" aria-label="anchor" href="#multinomial-marginals"><i class="fas fa-link"></i></a>
</h4>
<p>The marginals of a multinomial are binomial. For <span class="math inline">\(\boldsymbol{X} \sim Mult_k(n, \boldsymbol{p})\)</span>, <span class="math inline">\(X_j \sim Bin(n, p_j)\)</span>.</p>
<p>Going back to our toy example with American voters, this means that <span class="math inline">\(X_1 \sim Bin(n,0.28), X_2 \sim Bin(n,0.29), X_3 \sim Bin(n, 0.1), X_4 \sim Bin(n,0.33)\)</span>. Hopefully this example makes sense. If we look at <span class="math inline">\(X_1,\)</span> we are looking at the number of voters who are democrats and those who are not. The proportion of Democrats still remains the same, while the proportion of Republicans, other affiliations, and independents is the sum of their individual proportions, or 1 minus the proportion of Democrats.</p>
</div>
<div id="multinomial-lumping" class="section level4" number="5.6.1.2">
<h4>
<span class="header-section-number">5.6.1.2</span> Multinomial Lumping<a class="anchor" aria-label="anchor" href="#multinomial-lumping"><i class="fas fa-link"></i></a>
</h4>
<p>With discrete and categorical variables, it can be common to want to lump (or merge, or collapse, or combine) categories together. If <span class="math inline">\(\boldsymbol{X} \sim Mult_k(n, \boldsymbol{p})\)</span>, then <span class="math inline">\(X_i + X_j \sim Bin(n, p_i + p_j)\)</span>. If we decide to merge categories 1 and 2, we have <span class="math inline">\((X_1 + X_2, X_3, \cdots, X_k) \sim Mult_{k-1}(n, (p_1 + p_2, p_3, \cdots, p_k))\)</span>.</p>
<p>We go back to our toy example. Suppose we consider Democrats and Republicans to be the major parties, we may wish to combine everyone else into one category: those with other affiliations and independents. We can define this using a new random variable <span class="math inline">\(\boldsymbol{Y} = (X_1, X_2, X_3+X_4) \sim Mult_3(n,(0.29,0.29,0.43)\)</span>. Note we now have 3 categories instead of 4. The proportion for the lumped category is the sum of their individual proportions.</p>
</div>
<div id="multinomial-covariance" class="section level4" number="5.6.1.3">
<h4>
<span class="header-section-number">5.6.1.3</span> Multinomial Covariance<a class="anchor" aria-label="anchor" href="#multinomial-covariance"><i class="fas fa-link"></i></a>
</h4>
<p>For <span class="math inline">\(\boldsymbol{X} \sim Mult_k(n, \boldsymbol{p})\)</span> with <span class="math inline">\(\boldsymbol{p} = (p_1,p_2, \cdots, p_k)\)</span>. The covariance between any two distinct components <span class="math inline">\(X_i\)</span> and <span class="math inline">\(X_j\)</span> is</p>
<p><span class="math display" id="eq:5-multinomialcov">\[\begin{equation}
Cov(X_i, X_j) = -n p_i p_j,
\tag{5.29}
\end{equation}\]</span></p>
<p>for any <span class="math inline">\(i \neq j\)</span>. The book provides a nice proof, under Theorem 7.4.6, for those interested.</p>
<p>Looking at <a href="joint-distributions.html#eq:5-multinomialcov">(5.29)</a>, we notice the covariance between any two distinct components is negative (since probabilities are non negative). This means that the numerical values of <span class="math inline">\(X_i\)</span> and <span class="math inline">\(X_j\)</span> go in opposite directions. This should make intuitive sense since <span class="math inline">\(n = X_1 + \cdots + X_k\)</span> is fixed, so if <span class="math inline">\(X_i\)</span> is large, <span class="math inline">\(X_j\)</span> should be small since <span class="math inline">\(n\)</span> is fixed. An extreme example will be if <span class="math inline">\(X_i = n\)</span>, then <span class="math inline">\(X_j\)</span> must be 0.</p>
<p>We go back to our toy example. Suppose we want to find the correlation between <span class="math inline">\(X_1\)</span> and <span class="math inline">\(X_2\)</span>, the number of Democrats and Republicans in a sample of size <span class="math inline">\(n\)</span>. Note that <span class="math inline">\(X_1 \sim Bin(n,0.28), X_2 \sim Bin(n,0.29)\)</span>, and <span class="math inline">\(Cov(X_1,X_2) = -n \times 0.28 \times 0.29 = 0.0812n\)</span>,</p>
<p><span class="math display">\[
\begin{split}
Corr(X_1,X_2) &amp;= \frac{Cov(X_1,X_2)}{\sqrt{Var(X_1) Var(X_2)}}\\
&amp;= \frac{-n p_1 p_2}{\sqrt{n p_1 (1-p_1) n p_2 (1-p_2)}} \\
&amp;= -\sqrt{\frac{p_1 p_2}{(1-p_1)(1-p_2)}} \\
&amp;= -\sqrt{\frac{0.28 \times 0.29}{(1-0.28)(1-0.29)}} \\
&amp;= -0.3985498.
\end{split}
\]</span></p>
</div>
<div id="conditional-multinomial" class="section level4" number="5.6.1.4">
<h4>
<span class="header-section-number">5.6.1.4</span> Conditional Multinomial<a class="anchor" aria-label="anchor" href="#conditional-multinomial"><i class="fas fa-link"></i></a>
</h4>
<p>Sometimes, we have some observed data from a multinomial distribution, and wish to update the distribution. Suppose we have <span class="math inline">\(\boldsymbol{X} \sim Mult_k(n, \boldsymbol{p})\)</span>, and we observed that <span class="math inline">\(X_1 = n_1\)</span>, then <span class="math inline">\((X_2, \cdots, X_k)|X_1 = n_1 \sim Mult_{k-1}(n-n_1, (p_2^{\prime}, \cdots, p_k^{\prime}))\)</span> where <span class="math inline">\(p_j^{\prime} = \frac{p_j}{p_2 + \cdots + p_k}\)</span>.</p>
</div>
</div>
<div id="multivariate-normal" class="section level3" number="5.6.2">
<h3>
<span class="header-section-number">5.6.2</span> Multivariate Normal<a class="anchor" aria-label="anchor" href="#multivariate-normal"><i class="fas fa-link"></i></a>
</h3>
<p>The <strong>multivariate normal</strong> (MVN) distribution can be viewed as a generalization of the normal distribution into higher dimensions. Just like the univariate normal distribution, the central limit theorem also applies to higher dimensions.</p>
<p>A <span class="math inline">\(k\)</span>-dimensional random vector <span class="math inline">\(\boldsymbol{X} = (X_1, \cdots, X_k)\)</span> is said to have an MVN distribution if every linear combination of the <span class="math inline">\(X_j\)</span> is normal. This means that <span class="math inline">\(t_1 X_1 + \cdots + t_k X_k\)</span> is normally distributed for any constants <span class="math inline">\(t_1, \cdots, t_k\)</span>. When <span class="math inline">\(k=2\)</span>, the MVN is often called a <strong>bivariate normal</strong>.</p>
<p>In Section <a href="continuous-random-variables.html#norm">4.5.2.2</a>, we mentioned that the parameters of a normal distribution are its mean <span class="math inline">\(\mu\)</span> and variance <span class="math inline">\(\sigma^2\)</span>. This idea is generalized to a MVN <span class="math inline">\(\boldsymbol{X} = (X_1, \cdots, X_k)\)</span>. The parameters are:</p>
<ul>
<li><p>the <strong>mean vector</strong> <span class="math inline">\((\mu_1, \cdots, \mu_k)\)</span> where <span class="math inline">\(\mu_j = E(X_j)\)</span>. This is a vector of length <span class="math inline">\(k\)</span> where each entry is the expected value of that component.</p></li>
<li><p>the <strong>covariance matrix</strong>. This is a <span class="math inline">\(k \times k\)</span> matrix where the <span class="math inline">\((i,j)\)</span>th entry (i.e. row <span class="math inline">\(i\)</span>, column <span class="math inline">\(j\)</span>) is the covariance between <span class="math inline">\(X_i\)</span> and <span class="math inline">\(X_j\)</span>. This implies that the diagonal entries give the variance of each component (since <span class="math inline">\(Cov(X_i, X_i) = Var(X_i)\)</span>), and the covariance matrix is symmetric (since <span class="math inline">\(Cov(X_i, X_j) = Cov(X_j, X_i)\)</span>).</p></li>
</ul>
<p>For example, suppose we have <span class="math inline">\(\boldsymbol{X} = (X_1, X_2, X_3)\)</span> that is MVN with mean vector <span class="math inline">\((5, 2, 8)\)</span> and covariance matrix</p>
<p><span class="math display">\[
\begin{pmatrix}
3 &amp; 1.5 &amp; 2.5\\
1.5 &amp; 2 &amp; 4.2 \\
2.5 &amp; 4.2 &amp; 1
\end{pmatrix},
\]</span></p>
<p>then</p>
<ul>
<li>
<span class="math inline">\(E(X_1) = 5, E(X_2) = 2, E(X_3) = 8\)</span>,</li>
<li>
<span class="math inline">\(Var(X_1) = 3, Var(X_2) = 2, Var(X_3) = 1\)</span>,</li>
<li>
<span class="math inline">\(Cov(X_1, X_2) = Cov(X_2, X_1) = 1.5\)</span>,</li>
<li>
<span class="math inline">\(Cov(X_1, X_3) = Cov(X_3, X_1) = 2.5\)</span>, and</li>
<li>
<span class="math inline">\(Cov(X_2, X_3) = Cov(X_3, X_2) = 4.2\)</span>.</li>
</ul>
<p>Some properties of the MVN distribution:</p>
<ol style="list-style-type: decimal">
<li><p>If <span class="math inline">\(\boldsymbol{X} = (X_1, \cdots, X_k)\)</span> is MVN, the marginal distribution of each <span class="math inline">\(X_j\)</span> is normal, as we can set <span class="math inline">\(t_j =1\)</span> and all other constants to be 0.</p></li>
<li><p>However, the converse is not necessarily true. If each <span class="math inline">\(X_1, \cdots, X_k\)</span> is normal, <span class="math inline">\((X_1, \cdots, X_k)\)</span> is not necessarily MVN.</p></li>
<li><p>If <span class="math inline">\((X_1, \cdots, X_k)\)</span> is MVN, then so is any subvector, e.g. <span class="math inline">\((X_i, X_j)\)</span> is bivariate normal.</p></li>
<li><p>If <span class="math inline">\(\boldsymbol{X} = (X_1, \cdots, X_k)\)</span> and <span class="math inline">\(\boldsymbol{Y} = (Y_1, \cdots, Y_m)\)</span> are MVN with <span class="math inline">\(\boldsymbol{X}\)</span> independent of <span class="math inline">\(\boldsymbol{Y}\)</span>, then <span class="math inline">\(\boldsymbol{W} = (X_1, \cdots, X_k, Y_1, \cdots, Y_m)\)</span> is MVN.</p></li>
<li><p>Within an MVN random vector, uncorrelated implies independence. If <span class="math inline">\(\boldsymbol{X}\)</span> is MVN and <span class="math inline">\(\boldsymbol{X} = (\boldsymbol{X_1, X_2})\)</span> where <span class="math inline">\(\boldsymbol{X_1}\)</span> and <span class="math inline">\(\boldsymbol{X_2}\)</span> are subvectors, and every component of <span class="math inline">\(\boldsymbol{X_1}\)</span> is uncorrelated with every component of <span class="math inline">\(\boldsymbol{X_2}\)</span>, then <span class="math inline">\(\boldsymbol{X_1}\)</span> and <span class="math inline">\(\boldsymbol{X_2}\)</span> are independent.</p></li>
</ol>
<div id="simulations" class="section level4" number="5.6.2.1">
<h4>
<span class="header-section-number">5.6.2.1</span> Simulations<a class="anchor" aria-label="anchor" href="#simulations"><i class="fas fa-link"></i></a>
</h4>
<p>We can use simulations to verify the first property. For this simulation, we will do the following:</p>
<ul>
<li>Simulate 5000 draws from a MVN distribution with mean vector <span class="math inline">\((1,2,5)\)</span> and covariance matrix</li>
</ul>
<p><span class="math display">\[
\begin{pmatrix}
1 &amp; 0.5 &amp; 0.6\\
0.5 &amp; 2 &amp; 0.2 \\
0.6 &amp; 0.2 &amp; 4
\end{pmatrix}.
\]</span></p>
<ul>
<li>
<p>Assess if each component <span class="math inline">\(X_1, X_2, X_3\)</span> is normally distributed by using the Shapiro-Wilk test for normality.</p>
<ul>
<li>The null hypothesis is that the variable follows a normal distribution, and the alternative hypothesis is that the variable does not follow a normal distribution.</li>
<li>So rejecting the null hypothesis means the variable is inconsistent with a normal distribution, while not rejecting means we do not have evidence the variable is inconsistent with a normal distribution.</li>
<li>We will record the p-value of each test on <span class="math inline">\(X_1, X_2, X_3\)</span>.</li>
</ul>
</li>
<li><p>Repeat the previous 2 steps for a total of 10 thousand reps.</p></li>
<li>
<p>Count the proportion of reps where the Shapiro-Wilk test rejected the null hypothesis at significance level 0.05 for <span class="math inline">\(X_1, X_2, X_3\)</span>.</p>
<ul>
<li>If this property is correct, we will expect close to 5% of the p-values to (wrongly) reject the null hypothesis, since the tests are conducted at 0.05 significance level.</li>
</ul>
</li>
</ul>
<div class="sourceCode" id="cb83"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="kw"><a href="https://rdrr.io/r/base/library.html">library</a></span><span class="op">(</span><span class="va"><a href="http://mvtnorm.R-forge.R-project.org">mvtnorm</a></span><span class="op">)</span> <span class="co">##package to simulate from MVN</span></span>
<span></span>
<span><span class="va">reps</span><span class="op">&lt;-</span><span class="fl">1000</span> <span class="co">## how many reps</span></span>
<span><span class="va">pvalsx1</span><span class="op">&lt;-</span><span class="va">pvalsx2</span><span class="op">&lt;-</span><span class="va">pvalsx3</span><span class="op">&lt;-</span><span class="fu"><a href="https://rdrr.io/r/base/array.html">array</a></span><span class="op">(</span><span class="fl">0</span>,<span class="va">reps</span><span class="op">)</span> <span class="co">##initialize an array to store the pvalues from each test at each rep</span></span>
<span><span class="va">siglevel</span><span class="op">&lt;-</span><span class="fl">0.05</span> <span class="co">##sig level</span></span>
<span><span class="va">n</span><span class="op">&lt;-</span><span class="fl">5000</span> <span class="co">##number of draws for each rep</span></span>
<span></span>
<span><span class="va">mu_vector</span><span class="op">&lt;-</span><span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span><span class="fl">1</span>,<span class="fl">2</span>,<span class="fl">5</span><span class="op">)</span> <span class="co">##mean vector</span></span>
<span></span>
<span><span class="co">##set up covariance matrix</span></span>
<span><span class="va">sig12</span><span class="op">&lt;-</span><span class="fl">0.5</span></span>
<span><span class="va">sig13</span><span class="op">&lt;-</span><span class="fl">0.6</span></span>
<span><span class="va">sig23</span><span class="op">&lt;-</span><span class="fl">0.2</span></span>
<span><span class="va">cov_mat</span><span class="op">&lt;-</span><span class="fu"><a href="https://rdrr.io/r/base/matrix.html">matrix</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span><span class="fl">1</span>,<span class="va">sig12</span>,<span class="va">sig13</span>,<span class="va">sig12</span>,<span class="fl">2</span>,<span class="va">sig23</span>,<span class="va">sig13</span>,<span class="va">sig23</span>,<span class="fl">4</span><span class="op">)</span>, nrow<span class="op">=</span><span class="fl">3</span>, ncol<span class="op">=</span><span class="fl">3</span><span class="op">)</span></span>
<span></span>
<span><span class="co">##set.seed so you can replicate my result.</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/base/Random.html">set.seed</a></span><span class="op">(</span><span class="fl">30</span><span class="op">)</span></span>
<span></span>
<span><span class="co">##run steps 1 and 2 for 10 000 times</span></span>
<span><span class="kw">for</span> <span class="op">(</span><span class="va">i</span> <span class="kw">in</span> <span class="fl">1</span><span class="op">:</span><span class="va">reps</span><span class="op">)</span></span>
<span>  </span>
<span><span class="op">{</span></span>
<span></span>
<span>  <span class="va">data</span><span class="op">&lt;-</span><span class="fu"><a href="https://rdrr.io/pkg/mvtnorm/man/Mvnorm.html">rmvnorm</a></span><span class="op">(</span><span class="va">n</span>, <span class="va">mu_vector</span>, <span class="va">cov_mat</span><span class="op">)</span></span>
<span>  </span>
<span>  <span class="va">x1</span><span class="op">&lt;-</span><span class="va">data</span><span class="op">[</span>,<span class="fl">1</span><span class="op">]</span> <span class="co">##extract X1</span></span>
<span>  <span class="va">x2</span><span class="op">&lt;-</span><span class="va">data</span><span class="op">[</span>,<span class="fl">2</span><span class="op">]</span> <span class="co">##extract X2</span></span>
<span>  <span class="va">x3</span><span class="op">&lt;-</span><span class="va">data</span><span class="op">[</span>,<span class="fl">3</span><span class="op">]</span> <span class="co">##extract X3</span></span>
<span>  </span>
<span>  <span class="co">##store pvalue from Shapiro-Wilk test from each component</span></span>
<span>  <span class="va">pvalsx1</span><span class="op">[</span><span class="va">i</span><span class="op">]</span><span class="op">&lt;-</span><span class="fu"><a href="https://rdrr.io/r/stats/shapiro.test.html">shapiro.test</a></span><span class="op">(</span><span class="va">x1</span><span class="op">)</span><span class="op">$</span><span class="va">p.value</span> </span>
<span>  <span class="va">pvalsx2</span><span class="op">[</span><span class="va">i</span><span class="op">]</span><span class="op">&lt;-</span><span class="fu"><a href="https://rdrr.io/r/stats/shapiro.test.html">shapiro.test</a></span><span class="op">(</span><span class="va">x2</span><span class="op">)</span><span class="op">$</span><span class="va">p.value</span></span>
<span>  <span class="va">pvalsx3</span><span class="op">[</span><span class="va">i</span><span class="op">]</span><span class="op">&lt;-</span><span class="fu"><a href="https://rdrr.io/r/stats/shapiro.test.html">shapiro.test</a></span><span class="op">(</span><span class="va">x3</span><span class="op">)</span><span class="op">$</span><span class="va">p.value</span> </span>
<span>  </span>
<span><span class="op">}</span></span>
<span></span>
<span><span class="co">##proportion of tests that wrongly reject the null</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/base/sum.html">sum</a></span><span class="op">(</span><span class="va">pvalsx1</span><span class="op">&lt;</span><span class="va">siglevel</span><span class="op">)</span><span class="op">/</span><span class="va">reps</span> <span class="co">##for X1</span></span></code></pre></div>
<pre><code>## [1] 0.054</code></pre>
<div class="sourceCode" id="cb85"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="fu"><a href="https://rdrr.io/r/base/sum.html">sum</a></span><span class="op">(</span><span class="va">pvalsx2</span><span class="op">&lt;</span><span class="va">siglevel</span><span class="op">)</span><span class="op">/</span><span class="va">reps</span> <span class="co">##for X2</span></span></code></pre></div>
<pre><code>## [1] 0.037</code></pre>
<div class="sourceCode" id="cb87"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="fu"><a href="https://rdrr.io/r/base/sum.html">sum</a></span><span class="op">(</span><span class="va">pvalsx3</span><span class="op">&lt;</span><span class="va">siglevel</span><span class="op">)</span><span class="op">/</span><span class="va">reps</span> <span class="co">##for X3</span></span></code></pre></div>
<pre><code>## [1] 0.047</code></pre>
<p>Since close to 5% of each hypothesis test rejected the null hypothesis, it appears that each component is consistent with a normal distribution. (Or more accurately, we do not have evidence to say that each component is not normal.) It does appear that if <span class="math inline">\(\boldsymbol{X} = (X_1, \cdots, X_k)\)</span> is MVN, the marginal distribution of each <span class="math inline">\(X_j\)</span> is normal. Our simulation does not provide evidence against this property.</p>
<p>Note: What we have done is called a Monte Carlo simulation, and is often used in research to verify theorems. While you may not be involved in research, writing code to run simulations is a good way for you to understand these theorems and how they are applied. We will cover Monte Carlo simulations in more detail in a later module.</p>

</div>
</div>
</div>
</div>
  <div class="chapter-nav">
<div class="prev"><a href="continuous-random-variables.html"><span class="header-section-number">4</span> Continuous Random Variables</a></div>
<div class="next"><a href="inequalities-limit-theorems-and-simulations.html"><span class="header-section-number">6</span> Inequalities, Limit Theorems, and Simulations</a></div>
</div></main><div class="col-md-3 col-lg-2 d-none d-md-block sidebar sidebar-chapter">
    <nav id="toc" data-toggle="toc" aria-label="On this page"><h2>On this page</h2>
      <ul class="nav navbar-nav">
<li><a class="nav-link" href="#joint-distributions"><span class="header-section-number">5</span> Joint Distributions</a></li>
<li><a class="nav-link" href="#introduction-1"><span class="header-section-number">5.1</span> Introduction</a></li>
<li>
<a class="nav-link" href="#joint-distributions-for-discrete-rvs"><span class="header-section-number">5.2</span> Joint Distributions for Discrete RVs</a><ul class="nav navbar-nav">
<li><a class="nav-link" href="#marginal-distributions-for-discrete-rvs"><span class="header-section-number">5.2.1</span> Marginal Distributions for Discrete RVs</a></li>
<li><a class="nav-link" href="#conddist"><span class="header-section-number">5.2.2</span> Conditional Distributions for Discrete RVs</a></li>
<li><a class="nav-link" href="#bayes-rule-1"><span class="header-section-number">5.2.3</span> Bayes’ Rule</a></li>
<li><a class="nav-link" href="#law-of-total-probability-1"><span class="header-section-number">5.2.4</span> Law of Total Probability</a></li>
<li><a class="nav-link" href="#indepdence-of-discrete-rvs"><span class="header-section-number">5.2.5</span> Indepdence of Discrete RVs</a></li>
</ul>
</li>
<li><a class="nav-link" href="#joint-marginal-conditional-distributions-for-continuous-rvs"><span class="header-section-number">5.3</span> Joint, Marginal, Conditional Distributions for Continuous RVs</a></li>
<li>
<a class="nav-link" href="#covariance-and-correlation"><span class="header-section-number">5.4</span> Covariance and Correlation</a><ul class="nav navbar-nav">
<li><a class="nav-link" href="#id_5-cov"><span class="header-section-number">5.4.1</span> Covariance</a></li>
<li><a class="nav-link" href="#correlation"><span class="header-section-number">5.4.2</span> Correlation</a></li>
</ul>
</li>
<li>
<a class="nav-link" href="#conditional-expectation"><span class="header-section-number">5.5</span> Conditional Expectation</a><ul class="nav navbar-nav">
<li><a class="nav-link" href="#conditional-expectation-given-event"><span class="header-section-number">5.5.1</span> Conditional Expectation Given Event</a></li>
<li><a class="nav-link" href="#conditional-expectation-given-random-variable"><span class="header-section-number">5.5.2</span> Conditional Expectation Given Random Variable</a></li>
</ul>
</li>
<li>
<a class="nav-link" href="#common-multivariate-distributions"><span class="header-section-number">5.6</span> Common Multivariate Distributions</a><ul class="nav navbar-nav">
<li><a class="nav-link" href="#multinomial"><span class="header-section-number">5.6.1</span> Multinomial</a></li>
<li><a class="nav-link" href="#multivariate-normal"><span class="header-section-number">5.6.2</span> Multivariate Normal</a></li>
</ul>
</li>
</ul>

      <div class="book-extra">
        <ul class="list-unstyled">
          
        </ul>
</div>
    </nav>
</div>

</div>
</div> <!-- .container -->

<footer class="bg-primary text-light mt-5"><div class="container"><div class="row">

  <div class="col-12 col-md-6 mt-3">
    <p>"<strong>Understanding Uncertainty Course Notes</strong>" was written by Jeffrey Woo. It was last built on 2025-07-08.</p>
  </div>

  <div class="col-12 col-md-6 mt-3">
    <p>This book was built by the <a class="text-light" href="https://bookdown.org">bookdown</a> R package.</p>
  </div>

</div></div>
</footer><!-- dynamically load mathjax for compatibility with self-contained --><script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.9/latest.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script><script type="text/x-mathjax-config">const popovers = document.querySelectorAll('a.footnote-ref[data-toggle="popover"]');
for (let popover of popovers) {
  const div = document.createElement('div');
  div.setAttribute('style', 'position: absolute; top: 0, left:0; width:0, height:0, overflow: hidden; visibility: hidden;');
  div.innerHTML = popover.getAttribute('data-content');

  var has_math = div.querySelector("span.math");
  if (has_math) {
    document.body.appendChild(div);
    MathJax.Hub.Queue(["Typeset", MathJax.Hub, div]);
    MathJax.Hub.Queue(function() {
      popover.setAttribute('data-content', div.innerHTML);
      document.body.removeChild(div);
    })
  }
}
</script>
</body>
</html>
