# Inequalities, Limit Theorems, and Simulations

```{r, echo=FALSE}
rm(list = ls())
```

This module is based on Introduction to Probability (Blitzstein, Hwang), Chapter 10. You can access the book for free at https://stat110.hsites.harvard.edu/ (and then click on Book). Please note that I cover additional topics, and skip certain topics from the book. You may skip ??? from the book.

## Introduction

It can be difficult to calculate probabilities and expected values, for example, when the PDF of a distribution is unknown, or its integral is too difficult to work out. You may notice that we used simulations to approximate probabilities and expected values in some of the examples in previous modules. With improvement in computing capabilities, simulations can now be performed faster and is a tool that is used more and more. Other tools include using inequalities to bound the probabilities (e.g. the probability cannot be greater or less than a certain value), or approximating using known theorems. We'll look at these three tools in this module.

## Inequalities

If a probability or expected value is difficult to calculate, it may be easier to find a bound via an inequality. This usually means that we can guarantee that a certain probability or expected value is within a certain range of values, so we can narrow down the possible values for the exact answer. For example, instead of being able to calculate the probability of a certain event, we may be able to show that its probability is no more than 0.1, so we know the event is unlikely to happen. We will cover a coupleof the most well-known inequalities in probability.

### Cauchy-Schwartz Inequality

The **Cauchy-Schwarz inequality** is one of the most famous inequalities in mathematics and has many applications. In the context of probability, it is written as: For any random variables $X$ and $Y$ with finite variances

\begin{equation} 
|E(XY)| \leq \sqrt{E(X^2)E(Y^2)}.
(\#eq:6-CS)
\end{equation}

The Cauchy-Schwartz inequality can be used to show the correlation between any two random variables with finite variances must be between -1 and 1. A quick proof is as follows: we apply equation \@ref(eq:6-CS) to the **centered** random variables $X - \mu_X$ and $Y - \mu_Y$:

$$
\begin{split}
|E[(X - \mu_X)(Y - \mu_Y)]| & \leq \sqrt{E[(X - \mu_X)^2] E[(Y - \mu_Y)^2]} \\
\implies |Cov(X,Y)| & \leq \sqrt{Var(X) Var(Y)} \\
\implies |Corr(X,Y)| & \leq 1.
\end{split}
$$

The Cauchy-Schwarz inequality can also be used to show that the variance of any random variable has to be non negative. A quick proof is as follows: we apply equation \@ref(eq:6-CS) to the random variable $X$ and to a constant 1:

$$
\begin{split}
|E(X)| & \leq \sqrt{E(X^2)E(1^2)}. \\
\implies |E(X)| & \leq \sqrt{E(X^2)} \\
\implies E(X)^2 & \leq E(X^2) \\
\implies 0 & \leq E(X^2) - E(X)^2 = Var(X).
\end{split}
$$
Note: One other place that you may seen the Cauchy-Schwarz inequality is in the proof of the triangle inequality in geometry. 

### Jensen's Inequality

You may have noticed in previous modules, we have written about transforming a random variable. One way of transforming a random variable is through a scale change, in other words, the value of the random variable is multiplied by a constant. This happens when we change the units of the variable. For example we want to convert a random variable based on weight from kilograms to pounds. If $X$ and $Y$ denote the weight in kilograms and pounds respectively, we can write $Y = 2.2X$. If we know the expected value of $X$, we can easily find the expected value for $Y$ by multiplying $E(X)$ by 2.2. This is fairly intuitive and is based on the linearity of expectations using equation \@ref(eq:3-linEX). 

#### Linear and Non Linear Transformations

A way to think about transformations is to write $Y = g(X)$, where $g$ is a function that describes the transformation. In the kilograms to pounds example, $g$ is exactly 2.2, so $Y = 2.2X$. This transformation is a **linear transformation** since the graph of $Y = 2.2X$ is a straight line. In this example, $E(Y) = E(2.2X) = 2.2E(X)$.

What if we use a **non linear transformation**? A popular non linear transformation is a log transformation. This is used when a random variable is right skewed (which happens pretty often in real data, such as wages, since only a few people make really high wages and the vast majority of people have wages on the lower end). Expected values are often used in statistical models for predictions; however, we know that the mean may not be the best measure of centrality with skewed data. One way to transform right skewed data to become less skewed is to log transform the data. In this example, we have $Y = \log(X)$, so $g(x) = \log(x)$. If we know the expected value of the original variable, $E(X)$, can we easily find the expected value of $Y$? Can we write $E(Y) = E(\log(X)) = \log E(X)$? This is actually incorrect. It turns out that such operations do not work for non linear transformations, i.e. if $g$ is non linear, $E(g(X))$ is not necessarily equal to $g(E(X))$. A log transformation is not linear since the graph of $Y = \log(X)$ is not a straight line. 

Let us use a toy example to show this. Suppose we roll a fair six-sided die, and let $X$ denote the number of dots the die shows. For this game, we get to win money based on the result of the roll, specifically twice the result. Let $D$ denote the winnings for this game, so $D = 2X$. Since we know $E(X) = 3.5$, this means that the expected winnings for this game is $E(D) = E(2X) = 2E(X) = 7$, since we have linear transformation here. The code below verifies these:

```{r}
X<-c(1,2,3,4,5,6) ##support for X

D<-2* X ##winnings

mean(X) ##EX since fie is fair
mean(D) ##Expected winnings. This is equal to 2 times mean(X)
```
Now suppose the winnings is now defined as the squared of the number of dots the die shows. Let $T$ denote the new winnings, so $T = X^2$. Since this is a non linear transformation, $E(T) = E(X^2)$ may not equal to $E(X)^2$:

```{r}
X<-c(1,2,3,4,5,6) ##support for X

T<-X^2 ##winnings

mean(T) ##Expected winnings. 
mean(X)^2 ##not equal
```
In this example, we see that $E(T) > E(X)^2$, in other words, $E(g(X)) > g(E(X))$, when $g(x) = x^2$. Is $E(g(X)) > g(E(X))$ always for any non linear function $g$? It turns out that this is not always the case, and it depends on whether the function $g$ is concave or convex. 

#### Concave and Convex Functions


### Chebyshev's Inequality

## Limit Theorems

### Law of Large Numbers

### Central Limit Theorem

## Monte Carlo Simulations

