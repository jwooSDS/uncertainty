# Continuous Random Variables

```{r, echo=FALSE}
rm(list = ls())
```

This module is based on Introduction to Probability (Blitzstein, Hwang), Chapters 5 and 6. You can access the book for free at https://stat110.hsites.harvard.edu/ (and then click on Book). Please note that I cover additional topics, and skip certain topics from the book. You may skip Sections ??? from the book.

## Introduction

In the previous module, we learned about discrete random variables. We learned how their distributions can be described by the PMFs and CDFs, how to find their expected values and variances, as well as common distributions for discrete random variables. We will learn about their counterparts when dealing with continuous random variables. The concepts are similar, but how they are computed can be quite different. 

As a reminder:

- A **discrete random variable** can only take on a countable (finite or infinite) number of values.
- A **continuous random variable** can take on an uncountable number of values in an interval of real numbers. 

For example, height of an American adult is a continuous random variable, as height can take on any value in interval between 40 and 100 inches. All values between 40 and 100 are possible. We cannot list all possible real numbers in this range as the list is never ending.

The sample space associated with a continuous random variable will be difficult to list, since it takes on an uncountable number of values. Using the example of heights of American adults, any real number between 40 and 100 inches is possible. 

This is different from a discrete random variable where we would list the sample space, or support, and then find the probability associated with each value in the support. 

Similar to discrete random variables, we want to describe the shape of the distribution, the centrality, and the spread of a continuous random distribution so we have an idea of probabilities associated with different ranges of values of the random variable. 

## Cumulative Distribution Functions (CDFs)

We start by talking about the cumulative distribution function, as its definition applies to both discrete and continuous random variables. The CDF of a random variable $X$ is $F_X(x) = P(X \leq x)$. The difference lies in how a CDF looks visually and how it is calculated. 

Take a look at the CDF of a discrete random variable and the CDF of a continuous random variable below in Figure \@ref(fig:4-compare):

```{r 4-compare, fig.cap="CDF for Discrete RV vs CDF for Continuous RV", echo=FALSE}
y<-c(0,1,1,1,2,2,2,3)
## create plot of CDF vs each value in support
par(mfrow=c(1,2))
plot(ecdf(y) , main = "CDF for Discrete RV", xlab="", ylab="Probability")

curve(pnorm, from = -5, to = 5, main = "CDF for Continuous RV", ylab="Probability", xlab="")
```

As mentioned in the previous module, the CDF for a discrete random variable is what is called a step function, as it jumps at each value of the support. On the other hand, the CDF for a continuous random variable increases smoothly as its sample space is infinite. 

The height of the CDF informs us the percentile associated with the value of the random variable. Looking at the CDF for the continuous random variable in Figure \@ref(fig:4-compare), the height is 0.5 when the random variable is 0, so a value of 0 corresponds to the 50th percentile of this distribution. 

The technical definition of a continuous random variable is: A random variable has a continuous distribution if its CDF is differentiable.

A discrete random variable fails in this definition since its derivative is undefined at the jumps. 

### Valid CDFs

The criteria for a valid CDF is the same, it does not matter if the random variable is discrete or continuous:

- non decreasing. This means that as $x$ gets larger, the CDF either stays the same or increases. Visually, a graph of the CDF should never decreases as $x$ increases. 
- approach 1 as $x$ approaches infinity and approach 0 as $x$ approaches negative infinity. Visually, a graph of the CDF should be equal to or close to 1 for large values of x, and it should be equal to or close to 0 for small values of x. 

*Thought question*: Look at the CDFs for our example in Figure \@ref(fig:4-compare), and see how they satisfy the criteria listed above for a valid CDF. 

## Probability Density Functions (PDFs)

The **probability density function (PDF)** of a continuous random variable is analogous to the PMF of a discrete random variable. 

The definition of the PDF for continuous random variables is the following: for a continuous random variable $X$ with CDF $F_X(x)$, the PDF of $X$, $f_X(x)$, is the derivative of its CDF, in other words, $f_X(x) = F_X^{\prime}(x)$. The support of $X$ is the set of $x$ where $f_X(x) >0$.

The relationship between the PDF and CDF of a continuous random variable $X$ can be expressed as

\begin{equation} 
F_X(x) = P(X \leq x) = \int_{-\infty}^{x} f_X(x) dx.
(\#eq:4-PDFCDF)
\end{equation}

We take a look at an example below. Suppose we have a continuous random variable $X$ with its CDF and PDF displayed below, and we want to find $P(X \leq 1)$:

```{r 4-prob, fig.cap="Probabilities from CDF and PDF", echo=FALSE}
par(mfrow=c(1,2))
curve(pnorm, from = -4, to = 4, main = "CDF for Continuous RV", ylab="Probability", xlab="")
abline(h=0.84, col="blue")
abline(v=1, col="blue")

curve(dnorm, from = -4, to = 4, main = "PDF for Continuous RV", ylab="Density", xlab="")

colorArea <- function(from, to, density, ..., col="blue", dens=NULL){
    y_seq <- seq(from, to, length.out=500)
    d <- c(0, density(y_seq, ...), 0)
    polygon(c(from, y_seq, to), d, col=col, density=dens)
}

colorArea(from=-4, to=1, dnorm)
```

We can find $P(X \leq 1)$ in two different ways:

- from the CDF, find the value of 1 on the horizontal axis, and read off the corresponding value on the vertical axis (blue lines). This tells us that $P(X \leq 1) = 0.84$.
- from the PDF, find the area under the PDF where $X \leq 1$. This area corresponds to the shaded region in blue, and will be equal to 0.84 if we performed the integration per equation \@ref(eq:4-PDFCDF).

Compare equation \@ref(eq:4-PDFCDF) with equation \@ref(eq:3-CDF) and note the similarities and differences. For discrete CDFs, we sum the PMF over all values less than or equal to $x$, whereas for continuous CDFs, we integrate, or accumulate the area, under the PDF over all values less than or equal to $x$. Some people view the integral as a continuous version of a summation. 

From equation \@ref(eq:4-PDFCDF), we can generalize a way to find the probability $P(a<X<b)$  for a continuous random variable $X$:

\begin{equation} 
P(a<X<b) = F_X(b) - F_X(a) = \int_{a}^{b} f_X(x) dx.
(\#eq:4-integrate)
\end{equation}

In other words, to find the probability for a range of values for $X$, we just find the area under its PDF for that range of values. Going back to our example, if we want to find $P(0<X<1)$, we will find the area under its PDF for $0<X<1$, like in Figure \@ref(fig:4-prob2) below:

```{r 4-prob2, fig.cap="Probabilities from PDF", echo=FALSE}
curve(dnorm, from = -4, to = 4, main = "PDF for Continuous RV", ylab="Density", xlab="")

colorArea <- function(from, to, density, ..., col="blue", dens=NULL){
    y_seq <- seq(from, to, length.out=500)
    d <- c(0, density(y_seq, ...), 0)
    polygon(c(from, y_seq, to), d, col=col, density=dens)
}

colorArea(from=-0, to=1, dnorm)
```

As mentioned, the PDF of a continuous random variable is analogous, but not exactly the same as, to the PMF of a discrete random variable. One common misconception is that the PDF tells us a probability, for example, that the value of $f_X(2) = P(X=2)$, if $X$ is continuous. This is only correct if $X$ is discrete. In fact, if we look at equation \@ref(eq:4-integrate) a little more closely, $P(X=c) = 0$ if $X$ is continuous and $c$ is a constant, since the area under its PDF will be 0. 


### Valid PDFs

The PDF of a continuous random variable must satisfy the following criteria:

- Non negative: $f_X(x) \geq 0$,
- Integrates to 1: $\int_{-\infty}^{\infty}f_X(x) dx = 1$.

### Estimating PDFs with Data

## Summaries of a Distribution

Next, we will talk about some common summaries associated with a distribution. These involve measures of centrality and variance, which we have covered before. We will also talk about a couple of other measures: skewness and kurtosis.

### Expectations

The **expected value** of a continuous random variable $X$ is

\begin{equation} 
E(X) = \int_{-\infty}^{\infty} x f_X(x) dx.
(\#eq:4-EX)
\end{equation}

Another common notation for $E(X)$ is $\mu$, or sometimes $\mu_X$ show that we are writing the mean of the random variable $X$.

If we compare equation \@ref(eq:4-EX) with equation \@ref(eq:3-EX), we notice that we use an integral instead of a summation now that we are working with continuous random variables. 

The interpretation of expected values is still the same: the expectation of a random variable can be interpreted as the long-run mean of the random variable, i.e. if we were able to repeat the experiment an infinite number of times, the expectation of the random variable will be the average result among all the experiments. It is still a measure of centrality of the random variable. 

The **linearity of expectations** still hold in the same way, per equation \@ref(eq:3-linEX). It does not matter if the random variable is discrete or continuous.

The **Law of the Unconscious Statistician (LOTUS)** also still applies. For a continuous random variable $X$, it is (unsurprisingly):

\begin{equation} 
E(g(X)) = \int_{-\infty}^{\infty} g(x) f_X(x).
(\#eq:4-lotus)
\end{equation}

Notice again when we compare equation \@ref(eq:4-lotus) with its discrete counterpart in equation \@ref(eq:3-lotus): we have just replaced the summation with an integral. 

*Thought question*: Can you guess how to write the equation for the variance of a continuous random variable? Hint: Equation \@ref(eq:3-var2) is how to find the variance for a discrete random variable.

#### Median

The value $m$ is the **median** of a random variable $X$ if $P(X \leq c) \geq \frac{1}{2}$ and $P(X \geq c) \geq \frac{1}{2}$.

Intuitively, the median is the value $m$ which splits the area under the PDF into half (or as close to half as possible if the random variable is discrete). Half the area will be to the left of $m$, the other half of the area will be to the right of $m$. 

#### Mode

For a continuous random variable $X$, the mode is the value $c$ that maximizes the PDF: $f_X(c) \geq f_X(x)$ for all $x$.

For a discrete random variable $X$, the mode is the value $c$ that maximizes the PMF: $P(X=c) \geq P(X=x)$ for all $x$. Intuitively, the mode is the most commonly occurring value of a discrete random variable

#### Loss Functions

A goal of statistical modeling is to use the model to make predictions. We want to be able to quantify the quality of our prediction, or the prediction error. Consider we have an experiment that can be described by a random variable $X$, and we want to predict the value of the next experiment. The mean and median are natural guesses for the value of the next experiment.

It turns out there a several ways to quantify our prediction error. These are usually called loss functions. Suppose our predicted value is denoted by $x_{pred}$. A couple of common loss functions:

- **Mean squared error (MSE)**: $E(X-x_{pred})^2$,
- **Mean absolute error (MAE)**: $E|X-x_{pred}|$.

It turns out that the expected value $E(X)$ minimizes the MSE, and the median minimizes the MAE. So depending on what loss function suits our analysis, we could use either the mean or median for our predictions. We will cover these ideas in more detail in a later module (and indeed in later courses in this program).

### Variance

The **variance** of a continuous random variable $X$ is

\begin{equation} 
Var(X) = \int_{-\infty}^{\infty} (x-\mu)^2 f_X(x) dx.
(\#eq:4-var)
\end{equation}

The properties of variance is still the same as in Section \@ref(var-prop). It does not matter if the random variable is discrete or continuous. A common notation used for variance is $\sigma^2$, or sometimes $\sigma_X^2$ to show its the variance of the random variable $X$. 

### Moments

Before talking about measures that is used to describe distributions, we will cover some terminology that is used for these measures. Suppose we have a random variable $X$.

- The **$n$th moment** of $X$ is $E(X^n)$. So the expected value, or the mean, is sometimes called the first moment.
- The **$n$th central moment** of $X$ is $E((X-\mu)^n)$. So the variance is sometimes called the second central moment.
- The **$n$ standardized moment** of $X$ is $E(\frac{(X-\mu)^n}{\sigma})$.

### Skewness

One measure that is used to describe the shape of a distribution is skewness, which is a measure of symmetry (or measure of skewness). The **skew** of a random variable $X$ is the third standardized moment:

\begin{equation} 
Skew(X) = E \left(\frac{(X-\mu)^3}{\sigma} \right)
(\#eq:4-skew)
\end{equation}

A random variable $X$ has a **symmetric distribution about \mu** if $X - \mu$ has the same distribution as $\mu - X$. Fairly often, people will just say that $X$ is symmetric; it is almost always assumed that the symmetry is about its mean.

Intuitively, symmetry means that the PDF of $X$ to the left of its mean is the mirror image of the PDF of $X$ to the right of its mean. We look at a couple of examples below in Figure \@ref(fig:4-symm):

```{r 4-symm, fig.cap="PDFs for Symmetric RV vs Skewed RV", echo=FALSE}
par(mfrow=c(1,2))
curve(dnorm, from = -4, to = 4, main = "PDF for Symmetric RV", ylab="Probability", xlab="")
abline(v=0, col="blue")

curve(dexp, from = 0, to = 10, main = "PDF for Skewed RV", ylab="Probability", xlab="")
abline(v=1, col="blue")
```
The blue vertical lines indicate the mean of these distributions. Notice the mirror image in the first plot, but not in the second plot. 

If a distribution is not symmetric, we can say its distribution is asymmetric, or is skewed. The values of $Skew(X)$ that are associated with different shapes are:

- $Skew(X) = 0$: $X$ is symmetric.
- $Skew(X) > 0$: $X$ is right skewed.
- $Skew(X) < 0$: $X$ is left skewed.

### Kurtosis

One more measure deals with the **tail** behavior of a distribution. Visually, the tails of a PDF are associated with probabilities of extreme values for a random variable. A distribution that is heavy tailed means that extreme values (on both ends) are more likely to occur. Tail behavior is an important consideration in risk management in finance: e.g. a heavy left tail in the PDF could mean a financial crisis. Figure \@ref(fig:4-kurt) shows an example of a heavy tailed distribution (in blue), compared to a Gaussian distribution (in black). We will talk more about the Gaussian distribution in the next subsection.

```{r 4-kurt, fig.cap="PDF for Heavy Tailed Distribution", echo=FALSE}
x_values <- seq(-4, 4, length = 100)

# Plot the standard normal distribution
curve(dnorm, from = -4, to = 4, main = "PDF for Symmetric RV", ylab="Probability", xlab="")

# Add the t-distribution with 10 degrees of freedom
curve(dt(x,df=1), from = -4, to = 4, add=TRUE, col="blue")
```

A common measure of tail behavior is the **Kurtosis**. The kurtosis of a random variable $X$ is the shifted fourth standardized moment:

\begin{equation} 
Kurt(X) = E \left(\frac{(X-\mu)^4}{\sigma} \right) - 3.
(\#eq:4-kurt)
\end{equation}

The reason for subtracting (or shifting by) 3 is so that the Gaussian distribution (a commonly used distribution for continuous random variables) is 0. Note: Some people call equation \@ref(eq:4-kurt) the **excess kurtosis** and the kurtosis does not subtract the 3. 

The values of $Kurt(X)$ that are associated with tail behaviors are:

- $Kurt(X) = 0$: $X$ is similar tails to Gaussian distribution.
- $Kurt(X) > 0$: $X$ has heavier tails compared to Gaussian distribution (extreme values more likely).
- $SKurt(X) < 0$: $X$ has smaller tails compared to Gaussian distribution (extreme values less likely).

## Common Continuous Random Variables

### Uniform

#### Universality of Uniform

### Normal
