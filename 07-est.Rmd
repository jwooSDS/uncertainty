# Estimation

```{r, echo=FALSE}
rm(list = ls())
```

This module is based on Introduction to Probability for Data Science (Chan), Chapter 8. You can access the book for free at https://probability4datascience.com. Please note that I cover additional topics, and skip certain topics from the book. You may skip ??? from the book.

## Introduction

We consider building models based on the data we have. Many models are based on some distribution, for example, the linear regression model is based on the normal distribution, and the logistic regression model is based on the Bernoulli distribution. Recall that these distributions are specified by their parameters: the mean $\mu$ and variance $\sigma^2$ for the normal distribution, and the success probability $p$ for a Bernoulli distribution. The value of the parameters are almost always unknown in real life. This module deals with estimation: how do we estimate the values of these parameters, as well as quantify the level of uncertainty we have with these estimated values. 

### Big Picture Idea with Estimation

Consider this simple scenario. We want to find the distribution associated with the systolic blood pressure of American adults who work full-time. To be able to achieve this goal, we would have to get the systolic blood pressure of every single American adult who works full-time. This is usually not feasible as researchers are unlikely to have the time and money to interview every single American adult who works full-time. Instead, a representative sample of American adults will be obtained, for example, 750 randomly selected American adults who work full-time are interviewed. We can then create density plots, histograms, compute the mean, median, variance, skewness, and other summaries that may be of interest, based on these 750 American adults. 

#### Population Vs Sample

The above scenario illustrates a few concepts and terms that are fundamental in estimation. In any study, we must be clear as to who or what is the population of interest, and who or what is the sample. 

The **population** (sometimes called the population of interest) is the entire set of individuals, or objects, or events that a study is interested in. In the scenario described above, the population would be (all) American adults who work full-time. 

The **sample** is the set of individuals, or objects, or events which we have data on. In the scenario described above, the sample is the 750 randomly selected American adults who work full-time.

Ideally, the sample should be **representative** of the population. A representative sample is often achieved through a simple random sample, where each unit in the population has the same chance of being selected to be in the sample. In this module, we will assume that we have a representative sample. Note: You may feel that obtaining a simple random sample may be difficult. We will not get into a discussion of sampling (sometimes called survey sampling), which is a field of statistics that handles how to obtain representative samples, or how calculations should be adjusted if the sample is not representative. There is still a lot of research that is being done in survey sampling.

#### Variables & Observations

A **variable** is a characteristic or attribute of individuals, or objects, or events that make up the population and sample.  In the above scenario, a variable would be the systolic blood pressure of American adults. We can use the notation of random variables to describe variables. For example, we can let $X$ denote the systolic blood pressure of an American adult who works full-time, so writing $P(X<20,000)$ means we want to find the probability that an American adult who works full-time earns less than $20,000.

An **observation** is the individual person, object or event that we collect data from. In the above scenario, an observation is a single American adult who works full-time in our sample of 750.

One way to think about variables and observations is through a spreadsheet. Typically, each row represents an observation and each column represents a variable. Figure \@ref(fig:07-dataframe) below displays such an example, based on the described scenario. Each row represents an observation, i.e. a single American adult who works full time in our sample, and the column represents the variable, which is systolic blood pressure. 

```{r 07-dataframe, fig.cap = "Example of Data in a Spreadsheet", echo = FALSE}
knitr::include_graphics("images/07-dataframe.png")
```

#### Parameter Vs Estimator

Now that we have made the distinction between a population and a sample, we are ready to define parameters and estimators.

A **parameter** is a numerical summary associated with a population. In the scenario described above, an example of a population parameter would be the population mean systolic blood pressure of American adults who work full-time. 

An **estimator** is a numerical summary associated with a sample. An estimator is typically connected with its corresponding version in the population. In the scenario described above, an estimator of the population mean systolic blood pressure of American adults who work full-time could be the average systolic blood pressure of the 750 American adults who work full-time in our sample. So the sample mean is an estimator of the population mean. 

An **estimated value** is the actual value of the estimator based on a sample. In the scenario described above, suppose the average systolic blood pressure of the 750 American adults who work full-time is \$60,000. We will say the estimated value of the mean systolic blood pressure of American adults who work full-time is \$60,000. 

So a parameter is a number that is associated with a population, while an estimator is a number that is associated with a sample. Some other differences between parameters and estimators:

- The value of parameters are unknown, while we can actually calculate numerical values of estimators.
- The value of parameters are considered fixed (as there is only one population), while the numerical values of estimators can vary if we obtain multiple random samples of the same sample size. Using the scenario above again, suppose we obtain a second representative sample of 750 America adults who work full-time. The average systolic blood pressure of this second sample is likely to be different from the average systolic blood pressure of the first sample. This illustrates that there is variance, or uncertainty, associated with estimators due to random sampling. 

Whenever we propose an estimator for a parameter, we want to assess how "good" the estimator is. In some situations, there is an obvious choice for an estimator, for example, using the sample mean, $\bar{x} = \frac{\sum x_i}{n}$ to estimate the population mean. But in some instances, the choice may not be so obvious. For example, why do use the sample variance $s^2 = \frac{\sum (x_i - \bar{x})}{n-1}$ as an estimator for the population variance, and not $\frac{\sum (x_i - \bar{x})}{n}$? We will cover a few measures that are used to assess an estimator, mainly its bias, variance, and its mean-squared error.

We will also cover a couple of methods in estimating parameters: the method of moments, and the method of maximum likelihood. You will notice that we use probability rules in these methods. 

To sum up estimation: we use data from a sample to estimate unknown characteristics of a population, so that we can answer questions regarding variables in the population, as well as provide a measure of uncertainty for our answers. 

## Method of Moments Estimation

We will cover a couple of methods in estimation. The first method is the **method of moments**. It is the more intuitive method, although it lacks certain ideal properties. Before defining this method, we recall and define some terms.

In Section \@ref(moments), we defined **moments**. As a reminder, for a random variable $X$, its $k$th moment is $E(X^k)$, which can be found using LOTUS $\int_{-\infty}^{\infty} x^k f_X(x) dx$. 

Suppose we observe a random sample $X_1, \cdots, X_n$ that comes from $X$. The $k$th **sample moment** is $M_k = \frac{1}{n} \sum_{i=1}^n X_i^k$.

Using these definitions,

- The 1st moment is $E(X) = \mu_x$, the population mean of $X$.
- The 1st sample moment is $M_1 = \frac{1}{n} \sum_{i=1}^n X_i = \bar{X}$, the sample mean of $X$. 
- The 2nd moment is $E(X^2)$.
- The 2nd sample moment is $M_2 = \frac{1}{n} \sum_{i=1}^n X_i^2$. 

And so on. 

The method of moments estimation is: Let $X$ be a random variable with distribution depending on parameters $\theta_1, \cdots, \theta_m$. The **method of moments (MOM) estimators** $\hat{\theta}_1, \cdots, \hat{\theta}_m$ are found by equating the first $m$ sample moments to the corresponding first $m$ moments and solving for $\theta_1, \cdots, \theta_m$.

You might have noticed that the method of moments is based on the Law of Large Numbers. 

Note: By convention, parameters are typically denoted by Greek letters, and their estimators are denoted by Greek letters with a hat symbol over them. 

Let us look at a couple of examples:

1. Suppose I have a coin and I do not know if it is fair or not. There are only two outcomes on a flip, heads or tails. Each flip is independent of other flips. Let $X_i$ denote whether the $i$th flip lands heads, where $X_i = 1$ if heads and $X_i = 0$ if tails. We can see that $X_i \sim Bern(p)$, where $p$ is the probability it lands heads. Derive the MOM estimator for $p$.

A Bernoulli distribution has only 1 parameter, $p$, so when using the method of moments, we only need to equate the first sample moment to the first moment.

- The first moment is $E(X_i) = p$, since $X_i \sim Bern(p)$.
- The first sample moment is $M_1 = \frac{1}{n} \sum_{i=1}^n X_i = \bar{X}$. 

Set $E(X_i) = M_1$, i.e. $\hat{p} = \bar{X}$. Since $X_i = 1$ if heads and $X_i = 0$ if tails, $\frac{1}{n} \sum_{i=1}^n X_i = \bar{X}$$ actually represents the proportion of flips that land on heads, based on $n$ flips. So this is actually the sample proportion. 

The MOM estimator for this problem is $\hat{p}$, the proportion of $n$ flips that land on heads. This result should be fairly intuitive. If the true success probability is 0.7, we expect 70\% of $n$ flips to land on heads. 

2. Birth weights of newborn babies typically follow a normal distribution. We have data from births at Baystate Medical Center in Springfield, MA, during 1986. Assuming that the births at this hospital is representative of births in New England in 1986, derive the MOM estimators for $\mu$ and $\sigma^2$, the mean and variance of the distribution of birth weights in New England in 1986. 

A normal distribution has 2 parameters, $\mu$ and $\sigma^2$, so we need to equate the first two sample moments to the first two moments. Let $X$ denote the birth weights in New England in 1986, so $X \sim N(\mu, \sigma^2)$.

- The first moment is $E(X) = \mu$, since $X \sim N(\mu, \sigma^2)$.
- The first sample moment is $M_1 = \frac{1}{n} \sum_{i=1}^n X_i = \bar{X}$. 

So we set $E(X) = M_1$, i.e. $\hat{\mu} = \bar{X}$. This is just the sample average of the birth weights at Baystate Medical Center in 1986.

- The second moment is $E(X^2)$. But we know that since $X \sim N(\mu, \sigma^2)$.

$$
\begin{split}
Var(X) &= E(X^2) - E(X)^2\\
\implies E(X^2) &= Var(X) + E(X)^2 \\
\implies E(X^2) &= \sigma^2 + \mu^2
\end{split}
$$

- The second sample moment is $M_2 = \frac{1}{n} \sum_{i=1}^n X_i^2$.

So we set $E(X^2) = M_2$, i.e. $\hat{\sigma^2} + \hat{\mu}^2 = \frac{1}{n} \sum_{i=1}^n X_i^2$. Since we earlier found that $\hat{\mu} = \bar{X}$, we get $\hat{\sigma^2}  = \frac{1}{n} \sum_{i=1}^n X_i^2 - \bar{X}^2 = \frac{1}{n} \sum_{i=1}^n (X_i - \bar{X})^2$.

Therefore, the MOM estimators for $\mu$ and $\sigma^2$ are $\hat{\mu} = \bar{X}$ and $\hat{\sigma^2}  = \frac{1}{n} \sum_{i=1}^n (X_i - \bar{X})^2$ respectively.

We now use these MOM estimators on the data set of birth weights at Baystate Medical Center in 1986. It is well established in the literature that birth weights of babies follow a normal distribution. A quick check with the Shapiro-Wilk's test for normality shows no contradiction, so we proceed with finding the estimators for $\mu$ and $\sigma^2$. We then produce a density plot of the birth weights, and overlay a curve that corresponds to a normal distribution with $\hat{\mu} = \bar{X}$ and $\hat{\sigma^2}  = \frac{1}{n} \sum_{i=1}^n (X_i - \bar{X})^2$. This normal curve is pretty close to the density plot, so it appears reasonable to say that the birth weights follow a normal distribution with mean 2944.587 (grams) and variance 528940 (grams-squared).

```{r 7-MOM, fig.cap="Density Plot of Birth Weights. Normal Curve with Parameters Estimated by MOM Overlaid"}
library(MASS)
data<-MASS::birthwt ##dataset comes from MASS package

shapiro.test(data$bwt) ##check for normality

mu<-mean(data$bwt) ##MOM estimator for mu
mu

sigma2<-mean((data$bwt-mu)^2) ##MOM estimator for sigma2
sigma2

##create density plot for data, and overlay Normal curve with parameters estimated by MOM
plot(density(data$bwt), main="", ylim=c(0,6e-04))
curve(dnorm(x, mean=mu, sd=sqrt(sigma2)), 
      col="blue", lwd=2, add=TRUE)
```


### Alternative Form of Method of Moments Estimation

In Section \@ref(moments), we defined ** central moments**. As a reminder, for a random variable $X$, its $k$th central moment is $E((X-\mu)^k)$.

Suppose we observe a random sample $X_1, \cdots, X_n$ that comes from $X$. The $k$th **sample central moment** is $M_k^* = \frac{1}{n} \sum_{i=1}^n (X_i - \bar{X})^k$.

An alternative form for the method of moments estimation is: Let $X$ be a random variable with distribution depending on parameters $\theta_1, \cdots, \theta_m$. The method of moments (MOM) estimators $\hat{\theta}_1, \cdots, \hat{\theta}_m$ are found by equating the first sample moment to the first moments, and equating subsequent sample central moments to the corresponding central moments, and solving for $\theta_1, \cdots, \theta_m$.

We go back to the 2nd example in the previous subsection, where we are trying to find estimators for $\mu$ and $\sigma^2$ of a normal distribution. 

- The first moment is $E(X) = \mu$, since $X \sim N(\mu, \sigma^2)$.
- The first sample moment is $M_1 = \frac{1}{n} \sum_{i=1}^n X_i = \bar{X}$. 

So we set $E(X) = M_1$, i.e. $\hat{\mu} = \bar{X}$. This is just the sample average of the birth weights at Baystate Medical Center in 1986.

- The second central moment is $Var(x) = E[(X-\mu)^2] = \sigma^2$.
- The second sample central moment is $M_2^* = \frac{1}{n} \sum_{i=1}^n (X_i - \bar{X})^2$.

So we set $Var(X) = M_2^*$ i.e. $\hat{\sigma^2} = \frac{1}{n} \sum_{i=1}^n (X_i - \bar{X})^2$. If we compare this solution with the solution in the previous subsection, they are exactly the same. 

This alternative form is sometimes easier to work with, since the 2nd central moment is actually the variance of $X$.

The idea behind method of moments estimation is fairly intuitive; however, it has some drawbacks which we will talk about after introducing another method for estimation in the next section, the method of maximum likelihood.

## Method of Maximum Likelihood Estimation

The method of maximum likelihood is a workhorse in statistics and data science since it is widely used in estimating models. It is preferred to the method of moments as it is built upon a stronger theoretical framework, and its estimators tend to have more desirable properties. You are virtually guaranteed to see the method of maximum likelihood again in the future. 

As its name suggests, it is a method of estimating parameters by maximizing the likelihood. 

### Likelihood Function

Suppose we have $n$ observations, denoted by the vector $\boldsymbol{x} = (x_1, \cdots, x_n)^{T}$. We can use a PDF to generalize the distribution of these observations, $f_{\boldsymbol{X}}(\boldsymbol{x})$. This is a joint PDF of all the variables.

We have seen that PDFs (and hence joint PDFs) are always described by their parameters (e.g. Normal distribution has $\mu, \sigma^2$, Bernoulli has $p$). For this section, we will let $\boldsymbol{\theta}$ denote the parameters of a PDF. For example, if we are working with multivariate normal distribution, $\boldsymbol{\theta} = (\boldsymbol{\mu}, \boldsymbol{\Sigma})$, the mean vector and covariance matrix. 

To make it clear what the parameters are, we will write $f_{\boldsymbol{X}}(\boldsymbol{x}; \boldsymbol{\theta})$
to express the PDF of the random vector $\boldsymbol{X}$ with parameter $\boldsymbol{\theta}$. So the PDF is a function of two items:

- The first item is the vector $\boldsymbol{x} = (x_1, \cdots, x_n)^{T}$, which is basically a vector of the observed data. In previous modules, we expressed PDFs as a function of the observed data, since we calculate the PDF when $\boldsymbol{X} = \boldsymbol{x}$. In estimation, the vector of observed data is actually fixed as tt is something we are given in the data set. 

- The second item is the parameter $\boldsymbol{\theta}$. Estimating the parameter is our focus in estimation. The general idea in maximum likelihood is to find the value of $\boldsymbol{\theta}$ that "best explains" or "is most consistent" with the observed values of the data $\boldsymbol{x}$. We maximize $f_{\boldsymbol{X}}(\boldsymbol{x}; \boldsymbol{\theta})$ to achieve this goal. 

The **likelihood function** is the PDF, but written in a way that shifts the emphasis to the parameters. The likelihood function is denoted by $L(\boldsymbol{\theta} | \boldsymbol{x})$ and is defined as $f_{\boldsymbol{X}}(\boldsymbol{x})$.

Note: the likelihood function should be viewed as a function of $\boldsymbol{\theta}$, and its shape changes depending on the values of the observed data $\boldsymbol{x}$.

To simplify calculations involving the likelihood function, we make an assumption that the observations $\boldsymbol{x}$ are independent and come from an identical distribution with PDF $f_X(x)$, in other words, the observations are i.i.d. (independent and identically distributed). 

Given i.i.d. random variables $X_1, \cdots, X_n$, each having PDF $f_X(x)$, the likelihood function is

\begin{equation} 
L(\boldsymbol{\theta} | \boldsymbol{x} ) = \prod_i^n f_X(x; \boldsymbol{\theta}).
(\#eq:7-like)
\end{equation}

When maximizing the likelihood function, we often log transform the likelihood function first, then maximize the log transformed likelihood function. The log transformed likelihood function is called the **log-likelihood function**, and it is

\begin{equation} 
\ell(\boldsymbol{\theta} | \boldsymbol{x}) = \log L(\boldsymbol{x} | \boldsymbol{\theta}) = \sum_{i=1}^n \log f_X(x; \boldsymbol{\theta}).
(\#eq:7-loglike)
\end{equation}

It turns out that maximizing the log-likelihood function is often easier computationally than maximizing the likelihood function.

Note: As the logarithm is a monotonic increasing function (it never decreases), maximizing a log transformed function is equivalent to maximizing the original function. Next, we look at how to write the likelihood and log-likelihood functions with a couple of examples.

#### Example 1: Bernoulli

Let $X_1, \cdots, X_n$ be i.i.d. $Bern(p)$. Find the corresponding likelihood and log-likelihood functions.

From equation \@ref(eq:3-bern), the PMF of $X \sim Bern(p)$ is $f_X(x) = p^x (1-p)^{1-x}$, where the support of $x$ is $\{0,1\}$.

The likelihood function, per equation \@ref(eq:7-like), becomes

$$
L(p | \boldsymbol{x} ) = \prod_i^n f_X(x_i; p) = \prod_i^n p^{x_i} (1-p)^{1-x_i}.
$$

The log-likelihood function, per equation \@ref(eq:7-loglike), becomes

$$
\begin{split}
\ell (p | \boldsymbol{x}) &= \sum_{i=1}^n \log f_X(x_i; \boldsymbol{\theta}) \\
                          &= \sum_{i=1}^n \log \left( p^{x_i} (1-p)^{1-x_i} \right) \\
                          &= \sum_{i=1}^n x_i \log p + (1-x_i) \log (1-p) \\
                          &= \log p \left(\sum_{i=1}^n x_i \right) + \log (1-p) \left( n - \sum_{i=1}^n x_i \right)
\end{split}
$$

## Assessing Estimators

### Bias

### Standard Error and Variance

#### Consistency

### Mean-Squared Error


